{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "b8d47100ee7ce075d4c01e6b6ec4d1711d26a20d",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "input files in ../input/train.csv & ../input/test.csv\n",
    "- one-hot encoding (done)\n",
    "- r2 score custom function (done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "1d314983e90546f3284f13f36e4879a946c51354",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "0ef082f87f173e4ea2ea55c03984667769a04e7a",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "7f2a6d3c4ceb06dd8b28186bf793bee3bc540719",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID       y  X0 X1  X2 X3 X4 X5 X6 X8  ...   X375  X376  X377  X378  X379  \\\n",
      "0   0  130.81   k  v  at  a  d  u  j  o  ...      0     0     1     0     0   \n",
      "1   6   88.53   k  t  av  e  d  y  l  o  ...      1     0     0     0     0   \n",
      "2   7   76.26  az  w   n  c  d  x  j  x  ...      0     0     0     0     0   \n",
      "3   9   80.62  az  t   n  f  d  x  l  e  ...      0     0     0     0     0   \n",
      "4  13   78.02  az  v   n  f  d  h  d  n  ...      0     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     1     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 378 columns] (4209, 378) (4209, 377)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../Data/Mercedes-Benz/train.csv', header=0)\n",
    "test = pd.read_csv('../Data/Mercedes-Benz/test.csv', header=0)\n",
    "print(data[:5], data.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "c950d02f8cfa8d51b206f9859f404607c040db59",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "e1b05c27bafc4807939ab30b32c5c9692d011076",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID :  [   0    6    7 ..., 8412 8415 8417]\n",
      "y :  [ 130.81   88.53   76.26 ...,   85.71  108.77   87.48]\n",
      "X0 :  ['k' 'az' 't' 'al' 'o' 'w' 'j' 'h' 's' 'n' 'ay' 'f' 'x' 'y' 'aj' 'ak' 'am'\n",
      " 'z' 'q' 'at' 'ap' 'v' 'af' 'a' 'e' 'ai' 'd' 'aq' 'c' 'aa' 'ba' 'as' 'i'\n",
      " 'r' 'b' 'ax' 'bc' 'u' 'ad' 'au' 'm' 'l' 'aw' 'ao' 'ac' 'g' 'ab']\n",
      "X1 :  ['v' 't' 'w' 'b' 'r' 'l' 's' 'aa' 'c' 'a' 'e' 'h' 'z' 'j' 'o' 'u' 'p' 'n'\n",
      " 'i' 'y' 'd' 'f' 'm' 'k' 'g' 'q' 'ab']\n",
      "X2 :  ['at' 'av' 'n' 'e' 'as' 'aq' 'r' 'ai' 'ak' 'm' 'a' 'k' 'ae' 's' 'f' 'd'\n",
      " 'ag' 'ay' 'ac' 'ap' 'g' 'i' 'aw' 'y' 'b' 'ao' 'al' 'h' 'x' 'au' 't' 'an'\n",
      " 'z' 'ah' 'p' 'am' 'j' 'q' 'af' 'l' 'aa' 'c' 'o' 'ar']\n",
      "X3 :  ['a' 'e' 'c' 'f' 'd' 'b' 'g']\n",
      "X4 :  ['d' 'b' 'c' 'a']\n",
      "X5 :  ['u' 'y' 'x' 'h' 'g' 'f' 'j' 'i' 'd' 'c' 'af' 'ag' 'ab' 'ac' 'ad' 'ae' 'ah'\n",
      " 'l' 'k' 'n' 'm' 'p' 'q' 's' 'r' 'v' 'w' 'o' 'aa']\n",
      "X6 :  ['j' 'l' 'd' 'h' 'i' 'a' 'g' 'c' 'k' 'e' 'f' 'b']\n",
      "X8 :  ['o' 'x' 'e' 'n' 's' 'a' 'h' 'p' 'm' 'k' 'd' 'i' 'v' 'j' 'b' 'q' 'w' 'g'\n",
      " 'y' 'l' 'f' 'u' 'r' 't' 'c']\n",
      "X10 :  [0 1]\n",
      "X11 :  [0]\n",
      "X12 :  [0 1]\n",
      "X13 :  [1 0]\n",
      "X14 :  [0 1]\n",
      "X15 :  [0 1]\n",
      "X16 :  [0 1]\n",
      "X17 :  [0 1]\n",
      "X18 :  [1 0]\n",
      "X19 :  [0 1]\n",
      "X20 :  [0 1]\n",
      "X21 :  [1 0]\n",
      "X22 :  [0 1]\n",
      "X23 :  [0 1]\n",
      "X24 :  [0 1]\n",
      "X26 :  [0 1]\n",
      "X27 :  [0 1]\n",
      "X28 :  [0 1]\n",
      "X29 :  [0 1]\n",
      "X30 :  [0 1]\n",
      "X31 :  [1 0]\n",
      "X32 :  [0 1]\n",
      "X33 :  [0 1]\n",
      "X34 :  [0 1]\n",
      "X35 :  [1 0]\n",
      "X36 :  [0 1]\n",
      "X37 :  [1 0]\n",
      "X38 :  [0 1]\n",
      "X39 :  [0 1]\n",
      "X40 :  [0 1]\n",
      "X41 :  [0 1]\n",
      "X42 :  [0 1]\n",
      "X43 :  [0 1]\n",
      "X44 :  [0 1]\n",
      "X45 :  [0 1]\n",
      "X46 :  [1 0]\n",
      "X47 :  [0 1]\n",
      "X48 :  [0 1]\n",
      "X49 :  [0 1]\n",
      "X50 :  [0 1]\n",
      "X51 :  [0 1]\n",
      "X52 :  [0 1]\n",
      "X53 :  [0 1]\n",
      "X54 :  [0 1]\n",
      "X55 :  [0 1]\n",
      "X56 :  [0 1]\n",
      "X57 :  [0 1]\n",
      "X58 :  [1 0]\n",
      "X59 :  [0 1]\n",
      "X60 :  [0 1]\n",
      "X61 :  [0 1]\n",
      "X62 :  [0 1]\n",
      "X63 :  [0 1]\n",
      "X64 :  [0 1]\n",
      "X65 :  [0 1]\n",
      "X66 :  [0 1]\n",
      "X67 :  [0 1]\n",
      "X68 :  [1 0]\n",
      "X69 :  [0 1]\n",
      "X70 :  [1 0]\n",
      "X71 :  [0 1]\n",
      "X73 :  [0 1]\n",
      "X74 :  [1 0]\n",
      "X75 :  [0 1]\n",
      "X76 :  [0 1]\n",
      "X77 :  [0 1]\n",
      "X78 :  [0 1]\n",
      "X79 :  [0 1]\n",
      "X80 :  [0 1]\n",
      "X81 :  [0 1]\n",
      "X82 :  [0 1]\n",
      "X83 :  [0 1]\n",
      "X84 :  [0 1]\n",
      "X85 :  [1 0]\n",
      "X86 :  [0 1]\n",
      "X87 :  [0 1]\n",
      "X88 :  [0 1]\n",
      "X89 :  [0 1]\n",
      "X90 :  [0 1]\n",
      "X91 :  [0 1]\n",
      "X92 :  [0 1]\n",
      "X93 :  [0]\n",
      "X94 :  [0 1]\n",
      "X95 :  [0 1]\n",
      "X96 :  [0 1]\n",
      "X97 :  [0 1]\n",
      "X98 :  [0 1]\n",
      "X99 :  [0 1]\n",
      "X100 :  [0 1]\n",
      "X101 :  [0 1]\n",
      "X102 :  [0 1]\n",
      "X103 :  [0 1]\n",
      "X104 :  [0 1]\n",
      "X105 :  [0 1]\n",
      "X106 :  [0 1]\n",
      "X107 :  [0]\n",
      "X108 :  [0 1]\n",
      "X109 :  [0 1]\n",
      "X110 :  [0 1]\n",
      "X111 :  [1 0]\n",
      "X112 :  [0 1]\n",
      "X113 :  [0 1]\n",
      "X114 :  [1 0]\n",
      "X115 :  [0 1]\n",
      "X116 :  [1 0]\n",
      "X117 :  [0 1]\n",
      "X118 :  [1 0]\n",
      "X119 :  [1 0]\n",
      "X120 :  [1 0]\n",
      "X122 :  [0 1]\n",
      "X123 :  [0 1]\n",
      "X124 :  [0 1]\n",
      "X125 :  [0 1]\n",
      "X126 :  [0 1]\n",
      "X127 :  [0 1]\n",
      "X128 :  [1 0]\n",
      "X129 :  [0 1]\n",
      "X130 :  [0 1]\n",
      "X131 :  [1 0]\n",
      "X132 :  [0 1]\n",
      "X133 :  [0 1]\n",
      "X134 :  [0 1]\n",
      "X135 :  [0 1]\n",
      "X136 :  [1 0]\n",
      "X137 :  [1 0]\n",
      "X138 :  [0 1]\n",
      "X139 :  [0 1]\n",
      "X140 :  [0 1]\n",
      "X141 :  [0 1]\n",
      "X142 :  [1 0]\n",
      "X143 :  [0 1]\n",
      "X144 :  [1 0]\n",
      "X145 :  [0 1]\n",
      "X146 :  [0 1]\n",
      "X147 :  [0 1]\n",
      "X148 :  [0 1]\n",
      "X150 :  [1 0]\n",
      "X151 :  [0 1]\n",
      "X152 :  [0 1]\n",
      "X153 :  [0 1]\n",
      "X154 :  [0 1]\n",
      "X155 :  [0 1]\n",
      "X156 :  [1 0]\n",
      "X157 :  [0 1]\n",
      "X158 :  [0 1]\n",
      "X159 :  [0 1]\n",
      "X160 :  [0 1]\n",
      "X161 :  [0 1]\n",
      "X162 :  [0 1]\n",
      "X163 :  [0 1]\n",
      "X164 :  [0 1]\n",
      "X165 :  [0 1]\n",
      "X166 :  [0 1]\n",
      "X167 :  [0 1]\n",
      "X168 :  [0 1]\n",
      "X169 :  [0 1]\n",
      "X170 :  [1 0]\n",
      "X171 :  [0 1]\n",
      "X172 :  [0 1]\n",
      "X173 :  [0 1]\n",
      "X174 :  [0 1]\n",
      "X175 :  [0 1]\n",
      "X176 :  [0 1]\n",
      "X177 :  [0 1]\n",
      "X178 :  [0 1]\n",
      "X179 :  [1 0]\n",
      "X180 :  [0 1]\n",
      "X181 :  [0 1]\n",
      "X182 :  [0 1]\n",
      "X183 :  [0 1]\n",
      "X184 :  [1 0]\n",
      "X185 :  [0 1]\n",
      "X186 :  [0 1]\n",
      "X187 :  [1 0]\n",
      "X189 :  [1 0]\n",
      "X190 :  [0 1]\n",
      "X191 :  [0 1]\n",
      "X192 :  [0 1]\n",
      "X194 :  [1 0]\n",
      "X195 :  [0 1]\n",
      "X196 :  [0 1]\n",
      "X197 :  [0 1]\n",
      "X198 :  [0 1]\n",
      "X199 :  [0 1]\n",
      "X200 :  [0 1]\n",
      "X201 :  [0 1]\n",
      "X202 :  [0 1]\n",
      "X203 :  [0 1]\n",
      "X204 :  [1 0]\n",
      "X205 :  [0 1]\n",
      "X206 :  [0 1]\n",
      "X207 :  [0 1]\n",
      "X208 :  [0 1]\n",
      "X209 :  [1 0]\n",
      "X210 :  [0 1]\n",
      "X211 :  [0 1]\n",
      "X212 :  [0 1]\n",
      "X213 :  [0 1]\n",
      "X214 :  [0 1]\n",
      "X215 :  [0 1]\n",
      "X216 :  [0 1]\n",
      "X217 :  [0 1]\n",
      "X218 :  [0 1]\n",
      "X219 :  [0 1]\n",
      "X220 :  [1 0]\n",
      "X221 :  [0 1]\n",
      "X222 :  [0 1]\n",
      "X223 :  [0 1]\n",
      "X224 :  [0 1]\n",
      "X225 :  [0 1]\n",
      "X226 :  [0 1]\n",
      "X227 :  [0 1]\n",
      "X228 :  [0 1]\n",
      "X229 :  [0 1]\n",
      "X230 :  [0 1]\n",
      "X231 :  [0 1]\n",
      "X232 :  [0 1]\n",
      "X233 :  [0]\n",
      "X234 :  [1 0]\n",
      "X235 :  [0]\n",
      "X236 :  [0 1]\n",
      "X237 :  [1 0]\n",
      "X238 :  [0 1]\n",
      "X239 :  [0 1]\n",
      "X240 :  [0 1]\n",
      "X241 :  [0 1]\n",
      "X242 :  [0 1]\n",
      "X243 :  [0 1]\n",
      "X244 :  [0 1]\n",
      "X245 :  [0 1]\n",
      "X246 :  [0 1]\n",
      "X247 :  [0 1]\n",
      "X248 :  [0 1]\n",
      "X249 :  [0 1]\n",
      "X250 :  [0 1]\n",
      "X251 :  [0 1]\n",
      "X252 :  [0 1]\n",
      "X253 :  [0 1]\n",
      "X254 :  [0 1]\n",
      "X255 :  [0 1]\n",
      "X256 :  [0 1]\n",
      "X257 :  [0 1]\n",
      "X258 :  [0 1]\n",
      "X259 :  [0 1]\n",
      "X260 :  [0 1]\n",
      "X261 :  [0 1]\n",
      "X262 :  [1 0]\n",
      "X263 :  [1 0]\n",
      "X264 :  [0 1]\n",
      "X265 :  [0 1]\n",
      "X266 :  [1 0]\n",
      "X267 :  [0 1]\n",
      "X268 :  [0]\n",
      "X269 :  [0 1]\n",
      "X270 :  [0 1]\n",
      "X271 :  [0 1]\n",
      "X272 :  [0 1]\n",
      "X273 :  [1 0]\n",
      "X274 :  [0 1]\n",
      "X275 :  [1 0]\n",
      "X276 :  [0 1]\n",
      "X277 :  [0 1]\n",
      "X278 :  [0 1]\n",
      "X279 :  [0 1]\n",
      "X280 :  [0 1]\n",
      "X281 :  [0 1]\n",
      "X282 :  [0 1]\n",
      "X283 :  [0 1]\n",
      "X284 :  [0 1]\n",
      "X285 :  [1 0]\n",
      "X286 :  [0 1]\n",
      "X287 :  [0 1]\n",
      "X288 :  [0 1]\n",
      "X289 :  [0]\n",
      "X290 :  [0]\n",
      "X291 :  [0 1]\n",
      "X292 :  [0 1]\n",
      "X293 :  [0]\n",
      "X294 :  [0 1]\n",
      "X295 :  [0 1]\n",
      "X296 :  [0 1]\n",
      "X297 :  [0]\n",
      "X298 :  [0 1]\n",
      "X299 :  [0 1]\n",
      "X300 :  [0 1]\n",
      "X301 :  [0 1]\n",
      "X302 :  [0 1]\n",
      "X304 :  [0 1]\n",
      "X305 :  [0 1]\n",
      "X306 :  [1 0]\n",
      "X307 :  [0 1]\n",
      "X308 :  [0 1]\n",
      "X309 :  [0 1]\n",
      "X310 :  [0 1]\n",
      "X311 :  [0 1]\n",
      "X312 :  [0 1]\n",
      "X313 :  [0 1]\n",
      "X314 :  [0 1]\n",
      "X315 :  [0 1]\n",
      "X316 :  [1 0]\n",
      "X317 :  [0 1]\n",
      "X318 :  [0 1]\n",
      "X319 :  [0 1]\n",
      "X320 :  [0 1]\n",
      "X321 :  [0 1]\n",
      "X322 :  [0 1]\n",
      "X323 :  [0 1]\n",
      "X324 :  [1 0]\n",
      "X325 :  [0 1]\n",
      "X326 :  [0 1]\n",
      "X327 :  [1 0]\n",
      "X328 :  [0 1]\n",
      "X329 :  [1 0]\n",
      "X330 :  [0]\n",
      "X331 :  [0 1]\n",
      "X332 :  [0 1]\n",
      "X333 :  [0 1]\n",
      "X334 :  [1 0]\n",
      "X335 :  [0 1]\n",
      "X336 :  [0 1]\n",
      "X337 :  [0 1]\n",
      "X338 :  [0 1]\n",
      "X339 :  [0 1]\n",
      "X340 :  [0 1]\n",
      "X341 :  [0 1]\n",
      "X342 :  [0 1]\n",
      "X343 :  [0 1]\n",
      "X344 :  [0 1]\n",
      "X345 :  [0 1]\n",
      "X346 :  [0 1]\n",
      "X347 :  [0]\n",
      "X348 :  [0 1]\n",
      "X349 :  [0 1]\n",
      "X350 :  [0 1]\n",
      "X351 :  [0 1]\n",
      "X352 :  [0 1]\n",
      "X353 :  [0 1]\n",
      "X354 :  [1 0]\n",
      "X355 :  [0 1]\n",
      "X356 :  [0 1]\n",
      "X357 :  [0 1]\n",
      "X358 :  [0 1]\n",
      "X359 :  [0 1]\n",
      "X360 :  [0 1]\n",
      "X361 :  [1 0]\n",
      "X362 :  [0 1]\n",
      "X363 :  [0 1]\n",
      "X364 :  [0 1]\n",
      "X365 :  [0 1]\n",
      "X366 :  [0 1]\n",
      "X367 :  [0 1]\n",
      "X368 :  [0 1]\n",
      "X369 :  [0 1]\n",
      "X370 :  [0 1]\n",
      "X371 :  [0 1]\n",
      "X372 :  [0 1]\n",
      "X373 :  [0 1]\n",
      "X374 :  [0 1]\n",
      "X375 :  [0 1]\n",
      "X376 :  [0 1]\n",
      "X377 :  [1 0]\n",
      "X378 :  [0 1]\n",
      "X379 :  [0 1]\n",
      "X380 :  [0 1]\n",
      "X382 :  [0 1]\n",
      "X383 :  [0 1]\n",
      "X384 :  [0 1]\n",
      "X385 :  [0 1]\n"
     ]
    }
   ],
   "source": [
    "for n in range(data.shape[1]):\n",
    "    print(data.columns[n], \": \", data.iloc[:,n].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "1c20ab773c3ffd32911e07376ffa01b206067eb4",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target = data['y']\n",
    "data_old = data\n",
    "data = data.drop('y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4209.000000\n",
       "mean      100.669318\n",
       "std        12.679381\n",
       "min        72.110000\n",
       "25%        90.820000\n",
       "50%        99.150000\n",
       "75%       109.010000\n",
       "max       265.320000\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x115957d68>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c5a3080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "target.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'aa', 'ab', 'ad', 'ae', 'af', 'ag', 'ai', 'aj', 'ak', 'al',\n",
       "       'am', 'an', 'ao', 'ap', 'aq', 'as', 'at', 'au', 'av', 'aw', 'ax',\n",
       "       'ay', 'az', 'b', 'ba', 'bb', 'bc', 'c', 'd', 'e', 'f', 'g', 'h',\n",
       "       'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u',\n",
       "       'v', 'w', 'x', 'y', 'z'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.append(data.iloc[:,2].unique(), test.iloc[:,1].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "cd027f5231507492189a0b1134a884ba4170da12",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X0 : \n",
      "(4209, 377)  +  (4209, 53)\n",
      "(4209, 377)  +  (4209, 53)\n",
      "X1 : \n",
      "(4209, 431)  +  (4209, 27)\n",
      "(4209, 431)  +  (4209, 27)\n",
      "X2 : \n",
      "(4209, 459)  +  (4209, 50)\n",
      "(4209, 459)  +  (4209, 50)\n",
      "X3 : \n",
      "(4209, 510)  +  (4209, 7)\n",
      "(4209, 510)  +  (4209, 7)\n",
      "X4 : \n",
      "(4209, 518)  +  (4209, 4)\n",
      "(4209, 518)  +  (4209, 4)\n",
      "X5 : \n",
      "(4209, 523)  +  (4209, 33)\n",
      "(4209, 523)  +  (4209, 33)\n",
      "X6 : \n",
      "(4209, 557)  +  (4209, 12)\n",
      "(4209, 557)  +  (4209, 12)\n",
      "X8 : \n",
      "(4209, 570)  +  (4209, 25)\n",
      "(4209, 570)  +  (4209, 25)\n",
      "\n",
      "Final:  (4209, 596) (4209, 596)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = {}\n",
    "one_hot_encoder = {}\n",
    "for col in ['X0','X1','X2','X3','X4','X5','X6','X8']:\n",
    "    print(col, \": \")\n",
    "    \n",
    "    #fit encoder\n",
    "    label_unique = np.unique(np.append(data[col].unique(), test[col].unique()))\n",
    "    label_encoder[col] = LabelEncoder().fit(label_unique)\n",
    "    one_hot_encoder[col] = OneHotEncoder(sparse=False).fit(np.arange(len(label_encoder[col].classes_)).reshape(-1,1))\n",
    "    col_names = [col+'_'+str(n) for n in range(len(label_encoder[col].classes_))]\n",
    "    \n",
    "    #process train data\n",
    "    data[col] = label_encoder[col].transform(data[col])\n",
    "    features = one_hot_encoder[col].transform(data[col].values.reshape(-1,1))\n",
    "    print(data.shape, ' + ', features.shape)\n",
    "    data = pd.concat([data, pd.DataFrame(features, columns=col_names)], axis=1)\n",
    "    \n",
    "    #process test data\n",
    "    test[col] = label_encoder[col].transform(test[col])\n",
    "    features = one_hot_encoder[col].transform(test[col].values.reshape(-1,1))\n",
    "    print(test.shape, ' + ', features.shape)\n",
    "    test = pd.concat([test, pd.DataFrame(features, columns=col_names)], axis=1)\n",
    "    \n",
    "    #Y mean for each category\n",
    "    y_means = {}\n",
    "    for cat in range(features.shape[1]):\n",
    "        y_means[cat] = target[data[col+'_'+str(cat)] == 1].mean()\n",
    "    data[col+'_ymean'] = 0\n",
    "    for _, row in data.iterrows():\n",
    "        row[col+'_ymean'] = y_means[int(row[col])]\n",
    "    test[col+'_ymean'] = 0\n",
    "    for _, row in test.iterrows():\n",
    "        row[col+'_ymean'] = y_means[int(row[col])]\n",
    "    \n",
    "print('\\nFinal: ', data.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.1 Dimentionality Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA, TruncatedSVD\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"pca = PCA(n_components=n_comp, random_state=420)\\npca_results = pca.fit_transform(data)\\n\\n# ICA\\nica = FastICA(n_components=n_comp, random_state=420)\\nica_results = ica.fit_transform(data)\\n\\n# GRP\\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\\ngrp_results = grp.fit_transform(data)\\n\\n# SRP\\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\\nsrp_results = srp.fit_transform(data)\\n\\n# Append decomposition components to datasets\\nfor i in range(n_comp):\\n    data['pca_' + str(i)] = pca_results[:, i]\\n    data['ica_' + str(i)] = ica_results[:, i]\\n    data['tsvd_' + str(i)] = tsvd_results[:, i]\\n    data['grp_' + str(i)] = grp_results[:, i]\\n    data['srp_' + str(i)] = srp_results[:, i]\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_comp = 5\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "tsvd_results = tsvd.fit_transform(data)\n",
    "\n",
    "# PCA\n",
    "'''pca = PCA(n_components=n_comp, random_state=420)\n",
    "pca_results = pca.fit_transform(data)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=420)\n",
    "ica_results = ica.fit_transform(data)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results = grp.fit_transform(data)\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "srp_results = srp.fit_transform(data)\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(n_comp):\n",
    "    data['pca_' + str(i)] = pca_results[:, i]\n",
    "    data['ica_' + str(i)] = ica_results[:, i]\n",
    "    data['tsvd_' + str(i)] = tsvd_results[:, i]\n",
    "    data['grp_' + str(i)] = grp_results[:, i]\n",
    "    data['srp_' + str(i)] = srp_results[:, i]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#extract from test\n",
    "\n",
    "# tSVD\n",
    "tsvd_results = tsvd.transform(test)\n",
    "\n",
    "# PCA\n",
    "pca_results = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica_results = ica.transform(test)\n",
    "\n",
    "# GRP\n",
    "grp_results = grp.transform(test)\n",
    "\n",
    "# SRP\n",
    "srp_results = srp.transform(test)\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(n_comp):\n",
    "    test['pca_' + str(i)] = pca_results[:, i]\n",
    "    test['ica_' + str(i)] = ica_results[:, i]\n",
    "    test['tsvd_' + str(i)] = tsvd_results[:, i]\n",
    "    test['grp_' + str(i)] = grp_results[:, i]\n",
    "    test['srp_' + str(i)] = srp_results[:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.2 KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fit KMeans Cluster model - \n",
    "cluster_norm = Normalizer(copy=False)\n",
    "cluster_norm_result = cluster_norm.fit_transform(tsvd_results)\n",
    "\n",
    "cluster = KMeans(n_clusters=20, init='k-means++', max_iter=300, n_init=10,\n",
    "                        verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 1.92200472637\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Initialization complete\n",
      "Initialization complete\n",
      "Iteration 1, inertia 1.8424839362\n",
      "start iteration\n",
      "Initialization complete\n",
      "Initialization complete\n",
      "Initialization complete\n",
      "start iteration\n",
      "Initialization complete\n",
      "done sorting\n",
      "start iteration\n",
      "Initialization complete\n",
      "done sorting\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 0, inertia 1.83301140245\n",
      "start iteration\n",
      "Iteration 0, inertia 1.83692749566\n",
      "end inner loop\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 0, inertia 1.93575160255\n",
      "Iteration 2, inertia 1.82007379141\n",
      "start iteration\n",
      "Initialization complete\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 0, inertia 1.89747620148\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 0, inertia 1.87789660152\n",
      "end inner loop\n",
      "done sorting\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 1.74919057146\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 1, inertia 1.80112789678\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 3, inertia 1.79983380272\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 0, inertia 1.84479865894\n",
      "start iteration\n",
      "Iteration 0, inertia 1.86453779855\n",
      "done sorting\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 0, inertia 1.84147835144\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 1, inertia 1.77428004341\n",
      "Iteration 1, inertia 1.79425234444\n",
      "start iteration\n",
      "Iteration 1, inertia 1.82145786375\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 0, inertia 1.84001713709\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 2, inertia 1.78514185192\n",
      "Iteration 4, inertia 1.78921108159\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 1, inertia 1.79480366455\n",
      "Iteration 2, inertia 1.73346671057\n",
      "Iteration 1, inertia 1.75129950193\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 1, inertia 1.74038349156\n",
      "end inner loop\n",
      "Iteration 2, inertia 1.73912548306\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 2, inertia 1.75899767476\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 2, inertia 1.78823485693\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 1.77901218109\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 5, inertia 1.78505502912\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 3, inertia 1.7252836142\n",
      "Iteration 1, inertia 1.77630263043\n",
      "Iteration 2, inertia 1.7277149392\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 3, inertia 1.73202178337\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 3, inertia 1.74199244271\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 4, inertia 1.77513739328\n",
      "done sorting\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 2, inertia 1.77138727377\n",
      "Iteration 2, inertia 1.73003195749\n",
      "end inner loop\n",
      "Iteration 6, inertia 1.78107613132\n",
      "Iteration 2, inertia 1.76337548464\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 3, inertia 1.77016885226\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 4, inertia 1.72007930072\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 4, inertia 1.72647157471\n",
      "end inner loop\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 3, inertia 1.71784526568\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 4, inertia 1.73102674626\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 7, inertia 1.78045578848\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 3, inertia 1.75614150431\n",
      "Iteration 5, inertia 1.77034840942\n",
      "Iteration 3, inertia 1.75695271244\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 4, inertia 1.75498250701\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 5, inertia 1.7175188995\n",
      "done sorting\n",
      "Iteration 3, inertia 1.72420217121\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 5, inertia 1.71288610304\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 4, inertia 1.70879541556\n",
      "start iteration\n",
      "Iteration 5, inertia 1.73058695042\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "done sorting\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 6, inertia 1.76715902576\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 4, inertia 1.75578746213\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 8, inertia 1.78011024745\n",
      "Iteration 5, inertia 1.74913091268\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 4, inertia 1.71917768529\n",
      "Iteration 6, inertia 1.70884472649\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 5, inertia 1.70626230853\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 4, inertia 1.74739541618\n",
      "start iteration\n",
      "Iteration 6, inertia 1.71604116385\n",
      "Iteration 6, inertia 1.7303662862\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 5, inertia 1.75576817066\n",
      "end inner loop\n",
      "Iteration 9, inertia 1.77987309877\n",
      "Iteration 6, inertia 1.74524958715\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 7, inertia 1.76489417205\n",
      "start iteration\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 1.7148985523\n",
      "Iteration 7, inertia 1.70882070022\n",
      "Iteration 6, inertia 1.7057209791\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 7, inertia 1.73019478164\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 5, inertia 1.7375799662\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 6, inertia 1.75575958371\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 1.77959709331\n",
      "Iteration 5, inertia 1.71670892173\n",
      "end inner loop\n",
      "Iteration 7, inertia 1.73841313641\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 8, inertia 1.71433884972\n",
      "Iteration 8, inertia 1.70881846529\n",
      "done sorting\n",
      "Iteration 7, inertia 1.70535347182\n",
      "start iteration\n",
      "Iteration 8, inertia 1.76380058184\n",
      "center shift 2.440058e-05 within tolerance 2.604959e-08\n",
      "end inner loop\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 8, inertia 1.73000896011\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 6, inertia 1.72435215798\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 8, inertia 1.73464286936\n",
      "Iteration 11, inertia 1.77935721075\n",
      "Iteration 6, inertia 1.71544314115\n",
      "done sorting\n",
      "Iteration 7, inertia 1.75575194303\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 8, inertia 1.70469931702\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "center shift 1.281569e-04 within tolerance 2.604959e-08\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 9, inertia 1.762988796\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 9, inertia 1.71406317169\n",
      "start iteration\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 9, inertia 1.72977141286\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 1.73124047277\n",
      "Iteration 7, inertia 1.71870968732\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 12, inertia 1.77927241547\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 7, inertia 1.71476791454\n",
      "Iteration 9, inertia 1.70411268174\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 10, inertia 1.72953682875\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 10, inertia 1.76102287559\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 10, inertia 1.71385875703\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 10, inertia 1.7288805362\n",
      "Iteration 8, inertia 1.71570767881\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 10, inertia 1.70213576188\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 13, inertia 1.77911615875\n",
      "done sorting\n",
      "Iteration 8, inertia 1.71398943955\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 11, inertia 1.75809105671\n",
      "Iteration 11, inertia 1.71370011409\n",
      "end inner loop\n",
      "Iteration 11, inertia 1.72591681814\n",
      "start iteration\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 9, inertia 1.71325355888\n",
      "Iteration 14, inertia 1.77882448421\n",
      "Iteration 11, inertia 1.72934489247\n",
      "done sorting\n",
      "Iteration 11, inertia 1.69873968033\n",
      "start iteration\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 9, inertia 1.71305686177\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 12, inertia 1.72283440784\n",
      "Iteration 12, inertia 1.75700423569\n",
      "Iteration 12, inertia 1.71354955638\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 10, inertia 1.71208512514\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 12, inertia 1.69681484461\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 12, inertia 1.72919430355\n",
      "done sorting\n",
      "Iteration 15, inertia 1.77842574027\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 10, inertia 1.71242686473\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 13, inertia 1.72043542381\n",
      "Iteration 11, inertia 1.71129452407\n",
      "end inner loop\n",
      "Iteration 13, inertia 1.71347913278\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 13, inertia 1.72904937303\n",
      "done sorting\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 13, inertia 1.69624471954\n",
      "Iteration 16, inertia 1.7780324412\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 13, inertia 1.75648587157\n",
      "done sorting\n",
      "Iteration 11, inertia 1.71192857458\n",
      "start iteration\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 12, inertia 1.70952942032\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 14, inertia 1.71908997403\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 14, inertia 1.72891489955\n",
      "Iteration 17, inertia 1.7776288692\n",
      "start iteration\n",
      "Iteration 14, inertia 1.69581994254\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 14, inertia 1.75612867295\n",
      "done sorting\n",
      "Iteration 12, inertia 1.71117710597\n",
      "end inner loop\n",
      "Iteration 14, inertia 1.71342612231\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 18, inertia 1.77752838023\n",
      "end inner loop\n",
      "Iteration 15, inertia 1.71863956744\n",
      "start iteration\n",
      "Iteration 15, inertia 1.72878407201\n",
      "start iteration\n",
      "Iteration 13, inertia 1.70774480105\n",
      "done sorting\n",
      "Iteration 15, inertia 1.69521883996\n",
      "done sorting\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 13, inertia 1.71050824575\n",
      "end inner loop\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 15, inertia 1.71335523814\n",
      "Iteration 15, inertia 1.7557380316\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 19, inertia 1.77744091666\n",
      "Iteration 16, inertia 1.718468187\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 14, inertia 1.70705759915\n",
      "done sorting\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 16, inertia 1.7286484846\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 16, inertia 1.69468997063\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 16, inertia 1.71317763289\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 16, inertia 1.75509085713\n",
      "Iteration 14, inertia 1.71010838711\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 17, inertia 1.7183240258\n",
      "Iteration 20, inertia 1.77740408129\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 15, inertia 1.70613168547\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 17, inertia 1.69386994547\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 17, inertia 1.72851259195\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 15, inertia 1.70995696825\n",
      "end inner loop\n",
      "Iteration 17, inertia 1.75460990613\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 17, inertia 1.71293475614\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 21, inertia 1.77736742396\n",
      "Iteration 18, inertia 1.71818497084\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 16, inertia 1.7059769865\n",
      "Iteration 18, inertia 1.69258873908\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 18, inertia 1.72842870944\n",
      "Iteration 16, inertia 1.70991565896\n",
      "end inner loop\n",
      "Iteration 18, inertia 1.75408700112\n",
      "start iteration\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 19, inertia 1.71815326739\n",
      "Iteration 18, inertia 1.7126197512\n",
      "Iteration 22, inertia 1.77718368411\n",
      "Iteration 19, inertia 1.68871724877\n",
      "Iteration 17, inertia 1.70590563751\n",
      "start iteration\n",
      "start iteration\n",
      "start iteration\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 17, inertia 1.70989100595\n",
      "done sorting\n",
      "Iteration 19, inertia 1.75385987213\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 20, inertia 1.71812807422\n",
      "Iteration 19, inertia 1.72820110018\n",
      "end inner loop\n",
      "Iteration 23, inertia 1.77695550156\n",
      "start iteration\n",
      "Iteration 20, inertia 1.68585648562\n",
      "Iteration 19, inertia 1.71224064359\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 18, inertia 1.70578614438\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 20, inertia 1.75337089561\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 18, inertia 1.70986777282\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 20, inertia 1.72810221118\n",
      "end inner loop\n",
      "Iteration 24, inertia 1.77689128121\n",
      "start iteration\n",
      "Iteration 21, inertia 1.71812423312\n",
      "done sorting\n",
      "start iteration\n",
      "center shift 1.213697e-04 within tolerance 2.604959e-08\n",
      "Iteration 21, inertia 1.68368163465\n",
      "Iteration 19, inertia 1.70577794452\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 20, inertia 1.71125915649\n",
      "center shift 5.669269e-05 within tolerance 2.604959e-08\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 19, inertia 1.70986408452\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "center shift 1.102187e-04 within tolerance 2.604959e-08\n",
      "Iteration 21, inertia 1.7280436479\n",
      "Iteration 21, inertia 1.75243093911\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 25, inertia 1.77685667179\n",
      "Iteration 22, inertia 1.68219086245\n",
      "done sorting\n",
      "done sorting\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 21, inertia 1.7108244436\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 22, inertia 1.75176537034\n",
      "end inner loop\n",
      "Iteration 22, inertia 1.72801229737\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 26, inertia 1.77668321951\n",
      "end inner loop\n",
      "Iteration 23, inertia 1.681175396\n",
      "start iteration\n",
      "Iteration 22, inertia 1.71030228488\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 23, inertia 1.75168023246\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 23, inertia 1.72797762879\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 27, inertia 1.77658463814\n",
      "done sorting\n",
      "Iteration 24, inertia 1.68103964492\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 23, inertia 1.71001898559\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 24, inertia 1.72794105378\n",
      "Iteration 24, inertia 1.75164782928\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 28, inertia 1.77647046182\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 25, inertia 1.68103469386\n",
      "start iteration\n",
      "done sorting\n",
      "center shift 9.708638e-05 within tolerance 2.604959e-08\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 24, inertia 1.70977576801\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 25, inertia 1.72788666359\n",
      "end inner loop\n",
      "Iteration 29, inertia 1.77620938478\n",
      "start iteration\n",
      "Iteration 25, inertia 1.75163958005\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 25, inertia 1.70926370213\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 26, inertia 1.7277900779\n",
      "Iteration 30, inertia 1.77608588583\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 26, inertia 1.75163473779\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 26, inertia 1.70879046213\n",
      "center shift 1.149615e-04 within tolerance 2.604959e-08\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 31, inertia 1.77606204453\n",
      "Iteration 27, inertia 1.72765567609\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 27, inertia 1.70816577023\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 32, inertia 1.77601875421\n",
      "Iteration 28, inertia 1.72749815112\n",
      "Iteration 28, inertia 1.70777339739\n",
      "start iteration\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 33, inertia 1.77596686845\n",
      "Iteration 29, inertia 1.70734002979\n",
      "start iteration\n",
      "Iteration 29, inertia 1.72730330068\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 34, inertia 1.77593840059\n",
      "Iteration 30, inertia 1.70692227467\n",
      "center shift 1.190379e-04 within tolerance 2.604959e-08\n",
      "Iteration 30, inertia 1.72705695906\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 31, inertia 1.70661248884\n",
      "Iteration 31, inertia 1.72680555974\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 32, inertia 1.70634370421\n",
      "Iteration 32, inertia 1.72643088681\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 33, inertia 1.70589545459\n",
      "start iteration\n",
      "Iteration 33, inertia 1.72600092906\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 34, inertia 1.70563475864\n",
      "start iteration\n",
      "Iteration 34, inertia 1.72560528387\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 35, inertia 1.70545276589\n",
      "start iteration\n",
      "Iteration 35, inertia 1.72521674456\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 36, inertia 1.70537914133\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 36, inertia 1.72482172891\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 37, inertia 1.70534953176\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 37, inertia 1.72459919966\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 38, inertia 1.70533158363\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 38, inertia 1.72445881898\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 39, inertia 1.70527465427\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 39, inertia 1.72433787753\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 40, inertia 1.70521993363\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 40, inertia 1.72422966637\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 41, inertia 1.70511676971\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 41, inertia 1.72417985657\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 42, inertia 1.7049750703\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 42, inertia 1.7241466709\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 43, inertia 1.70496052417\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 43, inertia 1.72412742717\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 44, inertia 1.70492286873\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 44, inertia 1.72410438412\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 45, inertia 1.70491032456\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 45, inertia 1.72406145971\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 46, inertia 1.70490553522\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 46, inertia 1.72397311243\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 47, inertia 1.70490553522\n",
      "center shift 0.000000e+00 within tolerance 2.604959e-08\n",
      "Iteration 47, inertia 1.72385754449\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 48, inertia 1.72371091019\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 49, inertia 1.72355391791\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 50, inertia 1.72342030106\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 51, inertia 1.7232710508\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 52, inertia 1.7230859783\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 53, inertia 1.72289124306\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 54, inertia 1.72263536244\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 55, inertia 1.72228701778\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 56, inertia 1.7219437278\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 57, inertia 1.72146997841\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 58, inertia 1.72106863922\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 59, inertia 1.72082550804\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 60, inertia 1.72074398324\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 61, inertia 1.72071551969\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 62, inertia 1.72071191472\n",
      "center shift 3.721831e-05 within tolerance 2.604959e-08\n"
     ]
    }
   ],
   "source": [
    "cluster_result = cluster.fit_transform(cluster_norm_result)\n",
    "for i in range(40):\n",
    "    data['kc_' + str(i)] = cluster_result[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#transform test data\n",
    "cluster_norm_result = cluster_norm.transform(tsvd_results)\n",
    "cluster_result = cluster.transform(cluster_norm_result)\n",
    "for i in range(40):\n",
    "    test['kc_' + str(i)] = cluster_result[:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4209 entries, 0 to 4208\n",
      "Columns: 886 entries, ID to kc_39\n",
      "dtypes: float64(501), int64(385)\n",
      "memory usage: 28.5 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(886,) ['ID' 'X0' 'X1' 'X2' 'X3' 'X4' 'X5' 'X6' 'X8' 'X10' 'X11' 'X12' 'X13' 'X14'\n",
      " 'X15' 'X16' 'X17' 'X18' 'X19' 'X20' 'X21' 'X22' 'X23' 'X24' 'X26' 'X27'\n",
      " 'X28' 'X29' 'X30' 'X31' 'X32' 'X33' 'X34' 'X35' 'X36' 'X37' 'X38' 'X39'\n",
      " 'X40' 'X41' 'X42' 'X43' 'X44' 'X45' 'X46' 'X47' 'X48' 'X49' 'X50' 'X51'\n",
      " 'X52' 'X53' 'X54' 'X55' 'X56' 'X57' 'X58' 'X59' 'X60' 'X61' 'X62' 'X63'\n",
      " 'X64' 'X65' 'X66' 'X67' 'X68' 'X69' 'X70' 'X71' 'X73' 'X74' 'X75' 'X76'\n",
      " 'X77' 'X78' 'X79' 'X80' 'X81' 'X82' 'X83' 'X84' 'X85' 'X86' 'X87' 'X88'\n",
      " 'X89' 'X90' 'X91' 'X92' 'X93' 'X94' 'X95' 'X96' 'X97' 'X98' 'X99' 'X100'\n",
      " 'X101' 'X102' 'X103' 'X104' 'X105' 'X106' 'X107' 'X108' 'X109' 'X110'\n",
      " 'X111' 'X112' 'X113' 'X114' 'X115' 'X116' 'X117' 'X118' 'X119' 'X120'\n",
      " 'X122' 'X123' 'X124' 'X125' 'X126' 'X127' 'X128' 'X129' 'X130' 'X131'\n",
      " 'X132' 'X133' 'X134' 'X135' 'X136' 'X137' 'X138' 'X139' 'X140' 'X141'\n",
      " 'X142' 'X143' 'X144' 'X145' 'X146' 'X147' 'X148' 'X150' 'X151' 'X152'\n",
      " 'X153' 'X154' 'X155' 'X156' 'X157' 'X158' 'X159' 'X160' 'X161' 'X162'\n",
      " 'X163' 'X164' 'X165' 'X166' 'X167' 'X168' 'X169' 'X170' 'X171' 'X172'\n",
      " 'X173' 'X174' 'X175' 'X176' 'X177' 'X178' 'X179' 'X180' 'X181' 'X182'\n",
      " 'X183' 'X184' 'X185' 'X186' 'X187' 'X189' 'X190' 'X191' 'X192' 'X194'\n",
      " 'X195' 'X196' 'X197' 'X198' 'X199' 'X200' 'X201' 'X202' 'X203' 'X204'\n",
      " 'X205' 'X206' 'X207' 'X208' 'X209' 'X210' 'X211' 'X212' 'X213' 'X214'\n",
      " 'X215' 'X216' 'X217' 'X218' 'X219' 'X220' 'X221' 'X222' 'X223' 'X224'\n",
      " 'X225' 'X226' 'X227' 'X228' 'X229' 'X230' 'X231' 'X232' 'X233' 'X234'\n",
      " 'X235' 'X236' 'X237' 'X238' 'X239' 'X240' 'X241' 'X242' 'X243' 'X244'\n",
      " 'X245' 'X246' 'X247' 'X248' 'X249' 'X250' 'X251' 'X252' 'X253' 'X254'\n",
      " 'X255' 'X256' 'X257' 'X258' 'X259' 'X260' 'X261' 'X262' 'X263' 'X264'\n",
      " 'X265' 'X266' 'X267' 'X268' 'X269' 'X270' 'X271' 'X272' 'X273' 'X274'\n",
      " 'X275' 'X276' 'X277' 'X278' 'X279' 'X280' 'X281' 'X282' 'X283' 'X284'\n",
      " 'X285' 'X286' 'X287' 'X288' 'X289' 'X290' 'X291' 'X292' 'X293' 'X294'\n",
      " 'X295' 'X296' 'X297' 'X298' 'X299' 'X300' 'X301' 'X302' 'X304' 'X305'\n",
      " 'X306' 'X307' 'X308' 'X309' 'X310' 'X311' 'X312' 'X313' 'X314' 'X315'\n",
      " 'X316' 'X317' 'X318' 'X319' 'X320' 'X321' 'X322' 'X323' 'X324' 'X325'\n",
      " 'X326' 'X327' 'X328' 'X329' 'X330' 'X331' 'X332' 'X333' 'X334' 'X335'\n",
      " 'X336' 'X337' 'X338' 'X339' 'X340' 'X341' 'X342' 'X343' 'X344' 'X345'\n",
      " 'X346' 'X347' 'X348' 'X349' 'X350' 'X351' 'X352' 'X353' 'X354' 'X355'\n",
      " 'X356' 'X357' 'X358' 'X359' 'X360' 'X361' 'X362' 'X363' 'X364' 'X365'\n",
      " 'X366' 'X367' 'X368' 'X369' 'X370' 'X371' 'X372' 'X373' 'X374' 'X375'\n",
      " 'X376' 'X377' 'X378' 'X379' 'X380' 'X382' 'X383' 'X384' 'X385' 'X0_0'\n",
      " 'X0_1' 'X0_2' 'X0_3' 'X0_4' 'X0_5' 'X0_6' 'X0_7' 'X0_8' 'X0_9' 'X0_10'\n",
      " 'X0_11' 'X0_12' 'X0_13' 'X0_14' 'X0_15' 'X0_16' 'X0_17' 'X0_18' 'X0_19'\n",
      " 'X0_20' 'X0_21' 'X0_22' 'X0_23' 'X0_24' 'X0_25' 'X0_26' 'X0_27' 'X0_28'\n",
      " 'X0_29' 'X0_30' 'X0_31' 'X0_32' 'X0_33' 'X0_34' 'X0_35' 'X0_36' 'X0_37'\n",
      " 'X0_38' 'X0_39' 'X0_40' 'X0_41' 'X0_42' 'X0_43' 'X0_44' 'X0_45' 'X0_46'\n",
      " 'X0_47' 'X0_48' 'X0_49' 'X0_50' 'X0_51' 'X0_52' 'X0_ymean' 'X1_0' 'X1_1'\n",
      " 'X1_2' 'X1_3' 'X1_4' 'X1_5' 'X1_6' 'X1_7' 'X1_8' 'X1_9' 'X1_10' 'X1_11'\n",
      " 'X1_12' 'X1_13' 'X1_14' 'X1_15' 'X1_16' 'X1_17' 'X1_18' 'X1_19' 'X1_20'\n",
      " 'X1_21' 'X1_22' 'X1_23' 'X1_24' 'X1_25' 'X1_26' 'X1_ymean' 'X2_0' 'X2_1'\n",
      " 'X2_2' 'X2_3' 'X2_4' 'X2_5' 'X2_6' 'X2_7' 'X2_8' 'X2_9' 'X2_10' 'X2_11'\n",
      " 'X2_12' 'X2_13' 'X2_14' 'X2_15' 'X2_16' 'X2_17' 'X2_18' 'X2_19' 'X2_20'\n",
      " 'X2_21' 'X2_22' 'X2_23' 'X2_24' 'X2_25' 'X2_26' 'X2_27' 'X2_28' 'X2_29'\n",
      " 'X2_30' 'X2_31' 'X2_32' 'X2_33' 'X2_34' 'X2_35' 'X2_36' 'X2_37' 'X2_38'\n",
      " 'X2_39' 'X2_40' 'X2_41' 'X2_42' 'X2_43' 'X2_44' 'X2_45' 'X2_46' 'X2_47'\n",
      " 'X2_48' 'X2_49' 'X2_ymean' 'X3_0' 'X3_1' 'X3_2' 'X3_3' 'X3_4' 'X3_5'\n",
      " 'X3_6' 'X3_ymean' 'X4_0' 'X4_1' 'X4_2' 'X4_3' 'X4_ymean' 'X5_0' 'X5_1'\n",
      " 'X5_2' 'X5_3' 'X5_4' 'X5_5' 'X5_6' 'X5_7' 'X5_8' 'X5_9' 'X5_10' 'X5_11'\n",
      " 'X5_12' 'X5_13' 'X5_14' 'X5_15' 'X5_16' 'X5_17' 'X5_18' 'X5_19' 'X5_20'\n",
      " 'X5_21' 'X5_22' 'X5_23' 'X5_24' 'X5_25' 'X5_26' 'X5_27' 'X5_28' 'X5_29'\n",
      " 'X5_30' 'X5_31' 'X5_32' 'X5_ymean' 'X6_0' 'X6_1' 'X6_2' 'X6_3' 'X6_4'\n",
      " 'X6_5' 'X6_6' 'X6_7' 'X6_8' 'X6_9' 'X6_10' 'X6_11' 'X6_ymean' 'X8_0'\n",
      " 'X8_1' 'X8_2' 'X8_3' 'X8_4' 'X8_5' 'X8_6' 'X8_7' 'X8_8' 'X8_9' 'X8_10'\n",
      " 'X8_11' 'X8_12' 'X8_13' 'X8_14' 'X8_15' 'X8_16' 'X8_17' 'X8_18' 'X8_19'\n",
      " 'X8_20' 'X8_21' 'X8_22' 'X8_23' 'X8_24' 'X8_ymean' 'pca_0' 'ica_0'\n",
      " 'tsvd_0' 'grp_0' 'srp_0' 'pca_1' 'ica_1' 'tsvd_1' 'grp_1' 'srp_1' 'pca_2'\n",
      " 'ica_2' 'tsvd_2' 'grp_2' 'srp_2' 'pca_3' 'ica_3' 'tsvd_3' 'grp_3' 'srp_3'\n",
      " 'pca_4' 'ica_4' 'tsvd_4' 'grp_4' 'srp_4' 'pca_5' 'ica_5' 'tsvd_5' 'grp_5'\n",
      " 'srp_5' 'pca_6' 'ica_6' 'tsvd_6' 'grp_6' 'srp_6' 'pca_7' 'ica_7' 'tsvd_7'\n",
      " 'grp_7' 'srp_7' 'pca_8' 'ica_8' 'tsvd_8' 'grp_8' 'srp_8' 'pca_9' 'ica_9'\n",
      " 'tsvd_9' 'grp_9' 'srp_9' 'pca_10' 'ica_10' 'tsvd_10' 'grp_10' 'srp_10'\n",
      " 'pca_11' 'ica_11' 'tsvd_11' 'grp_11' 'srp_11' 'pca_12' 'ica_12' 'tsvd_12'\n",
      " 'grp_12' 'srp_12' 'pca_13' 'ica_13' 'tsvd_13' 'grp_13' 'srp_13' 'pca_14'\n",
      " 'ica_14' 'tsvd_14' 'grp_14' 'srp_14' 'pca_15' 'ica_15' 'tsvd_15' 'grp_15'\n",
      " 'srp_15' 'pca_16' 'ica_16' 'tsvd_16' 'grp_16' 'srp_16' 'pca_17' 'ica_17'\n",
      " 'tsvd_17' 'grp_17' 'srp_17' 'pca_18' 'ica_18' 'tsvd_18' 'grp_18' 'srp_18'\n",
      " 'pca_19' 'ica_19' 'tsvd_19' 'grp_19' 'srp_19' 'pca_20' 'ica_20' 'tsvd_20'\n",
      " 'grp_20' 'srp_20' 'pca_21' 'ica_21' 'tsvd_21' 'grp_21' 'srp_21' 'pca_22'\n",
      " 'ica_22' 'tsvd_22' 'grp_22' 'srp_22' 'pca_23' 'ica_23' 'tsvd_23' 'grp_23'\n",
      " 'srp_23' 'pca_24' 'ica_24' 'tsvd_24' 'grp_24' 'srp_24' 'pca_25' 'ica_25'\n",
      " 'tsvd_25' 'grp_25' 'srp_25' 'pca_26' 'ica_26' 'tsvd_26' 'grp_26' 'srp_26'\n",
      " 'pca_27' 'ica_27' 'tsvd_27' 'grp_27' 'srp_27' 'pca_28' 'ica_28' 'tsvd_28'\n",
      " 'grp_28' 'srp_28' 'pca_29' 'ica_29' 'tsvd_29' 'grp_29' 'srp_29' 'pca_30'\n",
      " 'ica_30' 'tsvd_30' 'grp_30' 'srp_30' 'pca_31' 'ica_31' 'tsvd_31' 'grp_31'\n",
      " 'srp_31' 'pca_32' 'ica_32' 'tsvd_32' 'grp_32' 'srp_32' 'pca_33' 'ica_33'\n",
      " 'tsvd_33' 'grp_33' 'srp_33' 'pca_34' 'ica_34' 'tsvd_34' 'grp_34' 'srp_34'\n",
      " 'pca_35' 'ica_35' 'tsvd_35' 'grp_35' 'srp_35' 'pca_36' 'ica_36' 'tsvd_36'\n",
      " 'grp_36' 'srp_36' 'pca_37' 'ica_37' 'tsvd_37' 'grp_37' 'srp_37' 'pca_38'\n",
      " 'ica_38' 'tsvd_38' 'grp_38' 'srp_38' 'pca_39' 'ica_39' 'tsvd_39' 'grp_39'\n",
      " 'srp_39' 'pca_40' 'ica_40' 'tsvd_40' 'grp_40' 'srp_40' 'pca_41' 'ica_41'\n",
      " 'tsvd_41' 'grp_41' 'srp_41' 'pca_42' 'ica_42' 'tsvd_42' 'grp_42' 'srp_42'\n",
      " 'pca_43' 'ica_43' 'tsvd_43' 'grp_43' 'srp_43' 'pca_44' 'ica_44' 'tsvd_44'\n",
      " 'grp_44' 'srp_44' 'pca_45' 'ica_45' 'tsvd_45' 'grp_45' 'srp_45' 'pca_46'\n",
      " 'ica_46' 'tsvd_46' 'grp_46' 'srp_46' 'pca_47' 'ica_47' 'tsvd_47' 'grp_47'\n",
      " 'srp_47' 'pca_48' 'ica_48' 'tsvd_48' 'grp_48' 'srp_48' 'pca_49' 'ica_49'\n",
      " 'tsvd_49' 'grp_49' 'srp_49' 'kc_0' 'kc_1' 'kc_2' 'kc_3' 'kc_4' 'kc_5'\n",
      " 'kc_6' 'kc_7' 'kc_8' 'kc_9' 'kc_10' 'kc_11' 'kc_12' 'kc_13' 'kc_14'\n",
      " 'kc_15' 'kc_16' 'kc_17' 'kc_18' 'kc_19' 'kc_20' 'kc_21' 'kc_22' 'kc_23'\n",
      " 'kc_24' 'kc_25' 'kc_26' 'kc_27' 'kc_28' 'kc_29' 'kc_30' 'kc_31' 'kc_32'\n",
      " 'kc_33' 'kc_34' 'kc_35' 'kc_36' 'kc_37' 'kc_38' 'kc_39']\n"
     ]
    }
   ],
   "source": [
    "print(data.columns.shape, data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(886,) ['ID' 'X0' 'X1' 'X2' 'X3' 'X4' 'X5' 'X6' 'X8' 'X10' 'X11' 'X12' 'X13' 'X14'\n",
      " 'X15' 'X16' 'X17' 'X18' 'X19' 'X20' 'X21' 'X22' 'X23' 'X24' 'X26' 'X27'\n",
      " 'X28' 'X29' 'X30' 'X31' 'X32' 'X33' 'X34' 'X35' 'X36' 'X37' 'X38' 'X39'\n",
      " 'X40' 'X41' 'X42' 'X43' 'X44' 'X45' 'X46' 'X47' 'X48' 'X49' 'X50' 'X51'\n",
      " 'X52' 'X53' 'X54' 'X55' 'X56' 'X57' 'X58' 'X59' 'X60' 'X61' 'X62' 'X63'\n",
      " 'X64' 'X65' 'X66' 'X67' 'X68' 'X69' 'X70' 'X71' 'X73' 'X74' 'X75' 'X76'\n",
      " 'X77' 'X78' 'X79' 'X80' 'X81' 'X82' 'X83' 'X84' 'X85' 'X86' 'X87' 'X88'\n",
      " 'X89' 'X90' 'X91' 'X92' 'X93' 'X94' 'X95' 'X96' 'X97' 'X98' 'X99' 'X100'\n",
      " 'X101' 'X102' 'X103' 'X104' 'X105' 'X106' 'X107' 'X108' 'X109' 'X110'\n",
      " 'X111' 'X112' 'X113' 'X114' 'X115' 'X116' 'X117' 'X118' 'X119' 'X120'\n",
      " 'X122' 'X123' 'X124' 'X125' 'X126' 'X127' 'X128' 'X129' 'X130' 'X131'\n",
      " 'X132' 'X133' 'X134' 'X135' 'X136' 'X137' 'X138' 'X139' 'X140' 'X141'\n",
      " 'X142' 'X143' 'X144' 'X145' 'X146' 'X147' 'X148' 'X150' 'X151' 'X152'\n",
      " 'X153' 'X154' 'X155' 'X156' 'X157' 'X158' 'X159' 'X160' 'X161' 'X162'\n",
      " 'X163' 'X164' 'X165' 'X166' 'X167' 'X168' 'X169' 'X170' 'X171' 'X172'\n",
      " 'X173' 'X174' 'X175' 'X176' 'X177' 'X178' 'X179' 'X180' 'X181' 'X182'\n",
      " 'X183' 'X184' 'X185' 'X186' 'X187' 'X189' 'X190' 'X191' 'X192' 'X194'\n",
      " 'X195' 'X196' 'X197' 'X198' 'X199' 'X200' 'X201' 'X202' 'X203' 'X204'\n",
      " 'X205' 'X206' 'X207' 'X208' 'X209' 'X210' 'X211' 'X212' 'X213' 'X214'\n",
      " 'X215' 'X216' 'X217' 'X218' 'X219' 'X220' 'X221' 'X222' 'X223' 'X224'\n",
      " 'X225' 'X226' 'X227' 'X228' 'X229' 'X230' 'X231' 'X232' 'X233' 'X234'\n",
      " 'X235' 'X236' 'X237' 'X238' 'X239' 'X240' 'X241' 'X242' 'X243' 'X244'\n",
      " 'X245' 'X246' 'X247' 'X248' 'X249' 'X250' 'X251' 'X252' 'X253' 'X254'\n",
      " 'X255' 'X256' 'X257' 'X258' 'X259' 'X260' 'X261' 'X262' 'X263' 'X264'\n",
      " 'X265' 'X266' 'X267' 'X268' 'X269' 'X270' 'X271' 'X272' 'X273' 'X274'\n",
      " 'X275' 'X276' 'X277' 'X278' 'X279' 'X280' 'X281' 'X282' 'X283' 'X284'\n",
      " 'X285' 'X286' 'X287' 'X288' 'X289' 'X290' 'X291' 'X292' 'X293' 'X294'\n",
      " 'X295' 'X296' 'X297' 'X298' 'X299' 'X300' 'X301' 'X302' 'X304' 'X305'\n",
      " 'X306' 'X307' 'X308' 'X309' 'X310' 'X311' 'X312' 'X313' 'X314' 'X315'\n",
      " 'X316' 'X317' 'X318' 'X319' 'X320' 'X321' 'X322' 'X323' 'X324' 'X325'\n",
      " 'X326' 'X327' 'X328' 'X329' 'X330' 'X331' 'X332' 'X333' 'X334' 'X335'\n",
      " 'X336' 'X337' 'X338' 'X339' 'X340' 'X341' 'X342' 'X343' 'X344' 'X345'\n",
      " 'X346' 'X347' 'X348' 'X349' 'X350' 'X351' 'X352' 'X353' 'X354' 'X355'\n",
      " 'X356' 'X357' 'X358' 'X359' 'X360' 'X361' 'X362' 'X363' 'X364' 'X365'\n",
      " 'X366' 'X367' 'X368' 'X369' 'X370' 'X371' 'X372' 'X373' 'X374' 'X375'\n",
      " 'X376' 'X377' 'X378' 'X379' 'X380' 'X382' 'X383' 'X384' 'X385' 'X0_0'\n",
      " 'X0_1' 'X0_2' 'X0_3' 'X0_4' 'X0_5' 'X0_6' 'X0_7' 'X0_8' 'X0_9' 'X0_10'\n",
      " 'X0_11' 'X0_12' 'X0_13' 'X0_14' 'X0_15' 'X0_16' 'X0_17' 'X0_18' 'X0_19'\n",
      " 'X0_20' 'X0_21' 'X0_22' 'X0_23' 'X0_24' 'X0_25' 'X0_26' 'X0_27' 'X0_28'\n",
      " 'X0_29' 'X0_30' 'X0_31' 'X0_32' 'X0_33' 'X0_34' 'X0_35' 'X0_36' 'X0_37'\n",
      " 'X0_38' 'X0_39' 'X0_40' 'X0_41' 'X0_42' 'X0_43' 'X0_44' 'X0_45' 'X0_46'\n",
      " 'X0_47' 'X0_48' 'X0_49' 'X0_50' 'X0_51' 'X0_52' 'X0_ymean' 'X1_0' 'X1_1'\n",
      " 'X1_2' 'X1_3' 'X1_4' 'X1_5' 'X1_6' 'X1_7' 'X1_8' 'X1_9' 'X1_10' 'X1_11'\n",
      " 'X1_12' 'X1_13' 'X1_14' 'X1_15' 'X1_16' 'X1_17' 'X1_18' 'X1_19' 'X1_20'\n",
      " 'X1_21' 'X1_22' 'X1_23' 'X1_24' 'X1_25' 'X1_26' 'X1_ymean' 'X2_0' 'X2_1'\n",
      " 'X2_2' 'X2_3' 'X2_4' 'X2_5' 'X2_6' 'X2_7' 'X2_8' 'X2_9' 'X2_10' 'X2_11'\n",
      " 'X2_12' 'X2_13' 'X2_14' 'X2_15' 'X2_16' 'X2_17' 'X2_18' 'X2_19' 'X2_20'\n",
      " 'X2_21' 'X2_22' 'X2_23' 'X2_24' 'X2_25' 'X2_26' 'X2_27' 'X2_28' 'X2_29'\n",
      " 'X2_30' 'X2_31' 'X2_32' 'X2_33' 'X2_34' 'X2_35' 'X2_36' 'X2_37' 'X2_38'\n",
      " 'X2_39' 'X2_40' 'X2_41' 'X2_42' 'X2_43' 'X2_44' 'X2_45' 'X2_46' 'X2_47'\n",
      " 'X2_48' 'X2_49' 'X2_ymean' 'X3_0' 'X3_1' 'X3_2' 'X3_3' 'X3_4' 'X3_5'\n",
      " 'X3_6' 'X3_ymean' 'X4_0' 'X4_1' 'X4_2' 'X4_3' 'X4_ymean' 'X5_0' 'X5_1'\n",
      " 'X5_2' 'X5_3' 'X5_4' 'X5_5' 'X5_6' 'X5_7' 'X5_8' 'X5_9' 'X5_10' 'X5_11'\n",
      " 'X5_12' 'X5_13' 'X5_14' 'X5_15' 'X5_16' 'X5_17' 'X5_18' 'X5_19' 'X5_20'\n",
      " 'X5_21' 'X5_22' 'X5_23' 'X5_24' 'X5_25' 'X5_26' 'X5_27' 'X5_28' 'X5_29'\n",
      " 'X5_30' 'X5_31' 'X5_32' 'X5_ymean' 'X6_0' 'X6_1' 'X6_2' 'X6_3' 'X6_4'\n",
      " 'X6_5' 'X6_6' 'X6_7' 'X6_8' 'X6_9' 'X6_10' 'X6_11' 'X6_ymean' 'X8_0'\n",
      " 'X8_1' 'X8_2' 'X8_3' 'X8_4' 'X8_5' 'X8_6' 'X8_7' 'X8_8' 'X8_9' 'X8_10'\n",
      " 'X8_11' 'X8_12' 'X8_13' 'X8_14' 'X8_15' 'X8_16' 'X8_17' 'X8_18' 'X8_19'\n",
      " 'X8_20' 'X8_21' 'X8_22' 'X8_23' 'X8_24' 'X8_ymean' 'pca_0' 'ica_0'\n",
      " 'tsvd_0' 'grp_0' 'srp_0' 'pca_1' 'ica_1' 'tsvd_1' 'grp_1' 'srp_1' 'pca_2'\n",
      " 'ica_2' 'tsvd_2' 'grp_2' 'srp_2' 'pca_3' 'ica_3' 'tsvd_3' 'grp_3' 'srp_3'\n",
      " 'pca_4' 'ica_4' 'tsvd_4' 'grp_4' 'srp_4' 'pca_5' 'ica_5' 'tsvd_5' 'grp_5'\n",
      " 'srp_5' 'pca_6' 'ica_6' 'tsvd_6' 'grp_6' 'srp_6' 'pca_7' 'ica_7' 'tsvd_7'\n",
      " 'grp_7' 'srp_7' 'pca_8' 'ica_8' 'tsvd_8' 'grp_8' 'srp_8' 'pca_9' 'ica_9'\n",
      " 'tsvd_9' 'grp_9' 'srp_9' 'pca_10' 'ica_10' 'tsvd_10' 'grp_10' 'srp_10'\n",
      " 'pca_11' 'ica_11' 'tsvd_11' 'grp_11' 'srp_11' 'pca_12' 'ica_12' 'tsvd_12'\n",
      " 'grp_12' 'srp_12' 'pca_13' 'ica_13' 'tsvd_13' 'grp_13' 'srp_13' 'pca_14'\n",
      " 'ica_14' 'tsvd_14' 'grp_14' 'srp_14' 'pca_15' 'ica_15' 'tsvd_15' 'grp_15'\n",
      " 'srp_15' 'pca_16' 'ica_16' 'tsvd_16' 'grp_16' 'srp_16' 'pca_17' 'ica_17'\n",
      " 'tsvd_17' 'grp_17' 'srp_17' 'pca_18' 'ica_18' 'tsvd_18' 'grp_18' 'srp_18'\n",
      " 'pca_19' 'ica_19' 'tsvd_19' 'grp_19' 'srp_19' 'pca_20' 'ica_20' 'tsvd_20'\n",
      " 'grp_20' 'srp_20' 'pca_21' 'ica_21' 'tsvd_21' 'grp_21' 'srp_21' 'pca_22'\n",
      " 'ica_22' 'tsvd_22' 'grp_22' 'srp_22' 'pca_23' 'ica_23' 'tsvd_23' 'grp_23'\n",
      " 'srp_23' 'pca_24' 'ica_24' 'tsvd_24' 'grp_24' 'srp_24' 'pca_25' 'ica_25'\n",
      " 'tsvd_25' 'grp_25' 'srp_25' 'pca_26' 'ica_26' 'tsvd_26' 'grp_26' 'srp_26'\n",
      " 'pca_27' 'ica_27' 'tsvd_27' 'grp_27' 'srp_27' 'pca_28' 'ica_28' 'tsvd_28'\n",
      " 'grp_28' 'srp_28' 'pca_29' 'ica_29' 'tsvd_29' 'grp_29' 'srp_29' 'pca_30'\n",
      " 'ica_30' 'tsvd_30' 'grp_30' 'srp_30' 'pca_31' 'ica_31' 'tsvd_31' 'grp_31'\n",
      " 'srp_31' 'pca_32' 'ica_32' 'tsvd_32' 'grp_32' 'srp_32' 'pca_33' 'ica_33'\n",
      " 'tsvd_33' 'grp_33' 'srp_33' 'pca_34' 'ica_34' 'tsvd_34' 'grp_34' 'srp_34'\n",
      " 'pca_35' 'ica_35' 'tsvd_35' 'grp_35' 'srp_35' 'pca_36' 'ica_36' 'tsvd_36'\n",
      " 'grp_36' 'srp_36' 'pca_37' 'ica_37' 'tsvd_37' 'grp_37' 'srp_37' 'pca_38'\n",
      " 'ica_38' 'tsvd_38' 'grp_38' 'srp_38' 'pca_39' 'ica_39' 'tsvd_39' 'grp_39'\n",
      " 'srp_39' 'pca_40' 'ica_40' 'tsvd_40' 'grp_40' 'srp_40' 'pca_41' 'ica_41'\n",
      " 'tsvd_41' 'grp_41' 'srp_41' 'pca_42' 'ica_42' 'tsvd_42' 'grp_42' 'srp_42'\n",
      " 'pca_43' 'ica_43' 'tsvd_43' 'grp_43' 'srp_43' 'pca_44' 'ica_44' 'tsvd_44'\n",
      " 'grp_44' 'srp_44' 'pca_45' 'ica_45' 'tsvd_45' 'grp_45' 'srp_45' 'pca_46'\n",
      " 'ica_46' 'tsvd_46' 'grp_46' 'srp_46' 'pca_47' 'ica_47' 'tsvd_47' 'grp_47'\n",
      " 'srp_47' 'pca_48' 'ica_48' 'tsvd_48' 'grp_48' 'srp_48' 'pca_49' 'ica_49'\n",
      " 'tsvd_49' 'grp_49' 'srp_49' 'kc_0' 'kc_1' 'kc_2' 'kc_3' 'kc_4' 'kc_5'\n",
      " 'kc_6' 'kc_7' 'kc_8' 'kc_9' 'kc_10' 'kc_11' 'kc_12' 'kc_13' 'kc_14'\n",
      " 'kc_15' 'kc_16' 'kc_17' 'kc_18' 'kc_19' 'kc_20' 'kc_21' 'kc_22' 'kc_23'\n",
      " 'kc_24' 'kc_25' 'kc_26' 'kc_27' 'kc_28' 'kc_29' 'kc_30' 'kc_31' 'kc_32'\n",
      " 'kc_33' 'kc_34' 'kc_35' 'kc_36' 'kc_37' 'kc_38' 'kc_39']\n"
     ]
    }
   ],
   "source": [
    "print(test.columns.shape, test.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_train = data.values\n",
    "data_test = test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Stage Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = data.loc[(target > 85) & (target < 90)]\n",
    "y = target[(target > 85) & (target < 90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 596)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=420)\n",
    "pca_results = pca.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=20)\n",
    "tsne_results = tsne.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "Initialization complete\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "Initialization complete\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 73487187.8089\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 80441016.894\n",
      "Iteration 0, inertia 80637926.9272\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 1, inertia 69622192.2467\n",
      "Iteration 0, inertia 76920537.6478\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 76236945.1972\n",
      "Iteration 1, inertia 73356560.1704\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 73183866.6972\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 2, inertia 68508408.0255\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 2, inertia 70516017.6391\n",
      "Iteration 2, inertia 74552632.5396\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 3, inertia 67925015.9013\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 72051127.9166\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 3, inertia 73422633.7103\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 3, inertia 68756502.4968\n",
      "Iteration 4, inertia 67480158.1662\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 3, inertia 71358828.583\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 67789488.3911\n",
      "Iteration 4, inertia 72221254.1234\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 67185561.7594\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 4, inertia 70912322.245\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 5, inertia 67240471.682\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 71356560.7073\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 5, inertia 70639421.5853\n",
      "Iteration 6, inertia 66900783.7984\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 66793939.3516\n",
      "Iteration 6, inertia 70694099.3752\n",
      "Iteration 6, inertia 70442660.6359\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 7, inertia 66653049.9639\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 70099090.913\n",
      "Iteration 7, inertia 70195900.807\n",
      "Iteration 7, inertia 66559300.4548\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 66436732.2453\n",
      "Iteration 8, inertia 69875818.1626\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 8, inertia 66432221.9018\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 69609074.4784\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 9, inertia 66268579.0746\n",
      "start iteration\n",
      "Iteration 9, inertia 69559120.1683\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 9, inertia 66356418.8908\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 69040834.3843\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 10, inertia 66143864.6534\n",
      "end inner loop\n",
      "Iteration 10, inertia 69271424.8721\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 10, inertia 66301137.1907\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 68577287.7302\n",
      "Iteration 11, inertia 69026198.4648\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 11, inertia 66003786.1838\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 11, inertia 66257907.5739\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 12, inertia 65859964.5932\n",
      "Iteration 11, inertia 68224199.9629\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 12, inertia 68854218.3174\n",
      "start iteration\n",
      "Iteration 12, inertia 66217919.554\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 12, inertia 67975472.2157\n",
      "Iteration 13, inertia 65744956.5594\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 13, inertia 68758103.354\n",
      "Iteration 13, inertia 66167743.3171\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 14, inertia 65642808.9269\n",
      "Iteration 13, inertia 67728421.7694\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 14, inertia 66121465.1933\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 14, inertia 68680127.2154\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 14, inertia 67417716.0781\n",
      "start iteration\n",
      "Iteration 15, inertia 68573097.1519\n",
      "done sorting\n",
      "Iteration 15, inertia 65581143.9857\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 15, inertia 66073578.1412\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 15, inertia 67068280.7087\n",
      "Iteration 16, inertia 68469643.0746\n",
      "Iteration 16, inertia 65538028.7595\n",
      "start iteration\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 16, inertia 66006118.9963\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 16, inertia 66799071.6375\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 17, inertia 65488209.8405\n",
      "start iteration\n",
      "Iteration 17, inertia 68373956.4824\n",
      "start iteration\n",
      "Iteration 17, inertia 65944226.1103\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 17, inertia 66623818.3443\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 18, inertia 65911080.4946\n",
      "start iteration\n",
      "Iteration 18, inertia 65430713.6174\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 18, inertia 68285948.0426\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 18, inertia 66489608.015\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 19, inertia 65371303.3389\n",
      "Iteration 19, inertia 68175650.7363\n",
      "start iteration\n",
      "Iteration 19, inertia 65868542.0902\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 19, inertia 66379856.3839\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 20, inertia 68054431.7233\n",
      "start iteration\n",
      "Iteration 20, inertia 65309214.8024\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 20, inertia 65826815.8505\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 20, inertia 66296092.8394\n",
      "end inner loop\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 21, inertia 67872736.9377\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 21, inertia 65786719.0549\n",
      "Iteration 21, inertia 65247573.2691\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 21, inertia 66214536.9269\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 22, inertia 67705582.2668\n",
      "Iteration 22, inertia 65710605.6594\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 22, inertia 66126870.685\n",
      "Iteration 22, inertia 65209803.5089\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 23, inertia 67503514.7746\n",
      "Iteration 23, inertia 65182159.9237\n",
      "Iteration 23, inertia 66034461.8791\n",
      "Iteration 23, inertia 65658491.1156\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 24, inertia 65135713.8923\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 24, inertia 65623425.6797\n",
      "Iteration 24, inertia 67278526.2204\n",
      "Iteration 24, inertia 65953750.7658\n",
      "start iteration\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 25, inertia 65109974.375\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 25, inertia 67030540.7763\n",
      "start iteration\n",
      "Iteration 25, inertia 65889121.1311\n",
      "done sorting\n",
      "Iteration 25, inertia 65579909.7065\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 26, inertia 65099006.5413\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 26, inertia 65821307.8687\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 26, inertia 66848934.9888\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 26, inertia 65538610.2849\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 27, inertia 65091157.1282\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 27, inertia 65767900.4886\n",
      "start iteration\n",
      "Iteration 27, inertia 66703560.4782\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 27, inertia 65504789.85\n",
      "Iteration 28, inertia 65081431.6671\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 28, inertia 65738146.8647\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 28, inertia 66561681.2063\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 28, inertia 65475040.1445\n",
      "Iteration 29, inertia 65068374.9546\n",
      "Iteration 29, inertia 65699382.5739\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 29, inertia 66427394.6078\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 30, inertia 65059191.135\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 30, inertia 65660708.0592\n",
      "start iteration\n",
      "Iteration 29, inertia 65448145.6857\n",
      "Iteration 30, inertia 66321504.3352\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 31, inertia 65050138.1014\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 31, inertia 65642379.8796\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 30, inertia 65426828.5999\n",
      "Iteration 31, inertia 66200777.0743\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 32, inertia 65042148.5849\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 31, inertia 65410040.4513\n",
      "Iteration 32, inertia 65630597.3885\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 32, inertia 66052298.4893\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 33, inertia 65032880.9957\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 32, inertia 65390944.5111\n",
      "start iteration\n",
      "Iteration 33, inertia 65601813.6777\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 33, inertia 65979586.6404\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 34, inertia 65020836.6051\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 33, inertia 65375167.5056\n",
      "Iteration 34, inertia 65568194.2268\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 34, inertia 65924129.6151\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 35, inertia 65006287.5647\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 35, inertia 65550344.2045\n",
      "start iteration\n",
      "Iteration 35, inertia 65860280.3141\n",
      "start iteration\n",
      "Iteration 34, inertia 65357578.3594\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 36, inertia 64993148.1051\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 35, inertia 65337679.0701\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 37, inertia 64980239.2574\n",
      "start iteration\n",
      "Iteration 36, inertia 65803945.872\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 36, inertia 65531298.1325\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 36, inertia 65326822.1082\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 38, inertia 64961843.966\n",
      "Iteration 37, inertia 65758216.8971\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 37, inertia 65514650.4048\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 37, inertia 65310525.7382\n",
      "start iteration\n",
      "Iteration 38, inertia 65720174.9423\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 39, inertia 64943690.6847\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 38, inertia 65500388.2415\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 38, inertia 65284665.6585\n",
      "Iteration 40, inertia 64935689.477\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 39, inertia 65682389.5799\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 39, inertia 65484098.5855\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 39, inertia 65248808.8449\n",
      "start iteration\n",
      "Iteration 41, inertia 64925488.3944\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 40, inertia 65646094.7458\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 40, inertia 65471264.9959\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 40, inertia 65226511.3827\n",
      "Iteration 42, inertia 64907944.2355\n",
      "start iteration\n",
      "Iteration 41, inertia 65605495.3431\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 41, inertia 65451889.4013\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 41, inertia 65212369.0938\n",
      "start iteration\n",
      "Iteration 43, inertia 64897924.3557\n",
      "done sorting\n",
      "Iteration 42, inertia 65543973.2588\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 42, inertia 65426067.4534\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 42, inertia 65197455.893\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 44, inertia 64890938.968\n",
      "Iteration 43, inertia 65486684.7339\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 43, inertia 65398018.1977\n",
      "Iteration 43, inertia 65184409.7407\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 45, inertia 64884583.9885\n",
      "Iteration 44, inertia 65421622.6481\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 44, inertia 65353374.7812\n",
      "Iteration 44, inertia 65166596.5039\n",
      "Iteration 45, inertia 65376340.3926\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 46, inertia 64879522.6064\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 45, inertia 65147113.5803\n",
      "Iteration 45, inertia 65284872.443\n",
      "start iteration\n",
      "Iteration 47, inertia 64874582.468\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 46, inertia 65335824.71\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 46, inertia 65222824.1718\n",
      "Iteration 46, inertia 65135695.4354\n",
      "Iteration 48, inertia 64867751.0276\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 47, inertia 65317901.2569\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 47, inertia 65116394.1459\n",
      "start iteration\n",
      "Iteration 47, inertia 65171206.6322\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 49, inertia 64862664.1831\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 48, inertia 65304578.7878\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 48, inertia 65092722.3584\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 50, inertia 64859370.5624\n",
      "Iteration 49, inertia 65288715.0902\n",
      "Iteration 48, inertia 65128014.3632\n",
      "start iteration\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 49, inertia 65059070.8718\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 51, inertia 64856893.7626\n",
      "Iteration 49, inertia 65077892.3653\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 50, inertia 65271673.8489\n",
      "start iteration\n",
      "Iteration 50, inertia 65036760.8475\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 50, inertia 65039683.735\n",
      "Iteration 52, inertia 64853464.2346\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 51, inertia 65255105.7273\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 51, inertia 65011404.4515\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 53, inertia 64852943.2133\n",
      "end inner loop\n",
      "Iteration 51, inertia 65003473.5877\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 52, inertia 65240090.5103\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 52, inertia 64994754.8555\n",
      "start iteration\n",
      "Iteration 52, inertia 64972311.0395\n",
      "start iteration\n",
      "Iteration 54, inertia 64852943.2133\n",
      "done sorting\n",
      "center shift 0.000000e+00 within tolerance 9.951913e-01\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 53, inertia 65231350.2842\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 53, inertia 64978948.0374\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 53, inertia 64944360.0175\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 54, inertia 65219152.9171\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 54, inertia 64969381.072\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 54, inertia 64916616.558\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 55, inertia 65198330.7725\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 55, inertia 64964007.8256\n",
      "start iteration\n",
      "Iteration 55, inertia 64891142.6485\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 56, inertia 65164766.0697\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 56, inertia 64962294.0828\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 56, inertia 64875887.8802\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 57, inertia 65136302.7187\n",
      "start iteration\n",
      "Initialization complete\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 57, inertia 64961816.1955\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 57, inertia 64870071.412\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 58, inertia 65110312.47\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 58, inertia 64860502.6259\n",
      "end inner loop\n",
      "Iteration 0, inertia 71826191.0473\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 58, inertia 64960237.0282\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 59, inertia 65100666.1493\n",
      "start iteration\n",
      "Iteration 1, inertia 69020148.9896\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 59, inertia 64839180.991\n",
      "end inner loop\n",
      "Iteration 59, inertia 64956761.8728\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 60, inertia 65100118.4131\n",
      "Iteration 2, inertia 67963979.8727\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 60, inertia 64949115.4212\n",
      "Iteration 60, inertia 64811675.8949\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 61, inertia 65098133.822\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 67370061.1434\n",
      "Iteration 61, inertia 64788274.9181\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 61, inertia 64940038.4547\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 62, inertia 65095908.7133\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 67003731.0334\n",
      "start iteration\n",
      "Iteration 62, inertia 64765420.54\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 62, inertia 64927009.578\n",
      "end inner loop\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 63, inertia 65092965.2673\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 5, inertia 66777290.6461\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 63, inertia 64757519.7356\n",
      "end inner loop\n",
      "Iteration 63, inertia 64905975.345\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 64, inertia 65088571.4402\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 66624052.7213\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 64, inertia 64754523.2102\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 64, inertia 64887129.9383\n",
      "Iteration 65, inertia 65080644.3998\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 66458296.3647\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 65, inertia 64749936.0962\n",
      "start iteration\n",
      "Iteration 66, inertia 65069460.6554\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 65, inertia 64855252.5267\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 66340772.34\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 67, inertia 65055900.1138\n",
      "end inner loop\n",
      "Iteration 66, inertia 64815737.33\n",
      "Iteration 66, inertia 64747609.5205\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 66246437.182\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 67, inertia 64796680.4962\n",
      "start iteration\n",
      "Iteration 67, inertia 64744136.9969\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 68, inertia 65050748.6257\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 66160104.0362\n",
      "start iteration\n",
      "Iteration 68, inertia 64740035.5105\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 68, inertia 64784831.4993\n",
      "end inner loop\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 69, inertia 65047507.6053\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 11, inertia 66087683.1003\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 69, inertia 64780079.4288\n",
      "Iteration 69, inertia 64727446.9644\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 70, inertia 65044408.9917\n",
      "end inner loop\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 12, inertia 66014698.2351\n",
      "start iteration\n",
      "Iteration 70, inertia 64777180.4978\n",
      "done sorting\n",
      "Iteration 70, inertia 64718349.4702\n",
      "Iteration 71, inertia 65041197.0885\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 13, inertia 65967679.824\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 71, inertia 64775548.5735\n",
      "start iteration\n",
      "Iteration 71, inertia 64715220.1127\n",
      "done sorting\n",
      "start iteration\n",
      "Iteration 72, inertia 65037050.8535\n",
      "end inner loop\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 14, inertia 65922415.9852\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 72, inertia 64773917.4626\n",
      "Iteration 73, inertia 65027213.0211\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 72, inertia 64712637.5808\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 15, inertia 65887209.024\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 74, inertia 65021451.84\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 73, inertia 64773917.4626\n",
      "Iteration 16, inertia 65858358.3293\n",
      "center shift 0.000000e+00 within tolerance 9.951913e-01\n",
      "start iteration\n",
      "Iteration 73, inertia 64711353.1564\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 75, inertia 65017287.846\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 17, inertia 65850469.8009\n",
      "Iteration 74, inertia 64711353.1564\n",
      "center shift 0.000000e+00 within tolerance 9.951913e-01\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 76, inertia 65015904.653\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 18, inertia 65835973.6981\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 77, inertia 65014130.0026\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 19, inertia 65817369.0225\n",
      "start iteration\n",
      "Iteration 78, inertia 65012045.1515\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Initialization complete\n",
      "start iteration\n",
      "Iteration 20, inertia 65794700.109\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 79, inertia 65009139.7346\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Initialization complete\n",
      "Iteration 0, inertia 72269918.0369\n",
      "Iteration 21, inertia 65766843.3877\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 80, inertia 65007518.1138\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 1, inertia 70588190.5154\n",
      "start iteration\n",
      "Iteration 22, inertia 65743974.2159\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 81, inertia 65007518.1138\n",
      "center shift 0.000000e+00 within tolerance 9.951913e-01\n",
      "Iteration 0, inertia 79120621.5372\n",
      "start iteration\n",
      "Iteration 2, inertia 69684165.5892\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 23, inertia 65718551.325\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 73997512.4065\n",
      "start iteration\n",
      "Iteration 3, inertia 69094883.0744\n",
      "done sorting\n",
      "Iteration 24, inertia 65684894.7221\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 2, inertia 72205890.7226\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 68497516.971\n",
      "start iteration\n",
      "Iteration 25, inertia 65632589.174\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 70863651.0755\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 68022744.0009\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 26, inertia 65540109.7536\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 69995146.1368\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Initialization complete\n",
      "Iteration 6, inertia 67598911.0943\n",
      "Iteration 27, inertia 65453988.6158\n",
      "start iteration\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 5, inertia 69487103.9763\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 69725468.3426\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 28, inertia 65378440.5162\n",
      "Iteration 7, inertia 67185229.6131\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 69063699.6551\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 67186357.6871\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 8, inertia 66839219.2439\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 29, inertia 65328652.4811\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 7, inertia 68706623.3379\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 2, inertia 66466535.1359\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 66536152.4147\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 30, inertia 65285253.1358\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 68320092.0997\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 3, inertia 66094094.8063\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 10, inertia 66307279.9198\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 67977705.784\n",
      "Iteration 31, inertia 65239888.4905\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 4, inertia 65902540.3963\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 11, inertia 66090286.3248\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 32, inertia 65196937.8246\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 65806732.0479\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 67690029.1222\n",
      "start iteration\n",
      "Iteration 12, inertia 65918234.4922\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 33, inertia 65156751.9926\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 65700311.5764\n",
      "start iteration\n",
      "Iteration 11, inertia 67455671.2595\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 13, inertia 65754730.5782\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 34, inertia 65122660.5326\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 65603868.373\n",
      "start iteration\n",
      "Iteration 12, inertia 67257310.5264\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 14, inertia 65601304.9707\n",
      "Iteration 35, inertia 65094986.5566\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 13, inertia 67125370.0864\n",
      "Iteration 8, inertia 65546861.1929\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 15, inertia 65475035.811\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 36, inertia 65069036.0922\n",
      "end inner loop\n",
      "Iteration 9, inertia 65507513.2431\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 14, inertia 67047854.2991\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 16, inertia 65371887.6968\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 37, inertia 65041444.358\n",
      "Iteration 15, inertia 66965909.9969\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 65477456.8849\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 17, inertia 65302299.4698\n",
      "Iteration 16, inertia 66900212.5392\n",
      "start iteration\n",
      "Iteration 38, inertia 65013198.8514\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 39, inertia 64989444.613\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 11, inertia 65435208.6618\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 17, inertia 66831299.2609\n",
      "Iteration 18, inertia 65259810.096\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 12, inertia 65409621.0569\n",
      "Iteration 19, inertia 65230183.7483\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 40, inertia 64965375.4623\n",
      "start iteration\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 18, inertia 66725968.9684\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 13, inertia 65391197.1796\n",
      "start iteration\n",
      "Iteration 20, inertia 65207541.8053\n",
      "start iteration\n",
      "Iteration 41, inertia 64957092.6661\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 19, inertia 66564984.9783\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 14, inertia 65376705.9276\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 42, inertia 64950107.5172\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 20, inertia 66393914.4043\n",
      "Iteration 21, inertia 65194676.2269\n",
      "done sorting\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 22, inertia 65183336.1685\n",
      "Iteration 15, inertia 65363080.2441\n",
      "start iteration\n",
      "Iteration 21, inertia 66215431.0299\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 43, inertia 64937604.6256\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 23, inertia 65167496.4055\n",
      "Iteration 44, inertia 64918298.1868\n",
      "start iteration\n",
      "Iteration 16, inertia 65350108.6469\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 22, inertia 66110308.7079\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 24, inertia 65161103.6612\n",
      "start iteration\n",
      "Iteration 45, inertia 64891815.5528\n",
      "done sorting\n",
      "Iteration 17, inertia 65336172.618\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 23, inertia 66039466.5565\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 18, inertia 65327789.1531\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 25, inertia 65152387.3811\n",
      "Iteration 46, inertia 64848934.498\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 24, inertia 65984792.4768\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 26, inertia 65134338.0735\n",
      "start iteration\n",
      "Iteration 19, inertia 65319346.928\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 47, inertia 64807479.563\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "end inner loop\n",
      "Iteration 25, inertia 65926597.1276\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 48, inertia 64773327.507\n",
      "Iteration 20, inertia 65312438.8408\n",
      "start iteration\n",
      "start iteration\n",
      "Iteration 27, inertia 65117090.1935\n",
      "Iteration 26, inertia 65875673.4699\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 28, inertia 65103123.0048\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 21, inertia 65306692.943\n",
      "Iteration 49, inertia 64741713.6163\n",
      "Iteration 27, inertia 65815322.0352\n",
      "end inner loop\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 28, inertia 65775193.6374\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 29, inertia 65092967.5348\n",
      "Iteration 22, inertia 65299072.5811\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 50, inertia 64701624.1452\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 29, inertia 65737163.6044\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 51, inertia 64650911.9153\n",
      "Iteration 30, inertia 65083491.9571\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 23, inertia 65295892.1646\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 30, inertia 65679049.8909\n",
      "start iteration\n",
      "Iteration 31, inertia 65078390.2316\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 52, inertia 64609886.74\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 24, inertia 65292905.9169\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 32, inertia 65072310.6025\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 31, inertia 65628693.2658\n",
      "start iteration\n",
      "Iteration 53, inertia 64573272.7616\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 25, inertia 65289388.2916\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 33, inertia 65068552.5954\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 32, inertia 65602329.5461\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 54, inertia 64525503.2329\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 26, inertia 65285755.1316\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 34, inertia 65063242.7613\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 27, inertia 65283251.6095\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 33, inertia 65579961.329\n",
      "start iteration\n",
      "Iteration 55, inertia 64487008.9245\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 28, inertia 65279674.1546\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 35, inertia 65062762.1447\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 34, inertia 65557806.7965\n",
      "Iteration 56, inertia 64459682.1324\n",
      "start iteration\n",
      "Iteration 29, inertia 65279674.1546\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "center shift 0.000000e+00 within tolerance 9.951913e-01\n",
      "Iteration 36, inertia 65062762.1447\n",
      "end inner loop\n",
      "center shift 0.000000e+00 within tolerance 9.951913e-01\n",
      "Iteration 57, inertia 64424231.3672\n",
      "Iteration 35, inertia 65517733.9432\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 58, inertia 64389530.9146\n",
      "Iteration 36, inertia 65489132.2822\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 37, inertia 65480947.8509\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 59, inertia 64373563.6811\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 60, inertia 64371147.9875\n",
      "start iteration\n",
      "Iteration 38, inertia 65471646.3701\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Initialization complete\n",
      "Initialization complete\n",
      "Iteration 39, inertia 65460283.4705\n",
      "Iteration 61, inertia 64371147.9875\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "center shift 0.000000e+00 within tolerance 9.951913e-01\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 0, inertia 74742509.7551\n",
      "Iteration 0, inertia 72512706.0638\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "Iteration 40, inertia 65446267.2821\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 1, inertia 70501424.1556\n",
      "Iteration 41, inertia 65420466.616\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 69527353.5985\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 69290419.4422\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 42, inertia 65394696.339\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 2, inertia 68393552.4946\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 68804076.7104\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 43, inertia 65373813.7742\n",
      "end inner loop\n",
      "Iteration 3, inertia 67756542.5147\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 4, inertia 68436049.1525\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 4, inertia 67427285.7782\n",
      "Iteration 44, inertia 65351326.4752\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 68238979.3826\n",
      "Iteration 45, inertia 65314351.5042\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 67264491.3944\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 68087460.3466\n",
      "start iteration\n",
      "Iteration 46, inertia 65274740.5259\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 6, inertia 67144792.0248\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 7, inertia 67063770.4415\n",
      "Iteration 7, inertia 67891485.8478\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 47, inertia 65212424.0817\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 67762446.319\n",
      "start iteration\n",
      "Iteration 8, inertia 67002388.6334\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 48, inertia 65177907.0659\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 49, inertia 65143321.3966\n",
      "Iteration 9, inertia 66945256.3237\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 9, inertia 67641758.9094\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 50, inertia 65111683.6512\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 10, inertia 66882583.1286\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 10, inertia 67528582.354\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 51, inertia 65080949.1025\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 11, inertia 66854030.3275\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 11, inertia 67403203.571\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 52, inertia 65048358.4239\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 12, inertia 66841520.4913\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "Iteration 12, inertia 67291348.5174\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 53, inertia 64994148.8803\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 13, inertia 66824055.0368\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 13, inertia 67222996.0097\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 54, inertia 64960021.2817\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 14, inertia 66806590.1866\n",
      "start iteration\n",
      "Iteration 14, inertia 67159226.6742\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 55, inertia 64928906.4798\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 15, inertia 66793325.5216\n",
      "Iteration 15, inertia 67113374.2205\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 56, inertia 64905577.4298\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 16, inertia 67076952.4484\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 16, inertia 66782751.8819\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 57, inertia 64887654.1704\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 17, inertia 67033691.4635\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 17, inertia 66765300.9404\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 58, inertia 64878780.7042\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 18, inertia 67011642.7719\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 18, inertia 66746904.3429\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 59, inertia 64864933.857\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 19, inertia 66997941.9162\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 19, inertia 66733137.5282\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 60, inertia 64840844.4803\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 20, inertia 66987272.0959\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 20, inertia 66723569.3294\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 61, inertia 64810735.8214\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 21, inertia 66976234.259\n",
      "end inner loop\n",
      "start iteration\n",
      "Iteration 21, inertia 66719767.0994\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 62, inertia 64785717.5429\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 22, inertia 66966858.2072\n",
      "start iteration\n",
      "Iteration 22, inertia 66717768.9249\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 63, inertia 64764263.7796\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 23, inertia 66955575.7585\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 23, inertia 66717193.1804\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 64, inertia 64749829.5237\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 24, inertia 66941603.5177\n",
      "Iteration 24, inertia 66716340.8868\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 65, inertia 64746337.7214\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 25, inertia 66925720.3312\n",
      "Iteration 25, inertia 66715556.8885\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 66, inertia 64743715.3852\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 26, inertia 66713636.8722\n",
      "Iteration 26, inertia 66907140.96\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 67, inertia 64739831.8994\n",
      "Iteration 27, inertia 66885654.1056\n",
      "start iteration\n",
      "Iteration 27, inertia 66707040.7209\n",
      "done sorting\n",
      "start iteration\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 68, inertia 64736379.8091\n",
      "Iteration 28, inertia 66701585.0112\n",
      "start iteration\n",
      "start iteration\n",
      "done sorting\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 28, inertia 66869617.5485\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 69, inertia 64732329.8149\n",
      "Iteration 29, inertia 66691848.2647\n",
      "start iteration\n",
      "Iteration 29, inertia 66865878.6379\n",
      "done sorting\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 70, inertia 64723407.9838\n",
      "Iteration 30, inertia 66865328.3442\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 30, inertia 66680231.2588\n",
      "start iteration\n",
      "done sorting\n",
      "start iteration\n",
      "end inner loop\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 71, inertia 64708904.5625\n",
      "start iteration\n",
      "Iteration 31, inertia 66865328.3442\n",
      "Iteration 31, inertia 66673158.472\n",
      "done sorting\n",
      "start iteration\n",
      "center shift 0.000000e+00 within tolerance 9.951913e-01\n",
      "done sorting\n",
      "end inner loop\n",
      "end inner loop\n",
      "Iteration 72, inertia 64705775.205\n",
      "Iteration 32, inertia 66668119.4548\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 33, inertia 66662191.5848\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 73, inertia 64703192.6731\n",
      "end inner loop\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 34, inertia 66656146.3426\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 74, inertia 64701908.2487\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 35, inertia 66650015.3761\n",
      "start iteration\n",
      "done sorting\n",
      "Iteration 75, inertia 64701908.2487\n",
      "end inner loop\n",
      "center shift 0.000000e+00 within tolerance 9.951913e-01\n",
      "Iteration 36, inertia 66645230.0923\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 37, inertia 66637413.0227\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 38, inertia 66622250.7372\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 39, inertia 66601142.706\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 40, inertia 66592189.4362\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 41, inertia 66582950.9941\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 42, inertia 66576479.0776\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 43, inertia 66572301.61\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 44, inertia 66569136.8558\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 45, inertia 66567080.3076\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 46, inertia 66561512.3165\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 47, inertia 66557868.0431\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 48, inertia 66556356.9275\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 49, inertia 66555801.6386\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 50, inertia 66553989.9231\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 51, inertia 66551417.3952\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 52, inertia 66549783.8825\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 53, inertia 66549783.8825\n",
      "center shift 0.000000e+00 within tolerance 9.951913e-01\n"
     ]
    }
   ],
   "source": [
    "# Fit KMeans Cluster model - \n",
    "cluster = KMeans(n_clusters=20, init='k-means++', max_iter=1000, n_init=10,\n",
    "                        verbose=1, n_jobs=-1)\n",
    "kc = cluster.fit_predict(pd.concat([data, target], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-27bfe9fd08c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#sns.lmplot(x=pca_results[:,0], y=y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "#sns.lmplot(x=pca_results[:,0], y=y)\n",
    "pyplot.scatter(kc[:100,10], target[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11da75518>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD3CAYAAADxJYRbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE+5JREFUeJzt3X+MH3ldx/HntnttrdnWNX6xMUHRKO80moI5AsjRu+Y4\nrtRfVWKUGNDzYqlaLSIBjmuRaHo5UDykKCILtUBCNBYxpkkFw93VUsUeeCSi5X1WVExEs+K2Xahd\n6Hb9Y2Z16X1397uzs7vT+zwfSZP5fub7ne9rZ6ev7+zM9/udoZmZGSRJZVi31gEkSavH0pekglj6\nklQQS1+SCmLpS1JBhtfyycfHJ1t969Do6GYmJq60ucjWdDkbdDuf2Zrrcr4uZ4Nu5+v1RoaaPvYp\ntac/PLx+rSPMq8vZoNv5zNZcl/N1ORt0P19TT6nSlyQtzNKXpIJY+pJUEEtfkgpi6UtSQSx9SSqI\npS9JBbH0Jakglr4kFWRNv4ah6+5988N9x4/dd+cqJ5GkdrinL0kFsfQlqSCWviQVxNKXpIIseiI3\nIm4B3gc8A5gG9gHXgOPADPAZ4EBmXo+IfcD+ev6RzDy5MrElSU0Msqf//cBwZr4A+HXgAeAh4HBm\n7gSGgL0RsQ04CNwG7AYejIiNKxNbktTEIG/ZfAIYjoh1wBbgq8DzgdP1/FPA3VR/BZzNzClgKiIu\nADuAx+Zb8Ojo5tYvVNDrjbS6vDafYzWyLUeX85mtuS7n63I26H6+JgYp/S9RHdr5LPBNwA8Ct2fm\n7KUOJ4GtVC8Il+Y8bnZ8Xm1fiqzXG2F8fLLVZfbT5DlWK1tTXc5ntua6nK/L2aDb+ZbzYjTI4Z1X\nAx/JzGcCz6I6vr9hzvwR4CJwuZ6+cVyS1BGDlP4E/78H/9/ALcDjEbGrHtsDnAHOATsjYlNEbAW2\nU53klSR1xCCHd94GHIuIM1R7+PcDnwTGImIDcB44kZnTEXGU6gVgHXAoM6+uUG5JUgOLln5mfgn4\n8T6z7uhz3zFgrIVckqQV4IezJKkglr4kFcTSl6SCWPqSVBBLX5IKYulLUkEsfUkqiKUvSQWx9CWp\nIJa+JBXE0pekglj6klQQS1+SCmLpS1JBLH1JKoilL0kFsfQlqSCLXjkrIu4B7qlvbgKeDbwQ+G1g\nhuo6uAcy83pE7AP2A9eAI5l5cgUyS5IaWnRPPzOPZ+auzNwFfAo4CPwqcDgzdwJDwN6I2FbPuw3Y\nDTwYERtXLLkkackGuTA6ABHxHOC7M/NARLwJOF3POgXcDUwDZzNzCpiKiAvADuCx+ZY5OrqZ4eH1\njcP30+uNtLq8Np9jNbItR5fzma25Lufrcjbofr4mBi594H7g1+rpocycqacnga3AFuDSnPvPjs9r\nYuLKEp5+cb3eCOPjk60us58mz7Fa2Zrqcj6zNdflfF3OBt3Ot5wXo4FO5EbENwCRmY/UQ9fnzB4B\nLgKX6+kbxyVJHTHou3duBz425/bjEbGrnt4DnAHOATsjYlNEbAW2U53klSR1xKCHdwL43JzbrwHG\nImIDcB44kZnTEXGU6gVgHXAoM6+2mlaStCwDlX5m/uYNt58A7uhzvzFgrJ1okqS2+eEsSSqIpS9J\nBbH0Jakglr4kFcTSl6SCWPqSVBBLX5IKYulLUkEsfUkqiKUvSQWx9CWpIJa+JBXE0pekglj6klQQ\nS1+SCmLpS1JBLH1JKshAV86KiDcAPwxsAN4JnAaOAzNU18E9kJnXI2IfsB+4BhzJzJMrEVqS1Myi\ne/r1BdBfANxGdYnEpwMPAYczcycwBOyNiG3Awfp+u4EHI2LjCuWWJDUwyJ7+buDvgA8DW4DXAvuo\n9vYBTgF3A9PA2cycAqYi4gKwA3hsvgWPjm5meHh98/R99HojrS6vzedYjWzL0eV8Zmuuy/m6nA26\nn6+JQUr/m4BvA34Q+Hbgz4B1mTlTz58EtlK9IFya87jZ8XlNTFxZat4F9XojjI9PtrrMfpo8x2pl\na6rL+czWXJfzdTkbdDvfcl6MBin9LwKfzcyvABkRV6kO8cwaAS4Cl+vpG8clSR0xyLt3Pg68JCKG\nIuJbgK8HPlYf6wfYA5wBzgE7I2JTRGwFtlOd5JUkdcSie/qZeTIibqcq9XXAAeCfgbGI2ACcB05k\n5nREHKV6AVgHHMrMqysXXZK0VAO9ZTMzX9dn+I4+9xsDxpYbSpK0MvxwliQVxNKXpIJY+pJUEEtf\nkgpi6UtSQSx9SSqIpS9JBbH0JakgA304S1/r3jc/3Hf82H133hTLl1Qu9/QlqSCWviQVxNKXpIJY\n+pJUEEtfkgpi6UtSQSx9SSqIpS9JBRnow1kR8bdUFz6H6lKJDwDHgRmq6+AeyMzrEbEP2A9cA45k\n5snWE0uSGlu09CNiEzCUmbvmjP0ZcDgzH42IdwF7I+KvgYPAc4BNwMcj4i8yc2plokuSlmqQPf1n\nAZsj4qP1/e8HbgVO1/NPAXcD08DZuuSnIuICsAN4bL4Fj45uZnh4/TLiP1mvN9Lq8tp87uVmW+mf\nbS3X3WLM1lyX83U5G3Q/XxODlP4V4K3Ae4Dvoir5ocycqedPAluBLcClOY+bHZ/XxMSVpeZdUK83\nwvj4ZKvLXIqFnruNbCv5s631uluI2Zrrcr4uZ4Nu51vOi9Egpf8EcKEu+Sci4otUe/qzRoCLVMf8\nR/qMS5I6YpB379wL/BZARHwL1R79RyNiVz1/D3AGOAfsjIhNEbEV2E51kleS1BGD7Om/FzgeER+n\nerfOvcB/AWMRsQE4D5zIzOmIOEr1ArAOOJSZV1cotySpgUVLPzO/Avxkn1l39LnvGDDWQi5J0grw\nw1mSVBBLX5IKYulLUkEsfUkqiKUvSQWx9CWpIJa+JBXE0pekglj6klQQS1+SCmLpS1JBLH1JKoil\nL0kFsfQlqSCWviQVxNKXpIIMcuUsIuJpwKeAFwPXgONUV9H6DHAgM69HxD5gfz3/SGaeXJHEkqTG\nFt3Tj4hbgN8H/qceegg4nJk7gSFgb0RsAw4CtwG7gQcjYuPKRJYkNTXI4Z23Au8C/r2+fStwup4+\nBdwFPBc4m5lTmXkJuADsaDmrJGmZFjy8ExH3AOOZ+ZGIeEM9PJSZM/X0JLAV2AJcmvPQ2fEFjY5u\nZnh4/ZJDL6TXG2l1eW0+93KzrfTPtpbrbjFma67L+bqcDbqfr4nFjunfC8xExF3As4H3A0+bM38E\nuAhcrqdvHF/QxMSVJYVdTK83wvj4ZKvLXIqFnruNbCv5s631uluI2Zrrcr4uZ4Nu51vOi9GCpZ+Z\nt89OR8SjwM8BvxkRuzLzUWAP8AhwDnggIjYBG4HtVCd5JUkdMtC7d27wGmAsIjYA54ETmTkdEUeB\nM1TnCQ5l5tUWc94U7n3zw33Hj9135yonkaT+Bi79zNw15+YdfeaPAWMtZJIkrRA/nCVJBbH0Jakg\nTY7pa4k81i+pK9zTl6SCWPqSVBAP76yh+Q77LPX+HiaSNCj39CWpIJa+JBXE0pekglj6klQQS1+S\nCmLpS1JBLH1JKoilL0kFsfQlqSCWviQVxNKXpIIs+t07EbGe6opYAcxQXSf3KnC8vv0Z4EBmXo+I\nfcB+4BpwJDNPrlBuSVIDg+zp/xBAZt4GHAYeAB4CDmfmTmAI2BsR24CDwG3AbuDBiNi4IqklSY0s\nWvqZ+afAK+ub3wZcBG4FTtdjp4C7gOcCZzNzKjMvAReAHa0nliQ1NtBXK2fmtYh4H/CjwI8BL87M\nmXr2JLAV2AJcmvOw2fF5jY5uZnh4/ZJDL6TXG2l1eTeDtn7mLq87szXX5Xxdzgbdz9fEwN+nn5k/\nHRGvB/4G+Lo5s0ao9v4v19M3js9rYuLK4EkH0OuNMD4+2eoybwZt/MxdXndma67L+bqcDbqdbzkv\nRose3omIV0TEG+qbV4DrwCcjYlc9tgc4A5wDdkbEpojYCmynOskrSeqIQfb0/wT4g4j4S+AW4JeB\n88BYRGyop09k5nREHKV6AVgHHMrMqyuUW5LUwKKln5lfBn68z6w7+tx3jOrtnZKkDvLDWZJUEEtf\nkgpi6UtSQSx9SSqIpS9JBbH0Jakglr4kFcTSl6SCWPqSVBBLX5IKYulLUkEsfUkqiKUvSQWx9CWp\nIJa+JBXE0pekglj6klSQBa+cFRG3AMeAZwAbgSPAPwDHgRmqa+AeyMzrEbEP2A9cA45k5smViy1J\namKxPf2XA1/MzJ3AS4DfAR4CDtdjQ8DeiNgGHARuA3YDD0bExpWLLUlqYrFr5P4xcKKeHqLai78V\nOF2PnQLuBqaBs5k5BUxFxAVgB/BY64klSY0tWPqZ+SWAiBihKv/DwFszc6a+yySwFdgCXJrz0Nnx\nBY2ObmZ4eH2D2PPr9UZaXd7NoK2fucvrzmzNdTlfl7NB9/M1sdiePhHxdODDwDsz84MR8RtzZo8A\nF4HL9fSN4wuamLiytLSL6PVGGB+fbHWZN4M2fuYurzuzNdflfF3OBt3Ot5wXo8VO5H4z8FHgFzPz\nY/Xw4xGxKzMfBfYAjwDngAciYhPVCd/tVCd5tQruffPDfceP3XfnKieR1HWL7enfD4wCb4yIN9Zj\nrwKORsQG4DxwIjOnI+IocIbq5PChzLy6UqElSc0sdkz/VVQlf6M7+tx3DBhrKZckaQUseky/BPMd\nHpGkpxo/kStJBbH0Jakglr4kFcTSl6SCWPqSVBBLX5IKYulLUkEsfUkqiKUvSQWx9CWpIJa+JBXE\n0pekglj6klQQS1+SCmLpS1JBLH1JKshAF1GJiOcBb8nMXRHxncBxYIbqOrgHMvN6ROwD9gPXgCOZ\neXKFMkuSGlp0Tz8iXge8B9hUDz0EHM7MncAQsDcitgEHgduA3cCDEbFxZSJLkpoaZE//n4CXAh+o\nb98KnK6nTwF3A9PA2cycAqYi4gKwA3is3bhaivkuA3nsvjtXOYmkrli09DPzQxHxjDlDQ5k5U09P\nAluBLcClOfeZHV/Q6OhmhofXD552AL3eSKvLeyqabx11ed2Zrbku5+tyNuh+viaaXBj9+pzpEeAi\ncLmevnF8QRMTVxo8/fx6vRHGxydbXeZTUb911OV1Z7bmupyvy9mg2/mW82LU5N07j0fErnp6D3AG\nOAfsjIhNEbEV2E51kleS1CFN9vRfA4xFxAbgPHAiM6cj4ijVC8A64FBmXm0xpySpBQOVfmb+C/D8\nevoJ4I4+9xkDxtoMJ0lqlx/OkqSCWPqSVJAmx/Q7wfegS9LS3bSlPx9fDCRpfh7ekaSCWPqSVBBL\nX5IKYulLUkGecidytbj5TnaDJ7ylpzr39CWpIO7p62v4llfpqc09fUkqiKUvSQUp5vDOQicvJakU\n7ulLUkGK2dPX8iz1LyVP/ErdZOlrRfguIKmbWi39iFgHvBN4FjAF/GxmXmjzOSRJzbV9TP9HgE2Z\n+X3AfcBvtbx8SdIytH1454XAnwNk5ici4jktL183ubV6F9V8h5U8DKXSDM3MzLS2sIh4D/ChzDxV\n3/488B2Zea21J5EkNdb24Z3LwMjc5Vv4ktQdbZf+WeD7ASLi+cDftbx8SdIytH1M/8PAiyPir4Ah\n4GdaXr4kaRlaPaYvSeo2v4ZBkgpi6UtSQSx9SSrITfvdOxFxD3BPfXMT8Gzg+4CTwD/W47+XmX+0\nyrmeB7wlM3dFxHcCx4EZ4DPAgcy8HhH7gP3ANeBIZp5cg2zPBt4BTFN9ZcZPZeZ/RsTbqT5kN1k/\nbG9mXlqDfN9Ln99lR9bdHwLb6lnPAD6RmS9bi3UXEbcAx+ocG4EjwD/Qge1unmyfpwPb3TzZ/o2O\nbHPz5PtJWtjubtrSz8zjVBs2EfG7VCvoVuChzFyTr3+IiNcBrwC+XA89BBzOzEcj4l3A3oj4a+Ag\n8ByqF6uPR8RfZObUKmd7O/BLmfnpiNgPvB74Fap1uDsz/2sl8wyQ70m/y4jYRgfWXWa+rB4fBR4B\nXj0n82qvu5cDX8zMV0TENwKfrv91Ybvrl+2f6cZ21y/br9ORba5fvsz81jrTsra7m/7wTv1VD9+d\nme+m+uF/ICL+MiLeGxEjizy8bf8EvHTO7VuB0/X0KeAu4LnA2cycql+NLwA71iDbyzLz0/X0MHC1\n/sK87wLeHRFnI+LeVcg1X75+v8uurLtZvwa8IzO/sIbr7o+BN9bTQ1R7o13Z7vpl68p2N99668o2\n1y/frGVtdzd96QP3U60EgHPAazPzduBzwJtWM0hmfgj46pyhocycfU/sJLAV2ALM/dNrdnxVs2Xm\nFwAi4gXALwJvA76e6k/vlwMvAX4hIlZjA++37vr9Ljux7gAi4mnAi6j/2mSN1l1mfikzJ+uCOgEc\npiPbXb9sXdnu5llvXdrm+uVrZbu7qUs/Ir4BiMx8pB76cGZ+anYa+N61SfZ/rs+ZHgEu8uSvqpgd\nX3UR8RPAu4AfyMxx4Arw9sy8kpmTwMNUX5O9Fvr9Ljuz7oAfAz6YmdP17TVbdxHxdKo/9z+QmR+k\nQ9tdn2yd2e76ZOvUNtdv3dHCdndTlz5wO/CxObc/EhHPradfBHzqyQ9ZVY9HxK56eg9whmpvYmdE\nbIqIrcB2qpNtqyoiXk61p7UrMz9XDz8TOBsR6+sTSS8E/na1s9X6/S47se5qd1EdOpm1JusuIr4Z\n+Cjw+sw8Vg93Yrvrl60r2908660z29w8+aCF7e6mPZFbC6o/w2b9PPCOiPgq8B/AK9ck1f97DTAW\nERuA88CJzJyOiKNU/xHXAYcy8+pqhoqI9cBRqndS/ElEAJzOzDdFxAeAT1Adznh/Zv79amab40m/\ny8y8vNbrbo6v2fYy8/warbv7gVHgjRExewz4VcDRDmx3N2ZbD3wP8K+s/XbXb739CvC2jmxz/fLt\noYXtzq9hkKSC3OyHdyRJS2DpS1JBLH1JKoilL0kFsfQlqSCWviQVxNKXpIL8LyD4Pzo7DFM7AAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d7bd128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "target.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAGDCAYAAADtZ0xmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c9e60b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_order = list(range(20))\n",
    "plt.figure(figsize=(12,6))\n",
    "clusters = pd.DataFrame({'cluster': kc, 'y': target})\n",
    "sns.boxplot(x='cluster', y='y', data=clusters, order=col_order)\n",
    "plt.xlabel('cluster', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title(\"Distribution of y variable with cluster\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence after  11  iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
       "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
       "       normalize=False, tol=0.001, verbose=2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = LinearRegression()\n",
    "model = BayesianRidge(n_iter=300, verbose=2)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49833506578216835"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_val, model.predict(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "Multiple eval metrics have been passed: 'validation_1-r2' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-r2 hasn't improved in 200 rounds.\n",
      "[1]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[2]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[3]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[4]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[5]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[6]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[7]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[8]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[9]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[10]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[11]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[12]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[13]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[14]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[15]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[16]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[17]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[18]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[19]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[20]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[21]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[22]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[23]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[24]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[25]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[26]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[27]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[28]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[29]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[30]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[31]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[32]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[33]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[34]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[35]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[36]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[37]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[38]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[39]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[40]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[41]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[42]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[43]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[44]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[45]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[46]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[47]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[48]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[49]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[50]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[51]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[52]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[53]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[54]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[55]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[56]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[57]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[58]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[59]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[60]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[61]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[62]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[63]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[64]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[65]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[66]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[67]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[68]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[69]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[70]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[71]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[72]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[73]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[74]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[75]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[76]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[77]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[78]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[79]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[80]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[81]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[82]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[83]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[84]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[85]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[86]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[87]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[88]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[89]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[90]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[91]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[92]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[93]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[94]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[95]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[96]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[97]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[98]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[99]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[100]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[101]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[102]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[103]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[104]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[105]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[106]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[107]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[108]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[109]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[110]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[111]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[112]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[113]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[114]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[115]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[116]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[117]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[118]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[119]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[120]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[121]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[122]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[123]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[124]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[125]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[126]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[127]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[128]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[129]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[130]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[131]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[132]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[133]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[134]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[135]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[136]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[137]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[138]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[139]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[140]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[141]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[142]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[143]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[144]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[145]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[146]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[147]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[148]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[149]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[150]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[151]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[152]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[153]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[154]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[155]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[156]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[157]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[158]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[159]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[160]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[161]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[162]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[163]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[164]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[165]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[166]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[167]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[168]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[169]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[170]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[171]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[172]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[173]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[174]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[175]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[176]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[177]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[178]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[179]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[180]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[181]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[182]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[183]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[184]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[185]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[186]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[187]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[188]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[189]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[190]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[191]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[192]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[193]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[194]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[195]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[196]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[197]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[198]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[199]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "[200]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "Stopping. Best iteration:\n",
      "[0]\tvalidation_0-rmse:1.13548\tvalidation_1-rmse:1.1506\tvalidation_0-r2:0.024964\tvalidation_1-r2:0.001462\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=88.331581325301215, booster='gbtree',\n",
       "       colsample_bylevel=1, colsample_bytree=0.8, gamma=0,\n",
       "       learning_rate=0.03, max_delta_step=0, max_depth=4,\n",
       "       min_child_weight=5, missing=None, n_estimators=1000, n_jobs=1,\n",
       "       nthread=1, objective='reg:linear', random_state=420, reg_alpha=50,\n",
       "       reg_lambda=100, scale_pos_weight=1, seed=420, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = XGBRegressor(learning_rate=0.03, max_depth=4,\n",
    "                         subsample=1, colsample_bytree=0.8, min_child_weight=5,\n",
    "                         reg_alpha=50, reg_lambda=100, \n",
    "                         n_estimators=1000, base_score=y.mean(), seed=420)\n",
    "model_xgb.fit(x_train, y_train, eval_metric=r2_eval_xgb, \n",
    "          eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "         early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "ceda23fdf7022688f4d27298d11cb5137f8e01f9",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5.0 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def r2_eval_lgb(Y_pred, Y_true):\n",
    "    return 'r2', r2_score(Y_true, Y_pred), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's r2: -3393.02\tvalid_1's r2: -3053\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[2]\ttraining's r2: -831.316\tvalid_1's r2: -755.664\n",
      "[3]\ttraining's r2: -359.243\tvalid_1's r2: -325.684\n",
      "[4]\ttraining's r2: -199.507\tvalid_1's r2: -179.615\n",
      "[5]\ttraining's r2: -127.566\tvalid_1's r2: -114.301\n",
      "[6]\ttraining's r2: -87.1468\tvalid_1's r2: -77.8517\n",
      "[7]\ttraining's r2: -63.636\tvalid_1's r2: -56.4979\n",
      "[8]\ttraining's r2: -48.7139\tvalid_1's r2: -43.1658\n",
      "[9]\ttraining's r2: -38.0585\tvalid_1's r2: -33.5821\n",
      "[10]\ttraining's r2: -30.8822\tvalid_1's r2: -27.2277\n",
      "[11]\ttraining's r2: -25.4398\tvalid_1's r2: -22.3718\n",
      "[12]\ttraining's r2: -21.2366\tvalid_1's r2: -18.6668\n",
      "[13]\ttraining's r2: -17.96\tvalid_1's r2: -15.6961\n",
      "[14]\ttraining's r2: -15.3619\tvalid_1's r2: -13.3869\n",
      "[15]\ttraining's r2: -13.3278\tvalid_1's r2: -11.5399\n",
      "[16]\ttraining's r2: -11.6511\tvalid_1's r2: -10.0521\n",
      "[17]\ttraining's r2: -10.2502\tvalid_1's r2: -8.80767\n",
      "[18]\ttraining's r2: -9.11398\tvalid_1's r2: -7.8057\n",
      "[19]\ttraining's r2: -8.1371\tvalid_1's r2: -6.94893\n",
      "[20]\ttraining's r2: -7.28583\tvalid_1's r2: -6.19318\n",
      "[21]\ttraining's r2: -6.57567\tvalid_1's r2: -5.56987\n",
      "[22]\ttraining's r2: -5.95404\tvalid_1's r2: -5.01127\n",
      "[23]\ttraining's r2: -5.41888\tvalid_1's r2: -4.54362\n",
      "[24]\ttraining's r2: -4.94455\tvalid_1's r2: -4.1304\n",
      "[25]\ttraining's r2: -4.5445\tvalid_1's r2: -3.78412\n",
      "[26]\ttraining's r2: -4.17806\tvalid_1's r2: -3.45521\n",
      "[27]\ttraining's r2: -3.85026\tvalid_1's r2: -3.16996\n",
      "[28]\ttraining's r2: -3.5405\tvalid_1's r2: -2.89517\n",
      "[29]\ttraining's r2: -3.27866\tvalid_1's r2: -2.66744\n",
      "[30]\ttraining's r2: -3.03665\tvalid_1's r2: -2.45413\n",
      "[31]\ttraining's r2: -2.81381\tvalid_1's r2: -2.25625\n",
      "[32]\ttraining's r2: -2.63185\tvalid_1's r2: -2.10031\n",
      "[33]\ttraining's r2: -2.45426\tvalid_1's r2: -1.94512\n",
      "[34]\ttraining's r2: -2.29889\tvalid_1's r2: -1.81209\n",
      "[35]\ttraining's r2: -2.14449\tvalid_1's r2: -1.68076\n",
      "[36]\ttraining's r2: -2.0101\tvalid_1's r2: -1.56364\n",
      "[37]\ttraining's r2: -1.88324\tvalid_1's r2: -1.45646\n",
      "[38]\ttraining's r2: -1.76248\tvalid_1's r2: -1.35201\n",
      "[39]\ttraining's r2: -1.64844\tvalid_1's r2: -1.25109\n",
      "[40]\ttraining's r2: -1.54888\tvalid_1's r2: -1.16484\n",
      "[41]\ttraining's r2: -1.45047\tvalid_1's r2: -1.07865\n",
      "[42]\ttraining's r2: -1.36123\tvalid_1's r2: -1.00126\n",
      "[43]\ttraining's r2: -1.28282\tvalid_1's r2: -0.936813\n",
      "[44]\ttraining's r2: -1.20984\tvalid_1's r2: -0.874966\n",
      "[45]\ttraining's r2: -1.13872\tvalid_1's r2: -0.815331\n",
      "[46]\ttraining's r2: -1.07032\tvalid_1's r2: -0.757314\n",
      "[47]\ttraining's r2: -1.01209\tvalid_1's r2: -0.70776\n",
      "[48]\ttraining's r2: -0.954161\tvalid_1's r2: -0.657447\n",
      "[49]\ttraining's r2: -0.899313\tvalid_1's r2: -0.613937\n",
      "[50]\ttraining's r2: -0.847982\tvalid_1's r2: -0.575012\n",
      "[51]\ttraining's r2: -0.800986\tvalid_1's r2: -0.537886\n",
      "[52]\ttraining's r2: -0.757687\tvalid_1's r2: -0.501802\n",
      "[53]\ttraining's r2: -0.713899\tvalid_1's r2: -0.46602\n",
      "[54]\ttraining's r2: -0.66967\tvalid_1's r2: -0.428926\n",
      "[55]\ttraining's r2: -0.629576\tvalid_1's r2: -0.395164\n",
      "[56]\ttraining's r2: -0.591023\tvalid_1's r2: -0.362922\n",
      "[57]\ttraining's r2: -0.556191\tvalid_1's r2: -0.332965\n",
      "[58]\ttraining's r2: -0.524583\tvalid_1's r2: -0.307049\n",
      "[59]\ttraining's r2: -0.494437\tvalid_1's r2: -0.283288\n",
      "[60]\ttraining's r2: -0.465218\tvalid_1's r2: -0.261792\n",
      "[61]\ttraining's r2: -0.437778\tvalid_1's r2: -0.239433\n",
      "[62]\ttraining's r2: -0.410189\tvalid_1's r2: -0.217765\n",
      "[63]\ttraining's r2: -0.381868\tvalid_1's r2: -0.194955\n",
      "[64]\ttraining's r2: -0.35581\tvalid_1's r2: -0.173221\n",
      "[65]\ttraining's r2: -0.330414\tvalid_1's r2: -0.153359\n",
      "[66]\ttraining's r2: -0.307911\tvalid_1's r2: -0.136181\n",
      "[67]\ttraining's r2: -0.284801\tvalid_1's r2: -0.118698\n",
      "[68]\ttraining's r2: -0.26392\tvalid_1's r2: -0.103558\n",
      "[69]\ttraining's r2: -0.242997\tvalid_1's r2: -0.0874945\n",
      "[70]\ttraining's r2: -0.224779\tvalid_1's r2: -0.0731846\n",
      "[71]\ttraining's r2: -0.206054\tvalid_1's r2: -0.0580502\n",
      "[72]\ttraining's r2: -0.189832\tvalid_1's r2: -0.0473491\n",
      "[73]\ttraining's r2: -0.172037\tvalid_1's r2: -0.0353252\n",
      "[74]\ttraining's r2: -0.15561\tvalid_1's r2: -0.0235276\n",
      "[75]\ttraining's r2: -0.138409\tvalid_1's r2: -0.0107675\n",
      "[76]\ttraining's r2: -0.122718\tvalid_1's r2: 0.000392005\n",
      "[77]\ttraining's r2: -0.107336\tvalid_1's r2: 0.0124522\n",
      "[78]\ttraining's r2: -0.0934398\tvalid_1's r2: 0.0230415\n",
      "[79]\ttraining's r2: -0.0798228\tvalid_1's r2: 0.0337646\n",
      "[80]\ttraining's r2: -0.0660707\tvalid_1's r2: 0.0435432\n",
      "[81]\ttraining's r2: -0.0532694\tvalid_1's r2: 0.0540608\n",
      "[82]\ttraining's r2: -0.0422376\tvalid_1's r2: 0.0609494\n",
      "[83]\ttraining's r2: -0.0322228\tvalid_1's r2: 0.06668\n",
      "[84]\ttraining's r2: -0.0190111\tvalid_1's r2: 0.0768953\n",
      "[85]\ttraining's r2: -0.00745183\tvalid_1's r2: 0.0864616\n",
      "[86]\ttraining's r2: 0.0043186\tvalid_1's r2: 0.0942471\n",
      "[87]\ttraining's r2: 0.0158173\tvalid_1's r2: 0.102237\n",
      "[88]\ttraining's r2: 0.0267442\tvalid_1's r2: 0.109178\n",
      "[89]\ttraining's r2: 0.037409\tvalid_1's r2: 0.117792\n",
      "[90]\ttraining's r2: 0.0475503\tvalid_1's r2: 0.125534\n",
      "[91]\ttraining's r2: 0.0583811\tvalid_1's r2: 0.133483\n",
      "[92]\ttraining's r2: 0.0684183\tvalid_1's r2: 0.141444\n",
      "[93]\ttraining's r2: 0.0787132\tvalid_1's r2: 0.148747\n",
      "[94]\ttraining's r2: 0.0882225\tvalid_1's r2: 0.154779\n",
      "[95]\ttraining's r2: 0.0964846\tvalid_1's r2: 0.161371\n",
      "[96]\ttraining's r2: 0.105541\tvalid_1's r2: 0.168021\n",
      "[97]\ttraining's r2: 0.113517\tvalid_1's r2: 0.173187\n",
      "[98]\ttraining's r2: 0.122174\tvalid_1's r2: 0.179836\n",
      "[99]\ttraining's r2: 0.130037\tvalid_1's r2: 0.185159\n",
      "[100]\ttraining's r2: 0.137765\tvalid_1's r2: 0.189993\n",
      "[101]\ttraining's r2: 0.144547\tvalid_1's r2: 0.194255\n",
      "[102]\ttraining's r2: 0.152087\tvalid_1's r2: 0.19903\n",
      "[103]\ttraining's r2: 0.159569\tvalid_1's r2: 0.20451\n",
      "[104]\ttraining's r2: 0.166375\tvalid_1's r2: 0.208926\n",
      "[105]\ttraining's r2: 0.171557\tvalid_1's r2: 0.211731\n",
      "[106]\ttraining's r2: 0.178726\tvalid_1's r2: 0.216674\n",
      "[107]\ttraining's r2: 0.18429\tvalid_1's r2: 0.220153\n",
      "[108]\ttraining's r2: 0.189409\tvalid_1's r2: 0.222318\n",
      "[109]\ttraining's r2: 0.195407\tvalid_1's r2: 0.226815\n",
      "[110]\ttraining's r2: 0.201888\tvalid_1's r2: 0.231176\n",
      "[111]\ttraining's r2: 0.207627\tvalid_1's r2: 0.235048\n",
      "[112]\ttraining's r2: 0.213668\tvalid_1's r2: 0.239443\n",
      "[113]\ttraining's r2: 0.219005\tvalid_1's r2: 0.242298\n",
      "[114]\ttraining's r2: 0.224472\tvalid_1's r2: 0.245814\n",
      "[115]\ttraining's r2: 0.22993\tvalid_1's r2: 0.248289\n",
      "[116]\ttraining's r2: 0.23429\tvalid_1's r2: 0.250187\n",
      "[117]\ttraining's r2: 0.239651\tvalid_1's r2: 0.25376\n",
      "[118]\ttraining's r2: 0.244138\tvalid_1's r2: 0.2554\n",
      "[119]\ttraining's r2: 0.249611\tvalid_1's r2: 0.259131\n",
      "[120]\ttraining's r2: 0.254008\tvalid_1's r2: 0.262051\n",
      "[121]\ttraining's r2: 0.258881\tvalid_1's r2: 0.264903\n",
      "[122]\ttraining's r2: 0.262713\tvalid_1's r2: 0.266597\n",
      "[123]\ttraining's r2: 0.267991\tvalid_1's r2: 0.269358\n",
      "[124]\ttraining's r2: 0.271948\tvalid_1's r2: 0.271267\n",
      "[125]\ttraining's r2: 0.275525\tvalid_1's r2: 0.272323\n",
      "[126]\ttraining's r2: 0.280156\tvalid_1's r2: 0.275154\n",
      "[127]\ttraining's r2: 0.283853\tvalid_1's r2: 0.276617\n",
      "[128]\ttraining's r2: 0.287888\tvalid_1's r2: 0.279026\n",
      "[129]\ttraining's r2: 0.291952\tvalid_1's r2: 0.281405\n",
      "[130]\ttraining's r2: 0.295951\tvalid_1's r2: 0.284378\n",
      "[131]\ttraining's r2: 0.299613\tvalid_1's r2: 0.286357\n",
      "[132]\ttraining's r2: 0.302416\tvalid_1's r2: 0.287016\n",
      "[133]\ttraining's r2: 0.306989\tvalid_1's r2: 0.289696\n",
      "[134]\ttraining's r2: 0.310191\tvalid_1's r2: 0.291078\n",
      "[135]\ttraining's r2: 0.313486\tvalid_1's r2: 0.292254\n",
      "[136]\ttraining's r2: 0.317718\tvalid_1's r2: 0.294443\n",
      "[137]\ttraining's r2: 0.320647\tvalid_1's r2: 0.294945\n",
      "[138]\ttraining's r2: 0.323789\tvalid_1's r2: 0.296474\n",
      "[139]\ttraining's r2: 0.326566\tvalid_1's r2: 0.297393\n",
      "[140]\ttraining's r2: 0.329926\tvalid_1's r2: 0.298897\n",
      "[141]\ttraining's r2: 0.33379\tvalid_1's r2: 0.301159\n",
      "[142]\ttraining's r2: 0.337155\tvalid_1's r2: 0.302213\n",
      "[143]\ttraining's r2: 0.340229\tvalid_1's r2: 0.303518\n",
      "[144]\ttraining's r2: 0.343076\tvalid_1's r2: 0.304388\n",
      "[145]\ttraining's r2: 0.345975\tvalid_1's r2: 0.305455\n",
      "[146]\ttraining's r2: 0.348444\tvalid_1's r2: 0.306104\n",
      "[147]\ttraining's r2: 0.351353\tvalid_1's r2: 0.306739\n",
      "[148]\ttraining's r2: 0.354238\tvalid_1's r2: 0.307262\n",
      "[149]\ttraining's r2: 0.356901\tvalid_1's r2: 0.308218\n",
      "[150]\ttraining's r2: 0.359677\tvalid_1's r2: 0.308924\n",
      "[151]\ttraining's r2: 0.362086\tvalid_1's r2: 0.30972\n",
      "[152]\ttraining's r2: 0.364282\tvalid_1's r2: 0.310633\n",
      "[153]\ttraining's r2: 0.367605\tvalid_1's r2: 0.312565\n",
      "[154]\ttraining's r2: 0.370432\tvalid_1's r2: 0.314455\n",
      "[155]\ttraining's r2: 0.372371\tvalid_1's r2: 0.314771\n",
      "[156]\ttraining's r2: 0.374387\tvalid_1's r2: 0.315254\n",
      "[157]\ttraining's r2: 0.377401\tvalid_1's r2: 0.316149\n",
      "[158]\ttraining's r2: 0.379855\tvalid_1's r2: 0.316434\n",
      "[159]\ttraining's r2: 0.382191\tvalid_1's r2: 0.317313\n",
      "[160]\ttraining's r2: 0.38472\tvalid_1's r2: 0.318312\n",
      "[161]\ttraining's r2: 0.387162\tvalid_1's r2: 0.318509\n",
      "[162]\ttraining's r2: 0.389523\tvalid_1's r2: 0.319239\n",
      "[163]\ttraining's r2: 0.391292\tvalid_1's r2: 0.319589\n",
      "[164]\ttraining's r2: 0.393804\tvalid_1's r2: 0.319944\n",
      "[165]\ttraining's r2: 0.395789\tvalid_1's r2: 0.320762\n",
      "[166]\ttraining's r2: 0.397875\tvalid_1's r2: 0.321517\n",
      "[167]\ttraining's r2: 0.400063\tvalid_1's r2: 0.321885\n",
      "[168]\ttraining's r2: 0.402262\tvalid_1's r2: 0.322794\n",
      "[169]\ttraining's r2: 0.404188\tvalid_1's r2: 0.323291\n",
      "[170]\ttraining's r2: 0.406349\tvalid_1's r2: 0.324029\n",
      "[171]\ttraining's r2: 0.408285\tvalid_1's r2: 0.324532\n",
      "[172]\ttraining's r2: 0.410096\tvalid_1's r2: 0.325418\n",
      "[173]\ttraining's r2: 0.412421\tvalid_1's r2: 0.3267\n",
      "[174]\ttraining's r2: 0.414874\tvalid_1's r2: 0.327695\n",
      "[175]\ttraining's r2: 0.41687\tvalid_1's r2: 0.327913\n",
      "[176]\ttraining's r2: 0.418876\tvalid_1's r2: 0.32873\n",
      "[177]\ttraining's r2: 0.421011\tvalid_1's r2: 0.329406\n",
      "[178]\ttraining's r2: 0.422579\tvalid_1's r2: 0.329786\n",
      "[179]\ttraining's r2: 0.425002\tvalid_1's r2: 0.331451\n",
      "[180]\ttraining's r2: 0.427113\tvalid_1's r2: 0.332197\n",
      "[181]\ttraining's r2: 0.42916\tvalid_1's r2: 0.333321\n",
      "[182]\ttraining's r2: 0.43102\tvalid_1's r2: 0.333591\n",
      "[183]\ttraining's r2: 0.433224\tvalid_1's r2: 0.334627\n",
      "[184]\ttraining's r2: 0.435462\tvalid_1's r2: 0.335484\n",
      "[185]\ttraining's r2: 0.436904\tvalid_1's r2: 0.335816\n",
      "[186]\ttraining's r2: 0.438643\tvalid_1's r2: 0.336169\n",
      "[187]\ttraining's r2: 0.440563\tvalid_1's r2: 0.336987\n",
      "[188]\ttraining's r2: 0.443068\tvalid_1's r2: 0.338451\n",
      "[189]\ttraining's r2: 0.444773\tvalid_1's r2: 0.338525\n",
      "[190]\ttraining's r2: 0.44657\tvalid_1's r2: 0.338906\n",
      "[191]\ttraining's r2: 0.448346\tvalid_1's r2: 0.339145\n",
      "[192]\ttraining's r2: 0.450378\tvalid_1's r2: 0.339635\n",
      "[193]\ttraining's r2: 0.452112\tvalid_1's r2: 0.340179\n",
      "[194]\ttraining's r2: 0.453929\tvalid_1's r2: 0.340107\n",
      "[195]\ttraining's r2: 0.455592\tvalid_1's r2: 0.339813\n",
      "[196]\ttraining's r2: 0.457522\tvalid_1's r2: 0.340097\n",
      "[197]\ttraining's r2: 0.459192\tvalid_1's r2: 0.340773\n",
      "[198]\ttraining's r2: 0.460765\tvalid_1's r2: 0.340784\n",
      "[199]\ttraining's r2: 0.46301\tvalid_1's r2: 0.341851\n",
      "[200]\ttraining's r2: 0.4646\tvalid_1's r2: 0.342295\n",
      "[201]\ttraining's r2: 0.466041\tvalid_1's r2: 0.342206\n",
      "[202]\ttraining's r2: 0.467888\tvalid_1's r2: 0.342263\n",
      "[203]\ttraining's r2: 0.469482\tvalid_1's r2: 0.342798\n",
      "[204]\ttraining's r2: 0.470976\tvalid_1's r2: 0.343413\n",
      "[205]\ttraining's r2: 0.472498\tvalid_1's r2: 0.343731\n",
      "[206]\ttraining's r2: 0.47466\tvalid_1's r2: 0.344582\n",
      "[207]\ttraining's r2: 0.475988\tvalid_1's r2: 0.345173\n",
      "[208]\ttraining's r2: 0.477521\tvalid_1's r2: 0.345465\n",
      "[209]\ttraining's r2: 0.479317\tvalid_1's r2: 0.345878\n",
      "[210]\ttraining's r2: 0.480852\tvalid_1's r2: 0.346332\n",
      "[211]\ttraining's r2: 0.482571\tvalid_1's r2: 0.346963\n",
      "[212]\ttraining's r2: 0.484265\tvalid_1's r2: 0.347149\n",
      "[213]\ttraining's r2: 0.485724\tvalid_1's r2: 0.347305\n",
      "[214]\ttraining's r2: 0.486999\tvalid_1's r2: 0.347964\n",
      "[215]\ttraining's r2: 0.488596\tvalid_1's r2: 0.348223\n",
      "[216]\ttraining's r2: 0.49028\tvalid_1's r2: 0.348685\n",
      "[217]\ttraining's r2: 0.492091\tvalid_1's r2: 0.34916\n",
      "[218]\ttraining's r2: 0.493241\tvalid_1's r2: 0.348787\n",
      "[219]\ttraining's r2: 0.495107\tvalid_1's r2: 0.349312\n",
      "[220]\ttraining's r2: 0.496939\tvalid_1's r2: 0.350175\n",
      "[221]\ttraining's r2: 0.498318\tvalid_1's r2: 0.350744\n",
      "[222]\ttraining's r2: 0.500018\tvalid_1's r2: 0.351166\n",
      "[223]\ttraining's r2: 0.501473\tvalid_1's r2: 0.351419\n",
      "[224]\ttraining's r2: 0.502878\tvalid_1's r2: 0.351952\n",
      "[225]\ttraining's r2: 0.504551\tvalid_1's r2: 0.352467\n",
      "[226]\ttraining's r2: 0.505597\tvalid_1's r2: 0.352378\n",
      "[227]\ttraining's r2: 0.507346\tvalid_1's r2: 0.352895\n",
      "[228]\ttraining's r2: 0.508564\tvalid_1's r2: 0.353053\n",
      "[229]\ttraining's r2: 0.509939\tvalid_1's r2: 0.353238\n",
      "[230]\ttraining's r2: 0.510949\tvalid_1's r2: 0.352766\n",
      "[231]\ttraining's r2: 0.512209\tvalid_1's r2: 0.353377\n",
      "[232]\ttraining's r2: 0.513227\tvalid_1's r2: 0.353685\n",
      "[233]\ttraining's r2: 0.514485\tvalid_1's r2: 0.353859\n",
      "[234]\ttraining's r2: 0.515829\tvalid_1's r2: 0.353784\n",
      "[235]\ttraining's r2: 0.517425\tvalid_1's r2: 0.354452\n",
      "[236]\ttraining's r2: 0.51866\tvalid_1's r2: 0.355186\n",
      "[237]\ttraining's r2: 0.519947\tvalid_1's r2: 0.355871\n",
      "[238]\ttraining's r2: 0.52154\tvalid_1's r2: 0.356478\n",
      "[239]\ttraining's r2: 0.523027\tvalid_1's r2: 0.356682\n",
      "[240]\ttraining's r2: 0.524556\tvalid_1's r2: 0.356765\n",
      "[241]\ttraining's r2: 0.525839\tvalid_1's r2: 0.357037\n",
      "[242]\ttraining's r2: 0.526952\tvalid_1's r2: 0.356968\n",
      "[243]\ttraining's r2: 0.528137\tvalid_1's r2: 0.356979\n",
      "[244]\ttraining's r2: 0.529403\tvalid_1's r2: 0.357455\n",
      "[245]\ttraining's r2: 0.530615\tvalid_1's r2: 0.357567\n",
      "[246]\ttraining's r2: 0.53195\tvalid_1's r2: 0.3579\n",
      "[247]\ttraining's r2: 0.533243\tvalid_1's r2: 0.357853\n",
      "[248]\ttraining's r2: 0.534729\tvalid_1's r2: 0.358149\n",
      "[249]\ttraining's r2: 0.536045\tvalid_1's r2: 0.358702\n",
      "[250]\ttraining's r2: 0.537246\tvalid_1's r2: 0.359012\n",
      "[251]\ttraining's r2: 0.538863\tvalid_1's r2: 0.359786\n",
      "[252]\ttraining's r2: 0.540161\tvalid_1's r2: 0.360137\n",
      "[253]\ttraining's r2: 0.541387\tvalid_1's r2: 0.359902\n",
      "[254]\ttraining's r2: 0.542493\tvalid_1's r2: 0.360104\n",
      "[255]\ttraining's r2: 0.543759\tvalid_1's r2: 0.360542\n",
      "[256]\ttraining's r2: 0.54504\tvalid_1's r2: 0.360554\n",
      "[257]\ttraining's r2: 0.54592\tvalid_1's r2: 0.360325\n",
      "[258]\ttraining's r2: 0.547235\tvalid_1's r2: 0.360735\n",
      "[259]\ttraining's r2: 0.548594\tvalid_1's r2: 0.36141\n",
      "[260]\ttraining's r2: 0.549706\tvalid_1's r2: 0.361667\n",
      "[261]\ttraining's r2: 0.550984\tvalid_1's r2: 0.36199\n",
      "[262]\ttraining's r2: 0.552487\tvalid_1's r2: 0.362375\n",
      "[263]\ttraining's r2: 0.553505\tvalid_1's r2: 0.36191\n",
      "[264]\ttraining's r2: 0.554583\tvalid_1's r2: 0.361398\n",
      "[265]\ttraining's r2: 0.555889\tvalid_1's r2: 0.3617\n",
      "[266]\ttraining's r2: 0.557014\tvalid_1's r2: 0.361903\n",
      "[267]\ttraining's r2: 0.558176\tvalid_1's r2: 0.362212\n",
      "[268]\ttraining's r2: 0.559347\tvalid_1's r2: 0.36259\n",
      "[269]\ttraining's r2: 0.560566\tvalid_1's r2: 0.362849\n",
      "[270]\ttraining's r2: 0.561655\tvalid_1's r2: 0.362499\n",
      "[271]\ttraining's r2: 0.562818\tvalid_1's r2: 0.362343\n",
      "[272]\ttraining's r2: 0.56399\tvalid_1's r2: 0.362668\n",
      "[273]\ttraining's r2: 0.565141\tvalid_1's r2: 0.362981\n",
      "[274]\ttraining's r2: 0.566152\tvalid_1's r2: 0.36334\n",
      "[275]\ttraining's r2: 0.567373\tvalid_1's r2: 0.36373\n",
      "[276]\ttraining's r2: 0.568799\tvalid_1's r2: 0.363878\n",
      "[277]\ttraining's r2: 0.570111\tvalid_1's r2: 0.363772\n",
      "[278]\ttraining's r2: 0.571331\tvalid_1's r2: 0.363957\n",
      "[279]\ttraining's r2: 0.572462\tvalid_1's r2: 0.364106\n",
      "[280]\ttraining's r2: 0.57353\tvalid_1's r2: 0.364488\n",
      "[281]\ttraining's r2: 0.574578\tvalid_1's r2: 0.36475\n",
      "[282]\ttraining's r2: 0.57563\tvalid_1's r2: 0.364559\n",
      "[283]\ttraining's r2: 0.577006\tvalid_1's r2: 0.365008\n",
      "[284]\ttraining's r2: 0.57807\tvalid_1's r2: 0.365004\n",
      "[285]\ttraining's r2: 0.579168\tvalid_1's r2: 0.365091\n",
      "[286]\ttraining's r2: 0.580215\tvalid_1's r2: 0.365129\n",
      "[287]\ttraining's r2: 0.581451\tvalid_1's r2: 0.365133\n",
      "[288]\ttraining's r2: 0.582498\tvalid_1's r2: 0.365566\n",
      "[289]\ttraining's r2: 0.583649\tvalid_1's r2: 0.365495\n",
      "[290]\ttraining's r2: 0.584849\tvalid_1's r2: 0.365306\n",
      "[291]\ttraining's r2: 0.585792\tvalid_1's r2: 0.365486\n",
      "[292]\ttraining's r2: 0.586661\tvalid_1's r2: 0.365603\n",
      "[293]\ttraining's r2: 0.587901\tvalid_1's r2: 0.365478\n",
      "[294]\ttraining's r2: 0.5889\tvalid_1's r2: 0.365509\n",
      "[295]\ttraining's r2: 0.590205\tvalid_1's r2: 0.365838\n",
      "[296]\ttraining's r2: 0.591393\tvalid_1's r2: 0.36625\n",
      "[297]\ttraining's r2: 0.592495\tvalid_1's r2: 0.366305\n",
      "[298]\ttraining's r2: 0.593536\tvalid_1's r2: 0.366898\n",
      "[299]\ttraining's r2: 0.594402\tvalid_1's r2: 0.367205\n",
      "[300]\ttraining's r2: 0.595311\tvalid_1's r2: 0.367149\n",
      "[301]\ttraining's r2: 0.596547\tvalid_1's r2: 0.367192\n",
      "[302]\ttraining's r2: 0.597689\tvalid_1's r2: 0.367775\n",
      "[303]\ttraining's r2: 0.5989\tvalid_1's r2: 0.36772\n",
      "[304]\ttraining's r2: 0.599995\tvalid_1's r2: 0.36759\n",
      "[305]\ttraining's r2: 0.60089\tvalid_1's r2: 0.367798\n",
      "[306]\ttraining's r2: 0.601728\tvalid_1's r2: 0.367749\n",
      "[307]\ttraining's r2: 0.602505\tvalid_1's r2: 0.367753\n",
      "[308]\ttraining's r2: 0.60363\tvalid_1's r2: 0.368035\n",
      "[309]\ttraining's r2: 0.604378\tvalid_1's r2: 0.367776\n",
      "[310]\ttraining's r2: 0.605545\tvalid_1's r2: 0.367857\n",
      "[311]\ttraining's r2: 0.606706\tvalid_1's r2: 0.367972\n",
      "[312]\ttraining's r2: 0.607715\tvalid_1's r2: 0.368256\n",
      "[313]\ttraining's r2: 0.609028\tvalid_1's r2: 0.36832\n",
      "[314]\ttraining's r2: 0.609915\tvalid_1's r2: 0.368646\n",
      "[315]\ttraining's r2: 0.610874\tvalid_1's r2: 0.368626\n",
      "[316]\ttraining's r2: 0.611749\tvalid_1's r2: 0.368629\n",
      "[317]\ttraining's r2: 0.612874\tvalid_1's r2: 0.368626\n",
      "[318]\ttraining's r2: 0.614031\tvalid_1's r2: 0.368507\n",
      "[319]\ttraining's r2: 0.615029\tvalid_1's r2: 0.368599\n",
      "[320]\ttraining's r2: 0.615789\tvalid_1's r2: 0.368304\n",
      "[321]\ttraining's r2: 0.616541\tvalid_1's r2: 0.368393\n",
      "[322]\ttraining's r2: 0.617587\tvalid_1's r2: 0.368496\n",
      "[323]\ttraining's r2: 0.61863\tvalid_1's r2: 0.368624\n",
      "[324]\ttraining's r2: 0.619572\tvalid_1's r2: 0.368974\n",
      "[325]\ttraining's r2: 0.620474\tvalid_1's r2: 0.369183\n",
      "[326]\ttraining's r2: 0.621302\tvalid_1's r2: 0.369139\n",
      "[327]\ttraining's r2: 0.622391\tvalid_1's r2: 0.369207\n",
      "[328]\ttraining's r2: 0.62329\tvalid_1's r2: 0.369234\n",
      "[329]\ttraining's r2: 0.624145\tvalid_1's r2: 0.369226\n",
      "[330]\ttraining's r2: 0.625037\tvalid_1's r2: 0.369237\n",
      "[331]\ttraining's r2: 0.625912\tvalid_1's r2: 0.369455\n",
      "[332]\ttraining's r2: 0.626748\tvalid_1's r2: 0.369484\n",
      "[333]\ttraining's r2: 0.627447\tvalid_1's r2: 0.369404\n",
      "[334]\ttraining's r2: 0.628213\tvalid_1's r2: 0.369809\n",
      "[335]\ttraining's r2: 0.629215\tvalid_1's r2: 0.370071\n",
      "[336]\ttraining's r2: 0.630169\tvalid_1's r2: 0.369985\n",
      "[337]\ttraining's r2: 0.63088\tvalid_1's r2: 0.369884\n",
      "[338]\ttraining's r2: 0.631953\tvalid_1's r2: 0.370354\n",
      "[339]\ttraining's r2: 0.633018\tvalid_1's r2: 0.370439\n",
      "[340]\ttraining's r2: 0.633899\tvalid_1's r2: 0.370688\n",
      "[341]\ttraining's r2: 0.634753\tvalid_1's r2: 0.370555\n",
      "[342]\ttraining's r2: 0.635565\tvalid_1's r2: 0.37034\n",
      "[343]\ttraining's r2: 0.636443\tvalid_1's r2: 0.370549\n",
      "[344]\ttraining's r2: 0.637143\tvalid_1's r2: 0.370422\n",
      "[345]\ttraining's r2: 0.638114\tvalid_1's r2: 0.370132\n",
      "[346]\ttraining's r2: 0.639043\tvalid_1's r2: 0.370221\n",
      "[347]\ttraining's r2: 0.63971\tvalid_1's r2: 0.370248\n",
      "[348]\ttraining's r2: 0.640467\tvalid_1's r2: 0.370128\n",
      "[349]\ttraining's r2: 0.641272\tvalid_1's r2: 0.370331\n",
      "[350]\ttraining's r2: 0.642278\tvalid_1's r2: 0.370359\n",
      "[351]\ttraining's r2: 0.643271\tvalid_1's r2: 0.370543\n",
      "[352]\ttraining's r2: 0.644081\tvalid_1's r2: 0.370675\n",
      "[353]\ttraining's r2: 0.644697\tvalid_1's r2: 0.370711\n",
      "[354]\ttraining's r2: 0.64553\tvalid_1's r2: 0.37051\n",
      "[355]\ttraining's r2: 0.646557\tvalid_1's r2: 0.370793\n",
      "[356]\ttraining's r2: 0.647318\tvalid_1's r2: 0.37094\n",
      "[357]\ttraining's r2: 0.647923\tvalid_1's r2: 0.371068\n",
      "[358]\ttraining's r2: 0.648715\tvalid_1's r2: 0.370957\n",
      "[359]\ttraining's r2: 0.649512\tvalid_1's r2: 0.371339\n",
      "[360]\ttraining's r2: 0.650217\tvalid_1's r2: 0.371346\n",
      "[361]\ttraining's r2: 0.651096\tvalid_1's r2: 0.371191\n",
      "[362]\ttraining's r2: 0.652031\tvalid_1's r2: 0.371169\n",
      "[363]\ttraining's r2: 0.652769\tvalid_1's r2: 0.371737\n",
      "[364]\ttraining's r2: 0.653382\tvalid_1's r2: 0.37181\n",
      "[365]\ttraining's r2: 0.654396\tvalid_1's r2: 0.371676\n",
      "[366]\ttraining's r2: 0.655165\tvalid_1's r2: 0.37199\n",
      "[367]\ttraining's r2: 0.655831\tvalid_1's r2: 0.372176\n",
      "[368]\ttraining's r2: 0.656704\tvalid_1's r2: 0.371923\n",
      "[369]\ttraining's r2: 0.657436\tvalid_1's r2: 0.37122\n",
      "[370]\ttraining's r2: 0.658282\tvalid_1's r2: 0.371581\n",
      "[371]\ttraining's r2: 0.659312\tvalid_1's r2: 0.37156\n",
      "[372]\ttraining's r2: 0.660377\tvalid_1's r2: 0.371536\n",
      "[373]\ttraining's r2: 0.661183\tvalid_1's r2: 0.371725\n",
      "[374]\ttraining's r2: 0.66201\tvalid_1's r2: 0.371867\n",
      "[375]\ttraining's r2: 0.662694\tvalid_1's r2: 0.372353\n",
      "[376]\ttraining's r2: 0.663401\tvalid_1's r2: 0.372302\n",
      "[377]\ttraining's r2: 0.664068\tvalid_1's r2: 0.372243\n",
      "[378]\ttraining's r2: 0.664857\tvalid_1's r2: 0.372025\n",
      "[379]\ttraining's r2: 0.665754\tvalid_1's r2: 0.372378\n",
      "[380]\ttraining's r2: 0.666653\tvalid_1's r2: 0.372386\n",
      "[381]\ttraining's r2: 0.667239\tvalid_1's r2: 0.372262\n",
      "[382]\ttraining's r2: 0.667803\tvalid_1's r2: 0.372412\n",
      "[383]\ttraining's r2: 0.668323\tvalid_1's r2: 0.37224\n",
      "[384]\ttraining's r2: 0.669086\tvalid_1's r2: 0.372646\n",
      "[385]\ttraining's r2: 0.669869\tvalid_1's r2: 0.372814\n",
      "[386]\ttraining's r2: 0.670511\tvalid_1's r2: 0.372585\n",
      "[387]\ttraining's r2: 0.671188\tvalid_1's r2: 0.372604\n",
      "[388]\ttraining's r2: 0.671999\tvalid_1's r2: 0.37235\n",
      "[389]\ttraining's r2: 0.672723\tvalid_1's r2: 0.372815\n",
      "[390]\ttraining's r2: 0.673432\tvalid_1's r2: 0.372876\n",
      "[391]\ttraining's r2: 0.674135\tvalid_1's r2: 0.372696\n",
      "[392]\ttraining's r2: 0.674788\tvalid_1's r2: 0.372558\n",
      "[393]\ttraining's r2: 0.675545\tvalid_1's r2: 0.372561\n",
      "[394]\ttraining's r2: 0.676269\tvalid_1's r2: 0.372923\n",
      "[395]\ttraining's r2: 0.677082\tvalid_1's r2: 0.372916\n",
      "[396]\ttraining's r2: 0.677815\tvalid_1's r2: 0.373161\n",
      "[397]\ttraining's r2: 0.678558\tvalid_1's r2: 0.373298\n",
      "[398]\ttraining's r2: 0.679383\tvalid_1's r2: 0.373822\n",
      "[399]\ttraining's r2: 0.680397\tvalid_1's r2: 0.373911\n",
      "[400]\ttraining's r2: 0.680952\tvalid_1's r2: 0.373783\n",
      "[401]\ttraining's r2: 0.681677\tvalid_1's r2: 0.373789\n",
      "[402]\ttraining's r2: 0.6824\tvalid_1's r2: 0.37408\n",
      "[403]\ttraining's r2: 0.683031\tvalid_1's r2: 0.373557\n",
      "[404]\ttraining's r2: 0.683807\tvalid_1's r2: 0.373772\n",
      "[405]\ttraining's r2: 0.68454\tvalid_1's r2: 0.374107\n",
      "[406]\ttraining's r2: 0.685189\tvalid_1's r2: 0.374545\n",
      "[407]\ttraining's r2: 0.685777\tvalid_1's r2: 0.374384\n",
      "[408]\ttraining's r2: 0.68637\tvalid_1's r2: 0.374332\n",
      "[409]\ttraining's r2: 0.687007\tvalid_1's r2: 0.374185\n",
      "[410]\ttraining's r2: 0.687681\tvalid_1's r2: 0.374381\n",
      "[411]\ttraining's r2: 0.688483\tvalid_1's r2: 0.374281\n",
      "[412]\ttraining's r2: 0.689085\tvalid_1's r2: 0.374302\n",
      "[413]\ttraining's r2: 0.689653\tvalid_1's r2: 0.374104\n",
      "[414]\ttraining's r2: 0.69038\tvalid_1's r2: 0.374216\n",
      "[415]\ttraining's r2: 0.691089\tvalid_1's r2: 0.374213\n",
      "[416]\ttraining's r2: 0.691615\tvalid_1's r2: 0.37395\n",
      "[417]\ttraining's r2: 0.692389\tvalid_1's r2: 0.374124\n",
      "[418]\ttraining's r2: 0.693028\tvalid_1's r2: 0.373881\n",
      "[419]\ttraining's r2: 0.693649\tvalid_1's r2: 0.373721\n",
      "[420]\ttraining's r2: 0.694141\tvalid_1's r2: 0.37345\n",
      "[421]\ttraining's r2: 0.694739\tvalid_1's r2: 0.373174\n",
      "[422]\ttraining's r2: 0.695155\tvalid_1's r2: 0.372866\n",
      "[423]\ttraining's r2: 0.695776\tvalid_1's r2: 0.372859\n",
      "[424]\ttraining's r2: 0.696628\tvalid_1's r2: 0.373248\n",
      "[425]\ttraining's r2: 0.697261\tvalid_1's r2: 0.373378\n",
      "[426]\ttraining's r2: 0.698161\tvalid_1's r2: 0.372984\n",
      "[427]\ttraining's r2: 0.698916\tvalid_1's r2: 0.373042\n",
      "[428]\ttraining's r2: 0.699599\tvalid_1's r2: 0.372938\n",
      "[429]\ttraining's r2: 0.700236\tvalid_1's r2: 0.373172\n",
      "[430]\ttraining's r2: 0.700846\tvalid_1's r2: 0.373204\n",
      "[431]\ttraining's r2: 0.701191\tvalid_1's r2: 0.373196\n",
      "[432]\ttraining's r2: 0.701804\tvalid_1's r2: 0.373134\n",
      "[433]\ttraining's r2: 0.702507\tvalid_1's r2: 0.373773\n",
      "[434]\ttraining's r2: 0.703075\tvalid_1's r2: 0.373976\n",
      "[435]\ttraining's r2: 0.703659\tvalid_1's r2: 0.373965\n",
      "[436]\ttraining's r2: 0.704258\tvalid_1's r2: 0.374118\n",
      "[437]\ttraining's r2: 0.70495\tvalid_1's r2: 0.374067\n",
      "[438]\ttraining's r2: 0.705697\tvalid_1's r2: 0.373981\n",
      "[439]\ttraining's r2: 0.706463\tvalid_1's r2: 0.374244\n",
      "[440]\ttraining's r2: 0.707083\tvalid_1's r2: 0.374192\n",
      "[441]\ttraining's r2: 0.707785\tvalid_1's r2: 0.374675\n",
      "[442]\ttraining's r2: 0.708424\tvalid_1's r2: 0.375104\n",
      "[443]\ttraining's r2: 0.709082\tvalid_1's r2: 0.375163\n",
      "[444]\ttraining's r2: 0.709799\tvalid_1's r2: 0.375627\n",
      "[445]\ttraining's r2: 0.710387\tvalid_1's r2: 0.37568\n",
      "[446]\ttraining's r2: 0.711057\tvalid_1's r2: 0.37581\n",
      "[447]\ttraining's r2: 0.711653\tvalid_1's r2: 0.37606\n",
      "[448]\ttraining's r2: 0.712286\tvalid_1's r2: 0.375998\n",
      "[449]\ttraining's r2: 0.712878\tvalid_1's r2: 0.376002\n",
      "[450]\ttraining's r2: 0.713544\tvalid_1's r2: 0.375865\n",
      "[451]\ttraining's r2: 0.714073\tvalid_1's r2: 0.376173\n",
      "[452]\ttraining's r2: 0.714593\tvalid_1's r2: 0.376045\n",
      "[453]\ttraining's r2: 0.715185\tvalid_1's r2: 0.375791\n",
      "[454]\ttraining's r2: 0.715791\tvalid_1's r2: 0.375717\n",
      "[455]\ttraining's r2: 0.716435\tvalid_1's r2: 0.375973\n",
      "[456]\ttraining's r2: 0.716997\tvalid_1's r2: 0.375856\n",
      "[457]\ttraining's r2: 0.717506\tvalid_1's r2: 0.375865\n",
      "[458]\ttraining's r2: 0.718045\tvalid_1's r2: 0.375791\n",
      "[459]\ttraining's r2: 0.718552\tvalid_1's r2: 0.375996\n",
      "[460]\ttraining's r2: 0.719046\tvalid_1's r2: 0.37599\n",
      "[461]\ttraining's r2: 0.719598\tvalid_1's r2: 0.375927\n",
      "[462]\ttraining's r2: 0.720269\tvalid_1's r2: 0.376121\n",
      "[463]\ttraining's r2: 0.720903\tvalid_1's r2: 0.376367\n",
      "[464]\ttraining's r2: 0.721478\tvalid_1's r2: 0.376404\n",
      "[465]\ttraining's r2: 0.722015\tvalid_1's r2: 0.376017\n",
      "[466]\ttraining's r2: 0.722687\tvalid_1's r2: 0.375897\n",
      "[467]\ttraining's r2: 0.723292\tvalid_1's r2: 0.375621\n",
      "[468]\ttraining's r2: 0.723808\tvalid_1's r2: 0.37576\n",
      "[469]\ttraining's r2: 0.724416\tvalid_1's r2: 0.375759\n",
      "[470]\ttraining's r2: 0.724911\tvalid_1's r2: 0.376011\n",
      "[471]\ttraining's r2: 0.725437\tvalid_1's r2: 0.375903\n",
      "[472]\ttraining's r2: 0.726217\tvalid_1's r2: 0.376162\n",
      "[473]\ttraining's r2: 0.726766\tvalid_1's r2: 0.376102\n",
      "[474]\ttraining's r2: 0.727229\tvalid_1's r2: 0.375982\n",
      "[475]\ttraining's r2: 0.727882\tvalid_1's r2: 0.376087\n",
      "[476]\ttraining's r2: 0.728482\tvalid_1's r2: 0.376139\n",
      "[477]\ttraining's r2: 0.728933\tvalid_1's r2: 0.375994\n",
      "[478]\ttraining's r2: 0.729494\tvalid_1's r2: 0.375992\n",
      "[479]\ttraining's r2: 0.730141\tvalid_1's r2: 0.376145\n",
      "[480]\ttraining's r2: 0.730648\tvalid_1's r2: 0.376264\n",
      "[481]\ttraining's r2: 0.73104\tvalid_1's r2: 0.37612\n",
      "[482]\ttraining's r2: 0.731408\tvalid_1's r2: 0.375989\n",
      "[483]\ttraining's r2: 0.73184\tvalid_1's r2: 0.376016\n",
      "[484]\ttraining's r2: 0.732206\tvalid_1's r2: 0.375776\n",
      "[485]\ttraining's r2: 0.732729\tvalid_1's r2: 0.375798\n",
      "[486]\ttraining's r2: 0.733247\tvalid_1's r2: 0.376097\n",
      "[487]\ttraining's r2: 0.733849\tvalid_1's r2: 0.376242\n",
      "[488]\ttraining's r2: 0.73447\tvalid_1's r2: 0.376228\n",
      "[489]\ttraining's r2: 0.735076\tvalid_1's r2: 0.376227\n",
      "[490]\ttraining's r2: 0.735679\tvalid_1's r2: 0.376791\n",
      "[491]\ttraining's r2: 0.73633\tvalid_1's r2: 0.377001\n",
      "[492]\ttraining's r2: 0.736779\tvalid_1's r2: 0.376956\n",
      "[493]\ttraining's r2: 0.737301\tvalid_1's r2: 0.376904\n",
      "[494]\ttraining's r2: 0.737803\tvalid_1's r2: 0.376675\n",
      "[495]\ttraining's r2: 0.738314\tvalid_1's r2: 0.37651\n",
      "[496]\ttraining's r2: 0.738821\tvalid_1's r2: 0.376535\n",
      "[497]\ttraining's r2: 0.739508\tvalid_1's r2: 0.376523\n",
      "[498]\ttraining's r2: 0.740092\tvalid_1's r2: 0.376445\n",
      "[499]\ttraining's r2: 0.74062\tvalid_1's r2: 0.376617\n",
      "[500]\ttraining's r2: 0.741242\tvalid_1's r2: 0.376582\n",
      "[501]\ttraining's r2: 0.741684\tvalid_1's r2: 0.376561\n",
      "[502]\ttraining's r2: 0.742273\tvalid_1's r2: 0.376773\n",
      "[503]\ttraining's r2: 0.742903\tvalid_1's r2: 0.376804\n",
      "[504]\ttraining's r2: 0.743377\tvalid_1's r2: 0.376897\n",
      "[505]\ttraining's r2: 0.743887\tvalid_1's r2: 0.37671\n",
      "[506]\ttraining's r2: 0.74442\tvalid_1's r2: 0.376651\n",
      "[507]\ttraining's r2: 0.74492\tvalid_1's r2: 0.376703\n",
      "[508]\ttraining's r2: 0.745341\tvalid_1's r2: 0.376427\n",
      "[509]\ttraining's r2: 0.745719\tvalid_1's r2: 0.376268\n",
      "[510]\ttraining's r2: 0.746075\tvalid_1's r2: 0.376189\n",
      "[511]\ttraining's r2: 0.746474\tvalid_1's r2: 0.376213\n",
      "[512]\ttraining's r2: 0.746892\tvalid_1's r2: 0.376306\n",
      "[513]\ttraining's r2: 0.747355\tvalid_1's r2: 0.37632\n",
      "[514]\ttraining's r2: 0.747849\tvalid_1's r2: 0.376426\n",
      "[515]\ttraining's r2: 0.748406\tvalid_1's r2: 0.376489\n",
      "[516]\ttraining's r2: 0.749017\tvalid_1's r2: 0.376364\n",
      "[517]\ttraining's r2: 0.749591\tvalid_1's r2: 0.376371\n",
      "[518]\ttraining's r2: 0.750101\tvalid_1's r2: 0.37645\n",
      "[519]\ttraining's r2: 0.750521\tvalid_1's r2: 0.376673\n",
      "[520]\ttraining's r2: 0.750979\tvalid_1's r2: 0.376749\n",
      "[521]\ttraining's r2: 0.751485\tvalid_1's r2: 0.376626\n",
      "[522]\ttraining's r2: 0.752034\tvalid_1's r2: 0.376778\n",
      "[523]\ttraining's r2: 0.752645\tvalid_1's r2: 0.37705\n",
      "[524]\ttraining's r2: 0.753115\tvalid_1's r2: 0.376908\n",
      "[525]\ttraining's r2: 0.753667\tvalid_1's r2: 0.377111\n",
      "[526]\ttraining's r2: 0.754163\tvalid_1's r2: 0.377127\n",
      "[527]\ttraining's r2: 0.754824\tvalid_1's r2: 0.377301\n",
      "[528]\ttraining's r2: 0.755355\tvalid_1's r2: 0.377333\n",
      "[529]\ttraining's r2: 0.755895\tvalid_1's r2: 0.377607\n",
      "[530]\ttraining's r2: 0.756343\tvalid_1's r2: 0.37752\n",
      "[531]\ttraining's r2: 0.756802\tvalid_1's r2: 0.37746\n",
      "[532]\ttraining's r2: 0.757298\tvalid_1's r2: 0.377461\n",
      "[533]\ttraining's r2: 0.75772\tvalid_1's r2: 0.377551\n",
      "[534]\ttraining's r2: 0.758078\tvalid_1's r2: 0.377488\n",
      "[535]\ttraining's r2: 0.758415\tvalid_1's r2: 0.377091\n",
      "[536]\ttraining's r2: 0.758896\tvalid_1's r2: 0.377134\n",
      "[537]\ttraining's r2: 0.759353\tvalid_1's r2: 0.377211\n",
      "[538]\ttraining's r2: 0.759832\tvalid_1's r2: 0.37738\n",
      "[539]\ttraining's r2: 0.760343\tvalid_1's r2: 0.377304\n",
      "[540]\ttraining's r2: 0.760915\tvalid_1's r2: 0.377144\n",
      "[541]\ttraining's r2: 0.761377\tvalid_1's r2: 0.377117\n",
      "[542]\ttraining's r2: 0.761797\tvalid_1's r2: 0.377375\n",
      "[543]\ttraining's r2: 0.762336\tvalid_1's r2: 0.37752\n",
      "[544]\ttraining's r2: 0.762894\tvalid_1's r2: 0.378134\n",
      "[545]\ttraining's r2: 0.763271\tvalid_1's r2: 0.377872\n",
      "[546]\ttraining's r2: 0.763662\tvalid_1's r2: 0.377937\n",
      "[547]\ttraining's r2: 0.764052\tvalid_1's r2: 0.377849\n",
      "[548]\ttraining's r2: 0.764546\tvalid_1's r2: 0.377841\n",
      "[549]\ttraining's r2: 0.76492\tvalid_1's r2: 0.377913\n",
      "[550]\ttraining's r2: 0.765389\tvalid_1's r2: 0.377808\n",
      "[551]\ttraining's r2: 0.765907\tvalid_1's r2: 0.377651\n",
      "[552]\ttraining's r2: 0.766404\tvalid_1's r2: 0.377653\n",
      "[553]\ttraining's r2: 0.766886\tvalid_1's r2: 0.377608\n",
      "[554]\ttraining's r2: 0.767338\tvalid_1's r2: 0.377495\n",
      "[555]\ttraining's r2: 0.767675\tvalid_1's r2: 0.377583\n",
      "[556]\ttraining's r2: 0.768068\tvalid_1's r2: 0.377494\n",
      "[557]\ttraining's r2: 0.768403\tvalid_1's r2: 0.377495\n",
      "[558]\ttraining's r2: 0.768677\tvalid_1's r2: 0.377271\n",
      "[559]\ttraining's r2: 0.769135\tvalid_1's r2: 0.377594\n",
      "[560]\ttraining's r2: 0.7695\tvalid_1's r2: 0.377551\n",
      "[561]\ttraining's r2: 0.769907\tvalid_1's r2: 0.377465\n",
      "[562]\ttraining's r2: 0.770377\tvalid_1's r2: 0.377546\n",
      "[563]\ttraining's r2: 0.770771\tvalid_1's r2: 0.377561\n",
      "[564]\ttraining's r2: 0.771256\tvalid_1's r2: 0.377665\n",
      "[565]\ttraining's r2: 0.771649\tvalid_1's r2: 0.378106\n",
      "[566]\ttraining's r2: 0.772072\tvalid_1's r2: 0.37829\n",
      "[567]\ttraining's r2: 0.772529\tvalid_1's r2: 0.378213\n",
      "[568]\ttraining's r2: 0.772888\tvalid_1's r2: 0.378318\n",
      "[569]\ttraining's r2: 0.773368\tvalid_1's r2: 0.378218\n",
      "[570]\ttraining's r2: 0.773801\tvalid_1's r2: 0.378417\n",
      "[571]\ttraining's r2: 0.774116\tvalid_1's r2: 0.378553\n",
      "[572]\ttraining's r2: 0.77463\tvalid_1's r2: 0.378788\n",
      "[573]\ttraining's r2: 0.775053\tvalid_1's r2: 0.378632\n",
      "[574]\ttraining's r2: 0.775541\tvalid_1's r2: 0.378662\n",
      "[575]\ttraining's r2: 0.77599\tvalid_1's r2: 0.378665\n",
      "[576]\ttraining's r2: 0.776407\tvalid_1's r2: 0.378445\n",
      "[577]\ttraining's r2: 0.776799\tvalid_1's r2: 0.378617\n",
      "[578]\ttraining's r2: 0.777311\tvalid_1's r2: 0.378748\n",
      "[579]\ttraining's r2: 0.777717\tvalid_1's r2: 0.379266\n",
      "[580]\ttraining's r2: 0.778104\tvalid_1's r2: 0.378966\n",
      "[581]\ttraining's r2: 0.778501\tvalid_1's r2: 0.379122\n",
      "[582]\ttraining's r2: 0.778852\tvalid_1's r2: 0.3792\n",
      "[583]\ttraining's r2: 0.779321\tvalid_1's r2: 0.379134\n",
      "[584]\ttraining's r2: 0.779676\tvalid_1's r2: 0.379006\n",
      "[585]\ttraining's r2: 0.780102\tvalid_1's r2: 0.37895\n",
      "[586]\ttraining's r2: 0.780441\tvalid_1's r2: 0.379048\n",
      "[587]\ttraining's r2: 0.780853\tvalid_1's r2: 0.379042\n",
      "[588]\ttraining's r2: 0.781265\tvalid_1's r2: 0.37912\n",
      "[589]\ttraining's r2: 0.781688\tvalid_1's r2: 0.379015\n",
      "[590]\ttraining's r2: 0.782049\tvalid_1's r2: 0.378879\n",
      "[591]\ttraining's r2: 0.782483\tvalid_1's r2: 0.378867\n",
      "[592]\ttraining's r2: 0.782917\tvalid_1's r2: 0.378851\n",
      "[593]\ttraining's r2: 0.783381\tvalid_1's r2: 0.37886\n",
      "[594]\ttraining's r2: 0.783795\tvalid_1's r2: 0.378887\n",
      "[595]\ttraining's r2: 0.784249\tvalid_1's r2: 0.379075\n",
      "[596]\ttraining's r2: 0.78465\tvalid_1's r2: 0.379199\n",
      "[597]\ttraining's r2: 0.78519\tvalid_1's r2: 0.379398\n",
      "[598]\ttraining's r2: 0.785483\tvalid_1's r2: 0.379401\n",
      "[599]\ttraining's r2: 0.785886\tvalid_1's r2: 0.379352\n",
      "[600]\ttraining's r2: 0.786381\tvalid_1's r2: 0.379173\n",
      "[601]\ttraining's r2: 0.786687\tvalid_1's r2: 0.378747\n",
      "[602]\ttraining's r2: 0.787027\tvalid_1's r2: 0.37866\n",
      "[603]\ttraining's r2: 0.787371\tvalid_1's r2: 0.378368\n",
      "[604]\ttraining's r2: 0.787718\tvalid_1's r2: 0.378333\n",
      "[605]\ttraining's r2: 0.788071\tvalid_1's r2: 0.378475\n",
      "[606]\ttraining's r2: 0.788383\tvalid_1's r2: 0.378553\n",
      "[607]\ttraining's r2: 0.788774\tvalid_1's r2: 0.37837\n",
      "[608]\ttraining's r2: 0.789129\tvalid_1's r2: 0.378527\n",
      "[609]\ttraining's r2: 0.78947\tvalid_1's r2: 0.378416\n",
      "[610]\ttraining's r2: 0.789803\tvalid_1's r2: 0.378331\n",
      "[611]\ttraining's r2: 0.790073\tvalid_1's r2: 0.378241\n",
      "[612]\ttraining's r2: 0.790426\tvalid_1's r2: 0.377887\n",
      "[613]\ttraining's r2: 0.790803\tvalid_1's r2: 0.377914\n",
      "[614]\ttraining's r2: 0.791218\tvalid_1's r2: 0.377984\n",
      "[615]\ttraining's r2: 0.791627\tvalid_1's r2: 0.378164\n",
      "[616]\ttraining's r2: 0.791999\tvalid_1's r2: 0.378233\n",
      "[617]\ttraining's r2: 0.792343\tvalid_1's r2: 0.378093\n",
      "[618]\ttraining's r2: 0.79266\tvalid_1's r2: 0.378086\n",
      "[619]\ttraining's r2: 0.792968\tvalid_1's r2: 0.378181\n",
      "[620]\ttraining's r2: 0.793284\tvalid_1's r2: 0.378365\n",
      "[621]\ttraining's r2: 0.793681\tvalid_1's r2: 0.378414\n",
      "[622]\ttraining's r2: 0.794066\tvalid_1's r2: 0.378576\n",
      "[623]\ttraining's r2: 0.794426\tvalid_1's r2: 0.378468\n",
      "[624]\ttraining's r2: 0.794924\tvalid_1's r2: 0.37873\n",
      "[625]\ttraining's r2: 0.795384\tvalid_1's r2: 0.378515\n",
      "[626]\ttraining's r2: 0.795805\tvalid_1's r2: 0.37851\n",
      "[627]\ttraining's r2: 0.796256\tvalid_1's r2: 0.378516\n",
      "[628]\ttraining's r2: 0.796583\tvalid_1's r2: 0.378353\n",
      "[629]\ttraining's r2: 0.796929\tvalid_1's r2: 0.378299\n",
      "[630]\ttraining's r2: 0.797326\tvalid_1's r2: 0.378584\n",
      "[631]\ttraining's r2: 0.797577\tvalid_1's r2: 0.37834\n",
      "[632]\ttraining's r2: 0.797945\tvalid_1's r2: 0.378286\n",
      "[633]\ttraining's r2: 0.798246\tvalid_1's r2: 0.378311\n",
      "[634]\ttraining's r2: 0.798576\tvalid_1's r2: 0.378119\n",
      "[635]\ttraining's r2: 0.798909\tvalid_1's r2: 0.378057\n",
      "[636]\ttraining's r2: 0.799121\tvalid_1's r2: 0.377715\n",
      "[637]\ttraining's r2: 0.799352\tvalid_1's r2: 0.377635\n",
      "[638]\ttraining's r2: 0.799659\tvalid_1's r2: 0.377886\n",
      "[639]\ttraining's r2: 0.80005\tvalid_1's r2: 0.377993\n",
      "[640]\ttraining's r2: 0.800382\tvalid_1's r2: 0.37787\n",
      "[641]\ttraining's r2: 0.800776\tvalid_1's r2: 0.378099\n",
      "[642]\ttraining's r2: 0.801123\tvalid_1's r2: 0.378097\n",
      "[643]\ttraining's r2: 0.801543\tvalid_1's r2: 0.378344\n",
      "[644]\ttraining's r2: 0.801884\tvalid_1's r2: 0.378365\n",
      "[645]\ttraining's r2: 0.802295\tvalid_1's r2: 0.378271\n",
      "[646]\ttraining's r2: 0.802673\tvalid_1's r2: 0.378588\n",
      "[647]\ttraining's r2: 0.80302\tvalid_1's r2: 0.378652\n",
      "[648]\ttraining's r2: 0.803401\tvalid_1's r2: 0.378746\n",
      "[649]\ttraining's r2: 0.803695\tvalid_1's r2: 0.378534\n",
      "[650]\ttraining's r2: 0.80402\tvalid_1's r2: 0.378545\n",
      "[651]\ttraining's r2: 0.804356\tvalid_1's r2: 0.378247\n",
      "[652]\ttraining's r2: 0.804748\tvalid_1's r2: 0.378247\n",
      "[653]\ttraining's r2: 0.805063\tvalid_1's r2: 0.378397\n",
      "[654]\ttraining's r2: 0.805435\tvalid_1's r2: 0.378501\n",
      "[655]\ttraining's r2: 0.805708\tvalid_1's r2: 0.378042\n",
      "[656]\ttraining's r2: 0.80604\tvalid_1's r2: 0.377844\n",
      "[657]\ttraining's r2: 0.806364\tvalid_1's r2: 0.377677\n",
      "[658]\ttraining's r2: 0.806702\tvalid_1's r2: 0.377716\n",
      "[659]\ttraining's r2: 0.807028\tvalid_1's r2: 0.377922\n",
      "[660]\ttraining's r2: 0.807283\tvalid_1's r2: 0.37806\n",
      "[661]\ttraining's r2: 0.807508\tvalid_1's r2: 0.377938\n",
      "[662]\ttraining's r2: 0.807758\tvalid_1's r2: 0.377876\n",
      "[663]\ttraining's r2: 0.808073\tvalid_1's r2: 0.377929\n",
      "[664]\ttraining's r2: 0.808418\tvalid_1's r2: 0.377726\n",
      "[665]\ttraining's r2: 0.808772\tvalid_1's r2: 0.377648\n",
      "[666]\ttraining's r2: 0.809053\tvalid_1's r2: 0.377567\n",
      "[667]\ttraining's r2: 0.80932\tvalid_1's r2: 0.377734\n",
      "[668]\ttraining's r2: 0.80961\tvalid_1's r2: 0.377521\n",
      "[669]\ttraining's r2: 0.809935\tvalid_1's r2: 0.377569\n",
      "[670]\ttraining's r2: 0.810262\tvalid_1's r2: 0.377631\n",
      "[671]\ttraining's r2: 0.810551\tvalid_1's r2: 0.377671\n",
      "[672]\ttraining's r2: 0.810911\tvalid_1's r2: 0.377664\n",
      "[673]\ttraining's r2: 0.811305\tvalid_1's r2: 0.377676\n",
      "[674]\ttraining's r2: 0.811717\tvalid_1's r2: 0.377888\n",
      "[675]\ttraining's r2: 0.812003\tvalid_1's r2: 0.378073\n",
      "[676]\ttraining's r2: 0.812344\tvalid_1's r2: 0.377951\n",
      "[677]\ttraining's r2: 0.812706\tvalid_1's r2: 0.377807\n",
      "[678]\ttraining's r2: 0.813039\tvalid_1's r2: 0.377979\n",
      "[679]\ttraining's r2: 0.813391\tvalid_1's r2: 0.378121\n",
      "[680]\ttraining's r2: 0.81367\tvalid_1's r2: 0.377787\n",
      "[681]\ttraining's r2: 0.813951\tvalid_1's r2: 0.377733\n",
      "[682]\ttraining's r2: 0.814321\tvalid_1's r2: 0.377605\n",
      "[683]\ttraining's r2: 0.814722\tvalid_1's r2: 0.377625\n",
      "[684]\ttraining's r2: 0.814919\tvalid_1's r2: 0.377526\n",
      "[685]\ttraining's r2: 0.815186\tvalid_1's r2: 0.377406\n",
      "[686]\ttraining's r2: 0.815608\tvalid_1's r2: 0.377547\n",
      "[687]\ttraining's r2: 0.815857\tvalid_1's r2: 0.377512\n",
      "[688]\ttraining's r2: 0.816119\tvalid_1's r2: 0.377432\n",
      "[689]\ttraining's r2: 0.81636\tvalid_1's r2: 0.377384\n",
      "[690]\ttraining's r2: 0.816663\tvalid_1's r2: 0.377228\n",
      "[691]\ttraining's r2: 0.816953\tvalid_1's r2: 0.377329\n",
      "[692]\ttraining's r2: 0.817249\tvalid_1's r2: 0.377407\n",
      "[693]\ttraining's r2: 0.817694\tvalid_1's r2: 0.377218\n",
      "[694]\ttraining's r2: 0.818031\tvalid_1's r2: 0.377201\n",
      "[695]\ttraining's r2: 0.81833\tvalid_1's r2: 0.377099\n",
      "[696]\ttraining's r2: 0.818644\tvalid_1's r2: 0.377096\n",
      "[697]\ttraining's r2: 0.818984\tvalid_1's r2: 0.376788\n",
      "[698]\ttraining's r2: 0.819377\tvalid_1's r2: 0.376948\n",
      "Early stopping, best iteration is:\n",
      "[598]\ttraining's r2: 0.785483\tvalid_1's r2: 0.379401\n",
      "[1]\ttraining's r2: -3347.37\tvalid_1's r2: -3172.62\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[2]\ttraining's r2: -818.871\tvalid_1's r2: -756.336\n",
      "[3]\ttraining's r2: -350.946\tvalid_1's r2: -325.837\n",
      "[4]\ttraining's r2: -196.621\tvalid_1's r2: -182.22\n",
      "[5]\ttraining's r2: -123.89\tvalid_1's r2: -115.048\n",
      "[6]\ttraining's r2: -85.5172\tvalid_1's r2: -78.916\n",
      "[7]\ttraining's r2: -62.9626\tvalid_1's r2: -57.8472\n",
      "[8]\ttraining's r2: -48.0378\tvalid_1's r2: -43.985\n",
      "[9]\ttraining's r2: -37.4595\tvalid_1's r2: -34.078\n",
      "[10]\ttraining's r2: -30.493\tvalid_1's r2: -27.5912\n",
      "[11]\ttraining's r2: -25.1033\tvalid_1's r2: -22.692\n",
      "[12]\ttraining's r2: -21.0887\tvalid_1's r2: -18.9755\n",
      "[13]\ttraining's r2: -17.8737\tvalid_1's r2: -16.0201\n",
      "[14]\ttraining's r2: -15.3346\tvalid_1's r2: -13.6828\n",
      "[15]\ttraining's r2: -13.3028\tvalid_1's r2: -11.8289\n",
      "[16]\ttraining's r2: -11.6396\tvalid_1's r2: -10.2963\n",
      "[17]\ttraining's r2: -10.2931\tvalid_1's r2: -9.08421\n",
      "[18]\ttraining's r2: -9.15995\tvalid_1's r2: -8.06989\n",
      "[19]\ttraining's r2: -8.18807\tvalid_1's r2: -7.18762\n",
      "[20]\ttraining's r2: -7.3479\tvalid_1's r2: -6.43343\n",
      "[21]\ttraining's r2: -6.6342\tvalid_1's r2: -5.77912\n",
      "[22]\ttraining's r2: -6.0211\tvalid_1's r2: -5.22241\n",
      "[23]\ttraining's r2: -5.48618\tvalid_1's r2: -4.73244\n",
      "[24]\ttraining's r2: -4.99832\tvalid_1's r2: -4.29652\n",
      "[25]\ttraining's r2: -4.58521\tvalid_1's r2: -3.92083\n",
      "[26]\ttraining's r2: -4.20378\tvalid_1's r2: -3.57043\n",
      "[27]\ttraining's r2: -3.87141\tvalid_1's r2: -3.27322\n",
      "[28]\ttraining's r2: -3.54943\tvalid_1's r2: -2.98645\n",
      "[29]\ttraining's r2: -3.28444\tvalid_1's r2: -2.75455\n",
      "[30]\ttraining's r2: -3.04201\tvalid_1's r2: -2.54241\n",
      "[31]\ttraining's r2: -2.82291\tvalid_1's r2: -2.3431\n",
      "[32]\ttraining's r2: -2.63122\tvalid_1's r2: -2.16718\n",
      "[33]\ttraining's r2: -2.44644\tvalid_1's r2: -1.99843\n",
      "[34]\ttraining's r2: -2.28906\tvalid_1's r2: -1.85993\n",
      "[35]\ttraining's r2: -2.14334\tvalid_1's r2: -1.7348\n",
      "[36]\ttraining's r2: -2.01048\tvalid_1's r2: -1.61855\n",
      "[37]\ttraining's r2: -1.88363\tvalid_1's r2: -1.50781\n",
      "[38]\ttraining's r2: -1.76246\tvalid_1's r2: -1.39572\n",
      "[39]\ttraining's r2: -1.65557\tvalid_1's r2: -1.30364\n",
      "[40]\ttraining's r2: -1.55308\tvalid_1's r2: -1.21522\n",
      "[41]\ttraining's r2: -1.45888\tvalid_1's r2: -1.13091\n",
      "[42]\ttraining's r2: -1.37107\tvalid_1's r2: -1.05285\n",
      "[43]\ttraining's r2: -1.2889\tvalid_1's r2: -0.981209\n",
      "[44]\ttraining's r2: -1.21302\tvalid_1's r2: -0.913778\n",
      "[45]\ttraining's r2: -1.14197\tvalid_1's r2: -0.853119\n",
      "[46]\ttraining's r2: -1.07899\tvalid_1's r2: -0.799044\n",
      "[47]\ttraining's r2: -1.01894\tvalid_1's r2: -0.747213\n",
      "[48]\ttraining's r2: -0.96138\tvalid_1's r2: -0.696666\n",
      "[49]\ttraining's r2: -0.904912\tvalid_1's r2: -0.646958\n",
      "[50]\ttraining's r2: -0.854421\tvalid_1's r2: -0.603446\n",
      "[51]\ttraining's r2: -0.808199\tvalid_1's r2: -0.564201\n",
      "[52]\ttraining's r2: -0.761716\tvalid_1's r2: -0.524068\n",
      "[53]\ttraining's r2: -0.719891\tvalid_1's r2: -0.487229\n",
      "[54]\ttraining's r2: -0.677175\tvalid_1's r2: -0.450419\n",
      "[55]\ttraining's r2: -0.636696\tvalid_1's r2: -0.415953\n",
      "[56]\ttraining's r2: -0.597983\tvalid_1's r2: -0.380828\n",
      "[57]\ttraining's r2: -0.562485\tvalid_1's r2: -0.352256\n",
      "[58]\ttraining's r2: -0.52963\tvalid_1's r2: -0.324873\n",
      "[59]\ttraining's r2: -0.497605\tvalid_1's r2: -0.298343\n",
      "[60]\ttraining's r2: -0.468414\tvalid_1's r2: -0.274515\n",
      "[61]\ttraining's r2: -0.439859\tvalid_1's r2: -0.250366\n",
      "[62]\ttraining's r2: -0.412818\tvalid_1's r2: -0.228994\n",
      "[63]\ttraining's r2: -0.386627\tvalid_1's r2: -0.206955\n",
      "[64]\ttraining's r2: -0.361775\tvalid_1's r2: -0.187268\n",
      "[65]\ttraining's r2: -0.338701\tvalid_1's r2: -0.169512\n",
      "[66]\ttraining's r2: -0.316039\tvalid_1's r2: -0.152512\n",
      "[67]\ttraining's r2: -0.292311\tvalid_1's r2: -0.132596\n",
      "[68]\ttraining's r2: -0.270376\tvalid_1's r2: -0.115724\n",
      "[69]\ttraining's r2: -0.249762\tvalid_1's r2: -0.0985314\n",
      "[70]\ttraining's r2: -0.229501\tvalid_1's r2: -0.0821183\n",
      "[71]\ttraining's r2: -0.209959\tvalid_1's r2: -0.0656071\n",
      "[72]\ttraining's r2: -0.192325\tvalid_1's r2: -0.0521588\n",
      "[73]\ttraining's r2: -0.175491\tvalid_1's r2: -0.0390938\n",
      "[74]\ttraining's r2: -0.157803\tvalid_1's r2: -0.0247775\n",
      "[75]\ttraining's r2: -0.141742\tvalid_1's r2: -0.0123356\n",
      "[76]\ttraining's r2: -0.125965\tvalid_1's r2: 0.00155972\n",
      "[77]\ttraining's r2: -0.110751\tvalid_1's r2: 0.0134599\n",
      "[78]\ttraining's r2: -0.0952493\tvalid_1's r2: 0.025256\n",
      "[79]\ttraining's r2: -0.0818631\tvalid_1's r2: 0.035263\n",
      "[80]\ttraining's r2: -0.0675333\tvalid_1's r2: 0.0458813\n",
      "[81]\ttraining's r2: -0.0548708\tvalid_1's r2: 0.0551133\n",
      "[82]\ttraining's r2: -0.0421976\tvalid_1's r2: 0.0656968\n",
      "[83]\ttraining's r2: -0.033777\tvalid_1's r2: 0.0719261\n",
      "[84]\ttraining's r2: -0.0214805\tvalid_1's r2: 0.0801749\n",
      "[85]\ttraining's r2: -0.0091391\tvalid_1's r2: 0.0896337\n",
      "[86]\ttraining's r2: 0.00281825\tvalid_1's r2: 0.0997097\n",
      "[87]\ttraining's r2: 0.0140341\tvalid_1's r2: 0.108758\n",
      "[88]\ttraining's r2: 0.0238659\tvalid_1's r2: 0.11563\n",
      "[89]\ttraining's r2: 0.0347529\tvalid_1's r2: 0.123227\n",
      "[90]\ttraining's r2: 0.0443027\tvalid_1's r2: 0.130672\n",
      "[91]\ttraining's r2: 0.0527639\tvalid_1's r2: 0.136024\n",
      "[92]\ttraining's r2: 0.062885\tvalid_1's r2: 0.143546\n",
      "[93]\ttraining's r2: 0.0729809\tvalid_1's r2: 0.152412\n",
      "[94]\ttraining's r2: 0.0822325\tvalid_1's r2: 0.159979\n",
      "[95]\ttraining's r2: 0.0917928\tvalid_1's r2: 0.168391\n",
      "[96]\ttraining's r2: 0.100476\tvalid_1's r2: 0.175486\n",
      "[97]\ttraining's r2: 0.106286\tvalid_1's r2: 0.179736\n",
      "[98]\ttraining's r2: 0.114285\tvalid_1's r2: 0.186773\n",
      "[99]\ttraining's r2: 0.122114\tvalid_1's r2: 0.192966\n",
      "[100]\ttraining's r2: 0.129063\tvalid_1's r2: 0.197854\n",
      "[101]\ttraining's r2: 0.136533\tvalid_1's r2: 0.20369\n",
      "[102]\ttraining's r2: 0.143938\tvalid_1's r2: 0.208571\n",
      "[103]\ttraining's r2: 0.150637\tvalid_1's r2: 0.213072\n",
      "[104]\ttraining's r2: 0.157636\tvalid_1's r2: 0.217492\n",
      "[105]\ttraining's r2: 0.163083\tvalid_1's r2: 0.220705\n",
      "[106]\ttraining's r2: 0.170088\tvalid_1's r2: 0.225204\n",
      "[107]\ttraining's r2: 0.175789\tvalid_1's r2: 0.229165\n",
      "[108]\ttraining's r2: 0.181569\tvalid_1's r2: 0.233603\n",
      "[109]\ttraining's r2: 0.187651\tvalid_1's r2: 0.238153\n",
      "[110]\ttraining's r2: 0.193399\tvalid_1's r2: 0.241976\n",
      "[111]\ttraining's r2: 0.199697\tvalid_1's r2: 0.246457\n",
      "[112]\ttraining's r2: 0.205713\tvalid_1's r2: 0.250373\n",
      "[113]\ttraining's r2: 0.211406\tvalid_1's r2: 0.254371\n",
      "[114]\ttraining's r2: 0.217313\tvalid_1's r2: 0.25787\n",
      "[115]\ttraining's r2: 0.223094\tvalid_1's r2: 0.261798\n",
      "[116]\ttraining's r2: 0.226643\tvalid_1's r2: 0.263771\n",
      "[117]\ttraining's r2: 0.230652\tvalid_1's r2: 0.265463\n",
      "[118]\ttraining's r2: 0.236113\tvalid_1's r2: 0.269895\n",
      "[119]\ttraining's r2: 0.240824\tvalid_1's r2: 0.27346\n",
      "[120]\ttraining's r2: 0.244833\tvalid_1's r2: 0.275482\n",
      "[121]\ttraining's r2: 0.249543\tvalid_1's r2: 0.278458\n",
      "[122]\ttraining's r2: 0.253247\tvalid_1's r2: 0.280298\n",
      "[123]\ttraining's r2: 0.25818\tvalid_1's r2: 0.283249\n",
      "[124]\ttraining's r2: 0.261819\tvalid_1's r2: 0.284977\n",
      "[125]\ttraining's r2: 0.265307\tvalid_1's r2: 0.286899\n",
      "[126]\ttraining's r2: 0.268859\tvalid_1's r2: 0.288667\n",
      "[127]\ttraining's r2: 0.273562\tvalid_1's r2: 0.291303\n",
      "[128]\ttraining's r2: 0.277039\tvalid_1's r2: 0.292349\n",
      "[129]\ttraining's r2: 0.281127\tvalid_1's r2: 0.294248\n",
      "[130]\ttraining's r2: 0.285237\tvalid_1's r2: 0.29618\n",
      "[131]\ttraining's r2: 0.289592\tvalid_1's r2: 0.29954\n",
      "[132]\ttraining's r2: 0.293256\tvalid_1's r2: 0.301588\n",
      "[133]\ttraining's r2: 0.297116\tvalid_1's r2: 0.304083\n",
      "[134]\ttraining's r2: 0.300555\tvalid_1's r2: 0.306253\n",
      "[135]\ttraining's r2: 0.30378\tvalid_1's r2: 0.307653\n",
      "[136]\ttraining's r2: 0.306798\tvalid_1's r2: 0.309652\n",
      "[137]\ttraining's r2: 0.310011\tvalid_1's r2: 0.311366\n",
      "[138]\ttraining's r2: 0.313238\tvalid_1's r2: 0.312808\n",
      "[139]\ttraining's r2: 0.316658\tvalid_1's r2: 0.314796\n",
      "[140]\ttraining's r2: 0.319752\tvalid_1's r2: 0.315937\n",
      "[141]\ttraining's r2: 0.322509\tvalid_1's r2: 0.316754\n",
      "[142]\ttraining's r2: 0.325401\tvalid_1's r2: 0.318335\n",
      "[143]\ttraining's r2: 0.32817\tvalid_1's r2: 0.318897\n",
      "[144]\ttraining's r2: 0.330968\tvalid_1's r2: 0.32007\n",
      "[145]\ttraining's r2: 0.333816\tvalid_1's r2: 0.32128\n",
      "[146]\ttraining's r2: 0.336744\tvalid_1's r2: 0.322829\n",
      "[147]\ttraining's r2: 0.339917\tvalid_1's r2: 0.323361\n",
      "[148]\ttraining's r2: 0.342516\tvalid_1's r2: 0.323737\n",
      "[149]\ttraining's r2: 0.34544\tvalid_1's r2: 0.324426\n",
      "[150]\ttraining's r2: 0.348386\tvalid_1's r2: 0.325471\n",
      "[151]\ttraining's r2: 0.351214\tvalid_1's r2: 0.327169\n",
      "[152]\ttraining's r2: 0.353884\tvalid_1's r2: 0.328256\n",
      "[153]\ttraining's r2: 0.356741\tvalid_1's r2: 0.329425\n",
      "[154]\ttraining's r2: 0.35937\tvalid_1's r2: 0.330215\n",
      "[155]\ttraining's r2: 0.362008\tvalid_1's r2: 0.331115\n",
      "[156]\ttraining's r2: 0.364916\tvalid_1's r2: 0.331997\n",
      "[157]\ttraining's r2: 0.368112\tvalid_1's r2: 0.333521\n",
      "[158]\ttraining's r2: 0.370791\tvalid_1's r2: 0.334869\n",
      "[159]\ttraining's r2: 0.373339\tvalid_1's r2: 0.335399\n",
      "[160]\ttraining's r2: 0.376545\tvalid_1's r2: 0.337526\n",
      "[161]\ttraining's r2: 0.378626\tvalid_1's r2: 0.33872\n",
      "[162]\ttraining's r2: 0.380584\tvalid_1's r2: 0.339365\n",
      "[163]\ttraining's r2: 0.383178\tvalid_1's r2: 0.340235\n",
      "[164]\ttraining's r2: 0.385871\tvalid_1's r2: 0.341269\n",
      "[165]\ttraining's r2: 0.388121\tvalid_1's r2: 0.341857\n",
      "[166]\ttraining's r2: 0.390443\tvalid_1's r2: 0.342207\n",
      "[167]\ttraining's r2: 0.392367\tvalid_1's r2: 0.34255\n",
      "[168]\ttraining's r2: 0.394808\tvalid_1's r2: 0.343159\n",
      "[169]\ttraining's r2: 0.396596\tvalid_1's r2: 0.344248\n",
      "[170]\ttraining's r2: 0.398945\tvalid_1's r2: 0.345037\n",
      "[171]\ttraining's r2: 0.401265\tvalid_1's r2: 0.345732\n",
      "[172]\ttraining's r2: 0.404126\tvalid_1's r2: 0.347507\n",
      "[173]\ttraining's r2: 0.406122\tvalid_1's r2: 0.347761\n",
      "[174]\ttraining's r2: 0.40822\tvalid_1's r2: 0.34839\n",
      "[175]\ttraining's r2: 0.410228\tvalid_1's r2: 0.34883\n",
      "[176]\ttraining's r2: 0.412263\tvalid_1's r2: 0.349535\n",
      "[177]\ttraining's r2: 0.413971\tvalid_1's r2: 0.350064\n",
      "[178]\ttraining's r2: 0.416149\tvalid_1's r2: 0.350335\n",
      "[179]\ttraining's r2: 0.418427\tvalid_1's r2: 0.35136\n",
      "[180]\ttraining's r2: 0.420383\tvalid_1's r2: 0.351961\n",
      "[181]\ttraining's r2: 0.422616\tvalid_1's r2: 0.352692\n",
      "[182]\ttraining's r2: 0.424689\tvalid_1's r2: 0.353343\n",
      "[183]\ttraining's r2: 0.426723\tvalid_1's r2: 0.354091\n",
      "[184]\ttraining's r2: 0.428716\tvalid_1's r2: 0.355108\n",
      "[185]\ttraining's r2: 0.430703\tvalid_1's r2: 0.355999\n",
      "[186]\ttraining's r2: 0.432501\tvalid_1's r2: 0.35627\n",
      "[187]\ttraining's r2: 0.434403\tvalid_1's r2: 0.356886\n",
      "[188]\ttraining's r2: 0.436272\tvalid_1's r2: 0.357775\n",
      "[189]\ttraining's r2: 0.438254\tvalid_1's r2: 0.358371\n",
      "[190]\ttraining's r2: 0.439893\tvalid_1's r2: 0.358303\n",
      "[191]\ttraining's r2: 0.441839\tvalid_1's r2: 0.358728\n",
      "[192]\ttraining's r2: 0.443516\tvalid_1's r2: 0.359186\n",
      "[193]\ttraining's r2: 0.445739\tvalid_1's r2: 0.360292\n",
      "[194]\ttraining's r2: 0.447508\tvalid_1's r2: 0.360824\n",
      "[195]\ttraining's r2: 0.449149\tvalid_1's r2: 0.361075\n",
      "[196]\ttraining's r2: 0.451371\tvalid_1's r2: 0.362227\n",
      "[197]\ttraining's r2: 0.453\tvalid_1's r2: 0.362753\n",
      "[198]\ttraining's r2: 0.454658\tvalid_1's r2: 0.363341\n",
      "[199]\ttraining's r2: 0.456387\tvalid_1's r2: 0.36418\n",
      "[200]\ttraining's r2: 0.458253\tvalid_1's r2: 0.365001\n",
      "[201]\ttraining's r2: 0.459593\tvalid_1's r2: 0.364723\n",
      "[202]\ttraining's r2: 0.461273\tvalid_1's r2: 0.364788\n",
      "[203]\ttraining's r2: 0.462853\tvalid_1's r2: 0.365456\n",
      "[204]\ttraining's r2: 0.46427\tvalid_1's r2: 0.365894\n",
      "[205]\ttraining's r2: 0.466018\tvalid_1's r2: 0.366456\n",
      "[206]\ttraining's r2: 0.467623\tvalid_1's r2: 0.367045\n",
      "[207]\ttraining's r2: 0.468651\tvalid_1's r2: 0.36666\n",
      "[208]\ttraining's r2: 0.470153\tvalid_1's r2: 0.36686\n",
      "[209]\ttraining's r2: 0.47171\tvalid_1's r2: 0.367523\n",
      "[210]\ttraining's r2: 0.473103\tvalid_1's r2: 0.36751\n",
      "[211]\ttraining's r2: 0.474873\tvalid_1's r2: 0.368335\n",
      "[212]\ttraining's r2: 0.47663\tvalid_1's r2: 0.368995\n",
      "[213]\ttraining's r2: 0.478432\tvalid_1's r2: 0.36951\n",
      "[214]\ttraining's r2: 0.479655\tvalid_1's r2: 0.369382\n",
      "[215]\ttraining's r2: 0.481371\tvalid_1's r2: 0.37024\n",
      "[216]\ttraining's r2: 0.482808\tvalid_1's r2: 0.370109\n",
      "[217]\ttraining's r2: 0.484315\tvalid_1's r2: 0.370534\n",
      "[218]\ttraining's r2: 0.485723\tvalid_1's r2: 0.370772\n",
      "[219]\ttraining's r2: 0.487131\tvalid_1's r2: 0.370882\n",
      "[220]\ttraining's r2: 0.488675\tvalid_1's r2: 0.371091\n",
      "[221]\ttraining's r2: 0.490154\tvalid_1's r2: 0.371335\n",
      "[222]\ttraining's r2: 0.491778\tvalid_1's r2: 0.371809\n",
      "[223]\ttraining's r2: 0.493001\tvalid_1's r2: 0.372054\n",
      "[224]\ttraining's r2: 0.494517\tvalid_1's r2: 0.372308\n",
      "[225]\ttraining's r2: 0.49597\tvalid_1's r2: 0.372653\n",
      "[226]\ttraining's r2: 0.497221\tvalid_1's r2: 0.37254\n",
      "[227]\ttraining's r2: 0.498429\tvalid_1's r2: 0.372774\n",
      "[228]\ttraining's r2: 0.499737\tvalid_1's r2: 0.372975\n",
      "[229]\ttraining's r2: 0.501256\tvalid_1's r2: 0.373142\n",
      "[230]\ttraining's r2: 0.50295\tvalid_1's r2: 0.373598\n",
      "[231]\ttraining's r2: 0.504224\tvalid_1's r2: 0.374271\n",
      "[232]\ttraining's r2: 0.505496\tvalid_1's r2: 0.374299\n",
      "[233]\ttraining's r2: 0.506832\tvalid_1's r2: 0.374464\n",
      "[234]\ttraining's r2: 0.508461\tvalid_1's r2: 0.37427\n",
      "[235]\ttraining's r2: 0.509951\tvalid_1's r2: 0.374605\n",
      "[236]\ttraining's r2: 0.511151\tvalid_1's r2: 0.375361\n",
      "[237]\ttraining's r2: 0.512444\tvalid_1's r2: 0.375739\n",
      "[238]\ttraining's r2: 0.514063\tvalid_1's r2: 0.375674\n",
      "[239]\ttraining's r2: 0.515731\tvalid_1's r2: 0.376222\n",
      "[240]\ttraining's r2: 0.517179\tvalid_1's r2: 0.376214\n",
      "[241]\ttraining's r2: 0.518567\tvalid_1's r2: 0.376709\n",
      "[242]\ttraining's r2: 0.519936\tvalid_1's r2: 0.376612\n",
      "[243]\ttraining's r2: 0.521254\tvalid_1's r2: 0.376967\n",
      "[244]\ttraining's r2: 0.522571\tvalid_1's r2: 0.377101\n",
      "[245]\ttraining's r2: 0.523981\tvalid_1's r2: 0.377264\n",
      "[246]\ttraining's r2: 0.525475\tvalid_1's r2: 0.377566\n",
      "[247]\ttraining's r2: 0.526666\tvalid_1's r2: 0.377611\n",
      "[248]\ttraining's r2: 0.528024\tvalid_1's r2: 0.378237\n",
      "[249]\ttraining's r2: 0.529266\tvalid_1's r2: 0.378828\n",
      "[250]\ttraining's r2: 0.530554\tvalid_1's r2: 0.379009\n",
      "[251]\ttraining's r2: 0.531841\tvalid_1's r2: 0.379041\n",
      "[252]\ttraining's r2: 0.533397\tvalid_1's r2: 0.379309\n",
      "[253]\ttraining's r2: 0.534678\tvalid_1's r2: 0.379346\n",
      "[254]\ttraining's r2: 0.536027\tvalid_1's r2: 0.379929\n",
      "[255]\ttraining's r2: 0.537327\tvalid_1's r2: 0.380225\n",
      "[256]\ttraining's r2: 0.538422\tvalid_1's r2: 0.379904\n",
      "[257]\ttraining's r2: 0.539755\tvalid_1's r2: 0.380091\n",
      "[258]\ttraining's r2: 0.541263\tvalid_1's r2: 0.380172\n",
      "[259]\ttraining's r2: 0.542433\tvalid_1's r2: 0.380792\n",
      "[260]\ttraining's r2: 0.543624\tvalid_1's r2: 0.380444\n",
      "[261]\ttraining's r2: 0.545121\tvalid_1's r2: 0.380843\n",
      "[262]\ttraining's r2: 0.54641\tvalid_1's r2: 0.381481\n",
      "[263]\ttraining's r2: 0.547847\tvalid_1's r2: 0.381765\n",
      "[264]\ttraining's r2: 0.549091\tvalid_1's r2: 0.382053\n",
      "[265]\ttraining's r2: 0.550284\tvalid_1's r2: 0.382051\n",
      "[266]\ttraining's r2: 0.551453\tvalid_1's r2: 0.382407\n",
      "[267]\ttraining's r2: 0.552739\tvalid_1's r2: 0.382399\n",
      "[268]\ttraining's r2: 0.554174\tvalid_1's r2: 0.382833\n",
      "[269]\ttraining's r2: 0.555516\tvalid_1's r2: 0.383777\n",
      "[270]\ttraining's r2: 0.556545\tvalid_1's r2: 0.383342\n",
      "[271]\ttraining's r2: 0.557865\tvalid_1's r2: 0.383784\n",
      "[272]\ttraining's r2: 0.559064\tvalid_1's r2: 0.38418\n",
      "[273]\ttraining's r2: 0.560448\tvalid_1's r2: 0.384467\n",
      "[274]\ttraining's r2: 0.561664\tvalid_1's r2: 0.384426\n",
      "[275]\ttraining's r2: 0.562795\tvalid_1's r2: 0.384606\n",
      "[276]\ttraining's r2: 0.56404\tvalid_1's r2: 0.384774\n",
      "[277]\ttraining's r2: 0.565395\tvalid_1's r2: 0.385225\n",
      "[278]\ttraining's r2: 0.566842\tvalid_1's r2: 0.385248\n",
      "[279]\ttraining's r2: 0.568196\tvalid_1's r2: 0.385451\n",
      "[280]\ttraining's r2: 0.569421\tvalid_1's r2: 0.385489\n",
      "[281]\ttraining's r2: 0.570619\tvalid_1's r2: 0.385729\n",
      "[282]\ttraining's r2: 0.571722\tvalid_1's r2: 0.38643\n",
      "[283]\ttraining's r2: 0.572769\tvalid_1's r2: 0.386464\n",
      "[284]\ttraining's r2: 0.573946\tvalid_1's r2: 0.386373\n",
      "[285]\ttraining's r2: 0.574983\tvalid_1's r2: 0.386083\n",
      "[286]\ttraining's r2: 0.575963\tvalid_1's r2: 0.38628\n",
      "[287]\ttraining's r2: 0.577175\tvalid_1's r2: 0.386658\n",
      "[288]\ttraining's r2: 0.578243\tvalid_1's r2: 0.38735\n",
      "[289]\ttraining's r2: 0.579202\tvalid_1's r2: 0.387593\n",
      "[290]\ttraining's r2: 0.580348\tvalid_1's r2: 0.387923\n",
      "[291]\ttraining's r2: 0.581609\tvalid_1's r2: 0.388652\n",
      "[292]\ttraining's r2: 0.582658\tvalid_1's r2: 0.388562\n",
      "[293]\ttraining's r2: 0.58381\tvalid_1's r2: 0.388392\n",
      "[294]\ttraining's r2: 0.584914\tvalid_1's r2: 0.388629\n",
      "[295]\ttraining's r2: 0.586123\tvalid_1's r2: 0.38876\n",
      "[296]\ttraining's r2: 0.587335\tvalid_1's r2: 0.388941\n",
      "[297]\ttraining's r2: 0.588501\tvalid_1's r2: 0.389048\n",
      "[298]\ttraining's r2: 0.589844\tvalid_1's r2: 0.389602\n",
      "[299]\ttraining's r2: 0.591277\tvalid_1's r2: 0.390229\n",
      "[300]\ttraining's r2: 0.592403\tvalid_1's r2: 0.390339\n",
      "[301]\ttraining's r2: 0.593222\tvalid_1's r2: 0.390432\n",
      "[302]\ttraining's r2: 0.594433\tvalid_1's r2: 0.390614\n",
      "[303]\ttraining's r2: 0.595702\tvalid_1's r2: 0.391602\n",
      "[304]\ttraining's r2: 0.596485\tvalid_1's r2: 0.39163\n",
      "[305]\ttraining's r2: 0.597408\tvalid_1's r2: 0.391547\n",
      "[306]\ttraining's r2: 0.598541\tvalid_1's r2: 0.391524\n",
      "[307]\ttraining's r2: 0.599736\tvalid_1's r2: 0.391962\n",
      "[308]\ttraining's r2: 0.60057\tvalid_1's r2: 0.391619\n",
      "[309]\ttraining's r2: 0.601563\tvalid_1's r2: 0.391558\n",
      "[310]\ttraining's r2: 0.602573\tvalid_1's r2: 0.39167\n",
      "[311]\ttraining's r2: 0.603387\tvalid_1's r2: 0.391468\n",
      "[312]\ttraining's r2: 0.604313\tvalid_1's r2: 0.391724\n",
      "[313]\ttraining's r2: 0.605124\tvalid_1's r2: 0.391451\n",
      "[314]\ttraining's r2: 0.606057\tvalid_1's r2: 0.391373\n",
      "[315]\ttraining's r2: 0.606804\tvalid_1's r2: 0.391316\n",
      "[316]\ttraining's r2: 0.607641\tvalid_1's r2: 0.391623\n",
      "[317]\ttraining's r2: 0.608756\tvalid_1's r2: 0.392334\n",
      "[318]\ttraining's r2: 0.60951\tvalid_1's r2: 0.392063\n",
      "[319]\ttraining's r2: 0.610225\tvalid_1's r2: 0.39171\n",
      "[320]\ttraining's r2: 0.611076\tvalid_1's r2: 0.391536\n",
      "[321]\ttraining's r2: 0.611775\tvalid_1's r2: 0.390843\n",
      "[322]\ttraining's r2: 0.612318\tvalid_1's r2: 0.390771\n",
      "[323]\ttraining's r2: 0.613142\tvalid_1's r2: 0.390368\n",
      "[324]\ttraining's r2: 0.614005\tvalid_1's r2: 0.389981\n",
      "[325]\ttraining's r2: 0.614726\tvalid_1's r2: 0.38976\n",
      "[326]\ttraining's r2: 0.615596\tvalid_1's r2: 0.389456\n",
      "[327]\ttraining's r2: 0.616519\tvalid_1's r2: 0.389899\n",
      "[328]\ttraining's r2: 0.617439\tvalid_1's r2: 0.390038\n",
      "[329]\ttraining's r2: 0.618355\tvalid_1's r2: 0.390275\n",
      "[330]\ttraining's r2: 0.619175\tvalid_1's r2: 0.390343\n",
      "[331]\ttraining's r2: 0.62006\tvalid_1's r2: 0.390218\n",
      "[332]\ttraining's r2: 0.620916\tvalid_1's r2: 0.390277\n",
      "[333]\ttraining's r2: 0.62166\tvalid_1's r2: 0.390498\n",
      "[334]\ttraining's r2: 0.62261\tvalid_1's r2: 0.390805\n",
      "[335]\ttraining's r2: 0.623508\tvalid_1's r2: 0.391056\n",
      "[336]\ttraining's r2: 0.624361\tvalid_1's r2: 0.391407\n",
      "[337]\ttraining's r2: 0.625086\tvalid_1's r2: 0.391581\n",
      "[338]\ttraining's r2: 0.625916\tvalid_1's r2: 0.391488\n",
      "[339]\ttraining's r2: 0.626916\tvalid_1's r2: 0.391729\n",
      "[340]\ttraining's r2: 0.627966\tvalid_1's r2: 0.391782\n",
      "[341]\ttraining's r2: 0.628708\tvalid_1's r2: 0.391741\n",
      "[342]\ttraining's r2: 0.629732\tvalid_1's r2: 0.391982\n",
      "[343]\ttraining's r2: 0.630715\tvalid_1's r2: 0.39216\n",
      "[344]\ttraining's r2: 0.631605\tvalid_1's r2: 0.39229\n",
      "[345]\ttraining's r2: 0.632311\tvalid_1's r2: 0.392232\n",
      "[346]\ttraining's r2: 0.633088\tvalid_1's r2: 0.392124\n",
      "[347]\ttraining's r2: 0.634139\tvalid_1's r2: 0.392585\n",
      "[348]\ttraining's r2: 0.634934\tvalid_1's r2: 0.392265\n",
      "[349]\ttraining's r2: 0.635775\tvalid_1's r2: 0.392197\n",
      "[350]\ttraining's r2: 0.636712\tvalid_1's r2: 0.392477\n",
      "[351]\ttraining's r2: 0.637418\tvalid_1's r2: 0.392431\n",
      "[352]\ttraining's r2: 0.638484\tvalid_1's r2: 0.39273\n",
      "[353]\ttraining's r2: 0.639332\tvalid_1's r2: 0.392627\n",
      "[354]\ttraining's r2: 0.640135\tvalid_1's r2: 0.392674\n",
      "[355]\ttraining's r2: 0.64104\tvalid_1's r2: 0.392759\n",
      "[356]\ttraining's r2: 0.64184\tvalid_1's r2: 0.392453\n",
      "[357]\ttraining's r2: 0.642592\tvalid_1's r2: 0.392396\n",
      "[358]\ttraining's r2: 0.643594\tvalid_1's r2: 0.392761\n",
      "[359]\ttraining's r2: 0.644357\tvalid_1's r2: 0.393\n",
      "[360]\ttraining's r2: 0.64529\tvalid_1's r2: 0.393525\n",
      "[361]\ttraining's r2: 0.646247\tvalid_1's r2: 0.393741\n",
      "[362]\ttraining's r2: 0.647099\tvalid_1's r2: 0.394129\n",
      "[363]\ttraining's r2: 0.647989\tvalid_1's r2: 0.393962\n",
      "[364]\ttraining's r2: 0.648857\tvalid_1's r2: 0.39371\n",
      "[365]\ttraining's r2: 0.649579\tvalid_1's r2: 0.393514\n",
      "[366]\ttraining's r2: 0.65034\tvalid_1's r2: 0.393574\n",
      "[367]\ttraining's r2: 0.651086\tvalid_1's r2: 0.393181\n",
      "[368]\ttraining's r2: 0.651798\tvalid_1's r2: 0.392821\n",
      "[369]\ttraining's r2: 0.652702\tvalid_1's r2: 0.39319\n",
      "[370]\ttraining's r2: 0.653389\tvalid_1's r2: 0.393092\n",
      "[371]\ttraining's r2: 0.654288\tvalid_1's r2: 0.393278\n",
      "[372]\ttraining's r2: 0.655068\tvalid_1's r2: 0.393186\n",
      "[373]\ttraining's r2: 0.655854\tvalid_1's r2: 0.393271\n",
      "[374]\ttraining's r2: 0.656797\tvalid_1's r2: 0.393196\n",
      "[375]\ttraining's r2: 0.657632\tvalid_1's r2: 0.39321\n",
      "[376]\ttraining's r2: 0.658247\tvalid_1's r2: 0.392932\n",
      "[377]\ttraining's r2: 0.658975\tvalid_1's r2: 0.392838\n",
      "[378]\ttraining's r2: 0.659904\tvalid_1's r2: 0.393094\n",
      "[379]\ttraining's r2: 0.660586\tvalid_1's r2: 0.393532\n",
      "[380]\ttraining's r2: 0.661355\tvalid_1's r2: 0.393735\n",
      "[381]\ttraining's r2: 0.662175\tvalid_1's r2: 0.393671\n",
      "[382]\ttraining's r2: 0.663227\tvalid_1's r2: 0.393921\n",
      "[383]\ttraining's r2: 0.663948\tvalid_1's r2: 0.394032\n",
      "[384]\ttraining's r2: 0.664608\tvalid_1's r2: 0.393811\n",
      "[385]\ttraining's r2: 0.665382\tvalid_1's r2: 0.393523\n",
      "[386]\ttraining's r2: 0.666181\tvalid_1's r2: 0.393569\n",
      "[387]\ttraining's r2: 0.667014\tvalid_1's r2: 0.394066\n",
      "[388]\ttraining's r2: 0.667656\tvalid_1's r2: 0.394324\n",
      "[389]\ttraining's r2: 0.668321\tvalid_1's r2: 0.394272\n",
      "[390]\ttraining's r2: 0.669053\tvalid_1's r2: 0.394414\n",
      "[391]\ttraining's r2: 0.669745\tvalid_1's r2: 0.394348\n",
      "[392]\ttraining's r2: 0.670623\tvalid_1's r2: 0.394177\n",
      "[393]\ttraining's r2: 0.671587\tvalid_1's r2: 0.394337\n",
      "[394]\ttraining's r2: 0.672412\tvalid_1's r2: 0.394194\n",
      "[395]\ttraining's r2: 0.673297\tvalid_1's r2: 0.394461\n",
      "[396]\ttraining's r2: 0.67406\tvalid_1's r2: 0.394686\n",
      "[397]\ttraining's r2: 0.674768\tvalid_1's r2: 0.394681\n",
      "[398]\ttraining's r2: 0.675566\tvalid_1's r2: 0.394677\n",
      "[399]\ttraining's r2: 0.676547\tvalid_1's r2: 0.395219\n",
      "[400]\ttraining's r2: 0.677331\tvalid_1's r2: 0.395398\n",
      "[401]\ttraining's r2: 0.678044\tvalid_1's r2: 0.395328\n",
      "[402]\ttraining's r2: 0.678666\tvalid_1's r2: 0.395054\n",
      "[403]\ttraining's r2: 0.679269\tvalid_1's r2: 0.395093\n",
      "[404]\ttraining's r2: 0.679905\tvalid_1's r2: 0.394944\n",
      "[405]\ttraining's r2: 0.680714\tvalid_1's r2: 0.395342\n",
      "[406]\ttraining's r2: 0.681435\tvalid_1's r2: 0.395747\n",
      "[407]\ttraining's r2: 0.682146\tvalid_1's r2: 0.395855\n",
      "[408]\ttraining's r2: 0.682734\tvalid_1's r2: 0.396222\n",
      "[409]\ttraining's r2: 0.68356\tvalid_1's r2: 0.396213\n",
      "[410]\ttraining's r2: 0.684227\tvalid_1's r2: 0.396016\n",
      "[411]\ttraining's r2: 0.684803\tvalid_1's r2: 0.396008\n",
      "[412]\ttraining's r2: 0.685472\tvalid_1's r2: 0.396203\n",
      "[413]\ttraining's r2: 0.686209\tvalid_1's r2: 0.396338\n",
      "[414]\ttraining's r2: 0.686961\tvalid_1's r2: 0.396717\n",
      "[415]\ttraining's r2: 0.687642\tvalid_1's r2: 0.396621\n",
      "[416]\ttraining's r2: 0.688382\tvalid_1's r2: 0.396691\n",
      "[417]\ttraining's r2: 0.688992\tvalid_1's r2: 0.396715\n",
      "[418]\ttraining's r2: 0.689588\tvalid_1's r2: 0.396829\n",
      "[419]\ttraining's r2: 0.690311\tvalid_1's r2: 0.397092\n",
      "[420]\ttraining's r2: 0.690996\tvalid_1's r2: 0.397143\n",
      "[421]\ttraining's r2: 0.691768\tvalid_1's r2: 0.397263\n",
      "[422]\ttraining's r2: 0.692454\tvalid_1's r2: 0.397115\n",
      "[423]\ttraining's r2: 0.693123\tvalid_1's r2: 0.396972\n",
      "[424]\ttraining's r2: 0.693774\tvalid_1's r2: 0.396955\n",
      "[425]\ttraining's r2: 0.694366\tvalid_1's r2: 0.397197\n",
      "[426]\ttraining's r2: 0.694837\tvalid_1's r2: 0.396701\n",
      "[427]\ttraining's r2: 0.695574\tvalid_1's r2: 0.396696\n",
      "[428]\ttraining's r2: 0.696154\tvalid_1's r2: 0.396393\n",
      "[429]\ttraining's r2: 0.696762\tvalid_1's r2: 0.396611\n",
      "[430]\ttraining's r2: 0.697461\tvalid_1's r2: 0.396552\n",
      "[431]\ttraining's r2: 0.697922\tvalid_1's r2: 0.396273\n",
      "[432]\ttraining's r2: 0.698434\tvalid_1's r2: 0.396158\n",
      "[433]\ttraining's r2: 0.699158\tvalid_1's r2: 0.396366\n",
      "[434]\ttraining's r2: 0.69981\tvalid_1's r2: 0.396792\n",
      "[435]\ttraining's r2: 0.700359\tvalid_1's r2: 0.396777\n",
      "[436]\ttraining's r2: 0.700896\tvalid_1's r2: 0.396984\n",
      "[437]\ttraining's r2: 0.7014\tvalid_1's r2: 0.396625\n",
      "[438]\ttraining's r2: 0.70205\tvalid_1's r2: 0.397256\n",
      "[439]\ttraining's r2: 0.702661\tvalid_1's r2: 0.397356\n",
      "[440]\ttraining's r2: 0.703319\tvalid_1's r2: 0.397164\n",
      "[441]\ttraining's r2: 0.703777\tvalid_1's r2: 0.396927\n",
      "[442]\ttraining's r2: 0.704255\tvalid_1's r2: 0.397017\n",
      "[443]\ttraining's r2: 0.704825\tvalid_1's r2: 0.396949\n",
      "[444]\ttraining's r2: 0.705524\tvalid_1's r2: 0.397277\n",
      "[445]\ttraining's r2: 0.706154\tvalid_1's r2: 0.397534\n",
      "[446]\ttraining's r2: 0.706613\tvalid_1's r2: 0.397425\n",
      "[447]\ttraining's r2: 0.707145\tvalid_1's r2: 0.397079\n",
      "[448]\ttraining's r2: 0.707763\tvalid_1's r2: 0.397071\n",
      "[449]\ttraining's r2: 0.708269\tvalid_1's r2: 0.396596\n",
      "[450]\ttraining's r2: 0.708853\tvalid_1's r2: 0.396279\n",
      "[451]\ttraining's r2: 0.709458\tvalid_1's r2: 0.395976\n",
      "[452]\ttraining's r2: 0.710032\tvalid_1's r2: 0.396053\n",
      "[453]\ttraining's r2: 0.710522\tvalid_1's r2: 0.396316\n",
      "[454]\ttraining's r2: 0.711128\tvalid_1's r2: 0.39624\n",
      "[455]\ttraining's r2: 0.711905\tvalid_1's r2: 0.396093\n",
      "[456]\ttraining's r2: 0.712653\tvalid_1's r2: 0.39667\n",
      "[457]\ttraining's r2: 0.713225\tvalid_1's r2: 0.396725\n",
      "[458]\ttraining's r2: 0.713669\tvalid_1's r2: 0.396746\n",
      "[459]\ttraining's r2: 0.714274\tvalid_1's r2: 0.397029\n",
      "[460]\ttraining's r2: 0.714879\tvalid_1's r2: 0.396933\n",
      "[461]\ttraining's r2: 0.71546\tvalid_1's r2: 0.396688\n",
      "[462]\ttraining's r2: 0.716189\tvalid_1's r2: 0.396626\n",
      "[463]\ttraining's r2: 0.716859\tvalid_1's r2: 0.396612\n",
      "[464]\ttraining's r2: 0.717412\tvalid_1's r2: 0.39641\n",
      "[465]\ttraining's r2: 0.717909\tvalid_1's r2: 0.396855\n",
      "[466]\ttraining's r2: 0.718587\tvalid_1's r2: 0.396862\n",
      "[467]\ttraining's r2: 0.719321\tvalid_1's r2: 0.397014\n",
      "[468]\ttraining's r2: 0.719963\tvalid_1's r2: 0.397284\n",
      "[469]\ttraining's r2: 0.720535\tvalid_1's r2: 0.397164\n",
      "[470]\ttraining's r2: 0.721141\tvalid_1's r2: 0.397313\n",
      "[471]\ttraining's r2: 0.721796\tvalid_1's r2: 0.397521\n",
      "[472]\ttraining's r2: 0.72231\tvalid_1's r2: 0.397405\n",
      "[473]\ttraining's r2: 0.722786\tvalid_1's r2: 0.397207\n",
      "[474]\ttraining's r2: 0.723416\tvalid_1's r2: 0.397161\n",
      "[475]\ttraining's r2: 0.724112\tvalid_1's r2: 0.397303\n",
      "[476]\ttraining's r2: 0.724688\tvalid_1's r2: 0.397104\n",
      "[477]\ttraining's r2: 0.725241\tvalid_1's r2: 0.397001\n",
      "[478]\ttraining's r2: 0.725877\tvalid_1's r2: 0.396931\n",
      "[479]\ttraining's r2: 0.726354\tvalid_1's r2: 0.39664\n",
      "[480]\ttraining's r2: 0.726925\tvalid_1's r2: 0.396632\n",
      "[481]\ttraining's r2: 0.727503\tvalid_1's r2: 0.396878\n",
      "[482]\ttraining's r2: 0.728042\tvalid_1's r2: 0.396795\n",
      "[483]\ttraining's r2: 0.728564\tvalid_1's r2: 0.396737\n",
      "[484]\ttraining's r2: 0.729146\tvalid_1's r2: 0.396748\n",
      "[485]\ttraining's r2: 0.729808\tvalid_1's r2: 0.396972\n",
      "[486]\ttraining's r2: 0.730377\tvalid_1's r2: 0.396943\n",
      "[487]\ttraining's r2: 0.730974\tvalid_1's r2: 0.397273\n",
      "[488]\ttraining's r2: 0.731454\tvalid_1's r2: 0.396789\n",
      "[489]\ttraining's r2: 0.731977\tvalid_1's r2: 0.396793\n",
      "[490]\ttraining's r2: 0.732626\tvalid_1's r2: 0.397081\n",
      "[491]\ttraining's r2: 0.733106\tvalid_1's r2: 0.396895\n",
      "[492]\ttraining's r2: 0.733517\tvalid_1's r2: 0.396706\n",
      "[493]\ttraining's r2: 0.7341\tvalid_1's r2: 0.397027\n",
      "[494]\ttraining's r2: 0.734626\tvalid_1's r2: 0.397059\n",
      "[495]\ttraining's r2: 0.734999\tvalid_1's r2: 0.39728\n",
      "[496]\ttraining's r2: 0.735426\tvalid_1's r2: 0.396869\n",
      "[497]\ttraining's r2: 0.735941\tvalid_1's r2: 0.396843\n",
      "[498]\ttraining's r2: 0.736595\tvalid_1's r2: 0.397024\n",
      "[499]\ttraining's r2: 0.737042\tvalid_1's r2: 0.396679\n",
      "[500]\ttraining's r2: 0.737492\tvalid_1's r2: 0.396785\n",
      "[501]\ttraining's r2: 0.737894\tvalid_1's r2: 0.396579\n",
      "[502]\ttraining's r2: 0.738272\tvalid_1's r2: 0.396707\n",
      "[503]\ttraining's r2: 0.738827\tvalid_1's r2: 0.397238\n",
      "[504]\ttraining's r2: 0.739272\tvalid_1's r2: 0.396928\n",
      "[505]\ttraining's r2: 0.739729\tvalid_1's r2: 0.396723\n",
      "[506]\ttraining's r2: 0.740138\tvalid_1's r2: 0.396522\n",
      "[507]\ttraining's r2: 0.740731\tvalid_1's r2: 0.396549\n",
      "[508]\ttraining's r2: 0.741267\tvalid_1's r2: 0.396759\n",
      "[509]\ttraining's r2: 0.741816\tvalid_1's r2: 0.396597\n",
      "[510]\ttraining's r2: 0.742508\tvalid_1's r2: 0.397066\n",
      "[511]\ttraining's r2: 0.743005\tvalid_1's r2: 0.397148\n",
      "[512]\ttraining's r2: 0.743543\tvalid_1's r2: 0.397192\n",
      "[513]\ttraining's r2: 0.744124\tvalid_1's r2: 0.397122\n",
      "[514]\ttraining's r2: 0.744522\tvalid_1's r2: 0.39704\n",
      "[515]\ttraining's r2: 0.744916\tvalid_1's r2: 0.396897\n",
      "[516]\ttraining's r2: 0.7453\tvalid_1's r2: 0.396637\n",
      "[517]\ttraining's r2: 0.745706\tvalid_1's r2: 0.396587\n",
      "[518]\ttraining's r2: 0.746311\tvalid_1's r2: 0.396941\n",
      "[519]\ttraining's r2: 0.746885\tvalid_1's r2: 0.397558\n",
      "[520]\ttraining's r2: 0.747316\tvalid_1's r2: 0.397396\n",
      "[521]\ttraining's r2: 0.747899\tvalid_1's r2: 0.397716\n",
      "[522]\ttraining's r2: 0.748402\tvalid_1's r2: 0.398123\n",
      "[523]\ttraining's r2: 0.748869\tvalid_1's r2: 0.398135\n",
      "[524]\ttraining's r2: 0.749329\tvalid_1's r2: 0.398277\n",
      "[525]\ttraining's r2: 0.749657\tvalid_1's r2: 0.398311\n",
      "[526]\ttraining's r2: 0.750122\tvalid_1's r2: 0.398296\n",
      "[527]\ttraining's r2: 0.750616\tvalid_1's r2: 0.398301\n",
      "[528]\ttraining's r2: 0.751053\tvalid_1's r2: 0.398612\n",
      "[529]\ttraining's r2: 0.751472\tvalid_1's r2: 0.398297\n",
      "[530]\ttraining's r2: 0.751869\tvalid_1's r2: 0.398064\n",
      "[531]\ttraining's r2: 0.752338\tvalid_1's r2: 0.398065\n",
      "[532]\ttraining's r2: 0.752826\tvalid_1's r2: 0.398217\n",
      "[533]\ttraining's r2: 0.753419\tvalid_1's r2: 0.398343\n",
      "[534]\ttraining's r2: 0.753781\tvalid_1's r2: 0.398404\n",
      "[535]\ttraining's r2: 0.754272\tvalid_1's r2: 0.398292\n",
      "[536]\ttraining's r2: 0.754894\tvalid_1's r2: 0.399001\n",
      "[537]\ttraining's r2: 0.755557\tvalid_1's r2: 0.399392\n",
      "[538]\ttraining's r2: 0.756064\tvalid_1's r2: 0.399453\n",
      "[539]\ttraining's r2: 0.756502\tvalid_1's r2: 0.399492\n",
      "[540]\ttraining's r2: 0.756926\tvalid_1's r2: 0.399717\n",
      "[541]\ttraining's r2: 0.757337\tvalid_1's r2: 0.399932\n",
      "[542]\ttraining's r2: 0.757687\tvalid_1's r2: 0.399843\n",
      "[543]\ttraining's r2: 0.75809\tvalid_1's r2: 0.399672\n",
      "[544]\ttraining's r2: 0.758472\tvalid_1's r2: 0.399551\n",
      "[545]\ttraining's r2: 0.758766\tvalid_1's r2: 0.399151\n",
      "[546]\ttraining's r2: 0.759237\tvalid_1's r2: 0.399338\n",
      "[547]\ttraining's r2: 0.759689\tvalid_1's r2: 0.400021\n",
      "[548]\ttraining's r2: 0.760069\tvalid_1's r2: 0.400264\n",
      "[549]\ttraining's r2: 0.760477\tvalid_1's r2: 0.399966\n",
      "[550]\ttraining's r2: 0.760792\tvalid_1's r2: 0.399845\n",
      "[551]\ttraining's r2: 0.761063\tvalid_1's r2: 0.399863\n",
      "[552]\ttraining's r2: 0.761481\tvalid_1's r2: 0.399807\n",
      "[553]\ttraining's r2: 0.761859\tvalid_1's r2: 0.399955\n",
      "[554]\ttraining's r2: 0.762196\tvalid_1's r2: 0.399865\n",
      "[555]\ttraining's r2: 0.7626\tvalid_1's r2: 0.399945\n",
      "[556]\ttraining's r2: 0.763005\tvalid_1's r2: 0.399423\n",
      "[557]\ttraining's r2: 0.763499\tvalid_1's r2: 0.399602\n",
      "[558]\ttraining's r2: 0.763902\tvalid_1's r2: 0.39954\n",
      "[559]\ttraining's r2: 0.764201\tvalid_1's r2: 0.399596\n",
      "[560]\ttraining's r2: 0.764615\tvalid_1's r2: 0.399667\n",
      "[561]\ttraining's r2: 0.764939\tvalid_1's r2: 0.399822\n",
      "[562]\ttraining's r2: 0.765436\tvalid_1's r2: 0.400243\n",
      "[563]\ttraining's r2: 0.765879\tvalid_1's r2: 0.400152\n",
      "[564]\ttraining's r2: 0.766366\tvalid_1's r2: 0.400402\n",
      "[565]\ttraining's r2: 0.766728\tvalid_1's r2: 0.400438\n",
      "[566]\ttraining's r2: 0.767223\tvalid_1's r2: 0.40042\n",
      "[567]\ttraining's r2: 0.767704\tvalid_1's r2: 0.400241\n",
      "[568]\ttraining's r2: 0.768118\tvalid_1's r2: 0.399966\n",
      "[569]\ttraining's r2: 0.768514\tvalid_1's r2: 0.399976\n",
      "[570]\ttraining's r2: 0.768943\tvalid_1's r2: 0.400066\n",
      "[571]\ttraining's r2: 0.769461\tvalid_1's r2: 0.4002\n",
      "[572]\ttraining's r2: 0.769849\tvalid_1's r2: 0.400284\n",
      "[573]\ttraining's r2: 0.770285\tvalid_1's r2: 0.400006\n",
      "[574]\ttraining's r2: 0.770709\tvalid_1's r2: 0.399801\n",
      "[575]\ttraining's r2: 0.771239\tvalid_1's r2: 0.400048\n",
      "[576]\ttraining's r2: 0.771641\tvalid_1's r2: 0.400133\n",
      "[577]\ttraining's r2: 0.772052\tvalid_1's r2: 0.400192\n",
      "[578]\ttraining's r2: 0.772453\tvalid_1's r2: 0.400107\n",
      "[579]\ttraining's r2: 0.772827\tvalid_1's r2: 0.400171\n",
      "[580]\ttraining's r2: 0.773278\tvalid_1's r2: 0.399991\n",
      "[581]\ttraining's r2: 0.773575\tvalid_1's r2: 0.400243\n",
      "[582]\ttraining's r2: 0.773981\tvalid_1's r2: 0.400301\n",
      "[583]\ttraining's r2: 0.77439\tvalid_1's r2: 0.400219\n",
      "[584]\ttraining's r2: 0.774804\tvalid_1's r2: 0.400416\n",
      "[585]\ttraining's r2: 0.775195\tvalid_1's r2: 0.400228\n",
      "[586]\ttraining's r2: 0.775523\tvalid_1's r2: 0.400247\n",
      "[587]\ttraining's r2: 0.775902\tvalid_1's r2: 0.400111\n",
      "[588]\ttraining's r2: 0.776381\tvalid_1's r2: 0.400192\n",
      "[589]\ttraining's r2: 0.776735\tvalid_1's r2: 0.399911\n",
      "[590]\ttraining's r2: 0.777141\tvalid_1's r2: 0.39975\n",
      "[591]\ttraining's r2: 0.777599\tvalid_1's r2: 0.399691\n",
      "[592]\ttraining's r2: 0.777988\tvalid_1's r2: 0.399659\n",
      "[593]\ttraining's r2: 0.778438\tvalid_1's r2: 0.39981\n",
      "[594]\ttraining's r2: 0.778882\tvalid_1's r2: 0.399773\n",
      "[595]\ttraining's r2: 0.779361\tvalid_1's r2: 0.399851\n",
      "[596]\ttraining's r2: 0.779759\tvalid_1's r2: 0.399868\n",
      "[597]\ttraining's r2: 0.780085\tvalid_1's r2: 0.400008\n",
      "[598]\ttraining's r2: 0.780447\tvalid_1's r2: 0.399942\n",
      "[599]\ttraining's r2: 0.78079\tvalid_1's r2: 0.399944\n",
      "[600]\ttraining's r2: 0.78124\tvalid_1's r2: 0.400064\n",
      "[601]\ttraining's r2: 0.781655\tvalid_1's r2: 0.400301\n",
      "[602]\ttraining's r2: 0.782056\tvalid_1's r2: 0.400172\n",
      "[603]\ttraining's r2: 0.782443\tvalid_1's r2: 0.400153\n",
      "[604]\ttraining's r2: 0.782796\tvalid_1's r2: 0.400043\n",
      "[605]\ttraining's r2: 0.783172\tvalid_1's r2: 0.399921\n",
      "[606]\ttraining's r2: 0.783509\tvalid_1's r2: 0.399837\n",
      "[607]\ttraining's r2: 0.783899\tvalid_1's r2: 0.400112\n",
      "[608]\ttraining's r2: 0.784336\tvalid_1's r2: 0.399766\n",
      "[609]\ttraining's r2: 0.784719\tvalid_1's r2: 0.40005\n",
      "[610]\ttraining's r2: 0.785084\tvalid_1's r2: 0.399923\n",
      "[611]\ttraining's r2: 0.785421\tvalid_1's r2: 0.399935\n",
      "[612]\ttraining's r2: 0.785766\tvalid_1's r2: 0.399806\n",
      "[613]\ttraining's r2: 0.786148\tvalid_1's r2: 0.399644\n",
      "[614]\ttraining's r2: 0.786533\tvalid_1's r2: 0.39978\n",
      "[615]\ttraining's r2: 0.786894\tvalid_1's r2: 0.399646\n",
      "[616]\ttraining's r2: 0.78727\tvalid_1's r2: 0.399623\n",
      "[617]\ttraining's r2: 0.787524\tvalid_1's r2: 0.399403\n",
      "[618]\ttraining's r2: 0.787918\tvalid_1's r2: 0.399419\n",
      "[619]\ttraining's r2: 0.788287\tvalid_1's r2: 0.399465\n",
      "[620]\ttraining's r2: 0.788698\tvalid_1's r2: 0.399373\n",
      "[621]\ttraining's r2: 0.789069\tvalid_1's r2: 0.3994\n",
      "[622]\ttraining's r2: 0.789394\tvalid_1's r2: 0.399242\n",
      "[623]\ttraining's r2: 0.789718\tvalid_1's r2: 0.399044\n",
      "[624]\ttraining's r2: 0.790059\tvalid_1's r2: 0.3991\n",
      "[625]\ttraining's r2: 0.790434\tvalid_1's r2: 0.399199\n",
      "[626]\ttraining's r2: 0.790853\tvalid_1's r2: 0.39935\n",
      "[627]\ttraining's r2: 0.791212\tvalid_1's r2: 0.399297\n",
      "[628]\ttraining's r2: 0.791586\tvalid_1's r2: 0.399416\n",
      "[629]\ttraining's r2: 0.791929\tvalid_1's r2: 0.399442\n",
      "[630]\ttraining's r2: 0.792327\tvalid_1's r2: 0.399156\n",
      "[631]\ttraining's r2: 0.792689\tvalid_1's r2: 0.399365\n",
      "[632]\ttraining's r2: 0.793071\tvalid_1's r2: 0.399532\n",
      "[633]\ttraining's r2: 0.793468\tvalid_1's r2: 0.399665\n",
      "[634]\ttraining's r2: 0.793908\tvalid_1's r2: 0.399815\n",
      "[635]\ttraining's r2: 0.794309\tvalid_1's r2: 0.399918\n",
      "[636]\ttraining's r2: 0.794625\tvalid_1's r2: 0.399644\n",
      "[637]\ttraining's r2: 0.794977\tvalid_1's r2: 0.399523\n",
      "[638]\ttraining's r2: 0.795324\tvalid_1's r2: 0.399414\n",
      "[639]\ttraining's r2: 0.795577\tvalid_1's r2: 0.399222\n",
      "[640]\ttraining's r2: 0.795977\tvalid_1's r2: 0.399489\n",
      "[641]\ttraining's r2: 0.796245\tvalid_1's r2: 0.399317\n",
      "[642]\ttraining's r2: 0.796593\tvalid_1's r2: 0.399151\n",
      "[643]\ttraining's r2: 0.79702\tvalid_1's r2: 0.399266\n",
      "[644]\ttraining's r2: 0.797434\tvalid_1's r2: 0.399088\n",
      "[645]\ttraining's r2: 0.797782\tvalid_1's r2: 0.398856\n",
      "[646]\ttraining's r2: 0.798154\tvalid_1's r2: 0.398641\n",
      "[647]\ttraining's r2: 0.7985\tvalid_1's r2: 0.398496\n",
      "[648]\ttraining's r2: 0.798851\tvalid_1's r2: 0.398252\n",
      "[649]\ttraining's r2: 0.79927\tvalid_1's r2: 0.398348\n",
      "[650]\ttraining's r2: 0.799687\tvalid_1's r2: 0.398366\n",
      "[651]\ttraining's r2: 0.799973\tvalid_1's r2: 0.39844\n",
      "[652]\ttraining's r2: 0.800303\tvalid_1's r2: 0.398285\n",
      "[653]\ttraining's r2: 0.800657\tvalid_1's r2: 0.398238\n",
      "[654]\ttraining's r2: 0.800946\tvalid_1's r2: 0.398182\n",
      "[655]\ttraining's r2: 0.801276\tvalid_1's r2: 0.398172\n",
      "[656]\ttraining's r2: 0.801616\tvalid_1's r2: 0.398139\n",
      "[657]\ttraining's r2: 0.801876\tvalid_1's r2: 0.397959\n",
      "[658]\ttraining's r2: 0.80214\tvalid_1's r2: 0.39803\n",
      "[659]\ttraining's r2: 0.802437\tvalid_1's r2: 0.398151\n",
      "[660]\ttraining's r2: 0.802778\tvalid_1's r2: 0.398248\n",
      "[661]\ttraining's r2: 0.803081\tvalid_1's r2: 0.398299\n",
      "[662]\ttraining's r2: 0.803238\tvalid_1's r2: 0.398053\n",
      "[663]\ttraining's r2: 0.803449\tvalid_1's r2: 0.398074\n",
      "[664]\ttraining's r2: 0.803767\tvalid_1's r2: 0.398285\n",
      "[665]\ttraining's r2: 0.803978\tvalid_1's r2: 0.398179\n",
      "Early stopping, best iteration is:\n",
      "[565]\ttraining's r2: 0.766728\tvalid_1's r2: 0.400438\n",
      "[1]\ttraining's r2: -3474.54\tvalid_1's r2: -3546.68\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[2]\ttraining's r2: -824.415\tvalid_1's r2: -848.688\n",
      "[3]\ttraining's r2: -355.637\tvalid_1's r2: -362.215\n",
      "[4]\ttraining's r2: -195.17\tvalid_1's r2: -198.175\n",
      "[5]\ttraining's r2: -123.1\tvalid_1's r2: -124.417\n",
      "[6]\ttraining's r2: -85.3255\tvalid_1's r2: -86.2078\n",
      "[7]\ttraining's r2: -62.3144\tvalid_1's r2: -62.9579\n",
      "[8]\ttraining's r2: -47.2082\tvalid_1's r2: -47.6772\n",
      "[9]\ttraining's r2: -36.8054\tvalid_1's r2: -37.2069\n",
      "[10]\ttraining's r2: -29.8486\tvalid_1's r2: -30.3248\n",
      "[11]\ttraining's r2: -24.4803\tvalid_1's r2: -24.8206\n",
      "[12]\ttraining's r2: -20.4801\tvalid_1's r2: -20.7501\n",
      "[13]\ttraining's r2: -17.3767\tvalid_1's r2: -17.6301\n",
      "[14]\ttraining's r2: -14.9139\tvalid_1's r2: -15.1778\n",
      "[15]\ttraining's r2: -12.9455\tvalid_1's r2: -13.1565\n",
      "[16]\ttraining's r2: -11.2891\tvalid_1's r2: -11.4641\n",
      "[17]\ttraining's r2: -9.96327\tvalid_1's r2: -10.1374\n",
      "[18]\ttraining's r2: -8.84273\tvalid_1's r2: -8.98215\n",
      "[19]\ttraining's r2: -7.89674\tvalid_1's r2: -8.03576\n",
      "[20]\ttraining's r2: -7.07587\tvalid_1's r2: -7.21946\n",
      "[21]\ttraining's r2: -6.36034\tvalid_1's r2: -6.50222\n",
      "[22]\ttraining's r2: -5.75443\tvalid_1's r2: -5.89005\n",
      "[23]\ttraining's r2: -5.24068\tvalid_1's r2: -5.37992\n",
      "[24]\ttraining's r2: -4.77643\tvalid_1's r2: -4.90327\n",
      "[25]\ttraining's r2: -4.38092\tvalid_1's r2: -4.51177\n",
      "[26]\ttraining's r2: -4.01533\tvalid_1's r2: -4.14122\n",
      "[27]\ttraining's r2: -3.69207\tvalid_1's r2: -3.81883\n",
      "[28]\ttraining's r2: -3.38664\tvalid_1's r2: -3.51421\n",
      "[29]\ttraining's r2: -3.13438\tvalid_1's r2: -3.25287\n",
      "[30]\ttraining's r2: -2.90499\tvalid_1's r2: -3.01555\n",
      "[31]\ttraining's r2: -2.68417\tvalid_1's r2: -2.79377\n",
      "[32]\ttraining's r2: -2.50152\tvalid_1's r2: -2.61511\n",
      "[33]\ttraining's r2: -2.32927\tvalid_1's r2: -2.44225\n",
      "[34]\ttraining's r2: -2.17336\tvalid_1's r2: -2.28726\n",
      "[35]\ttraining's r2: -2.03222\tvalid_1's r2: -2.14417\n",
      "[36]\ttraining's r2: -1.90313\tvalid_1's r2: -2.01279\n",
      "[37]\ttraining's r2: -1.77994\tvalid_1's r2: -1.88193\n",
      "[38]\ttraining's r2: -1.6642\tvalid_1's r2: -1.76558\n",
      "[39]\ttraining's r2: -1.55702\tvalid_1's r2: -1.65588\n",
      "[40]\ttraining's r2: -1.46099\tvalid_1's r2: -1.55749\n",
      "[41]\ttraining's r2: -1.37151\tvalid_1's r2: -1.46722\n",
      "[42]\ttraining's r2: -1.28662\tvalid_1's r2: -1.38072\n",
      "[43]\ttraining's r2: -1.21159\tvalid_1's r2: -1.30553\n",
      "[44]\ttraining's r2: -1.13564\tvalid_1's r2: -1.22758\n",
      "[45]\ttraining's r2: -1.06671\tvalid_1's r2: -1.15664\n",
      "[46]\ttraining's r2: -1.00443\tvalid_1's r2: -1.09667\n",
      "[47]\ttraining's r2: -0.947253\tvalid_1's r2: -1.04039\n",
      "[48]\ttraining's r2: -0.888526\tvalid_1's r2: -0.980211\n",
      "[49]\ttraining's r2: -0.835256\tvalid_1's r2: -0.925428\n",
      "[50]\ttraining's r2: -0.786351\tvalid_1's r2: -0.877617\n",
      "[51]\ttraining's r2: -0.741262\tvalid_1's r2: -0.834891\n",
      "[52]\ttraining's r2: -0.696425\tvalid_1's r2: -0.788904\n",
      "[53]\ttraining's r2: -0.65628\tvalid_1's r2: -0.748553\n",
      "[54]\ttraining's r2: -0.615459\tvalid_1's r2: -0.70815\n",
      "[55]\ttraining's r2: -0.577084\tvalid_1's r2: -0.669612\n",
      "[56]\ttraining's r2: -0.540178\tvalid_1's r2: -0.631338\n",
      "[57]\ttraining's r2: -0.505053\tvalid_1's r2: -0.597131\n",
      "[58]\ttraining's r2: -0.474889\tvalid_1's r2: -0.565399\n",
      "[59]\ttraining's r2: -0.444368\tvalid_1's r2: -0.534876\n",
      "[60]\ttraining's r2: -0.416454\tvalid_1's r2: -0.507605\n",
      "[61]\ttraining's r2: -0.38795\tvalid_1's r2: -0.478335\n",
      "[62]\ttraining's r2: -0.362367\tvalid_1's r2: -0.452989\n",
      "[63]\ttraining's r2: -0.337368\tvalid_1's r2: -0.428126\n",
      "[64]\ttraining's r2: -0.314114\tvalid_1's r2: -0.405983\n",
      "[65]\ttraining's r2: -0.290141\tvalid_1's r2: -0.382362\n",
      "[66]\ttraining's r2: -0.267577\tvalid_1's r2: -0.357876\n",
      "[67]\ttraining's r2: -0.245296\tvalid_1's r2: -0.335918\n",
      "[68]\ttraining's r2: -0.223963\tvalid_1's r2: -0.316223\n",
      "[69]\ttraining's r2: -0.204269\tvalid_1's r2: -0.296207\n",
      "[70]\ttraining's r2: -0.186768\tvalid_1's r2: -0.279609\n",
      "[71]\ttraining's r2: -0.16968\tvalid_1's r2: -0.26108\n",
      "[72]\ttraining's r2: -0.152708\tvalid_1's r2: -0.245135\n",
      "[73]\ttraining's r2: -0.136157\tvalid_1's r2: -0.228708\n",
      "[74]\ttraining's r2: -0.118108\tvalid_1's r2: -0.211412\n",
      "[75]\ttraining's r2: -0.10308\tvalid_1's r2: -0.197481\n",
      "[76]\ttraining's r2: -0.0891886\tvalid_1's r2: -0.184942\n",
      "[77]\ttraining's r2: -0.0761938\tvalid_1's r2: -0.17181\n",
      "[78]\ttraining's r2: -0.0611214\tvalid_1's r2: -0.157756\n",
      "[79]\ttraining's r2: -0.0477765\tvalid_1's r2: -0.145911\n",
      "[80]\ttraining's r2: -0.0340647\tvalid_1's r2: -0.132004\n",
      "[81]\ttraining's r2: -0.0208374\tvalid_1's r2: -0.119932\n",
      "[82]\ttraining's r2: -0.00862009\tvalid_1's r2: -0.108099\n",
      "[83]\ttraining's r2: 0.00221583\tvalid_1's r2: -0.0978859\n",
      "[84]\ttraining's r2: 0.0140985\tvalid_1's r2: -0.0877251\n",
      "[85]\ttraining's r2: 0.0261766\tvalid_1's r2: -0.0755461\n",
      "[86]\ttraining's r2: 0.036354\tvalid_1's r2: -0.0656874\n",
      "[87]\ttraining's r2: 0.0464329\tvalid_1's r2: -0.0574878\n",
      "[88]\ttraining's r2: 0.0583922\tvalid_1's r2: -0.047038\n",
      "[89]\ttraining's r2: 0.0681214\tvalid_1's r2: -0.0380605\n",
      "[90]\ttraining's r2: 0.0781267\tvalid_1's r2: -0.0285923\n",
      "[91]\ttraining's r2: 0.0869569\tvalid_1's r2: -0.0196359\n",
      "[92]\ttraining's r2: 0.095882\tvalid_1's r2: -0.0119591\n",
      "[93]\ttraining's r2: 0.105495\tvalid_1's r2: -0.00314857\n",
      "[94]\ttraining's r2: 0.113994\tvalid_1's r2: 0.00469124\n",
      "[95]\ttraining's r2: 0.122615\tvalid_1's r2: 0.0130986\n",
      "[96]\ttraining's r2: 0.130652\tvalid_1's r2: 0.0214395\n",
      "[97]\ttraining's r2: 0.138321\tvalid_1's r2: 0.0291221\n",
      "[98]\ttraining's r2: 0.146362\tvalid_1's r2: 0.0361368\n",
      "[99]\ttraining's r2: 0.153434\tvalid_1's r2: 0.0417195\n",
      "[100]\ttraining's r2: 0.161133\tvalid_1's r2: 0.0478933\n",
      "[101]\ttraining's r2: 0.168489\tvalid_1's r2: 0.0536902\n",
      "[102]\ttraining's r2: 0.175709\tvalid_1's r2: 0.0603909\n",
      "[103]\ttraining's r2: 0.182543\tvalid_1's r2: 0.0664641\n",
      "[104]\ttraining's r2: 0.189046\tvalid_1's r2: 0.0721656\n",
      "[105]\ttraining's r2: 0.193665\tvalid_1's r2: 0.0750306\n",
      "[106]\ttraining's r2: 0.199758\tvalid_1's r2: 0.0791039\n",
      "[107]\ttraining's r2: 0.205737\tvalid_1's r2: 0.0846562\n",
      "[108]\ttraining's r2: 0.212196\tvalid_1's r2: 0.0899378\n",
      "[109]\ttraining's r2: 0.218638\tvalid_1's r2: 0.094962\n",
      "[110]\ttraining's r2: 0.224216\tvalid_1's r2: 0.100277\n",
      "[111]\ttraining's r2: 0.230028\tvalid_1's r2: 0.105314\n",
      "[112]\ttraining's r2: 0.23504\tvalid_1's r2: 0.109063\n",
      "[113]\ttraining's r2: 0.240109\tvalid_1's r2: 0.113057\n",
      "[114]\ttraining's r2: 0.245977\tvalid_1's r2: 0.117556\n",
      "[115]\ttraining's r2: 0.251726\tvalid_1's r2: 0.122614\n",
      "[116]\ttraining's r2: 0.256578\tvalid_1's r2: 0.126186\n",
      "[117]\ttraining's r2: 0.260733\tvalid_1's r2: 0.128138\n",
      "[118]\ttraining's r2: 0.265437\tvalid_1's r2: 0.132059\n",
      "[119]\ttraining's r2: 0.270224\tvalid_1's r2: 0.135989\n",
      "[120]\ttraining's r2: 0.273724\tvalid_1's r2: 0.137087\n",
      "[121]\ttraining's r2: 0.277742\tvalid_1's r2: 0.139217\n",
      "[122]\ttraining's r2: 0.281345\tvalid_1's r2: 0.141519\n",
      "[123]\ttraining's r2: 0.284722\tvalid_1's r2: 0.143614\n",
      "[124]\ttraining's r2: 0.289333\tvalid_1's r2: 0.147866\n",
      "[125]\ttraining's r2: 0.292706\tvalid_1's r2: 0.149849\n",
      "[126]\ttraining's r2: 0.297096\tvalid_1's r2: 0.153437\n",
      "[127]\ttraining's r2: 0.3009\tvalid_1's r2: 0.155446\n",
      "[128]\ttraining's r2: 0.304173\tvalid_1's r2: 0.156768\n",
      "[129]\ttraining's r2: 0.308817\tvalid_1's r2: 0.160148\n",
      "[130]\ttraining's r2: 0.312816\tvalid_1's r2: 0.16159\n",
      "[131]\ttraining's r2: 0.315867\tvalid_1's r2: 0.163397\n",
      "[132]\ttraining's r2: 0.319184\tvalid_1's r2: 0.164894\n",
      "[133]\ttraining's r2: 0.322584\tvalid_1's r2: 0.166567\n",
      "[134]\ttraining's r2: 0.324987\tvalid_1's r2: 0.168012\n",
      "[135]\ttraining's r2: 0.328078\tvalid_1's r2: 0.169474\n",
      "[136]\ttraining's r2: 0.330533\tvalid_1's r2: 0.170396\n",
      "[137]\ttraining's r2: 0.333727\tvalid_1's r2: 0.172478\n",
      "[138]\ttraining's r2: 0.336791\tvalid_1's r2: 0.174388\n",
      "[139]\ttraining's r2: 0.339395\tvalid_1's r2: 0.175871\n",
      "[140]\ttraining's r2: 0.342982\tvalid_1's r2: 0.178802\n",
      "[141]\ttraining's r2: 0.34577\tvalid_1's r2: 0.179537\n",
      "[142]\ttraining's r2: 0.348539\tvalid_1's r2: 0.180746\n",
      "[143]\ttraining's r2: 0.351043\tvalid_1's r2: 0.182165\n",
      "[144]\ttraining's r2: 0.3537\tvalid_1's r2: 0.18345\n",
      "[145]\ttraining's r2: 0.355883\tvalid_1's r2: 0.183766\n",
      "[146]\ttraining's r2: 0.359048\tvalid_1's r2: 0.185736\n",
      "[147]\ttraining's r2: 0.362087\tvalid_1's r2: 0.188449\n",
      "[148]\ttraining's r2: 0.364823\tvalid_1's r2: 0.190128\n",
      "[149]\ttraining's r2: 0.367933\tvalid_1's r2: 0.191805\n",
      "[150]\ttraining's r2: 0.370282\tvalid_1's r2: 0.192352\n",
      "[151]\ttraining's r2: 0.37291\tvalid_1's r2: 0.194129\n",
      "[152]\ttraining's r2: 0.375889\tvalid_1's r2: 0.195425\n",
      "[153]\ttraining's r2: 0.378844\tvalid_1's r2: 0.197159\n",
      "[154]\ttraining's r2: 0.381488\tvalid_1's r2: 0.198834\n",
      "[155]\ttraining's r2: 0.384325\tvalid_1's r2: 0.200757\n",
      "[156]\ttraining's r2: 0.386817\tvalid_1's r2: 0.203101\n",
      "[157]\ttraining's r2: 0.388773\tvalid_1's r2: 0.204196\n",
      "[158]\ttraining's r2: 0.390987\tvalid_1's r2: 0.205548\n",
      "[159]\ttraining's r2: 0.393004\tvalid_1's r2: 0.206506\n",
      "[160]\ttraining's r2: 0.394921\tvalid_1's r2: 0.20742\n",
      "[161]\ttraining's r2: 0.397159\tvalid_1's r2: 0.208165\n",
      "[162]\ttraining's r2: 0.399247\tvalid_1's r2: 0.20899\n",
      "[163]\ttraining's r2: 0.40147\tvalid_1's r2: 0.210064\n",
      "[164]\ttraining's r2: 0.403761\tvalid_1's r2: 0.211673\n",
      "[165]\ttraining's r2: 0.406295\tvalid_1's r2: 0.212729\n",
      "[166]\ttraining's r2: 0.408663\tvalid_1's r2: 0.2137\n",
      "[167]\ttraining's r2: 0.411443\tvalid_1's r2: 0.215942\n",
      "[168]\ttraining's r2: 0.413691\tvalid_1's r2: 0.215934\n",
      "[169]\ttraining's r2: 0.416075\tvalid_1's r2: 0.216974\n",
      "[170]\ttraining's r2: 0.418585\tvalid_1's r2: 0.217882\n",
      "[171]\ttraining's r2: 0.420093\tvalid_1's r2: 0.217907\n",
      "[172]\ttraining's r2: 0.422211\tvalid_1's r2: 0.21781\n",
      "[173]\ttraining's r2: 0.424291\tvalid_1's r2: 0.219121\n",
      "[174]\ttraining's r2: 0.425692\tvalid_1's r2: 0.219253\n",
      "[175]\ttraining's r2: 0.427788\tvalid_1's r2: 0.220301\n",
      "[176]\ttraining's r2: 0.429414\tvalid_1's r2: 0.220532\n",
      "[177]\ttraining's r2: 0.431247\tvalid_1's r2: 0.221561\n",
      "[178]\ttraining's r2: 0.433102\tvalid_1's r2: 0.22257\n",
      "[179]\ttraining's r2: 0.435546\tvalid_1's r2: 0.223692\n",
      "[180]\ttraining's r2: 0.437045\tvalid_1's r2: 0.224092\n",
      "[181]\ttraining's r2: 0.439121\tvalid_1's r2: 0.22474\n",
      "[182]\ttraining's r2: 0.440935\tvalid_1's r2: 0.225463\n",
      "[183]\ttraining's r2: 0.442638\tvalid_1's r2: 0.226052\n",
      "[184]\ttraining's r2: 0.444357\tvalid_1's r2: 0.226116\n",
      "[185]\ttraining's r2: 0.446211\tvalid_1's r2: 0.22672\n",
      "[186]\ttraining's r2: 0.447881\tvalid_1's r2: 0.227817\n",
      "[187]\ttraining's r2: 0.449542\tvalid_1's r2: 0.227622\n",
      "[188]\ttraining's r2: 0.451214\tvalid_1's r2: 0.228078\n",
      "[189]\ttraining's r2: 0.453333\tvalid_1's r2: 0.228927\n",
      "[190]\ttraining's r2: 0.454884\tvalid_1's r2: 0.229104\n",
      "[191]\ttraining's r2: 0.45666\tvalid_1's r2: 0.229611\n",
      "[192]\ttraining's r2: 0.458437\tvalid_1's r2: 0.230542\n",
      "[193]\ttraining's r2: 0.460218\tvalid_1's r2: 0.230933\n",
      "[194]\ttraining's r2: 0.462282\tvalid_1's r2: 0.23151\n",
      "[195]\ttraining's r2: 0.46414\tvalid_1's r2: 0.232256\n",
      "[196]\ttraining's r2: 0.465831\tvalid_1's r2: 0.232439\n",
      "[197]\ttraining's r2: 0.467451\tvalid_1's r2: 0.233194\n",
      "[198]\ttraining's r2: 0.468826\tvalid_1's r2: 0.233603\n",
      "[199]\ttraining's r2: 0.470437\tvalid_1's r2: 0.233328\n",
      "[200]\ttraining's r2: 0.472279\tvalid_1's r2: 0.233683\n",
      "[201]\ttraining's r2: 0.47393\tvalid_1's r2: 0.234203\n",
      "[202]\ttraining's r2: 0.475293\tvalid_1's r2: 0.234373\n",
      "[203]\ttraining's r2: 0.477212\tvalid_1's r2: 0.235154\n",
      "[204]\ttraining's r2: 0.479399\tvalid_1's r2: 0.236156\n",
      "[205]\ttraining's r2: 0.48101\tvalid_1's r2: 0.236683\n",
      "[206]\ttraining's r2: 0.482201\tvalid_1's r2: 0.237117\n",
      "[207]\ttraining's r2: 0.483415\tvalid_1's r2: 0.236991\n",
      "[208]\ttraining's r2: 0.485054\tvalid_1's r2: 0.237305\n",
      "[209]\ttraining's r2: 0.486415\tvalid_1's r2: 0.237422\n",
      "[210]\ttraining's r2: 0.488183\tvalid_1's r2: 0.238255\n",
      "[211]\ttraining's r2: 0.490027\tvalid_1's r2: 0.238949\n",
      "[212]\ttraining's r2: 0.491763\tvalid_1's r2: 0.239748\n",
      "[213]\ttraining's r2: 0.492943\tvalid_1's r2: 0.239619\n",
      "[214]\ttraining's r2: 0.494321\tvalid_1's r2: 0.239817\n",
      "[215]\ttraining's r2: 0.495693\tvalid_1's r2: 0.240104\n",
      "[216]\ttraining's r2: 0.497316\tvalid_1's r2: 0.240691\n",
      "[217]\ttraining's r2: 0.49857\tvalid_1's r2: 0.24114\n",
      "[218]\ttraining's r2: 0.50021\tvalid_1's r2: 0.24123\n",
      "[219]\ttraining's r2: 0.501922\tvalid_1's r2: 0.241961\n",
      "[220]\ttraining's r2: 0.503428\tvalid_1's r2: 0.242117\n",
      "[221]\ttraining's r2: 0.50472\tvalid_1's r2: 0.242591\n",
      "[222]\ttraining's r2: 0.506297\tvalid_1's r2: 0.242869\n",
      "[223]\ttraining's r2: 0.507405\tvalid_1's r2: 0.242851\n",
      "[224]\ttraining's r2: 0.508946\tvalid_1's r2: 0.243949\n",
      "[225]\ttraining's r2: 0.510217\tvalid_1's r2: 0.24398\n",
      "[226]\ttraining's r2: 0.511875\tvalid_1's r2: 0.245115\n",
      "[227]\ttraining's r2: 0.513149\tvalid_1's r2: 0.245556\n",
      "[228]\ttraining's r2: 0.514589\tvalid_1's r2: 0.246041\n",
      "[229]\ttraining's r2: 0.515959\tvalid_1's r2: 0.245751\n",
      "[230]\ttraining's r2: 0.517262\tvalid_1's r2: 0.245876\n",
      "[231]\ttraining's r2: 0.518248\tvalid_1's r2: 0.245723\n",
      "[232]\ttraining's r2: 0.51983\tvalid_1's r2: 0.246142\n",
      "[233]\ttraining's r2: 0.521038\tvalid_1's r2: 0.2461\n",
      "[234]\ttraining's r2: 0.522495\tvalid_1's r2: 0.246612\n",
      "[235]\ttraining's r2: 0.524014\tvalid_1's r2: 0.247786\n",
      "[236]\ttraining's r2: 0.525313\tvalid_1's r2: 0.248268\n",
      "[237]\ttraining's r2: 0.526629\tvalid_1's r2: 0.248651\n",
      "[238]\ttraining's r2: 0.527902\tvalid_1's r2: 0.248912\n",
      "[239]\ttraining's r2: 0.529441\tvalid_1's r2: 0.249227\n",
      "[240]\ttraining's r2: 0.530828\tvalid_1's r2: 0.25006\n",
      "[241]\ttraining's r2: 0.532179\tvalid_1's r2: 0.250084\n",
      "[242]\ttraining's r2: 0.533362\tvalid_1's r2: 0.250465\n",
      "[243]\ttraining's r2: 0.534658\tvalid_1's r2: 0.251652\n",
      "[244]\ttraining's r2: 0.535887\tvalid_1's r2: 0.252611\n",
      "[245]\ttraining's r2: 0.537528\tvalid_1's r2: 0.253306\n",
      "[246]\ttraining's r2: 0.538833\tvalid_1's r2: 0.253249\n",
      "[247]\ttraining's r2: 0.540191\tvalid_1's r2: 0.253576\n",
      "[248]\ttraining's r2: 0.54149\tvalid_1's r2: 0.254177\n",
      "[249]\ttraining's r2: 0.542856\tvalid_1's r2: 0.254071\n",
      "[250]\ttraining's r2: 0.544262\tvalid_1's r2: 0.254719\n",
      "[251]\ttraining's r2: 0.545606\tvalid_1's r2: 0.254711\n",
      "[252]\ttraining's r2: 0.546675\tvalid_1's r2: 0.254536\n",
      "[253]\ttraining's r2: 0.547726\tvalid_1's r2: 0.254498\n",
      "[254]\ttraining's r2: 0.549051\tvalid_1's r2: 0.255142\n",
      "[255]\ttraining's r2: 0.550171\tvalid_1's r2: 0.255615\n",
      "[256]\ttraining's r2: 0.551276\tvalid_1's r2: 0.256039\n",
      "[257]\ttraining's r2: 0.552734\tvalid_1's r2: 0.256999\n",
      "[258]\ttraining's r2: 0.55381\tvalid_1's r2: 0.256377\n",
      "[259]\ttraining's r2: 0.554994\tvalid_1's r2: 0.256719\n",
      "[260]\ttraining's r2: 0.556215\tvalid_1's r2: 0.256535\n",
      "[261]\ttraining's r2: 0.557387\tvalid_1's r2: 0.256482\n",
      "[262]\ttraining's r2: 0.558423\tvalid_1's r2: 0.256254\n",
      "[263]\ttraining's r2: 0.559681\tvalid_1's r2: 0.256256\n",
      "[264]\ttraining's r2: 0.561\tvalid_1's r2: 0.256604\n",
      "[265]\ttraining's r2: 0.562299\tvalid_1's r2: 0.256911\n",
      "[266]\ttraining's r2: 0.563339\tvalid_1's r2: 0.256984\n",
      "[267]\ttraining's r2: 0.564425\tvalid_1's r2: 0.256609\n",
      "[268]\ttraining's r2: 0.565303\tvalid_1's r2: 0.256669\n",
      "[269]\ttraining's r2: 0.566551\tvalid_1's r2: 0.256485\n",
      "[270]\ttraining's r2: 0.567607\tvalid_1's r2: 0.257028\n",
      "[271]\ttraining's r2: 0.568693\tvalid_1's r2: 0.25743\n",
      "[272]\ttraining's r2: 0.569804\tvalid_1's r2: 0.257509\n",
      "[273]\ttraining's r2: 0.571156\tvalid_1's r2: 0.25812\n",
      "[274]\ttraining's r2: 0.572199\tvalid_1's r2: 0.258278\n",
      "[275]\ttraining's r2: 0.573289\tvalid_1's r2: 0.258902\n",
      "[276]\ttraining's r2: 0.57451\tvalid_1's r2: 0.259141\n",
      "[277]\ttraining's r2: 0.575976\tvalid_1's r2: 0.259166\n",
      "[278]\ttraining's r2: 0.576923\tvalid_1's r2: 0.259212\n",
      "[279]\ttraining's r2: 0.577779\tvalid_1's r2: 0.259543\n",
      "[280]\ttraining's r2: 0.578864\tvalid_1's r2: 0.259235\n",
      "[281]\ttraining's r2: 0.579937\tvalid_1's r2: 0.259316\n",
      "[282]\ttraining's r2: 0.581237\tvalid_1's r2: 0.259379\n",
      "[283]\ttraining's r2: 0.582429\tvalid_1's r2: 0.25963\n",
      "[284]\ttraining's r2: 0.583509\tvalid_1's r2: 0.259687\n",
      "[285]\ttraining's r2: 0.584628\tvalid_1's r2: 0.259524\n",
      "[286]\ttraining's r2: 0.585954\tvalid_1's r2: 0.259918\n",
      "[287]\ttraining's r2: 0.587058\tvalid_1's r2: 0.260194\n",
      "[288]\ttraining's r2: 0.587761\tvalid_1's r2: 0.259623\n",
      "[289]\ttraining's r2: 0.588744\tvalid_1's r2: 0.259836\n",
      "[290]\ttraining's r2: 0.58986\tvalid_1's r2: 0.259867\n",
      "[291]\ttraining's r2: 0.590814\tvalid_1's r2: 0.259775\n",
      "[292]\ttraining's r2: 0.591905\tvalid_1's r2: 0.259784\n",
      "[293]\ttraining's r2: 0.593075\tvalid_1's r2: 0.260164\n",
      "[294]\ttraining's r2: 0.594269\tvalid_1's r2: 0.260697\n",
      "[295]\ttraining's r2: 0.595511\tvalid_1's r2: 0.26109\n",
      "[296]\ttraining's r2: 0.5963\tvalid_1's r2: 0.261384\n",
      "[297]\ttraining's r2: 0.597424\tvalid_1's r2: 0.261664\n",
      "[298]\ttraining's r2: 0.598435\tvalid_1's r2: 0.262082\n",
      "[299]\ttraining's r2: 0.599588\tvalid_1's r2: 0.262133\n",
      "[300]\ttraining's r2: 0.600423\tvalid_1's r2: 0.262476\n",
      "[301]\ttraining's r2: 0.601528\tvalid_1's r2: 0.26264\n",
      "[302]\ttraining's r2: 0.602295\tvalid_1's r2: 0.262971\n",
      "[303]\ttraining's r2: 0.603238\tvalid_1's r2: 0.2635\n",
      "[304]\ttraining's r2: 0.604127\tvalid_1's r2: 0.263361\n",
      "[305]\ttraining's r2: 0.605151\tvalid_1's r2: 0.263256\n",
      "[306]\ttraining's r2: 0.606016\tvalid_1's r2: 0.263542\n",
      "[307]\ttraining's r2: 0.606828\tvalid_1's r2: 0.263825\n",
      "[308]\ttraining's r2: 0.607939\tvalid_1's r2: 0.264457\n",
      "[309]\ttraining's r2: 0.608989\tvalid_1's r2: 0.264707\n",
      "[310]\ttraining's r2: 0.609816\tvalid_1's r2: 0.265148\n",
      "[311]\ttraining's r2: 0.610822\tvalid_1's r2: 0.265037\n",
      "[312]\ttraining's r2: 0.611471\tvalid_1's r2: 0.264771\n",
      "[313]\ttraining's r2: 0.612388\tvalid_1's r2: 0.264417\n",
      "[314]\ttraining's r2: 0.613263\tvalid_1's r2: 0.264178\n",
      "[315]\ttraining's r2: 0.614358\tvalid_1's r2: 0.264417\n",
      "[316]\ttraining's r2: 0.615282\tvalid_1's r2: 0.264043\n",
      "[317]\ttraining's r2: 0.616082\tvalid_1's r2: 0.264509\n",
      "[318]\ttraining's r2: 0.617162\tvalid_1's r2: 0.264458\n",
      "[319]\ttraining's r2: 0.617908\tvalid_1's r2: 0.264179\n",
      "[320]\ttraining's r2: 0.618698\tvalid_1's r2: 0.263898\n",
      "[321]\ttraining's r2: 0.619438\tvalid_1's r2: 0.263699\n",
      "[322]\ttraining's r2: 0.620407\tvalid_1's r2: 0.26374\n",
      "[323]\ttraining's r2: 0.621338\tvalid_1's r2: 0.263332\n",
      "[324]\ttraining's r2: 0.622156\tvalid_1's r2: 0.263054\n",
      "[325]\ttraining's r2: 0.622739\tvalid_1's r2: 0.262586\n",
      "[326]\ttraining's r2: 0.623456\tvalid_1's r2: 0.262434\n",
      "[327]\ttraining's r2: 0.624452\tvalid_1's r2: 0.263138\n",
      "[328]\ttraining's r2: 0.625278\tvalid_1's r2: 0.26305\n",
      "[329]\ttraining's r2: 0.626117\tvalid_1's r2: 0.263169\n",
      "[330]\ttraining's r2: 0.627005\tvalid_1's r2: 0.26357\n",
      "[331]\ttraining's r2: 0.627953\tvalid_1's r2: 0.263966\n",
      "[332]\ttraining's r2: 0.628805\tvalid_1's r2: 0.264546\n",
      "[333]\ttraining's r2: 0.629676\tvalid_1's r2: 0.264042\n",
      "[334]\ttraining's r2: 0.630532\tvalid_1's r2: 0.264565\n",
      "[335]\ttraining's r2: 0.631256\tvalid_1's r2: 0.264743\n",
      "[336]\ttraining's r2: 0.632238\tvalid_1's r2: 0.264993\n",
      "[337]\ttraining's r2: 0.633008\tvalid_1's r2: 0.265002\n",
      "[338]\ttraining's r2: 0.633847\tvalid_1's r2: 0.265298\n",
      "[339]\ttraining's r2: 0.634896\tvalid_1's r2: 0.265658\n",
      "[340]\ttraining's r2: 0.635896\tvalid_1's r2: 0.26564\n",
      "[341]\ttraining's r2: 0.636886\tvalid_1's r2: 0.265859\n",
      "[342]\ttraining's r2: 0.637767\tvalid_1's r2: 0.266206\n",
      "[343]\ttraining's r2: 0.638411\tvalid_1's r2: 0.26631\n",
      "[344]\ttraining's r2: 0.639203\tvalid_1's r2: 0.266232\n",
      "[345]\ttraining's r2: 0.640075\tvalid_1's r2: 0.265919\n",
      "[346]\ttraining's r2: 0.640881\tvalid_1's r2: 0.266276\n",
      "[347]\ttraining's r2: 0.641526\tvalid_1's r2: 0.265975\n",
      "[348]\ttraining's r2: 0.642407\tvalid_1's r2: 0.265859\n",
      "[349]\ttraining's r2: 0.643234\tvalid_1's r2: 0.265674\n",
      "[350]\ttraining's r2: 0.644049\tvalid_1's r2: 0.26512\n",
      "[351]\ttraining's r2: 0.644759\tvalid_1's r2: 0.264869\n",
      "[352]\ttraining's r2: 0.645485\tvalid_1's r2: 0.26519\n",
      "[353]\ttraining's r2: 0.646184\tvalid_1's r2: 0.265094\n",
      "[354]\ttraining's r2: 0.647286\tvalid_1's r2: 0.265122\n",
      "[355]\ttraining's r2: 0.648024\tvalid_1's r2: 0.265206\n",
      "[356]\ttraining's r2: 0.648735\tvalid_1's r2: 0.265236\n",
      "[357]\ttraining's r2: 0.64956\tvalid_1's r2: 0.265221\n",
      "[358]\ttraining's r2: 0.65043\tvalid_1's r2: 0.264963\n",
      "[359]\ttraining's r2: 0.651229\tvalid_1's r2: 0.265279\n",
      "[360]\ttraining's r2: 0.652028\tvalid_1's r2: 0.265113\n",
      "[361]\ttraining's r2: 0.65299\tvalid_1's r2: 0.265541\n",
      "[362]\ttraining's r2: 0.653949\tvalid_1's r2: 0.265282\n",
      "[363]\ttraining's r2: 0.654716\tvalid_1's r2: 0.265222\n",
      "[364]\ttraining's r2: 0.655502\tvalid_1's r2: 0.265307\n",
      "[365]\ttraining's r2: 0.656381\tvalid_1's r2: 0.265391\n",
      "[366]\ttraining's r2: 0.657006\tvalid_1's r2: 0.264873\n",
      "[367]\ttraining's r2: 0.657917\tvalid_1's r2: 0.26506\n",
      "[368]\ttraining's r2: 0.658605\tvalid_1's r2: 0.265094\n",
      "[369]\ttraining's r2: 0.65945\tvalid_1's r2: 0.265545\n",
      "[370]\ttraining's r2: 0.66029\tvalid_1's r2: 0.26506\n",
      "[371]\ttraining's r2: 0.661085\tvalid_1's r2: 0.264835\n",
      "[372]\ttraining's r2: 0.661825\tvalid_1's r2: 0.26455\n",
      "[373]\ttraining's r2: 0.662449\tvalid_1's r2: 0.26454\n",
      "[374]\ttraining's r2: 0.663337\tvalid_1's r2: 0.264661\n",
      "[375]\ttraining's r2: 0.66399\tvalid_1's r2: 0.26457\n",
      "[376]\ttraining's r2: 0.664692\tvalid_1's r2: 0.264924\n",
      "[377]\ttraining's r2: 0.665403\tvalid_1's r2: 0.26534\n",
      "[378]\ttraining's r2: 0.666081\tvalid_1's r2: 0.265086\n",
      "[379]\ttraining's r2: 0.666893\tvalid_1's r2: 0.265696\n",
      "[380]\ttraining's r2: 0.667734\tvalid_1's r2: 0.266139\n",
      "[381]\ttraining's r2: 0.668372\tvalid_1's r2: 0.2663\n",
      "[382]\ttraining's r2: 0.669164\tvalid_1's r2: 0.266547\n",
      "[383]\ttraining's r2: 0.6701\tvalid_1's r2: 0.266254\n",
      "[384]\ttraining's r2: 0.670863\tvalid_1's r2: 0.266498\n",
      "[385]\ttraining's r2: 0.671598\tvalid_1's r2: 0.265763\n",
      "[386]\ttraining's r2: 0.672171\tvalid_1's r2: 0.265725\n",
      "[387]\ttraining's r2: 0.672949\tvalid_1's r2: 0.265742\n",
      "[388]\ttraining's r2: 0.673705\tvalid_1's r2: 0.265642\n",
      "[389]\ttraining's r2: 0.674491\tvalid_1's r2: 0.266114\n",
      "[390]\ttraining's r2: 0.675237\tvalid_1's r2: 0.265942\n",
      "[391]\ttraining's r2: 0.675974\tvalid_1's r2: 0.265675\n",
      "[392]\ttraining's r2: 0.676724\tvalid_1's r2: 0.265696\n",
      "[393]\ttraining's r2: 0.677538\tvalid_1's r2: 0.26536\n",
      "[394]\ttraining's r2: 0.678095\tvalid_1's r2: 0.265329\n",
      "[395]\ttraining's r2: 0.678683\tvalid_1's r2: 0.265103\n",
      "[396]\ttraining's r2: 0.67941\tvalid_1's r2: 0.26524\n",
      "[397]\ttraining's r2: 0.680177\tvalid_1's r2: 0.265183\n",
      "[398]\ttraining's r2: 0.680712\tvalid_1's r2: 0.2653\n",
      "[399]\ttraining's r2: 0.681476\tvalid_1's r2: 0.26548\n",
      "[400]\ttraining's r2: 0.682139\tvalid_1's r2: 0.265372\n",
      "[401]\ttraining's r2: 0.682983\tvalid_1's r2: 0.266105\n",
      "[402]\ttraining's r2: 0.683683\tvalid_1's r2: 0.266569\n",
      "[403]\ttraining's r2: 0.684151\tvalid_1's r2: 0.266577\n",
      "[404]\ttraining's r2: 0.68498\tvalid_1's r2: 0.26697\n",
      "[405]\ttraining's r2: 0.685673\tvalid_1's r2: 0.266978\n",
      "[406]\ttraining's r2: 0.686388\tvalid_1's r2: 0.266877\n",
      "[407]\ttraining's r2: 0.686907\tvalid_1's r2: 0.266876\n",
      "[408]\ttraining's r2: 0.687732\tvalid_1's r2: 0.266581\n",
      "[409]\ttraining's r2: 0.688302\tvalid_1's r2: 0.26704\n",
      "[410]\ttraining's r2: 0.689091\tvalid_1's r2: 0.266584\n",
      "[411]\ttraining's r2: 0.689639\tvalid_1's r2: 0.266667\n",
      "[412]\ttraining's r2: 0.690422\tvalid_1's r2: 0.26652\n",
      "[413]\ttraining's r2: 0.690996\tvalid_1's r2: 0.266336\n",
      "[414]\ttraining's r2: 0.691709\tvalid_1's r2: 0.26618\n",
      "[415]\ttraining's r2: 0.69247\tvalid_1's r2: 0.266169\n",
      "[416]\ttraining's r2: 0.693117\tvalid_1's r2: 0.266447\n",
      "[417]\ttraining's r2: 0.693632\tvalid_1's r2: 0.266661\n",
      "[418]\ttraining's r2: 0.694267\tvalid_1's r2: 0.266805\n",
      "[419]\ttraining's r2: 0.69485\tvalid_1's r2: 0.266665\n",
      "[420]\ttraining's r2: 0.695429\tvalid_1's r2: 0.266664\n",
      "[421]\ttraining's r2: 0.696117\tvalid_1's r2: 0.26707\n",
      "[422]\ttraining's r2: 0.696696\tvalid_1's r2: 0.267056\n",
      "[423]\ttraining's r2: 0.697336\tvalid_1's r2: 0.266973\n",
      "[424]\ttraining's r2: 0.697821\tvalid_1's r2: 0.267201\n",
      "[425]\ttraining's r2: 0.698544\tvalid_1's r2: 0.267156\n",
      "[426]\ttraining's r2: 0.699274\tvalid_1's r2: 0.267328\n",
      "[427]\ttraining's r2: 0.699906\tvalid_1's r2: 0.267075\n",
      "[428]\ttraining's r2: 0.700573\tvalid_1's r2: 0.266776\n",
      "[429]\ttraining's r2: 0.701213\tvalid_1's r2: 0.266693\n",
      "[430]\ttraining's r2: 0.701877\tvalid_1's r2: 0.266693\n",
      "[431]\ttraining's r2: 0.70255\tvalid_1's r2: 0.266983\n",
      "[432]\ttraining's r2: 0.70319\tvalid_1's r2: 0.26681\n",
      "[433]\ttraining's r2: 0.703758\tvalid_1's r2: 0.266942\n",
      "[434]\ttraining's r2: 0.704341\tvalid_1's r2: 0.266999\n",
      "[435]\ttraining's r2: 0.705134\tvalid_1's r2: 0.266515\n",
      "[436]\ttraining's r2: 0.705855\tvalid_1's r2: 0.266439\n",
      "[437]\ttraining's r2: 0.706371\tvalid_1's r2: 0.266092\n",
      "[438]\ttraining's r2: 0.706999\tvalid_1's r2: 0.266291\n",
      "[439]\ttraining's r2: 0.707672\tvalid_1's r2: 0.266632\n",
      "[440]\ttraining's r2: 0.708289\tvalid_1's r2: 0.266628\n",
      "[441]\ttraining's r2: 0.708818\tvalid_1's r2: 0.26658\n",
      "[442]\ttraining's r2: 0.70953\tvalid_1's r2: 0.26599\n",
      "[443]\ttraining's r2: 0.710129\tvalid_1's r2: 0.265761\n",
      "[444]\ttraining's r2: 0.710735\tvalid_1's r2: 0.266281\n",
      "[445]\ttraining's r2: 0.711416\tvalid_1's r2: 0.266061\n",
      "[446]\ttraining's r2: 0.71193\tvalid_1's r2: 0.266297\n",
      "[447]\ttraining's r2: 0.712537\tvalid_1's r2: 0.266201\n",
      "[448]\ttraining's r2: 0.713082\tvalid_1's r2: 0.266104\n",
      "[449]\ttraining's r2: 0.713743\tvalid_1's r2: 0.266015\n",
      "[450]\ttraining's r2: 0.714258\tvalid_1's r2: 0.265865\n",
      "[451]\ttraining's r2: 0.714876\tvalid_1's r2: 0.266275\n",
      "[452]\ttraining's r2: 0.715476\tvalid_1's r2: 0.266295\n",
      "[453]\ttraining's r2: 0.716192\tvalid_1's r2: 0.266792\n",
      "[454]\ttraining's r2: 0.716776\tvalid_1's r2: 0.266807\n",
      "[455]\ttraining's r2: 0.717501\tvalid_1's r2: 0.266884\n",
      "[456]\ttraining's r2: 0.718192\tvalid_1's r2: 0.267029\n",
      "[457]\ttraining's r2: 0.718645\tvalid_1's r2: 0.266789\n",
      "[458]\ttraining's r2: 0.719279\tvalid_1's r2: 0.266599\n",
      "[459]\ttraining's r2: 0.71976\tvalid_1's r2: 0.266519\n",
      "[460]\ttraining's r2: 0.72038\tvalid_1's r2: 0.266515\n",
      "[461]\ttraining's r2: 0.721018\tvalid_1's r2: 0.266337\n",
      "[462]\ttraining's r2: 0.721618\tvalid_1's r2: 0.266381\n",
      "[463]\ttraining's r2: 0.722168\tvalid_1's r2: 0.266565\n",
      "[464]\ttraining's r2: 0.722761\tvalid_1's r2: 0.266633\n",
      "[465]\ttraining's r2: 0.723354\tvalid_1's r2: 0.266847\n",
      "[466]\ttraining's r2: 0.723979\tvalid_1's r2: 0.266958\n",
      "[467]\ttraining's r2: 0.724517\tvalid_1's r2: 0.266819\n",
      "[468]\ttraining's r2: 0.725064\tvalid_1's r2: 0.266709\n",
      "[469]\ttraining's r2: 0.725664\tvalid_1's r2: 0.266676\n",
      "[470]\ttraining's r2: 0.726098\tvalid_1's r2: 0.266527\n",
      "[471]\ttraining's r2: 0.726422\tvalid_1's r2: 0.266091\n",
      "[472]\ttraining's r2: 0.727065\tvalid_1's r2: 0.265892\n",
      "[473]\ttraining's r2: 0.727604\tvalid_1's r2: 0.265625\n",
      "[474]\ttraining's r2: 0.728251\tvalid_1's r2: 0.265913\n",
      "[475]\ttraining's r2: 0.728724\tvalid_1's r2: 0.265713\n",
      "[476]\ttraining's r2: 0.729252\tvalid_1's r2: 0.265593\n",
      "[477]\ttraining's r2: 0.729815\tvalid_1's r2: 0.265741\n",
      "[478]\ttraining's r2: 0.730442\tvalid_1's r2: 0.265714\n",
      "[479]\ttraining's r2: 0.731041\tvalid_1's r2: 0.265911\n",
      "[480]\ttraining's r2: 0.731571\tvalid_1's r2: 0.266635\n",
      "[481]\ttraining's r2: 0.732078\tvalid_1's r2: 0.266803\n",
      "[482]\ttraining's r2: 0.732702\tvalid_1's r2: 0.266638\n",
      "[483]\ttraining's r2: 0.733194\tvalid_1's r2: 0.266686\n",
      "[484]\ttraining's r2: 0.73378\tvalid_1's r2: 0.266928\n",
      "[485]\ttraining's r2: 0.734354\tvalid_1's r2: 0.267112\n",
      "[486]\ttraining's r2: 0.734944\tvalid_1's r2: 0.267345\n",
      "[487]\ttraining's r2: 0.735416\tvalid_1's r2: 0.267338\n",
      "[488]\ttraining's r2: 0.735809\tvalid_1's r2: 0.267091\n",
      "[489]\ttraining's r2: 0.736335\tvalid_1's r2: 0.267096\n",
      "[490]\ttraining's r2: 0.737028\tvalid_1's r2: 0.267271\n",
      "[491]\ttraining's r2: 0.737657\tvalid_1's r2: 0.267164\n",
      "[492]\ttraining's r2: 0.738223\tvalid_1's r2: 0.267308\n",
      "[493]\ttraining's r2: 0.738747\tvalid_1's r2: 0.267458\n",
      "[494]\ttraining's r2: 0.739271\tvalid_1's r2: 0.267614\n",
      "[495]\ttraining's r2: 0.7397\tvalid_1's r2: 0.267597\n",
      "[496]\ttraining's r2: 0.740345\tvalid_1's r2: 0.267687\n",
      "[497]\ttraining's r2: 0.740826\tvalid_1's r2: 0.267782\n",
      "[498]\ttraining's r2: 0.741224\tvalid_1's r2: 0.267792\n",
      "[499]\ttraining's r2: 0.741811\tvalid_1's r2: 0.267972\n",
      "[500]\ttraining's r2: 0.742329\tvalid_1's r2: 0.268284\n",
      "[501]\ttraining's r2: 0.742775\tvalid_1's r2: 0.268441\n",
      "[502]\ttraining's r2: 0.743313\tvalid_1's r2: 0.268485\n",
      "[503]\ttraining's r2: 0.743887\tvalid_1's r2: 0.268208\n",
      "[504]\ttraining's r2: 0.744334\tvalid_1's r2: 0.268327\n",
      "[505]\ttraining's r2: 0.744821\tvalid_1's r2: 0.268427\n",
      "[506]\ttraining's r2: 0.745215\tvalid_1's r2: 0.268648\n",
      "[507]\ttraining's r2: 0.745766\tvalid_1's r2: 0.268573\n",
      "[508]\ttraining's r2: 0.746285\tvalid_1's r2: 0.268552\n",
      "[509]\ttraining's r2: 0.746775\tvalid_1's r2: 0.268773\n",
      "[510]\ttraining's r2: 0.747238\tvalid_1's r2: 0.268669\n",
      "[511]\ttraining's r2: 0.747599\tvalid_1's r2: 0.268332\n",
      "[512]\ttraining's r2: 0.74804\tvalid_1's r2: 0.267873\n",
      "[513]\ttraining's r2: 0.748572\tvalid_1's r2: 0.268051\n",
      "[514]\ttraining's r2: 0.749019\tvalid_1's r2: 0.268095\n",
      "[515]\ttraining's r2: 0.749549\tvalid_1's r2: 0.267971\n",
      "[516]\ttraining's r2: 0.750057\tvalid_1's r2: 0.268041\n",
      "[517]\ttraining's r2: 0.750517\tvalid_1's r2: 0.268303\n",
      "[518]\ttraining's r2: 0.750917\tvalid_1's r2: 0.268245\n",
      "[519]\ttraining's r2: 0.751494\tvalid_1's r2: 0.268575\n",
      "[520]\ttraining's r2: 0.751998\tvalid_1's r2: 0.268472\n",
      "[521]\ttraining's r2: 0.752409\tvalid_1's r2: 0.268722\n",
      "[522]\ttraining's r2: 0.75293\tvalid_1's r2: 0.268861\n",
      "[523]\ttraining's r2: 0.753473\tvalid_1's r2: 0.269739\n",
      "[524]\ttraining's r2: 0.753901\tvalid_1's r2: 0.269608\n",
      "[525]\ttraining's r2: 0.754382\tvalid_1's r2: 0.269491\n",
      "[526]\ttraining's r2: 0.754804\tvalid_1's r2: 0.269039\n",
      "[527]\ttraining's r2: 0.755112\tvalid_1's r2: 0.268764\n",
      "[528]\ttraining's r2: 0.755565\tvalid_1's r2: 0.268702\n",
      "[529]\ttraining's r2: 0.756011\tvalid_1's r2: 0.268782\n",
      "[530]\ttraining's r2: 0.756501\tvalid_1's r2: 0.268726\n",
      "[531]\ttraining's r2: 0.756878\tvalid_1's r2: 0.268759\n",
      "[532]\ttraining's r2: 0.757346\tvalid_1's r2: 0.268578\n",
      "[533]\ttraining's r2: 0.757973\tvalid_1's r2: 0.269035\n",
      "[534]\ttraining's r2: 0.758505\tvalid_1's r2: 0.269364\n",
      "[535]\ttraining's r2: 0.759012\tvalid_1's r2: 0.269804\n",
      "[536]\ttraining's r2: 0.759507\tvalid_1's r2: 0.269805\n",
      "[537]\ttraining's r2: 0.759986\tvalid_1's r2: 0.269837\n",
      "[538]\ttraining's r2: 0.760539\tvalid_1's r2: 0.269756\n",
      "[539]\ttraining's r2: 0.760984\tvalid_1's r2: 0.269904\n",
      "[540]\ttraining's r2: 0.761387\tvalid_1's r2: 0.269804\n",
      "[541]\ttraining's r2: 0.761921\tvalid_1's r2: 0.270163\n",
      "[542]\ttraining's r2: 0.762377\tvalid_1's r2: 0.270172\n",
      "[543]\ttraining's r2: 0.762791\tvalid_1's r2: 0.27028\n",
      "[544]\ttraining's r2: 0.763121\tvalid_1's r2: 0.270399\n",
      "[545]\ttraining's r2: 0.763681\tvalid_1's r2: 0.270375\n",
      "[546]\ttraining's r2: 0.764091\tvalid_1's r2: 0.270156\n",
      "[547]\ttraining's r2: 0.764544\tvalid_1's r2: 0.27012\n",
      "[548]\ttraining's r2: 0.764934\tvalid_1's r2: 0.269945\n",
      "[549]\ttraining's r2: 0.765384\tvalid_1's r2: 0.269945\n",
      "[550]\ttraining's r2: 0.765832\tvalid_1's r2: 0.269935\n",
      "[551]\ttraining's r2: 0.766311\tvalid_1's r2: 0.26989\n",
      "[552]\ttraining's r2: 0.766775\tvalid_1's r2: 0.269195\n",
      "[553]\ttraining's r2: 0.767135\tvalid_1's r2: 0.268889\n",
      "[554]\ttraining's r2: 0.767589\tvalid_1's r2: 0.268773\n",
      "[555]\ttraining's r2: 0.768055\tvalid_1's r2: 0.268832\n",
      "[556]\ttraining's r2: 0.768431\tvalid_1's r2: 0.268746\n",
      "[557]\ttraining's r2: 0.768822\tvalid_1's r2: 0.268716\n",
      "[558]\ttraining's r2: 0.769345\tvalid_1's r2: 0.26892\n",
      "[559]\ttraining's r2: 0.769829\tvalid_1's r2: 0.26911\n",
      "[560]\ttraining's r2: 0.770244\tvalid_1's r2: 0.269489\n",
      "[561]\ttraining's r2: 0.770601\tvalid_1's r2: 0.269055\n",
      "[562]\ttraining's r2: 0.771115\tvalid_1's r2: 0.269243\n",
      "[563]\ttraining's r2: 0.771559\tvalid_1's r2: 0.269235\n",
      "[564]\ttraining's r2: 0.772047\tvalid_1's r2: 0.269525\n",
      "[565]\ttraining's r2: 0.772405\tvalid_1's r2: 0.269462\n",
      "[566]\ttraining's r2: 0.772839\tvalid_1's r2: 0.269526\n",
      "[567]\ttraining's r2: 0.773284\tvalid_1's r2: 0.269407\n",
      "[568]\ttraining's r2: 0.773769\tvalid_1's r2: 0.269646\n",
      "[569]\ttraining's r2: 0.774098\tvalid_1's r2: 0.26946\n",
      "[570]\ttraining's r2: 0.774398\tvalid_1's r2: 0.269323\n",
      "[571]\ttraining's r2: 0.774806\tvalid_1's r2: 0.269458\n",
      "[572]\ttraining's r2: 0.775206\tvalid_1's r2: 0.269326\n",
      "[573]\ttraining's r2: 0.775693\tvalid_1's r2: 0.269471\n",
      "[574]\ttraining's r2: 0.776062\tvalid_1's r2: 0.269547\n",
      "[575]\ttraining's r2: 0.776518\tvalid_1's r2: 0.269307\n",
      "[576]\ttraining's r2: 0.777007\tvalid_1's r2: 0.269391\n",
      "[577]\ttraining's r2: 0.777272\tvalid_1's r2: 0.269402\n",
      "[578]\ttraining's r2: 0.777604\tvalid_1's r2: 0.269111\n",
      "[579]\ttraining's r2: 0.778074\tvalid_1's r2: 0.269543\n",
      "[580]\ttraining's r2: 0.778553\tvalid_1's r2: 0.269438\n",
      "[581]\ttraining's r2: 0.778958\tvalid_1's r2: 0.26959\n",
      "[582]\ttraining's r2: 0.77936\tvalid_1's r2: 0.269556\n",
      "[583]\ttraining's r2: 0.779694\tvalid_1's r2: 0.269884\n",
      "[584]\ttraining's r2: 0.780097\tvalid_1's r2: 0.269924\n",
      "[585]\ttraining's r2: 0.780564\tvalid_1's r2: 0.269836\n",
      "[586]\ttraining's r2: 0.780998\tvalid_1's r2: 0.26969\n",
      "[587]\ttraining's r2: 0.781393\tvalid_1's r2: 0.269948\n",
      "[588]\ttraining's r2: 0.78189\tvalid_1's r2: 0.270304\n",
      "[589]\ttraining's r2: 0.782348\tvalid_1's r2: 0.270289\n",
      "[590]\ttraining's r2: 0.78281\tvalid_1's r2: 0.270349\n",
      "[591]\ttraining's r2: 0.783127\tvalid_1's r2: 0.270208\n",
      "[592]\ttraining's r2: 0.783568\tvalid_1's r2: 0.270299\n",
      "[593]\ttraining's r2: 0.784008\tvalid_1's r2: 0.269819\n",
      "[594]\ttraining's r2: 0.784315\tvalid_1's r2: 0.269856\n",
      "[595]\ttraining's r2: 0.78469\tvalid_1's r2: 0.269763\n",
      "[596]\ttraining's r2: 0.785047\tvalid_1's r2: 0.269875\n",
      "[597]\ttraining's r2: 0.785461\tvalid_1's r2: 0.269964\n",
      "[598]\ttraining's r2: 0.785822\tvalid_1's r2: 0.270248\n",
      "[599]\ttraining's r2: 0.786228\tvalid_1's r2: 0.27015\n",
      "[600]\ttraining's r2: 0.786587\tvalid_1's r2: 0.270068\n",
      "[601]\ttraining's r2: 0.786913\tvalid_1's r2: 0.270172\n",
      "[602]\ttraining's r2: 0.787303\tvalid_1's r2: 0.269879\n",
      "[603]\ttraining's r2: 0.787658\tvalid_1's r2: 0.269738\n",
      "[604]\ttraining's r2: 0.787977\tvalid_1's r2: 0.269791\n",
      "[605]\ttraining's r2: 0.78837\tvalid_1's r2: 0.269753\n",
      "[606]\ttraining's r2: 0.788701\tvalid_1's r2: 0.269566\n",
      "[607]\ttraining's r2: 0.789084\tvalid_1's r2: 0.269467\n",
      "[608]\ttraining's r2: 0.789402\tvalid_1's r2: 0.269402\n",
      "[609]\ttraining's r2: 0.789777\tvalid_1's r2: 0.269841\n",
      "[610]\ttraining's r2: 0.790159\tvalid_1's r2: 0.269978\n",
      "[611]\ttraining's r2: 0.790648\tvalid_1's r2: 0.270413\n",
      "[612]\ttraining's r2: 0.791027\tvalid_1's r2: 0.270523\n",
      "[613]\ttraining's r2: 0.791307\tvalid_1's r2: 0.270202\n",
      "[614]\ttraining's r2: 0.791604\tvalid_1's r2: 0.269961\n",
      "[615]\ttraining's r2: 0.792023\tvalid_1's r2: 0.269939\n",
      "[616]\ttraining's r2: 0.79231\tvalid_1's r2: 0.269966\n",
      "[617]\ttraining's r2: 0.792622\tvalid_1's r2: 0.269985\n",
      "[618]\ttraining's r2: 0.792976\tvalid_1's r2: 0.270095\n",
      "[619]\ttraining's r2: 0.793399\tvalid_1's r2: 0.270381\n",
      "[620]\ttraining's r2: 0.793827\tvalid_1's r2: 0.269941\n",
      "[621]\ttraining's r2: 0.794102\tvalid_1's r2: 0.269867\n",
      "[622]\ttraining's r2: 0.794572\tvalid_1's r2: 0.269828\n",
      "[623]\ttraining's r2: 0.794859\tvalid_1's r2: 0.269798\n",
      "[624]\ttraining's r2: 0.795222\tvalid_1's r2: 0.269903\n",
      "[625]\ttraining's r2: 0.795568\tvalid_1's r2: 0.269935\n",
      "[626]\ttraining's r2: 0.795898\tvalid_1's r2: 0.26935\n",
      "[627]\ttraining's r2: 0.796292\tvalid_1's r2: 0.269361\n",
      "[628]\ttraining's r2: 0.796598\tvalid_1's r2: 0.269185\n",
      "[629]\ttraining's r2: 0.796994\tvalid_1's r2: 0.269377\n",
      "[630]\ttraining's r2: 0.797309\tvalid_1's r2: 0.269383\n",
      "[631]\ttraining's r2: 0.797736\tvalid_1's r2: 0.26937\n",
      "[632]\ttraining's r2: 0.798053\tvalid_1's r2: 0.269147\n",
      "[633]\ttraining's r2: 0.798485\tvalid_1's r2: 0.268848\n",
      "[634]\ttraining's r2: 0.79882\tvalid_1's r2: 0.26877\n",
      "[635]\ttraining's r2: 0.799126\tvalid_1's r2: 0.268887\n",
      "[636]\ttraining's r2: 0.799537\tvalid_1's r2: 0.269049\n",
      "[637]\ttraining's r2: 0.799889\tvalid_1's r2: 0.269037\n",
      "[638]\ttraining's r2: 0.800192\tvalid_1's r2: 0.269273\n",
      "[639]\ttraining's r2: 0.800524\tvalid_1's r2: 0.269327\n",
      "[640]\ttraining's r2: 0.80091\tvalid_1's r2: 0.269541\n",
      "[641]\ttraining's r2: 0.801233\tvalid_1's r2: 0.269805\n",
      "[642]\ttraining's r2: 0.801686\tvalid_1's r2: 0.269936\n",
      "[643]\ttraining's r2: 0.802001\tvalid_1's r2: 0.270074\n",
      "[644]\ttraining's r2: 0.802356\tvalid_1's r2: 0.269945\n",
      "[645]\ttraining's r2: 0.802758\tvalid_1's r2: 0.269955\n",
      "[646]\ttraining's r2: 0.803087\tvalid_1's r2: 0.269886\n",
      "[647]\ttraining's r2: 0.803395\tvalid_1's r2: 0.269634\n",
      "[648]\ttraining's r2: 0.803813\tvalid_1's r2: 0.27016\n",
      "[649]\ttraining's r2: 0.804193\tvalid_1's r2: 0.270581\n",
      "[650]\ttraining's r2: 0.804582\tvalid_1's r2: 0.270525\n",
      "[651]\ttraining's r2: 0.804942\tvalid_1's r2: 0.27059\n",
      "[652]\ttraining's r2: 0.805246\tvalid_1's r2: 0.270543\n",
      "[653]\ttraining's r2: 0.805651\tvalid_1's r2: 0.270798\n",
      "[654]\ttraining's r2: 0.805987\tvalid_1's r2: 0.270635\n",
      "[655]\ttraining's r2: 0.806263\tvalid_1's r2: 0.270508\n",
      "[656]\ttraining's r2: 0.806531\tvalid_1's r2: 0.270161\n",
      "[657]\ttraining's r2: 0.806888\tvalid_1's r2: 0.270246\n",
      "[658]\ttraining's r2: 0.807176\tvalid_1's r2: 0.270419\n",
      "[659]\ttraining's r2: 0.807524\tvalid_1's r2: 0.270445\n",
      "[660]\ttraining's r2: 0.807846\tvalid_1's r2: 0.270389\n",
      "[661]\ttraining's r2: 0.808154\tvalid_1's r2: 0.270333\n",
      "[662]\ttraining's r2: 0.808474\tvalid_1's r2: 0.270113\n",
      "[663]\ttraining's r2: 0.808777\tvalid_1's r2: 0.270165\n",
      "[664]\ttraining's r2: 0.809066\tvalid_1's r2: 0.270402\n",
      "[665]\ttraining's r2: 0.809425\tvalid_1's r2: 0.270477\n",
      "[666]\ttraining's r2: 0.809716\tvalid_1's r2: 0.270297\n",
      "[667]\ttraining's r2: 0.810075\tvalid_1's r2: 0.270252\n",
      "[668]\ttraining's r2: 0.810299\tvalid_1's r2: 0.270138\n",
      "[669]\ttraining's r2: 0.810599\tvalid_1's r2: 0.270046\n",
      "[670]\ttraining's r2: 0.811025\tvalid_1's r2: 0.270381\n",
      "[671]\ttraining's r2: 0.81135\tvalid_1's r2: 0.270053\n",
      "[672]\ttraining's r2: 0.811651\tvalid_1's r2: 0.270174\n",
      "[673]\ttraining's r2: 0.811875\tvalid_1's r2: 0.270182\n",
      "[674]\ttraining's r2: 0.81221\tvalid_1's r2: 0.270113\n",
      "[675]\ttraining's r2: 0.812606\tvalid_1's r2: 0.27037\n",
      "[676]\ttraining's r2: 0.812963\tvalid_1's r2: 0.270186\n",
      "[677]\ttraining's r2: 0.813271\tvalid_1's r2: 0.270187\n",
      "[678]\ttraining's r2: 0.813559\tvalid_1's r2: 0.270159\n",
      "[679]\ttraining's r2: 0.813938\tvalid_1's r2: 0.270151\n",
      "[680]\ttraining's r2: 0.814251\tvalid_1's r2: 0.270261\n",
      "[681]\ttraining's r2: 0.81452\tvalid_1's r2: 0.270249\n",
      "[682]\ttraining's r2: 0.81481\tvalid_1's r2: 0.270083\n",
      "[683]\ttraining's r2: 0.815121\tvalid_1's r2: 0.27\n",
      "[684]\ttraining's r2: 0.815368\tvalid_1's r2: 0.269712\n",
      "[685]\ttraining's r2: 0.815706\tvalid_1's r2: 0.269534\n",
      "[686]\ttraining's r2: 0.816019\tvalid_1's r2: 0.269664\n",
      "[687]\ttraining's r2: 0.816332\tvalid_1's r2: 0.269654\n",
      "[688]\ttraining's r2: 0.816603\tvalid_1's r2: 0.26967\n",
      "[689]\ttraining's r2: 0.816927\tvalid_1's r2: 0.269732\n",
      "[690]\ttraining's r2: 0.817193\tvalid_1's r2: 0.269458\n",
      "[691]\ttraining's r2: 0.817534\tvalid_1's r2: 0.269774\n",
      "[692]\ttraining's r2: 0.817846\tvalid_1's r2: 0.269588\n",
      "[693]\ttraining's r2: 0.818216\tvalid_1's r2: 0.269743\n",
      "[694]\ttraining's r2: 0.818484\tvalid_1's r2: 0.269603\n",
      "[695]\ttraining's r2: 0.818785\tvalid_1's r2: 0.269593\n",
      "[696]\ttraining's r2: 0.819095\tvalid_1's r2: 0.269654\n",
      "[697]\ttraining's r2: 0.819378\tvalid_1's r2: 0.269493\n",
      "[698]\ttraining's r2: 0.819615\tvalid_1's r2: 0.269385\n",
      "[699]\ttraining's r2: 0.819902\tvalid_1's r2: 0.269542\n",
      "[700]\ttraining's r2: 0.820325\tvalid_1's r2: 0.269501\n",
      "[701]\ttraining's r2: 0.820659\tvalid_1's r2: 0.269497\n",
      "[702]\ttraining's r2: 0.820969\tvalid_1's r2: 0.269565\n",
      "[703]\ttraining's r2: 0.821232\tvalid_1's r2: 0.269253\n",
      "[704]\ttraining's r2: 0.821556\tvalid_1's r2: 0.26932\n",
      "[705]\ttraining's r2: 0.821836\tvalid_1's r2: 0.269489\n",
      "[706]\ttraining's r2: 0.822145\tvalid_1's r2: 0.269478\n",
      "[707]\ttraining's r2: 0.822377\tvalid_1's r2: 0.269415\n",
      "[708]\ttraining's r2: 0.822671\tvalid_1's r2: 0.269264\n",
      "[709]\ttraining's r2: 0.82293\tvalid_1's r2: 0.269266\n",
      "[710]\ttraining's r2: 0.823241\tvalid_1's r2: 0.269473\n",
      "[711]\ttraining's r2: 0.823513\tvalid_1's r2: 0.269271\n",
      "[712]\ttraining's r2: 0.823832\tvalid_1's r2: 0.269467\n",
      "[713]\ttraining's r2: 0.824115\tvalid_1's r2: 0.269665\n",
      "[714]\ttraining's r2: 0.824437\tvalid_1's r2: 0.269892\n",
      "[715]\ttraining's r2: 0.824797\tvalid_1's r2: 0.269676\n",
      "[716]\ttraining's r2: 0.824984\tvalid_1's r2: 0.269593\n",
      "[717]\ttraining's r2: 0.825247\tvalid_1's r2: 0.26941\n",
      "[718]\ttraining's r2: 0.825582\tvalid_1's r2: 0.269589\n",
      "[719]\ttraining's r2: 0.82588\tvalid_1's r2: 0.269575\n",
      "[720]\ttraining's r2: 0.826095\tvalid_1's r2: 0.269482\n",
      "[721]\ttraining's r2: 0.826432\tvalid_1's r2: 0.269775\n",
      "[722]\ttraining's r2: 0.82679\tvalid_1's r2: 0.269945\n",
      "[723]\ttraining's r2: 0.827044\tvalid_1's r2: 0.270052\n",
      "[724]\ttraining's r2: 0.827258\tvalid_1's r2: 0.269811\n",
      "[725]\ttraining's r2: 0.827518\tvalid_1's r2: 0.269847\n",
      "[726]\ttraining's r2: 0.827823\tvalid_1's r2: 0.269824\n",
      "[727]\ttraining's r2: 0.828077\tvalid_1's r2: 0.270043\n",
      "[728]\ttraining's r2: 0.828301\tvalid_1's r2: 0.269909\n",
      "[729]\ttraining's r2: 0.828565\tvalid_1's r2: 0.269976\n",
      "[730]\ttraining's r2: 0.82888\tvalid_1's r2: 0.27035\n",
      "[731]\ttraining's r2: 0.829124\tvalid_1's r2: 0.270347\n",
      "[732]\ttraining's r2: 0.82936\tvalid_1's r2: 0.270311\n",
      "[733]\ttraining's r2: 0.829693\tvalid_1's r2: 0.270492\n",
      "[734]\ttraining's r2: 0.829907\tvalid_1's r2: 0.270649\n",
      "[735]\ttraining's r2: 0.830151\tvalid_1's r2: 0.2705\n",
      "[736]\ttraining's r2: 0.830526\tvalid_1's r2: 0.27115\n",
      "[737]\ttraining's r2: 0.830716\tvalid_1's r2: 0.271225\n",
      "[738]\ttraining's r2: 0.831005\tvalid_1's r2: 0.271222\n",
      "[739]\ttraining's r2: 0.831349\tvalid_1's r2: 0.271546\n",
      "[740]\ttraining's r2: 0.831637\tvalid_1's r2: 0.271615\n",
      "[741]\ttraining's r2: 0.831899\tvalid_1's r2: 0.271626\n",
      "[742]\ttraining's r2: 0.832144\tvalid_1's r2: 0.271611\n",
      "[743]\ttraining's r2: 0.832453\tvalid_1's r2: 0.272194\n",
      "[744]\ttraining's r2: 0.832747\tvalid_1's r2: 0.272242\n",
      "[745]\ttraining's r2: 0.832973\tvalid_1's r2: 0.272429\n",
      "[746]\ttraining's r2: 0.83319\tvalid_1's r2: 0.272385\n",
      "[747]\ttraining's r2: 0.833502\tvalid_1's r2: 0.272283\n",
      "[748]\ttraining's r2: 0.833858\tvalid_1's r2: 0.272256\n",
      "[749]\ttraining's r2: 0.834096\tvalid_1's r2: 0.272258\n",
      "[750]\ttraining's r2: 0.834336\tvalid_1's r2: 0.27234\n",
      "[751]\ttraining's r2: 0.834537\tvalid_1's r2: 0.27192\n",
      "[752]\ttraining's r2: 0.834708\tvalid_1's r2: 0.271633\n",
      "[753]\ttraining's r2: 0.834923\tvalid_1's r2: 0.271913\n",
      "[754]\ttraining's r2: 0.835132\tvalid_1's r2: 0.271825\n",
      "[755]\ttraining's r2: 0.835355\tvalid_1's r2: 0.271233\n",
      "[756]\ttraining's r2: 0.8356\tvalid_1's r2: 0.271073\n",
      "[757]\ttraining's r2: 0.835885\tvalid_1's r2: 0.271007\n",
      "[758]\ttraining's r2: 0.836098\tvalid_1's r2: 0.271039\n",
      "[759]\ttraining's r2: 0.836388\tvalid_1's r2: 0.270927\n",
      "[760]\ttraining's r2: 0.836604\tvalid_1's r2: 0.271047\n",
      "[761]\ttraining's r2: 0.836912\tvalid_1's r2: 0.27087\n",
      "[762]\ttraining's r2: 0.837192\tvalid_1's r2: 0.271038\n",
      "[763]\ttraining's r2: 0.837387\tvalid_1's r2: 0.271101\n",
      "[764]\ttraining's r2: 0.837682\tvalid_1's r2: 0.271207\n",
      "[765]\ttraining's r2: 0.837923\tvalid_1's r2: 0.271285\n",
      "[766]\ttraining's r2: 0.838157\tvalid_1's r2: 0.270864\n",
      "[767]\ttraining's r2: 0.838397\tvalid_1's r2: 0.271046\n",
      "[768]\ttraining's r2: 0.838661\tvalid_1's r2: 0.271215\n",
      "[769]\ttraining's r2: 0.83892\tvalid_1's r2: 0.271095\n",
      "[770]\ttraining's r2: 0.839191\tvalid_1's r2: 0.271204\n",
      "[771]\ttraining's r2: 0.839426\tvalid_1's r2: 0.271369\n",
      "[772]\ttraining's r2: 0.839716\tvalid_1's r2: 0.271188\n",
      "[773]\ttraining's r2: 0.839991\tvalid_1's r2: 0.271298\n",
      "[774]\ttraining's r2: 0.840265\tvalid_1's r2: 0.271094\n",
      "[775]\ttraining's r2: 0.840485\tvalid_1's r2: 0.271186\n",
      "[776]\ttraining's r2: 0.840733\tvalid_1's r2: 0.271254\n",
      "[777]\ttraining's r2: 0.84096\tvalid_1's r2: 0.271496\n",
      "[778]\ttraining's r2: 0.841216\tvalid_1's r2: 0.271606\n",
      "[779]\ttraining's r2: 0.841436\tvalid_1's r2: 0.271608\n",
      "[780]\ttraining's r2: 0.841655\tvalid_1's r2: 0.271566\n",
      "[781]\ttraining's r2: 0.841875\tvalid_1's r2: 0.271449\n",
      "[782]\ttraining's r2: 0.842107\tvalid_1's r2: 0.271366\n",
      "[783]\ttraining's r2: 0.842414\tvalid_1's r2: 0.271268\n",
      "[784]\ttraining's r2: 0.842624\tvalid_1's r2: 0.271456\n",
      "[785]\ttraining's r2: 0.842885\tvalid_1's r2: 0.271362\n",
      "[786]\ttraining's r2: 0.843159\tvalid_1's r2: 0.271282\n",
      "[787]\ttraining's r2: 0.84341\tvalid_1's r2: 0.271041\n",
      "[788]\ttraining's r2: 0.843647\tvalid_1's r2: 0.271114\n",
      "[789]\ttraining's r2: 0.843926\tvalid_1's r2: 0.270922\n",
      "[790]\ttraining's r2: 0.844099\tvalid_1's r2: 0.271003\n",
      "[791]\ttraining's r2: 0.844343\tvalid_1's r2: 0.271222\n",
      "[792]\ttraining's r2: 0.84457\tvalid_1's r2: 0.271162\n",
      "[793]\ttraining's r2: 0.844818\tvalid_1's r2: 0.271155\n",
      "[794]\ttraining's r2: 0.845055\tvalid_1's r2: 0.271096\n",
      "[795]\ttraining's r2: 0.845277\tvalid_1's r2: 0.27126\n",
      "[796]\ttraining's r2: 0.845535\tvalid_1's r2: 0.271246\n",
      "[797]\ttraining's r2: 0.845796\tvalid_1's r2: 0.271435\n",
      "[798]\ttraining's r2: 0.846064\tvalid_1's r2: 0.271652\n",
      "[799]\ttraining's r2: 0.84636\tvalid_1's r2: 0.271691\n",
      "[800]\ttraining's r2: 0.846554\tvalid_1's r2: 0.271328\n",
      "[801]\ttraining's r2: 0.846731\tvalid_1's r2: 0.271323\n",
      "[802]\ttraining's r2: 0.84696\tvalid_1's r2: 0.271415\n",
      "[803]\ttraining's r2: 0.847216\tvalid_1's r2: 0.271446\n",
      "[804]\ttraining's r2: 0.847454\tvalid_1's r2: 0.271315\n",
      "[805]\ttraining's r2: 0.847627\tvalid_1's r2: 0.271512\n",
      "[806]\ttraining's r2: 0.847868\tvalid_1's r2: 0.271262\n",
      "[807]\ttraining's r2: 0.848091\tvalid_1's r2: 0.270852\n",
      "[808]\ttraining's r2: 0.848436\tvalid_1's r2: 0.270829\n",
      "[809]\ttraining's r2: 0.848659\tvalid_1's r2: 0.270869\n",
      "[810]\ttraining's r2: 0.848837\tvalid_1's r2: 0.271123\n",
      "[811]\ttraining's r2: 0.849122\tvalid_1's r2: 0.271378\n",
      "[812]\ttraining's r2: 0.849309\tvalid_1's r2: 0.271119\n",
      "[813]\ttraining's r2: 0.849477\tvalid_1's r2: 0.27109\n",
      "[814]\ttraining's r2: 0.849675\tvalid_1's r2: 0.270909\n",
      "[815]\ttraining's r2: 0.849946\tvalid_1's r2: 0.271066\n",
      "[816]\ttraining's r2: 0.850188\tvalid_1's r2: 0.271133\n",
      "[817]\ttraining's r2: 0.850359\tvalid_1's r2: 0.271104\n",
      "[818]\ttraining's r2: 0.850585\tvalid_1's r2: 0.271247\n",
      "[819]\ttraining's r2: 0.85078\tvalid_1's r2: 0.271207\n",
      "[820]\ttraining's r2: 0.851046\tvalid_1's r2: 0.271335\n",
      "[821]\ttraining's r2: 0.851289\tvalid_1's r2: 0.271567\n",
      "[822]\ttraining's r2: 0.85149\tvalid_1's r2: 0.271438\n",
      "[823]\ttraining's r2: 0.851734\tvalid_1's r2: 0.271287\n",
      "[824]\ttraining's r2: 0.851981\tvalid_1's r2: 0.271158\n",
      "[825]\ttraining's r2: 0.852164\tvalid_1's r2: 0.271126\n",
      "[826]\ttraining's r2: 0.852387\tvalid_1's r2: 0.270904\n",
      "[827]\ttraining's r2: 0.852586\tvalid_1's r2: 0.270737\n",
      "[828]\ttraining's r2: 0.852804\tvalid_1's r2: 0.270677\n",
      "[829]\ttraining's r2: 0.852993\tvalid_1's r2: 0.27052\n",
      "[830]\ttraining's r2: 0.853235\tvalid_1's r2: 0.27069\n",
      "[831]\ttraining's r2: 0.853476\tvalid_1's r2: 0.270939\n",
      "[832]\ttraining's r2: 0.853698\tvalid_1's r2: 0.270937\n",
      "[833]\ttraining's r2: 0.853865\tvalid_1's r2: 0.270836\n",
      "[834]\ttraining's r2: 0.854069\tvalid_1's r2: 0.271036\n",
      "[835]\ttraining's r2: 0.8542\tvalid_1's r2: 0.271224\n",
      "[836]\ttraining's r2: 0.854383\tvalid_1's r2: 0.27104\n",
      "[837]\ttraining's r2: 0.854619\tvalid_1's r2: 0.270878\n",
      "[838]\ttraining's r2: 0.854853\tvalid_1's r2: 0.270878\n",
      "[839]\ttraining's r2: 0.855064\tvalid_1's r2: 0.270996\n",
      "[840]\ttraining's r2: 0.855303\tvalid_1's r2: 0.270864\n",
      "[841]\ttraining's r2: 0.855528\tvalid_1's r2: 0.271188\n",
      "[842]\ttraining's r2: 0.855749\tvalid_1's r2: 0.271255\n",
      "[843]\ttraining's r2: 0.855971\tvalid_1's r2: 0.271194\n",
      "[844]\ttraining's r2: 0.856112\tvalid_1's r2: 0.271039\n",
      "[845]\ttraining's r2: 0.856283\tvalid_1's r2: 0.27081\n",
      "Early stopping, best iteration is:\n",
      "[745]\ttraining's r2: 0.832973\tvalid_1's r2: 0.272429\n",
      "[1]\ttraining's r2: -3653.09\tvalid_1's r2: -3346.39\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[2]\ttraining's r2: -845.81\tvalid_1's r2: -797.683\n",
      "[3]\ttraining's r2: -362.098\tvalid_1's r2: -345.222\n",
      "[4]\ttraining's r2: -200.286\tvalid_1's r2: -189.329\n",
      "[5]\ttraining's r2: -125.622\tvalid_1's r2: -118.064\n",
      "[6]\ttraining's r2: -86.4579\tvalid_1's r2: -81.4899\n",
      "[7]\ttraining's r2: -63.045\tvalid_1's r2: -59.4207\n",
      "[8]\ttraining's r2: -47.9061\tvalid_1's r2: -45.1854\n",
      "[9]\ttraining's r2: -37.2568\tvalid_1's r2: -35.0212\n",
      "[10]\ttraining's r2: -30.1281\tvalid_1's r2: -28.3589\n",
      "[11]\ttraining's r2: -24.7251\tvalid_1's r2: -23.295\n",
      "[12]\ttraining's r2: -20.6382\tvalid_1's r2: -19.4156\n",
      "[13]\ttraining's r2: -17.5216\tvalid_1's r2: -16.4847\n",
      "[14]\ttraining's r2: -15.0199\tvalid_1's r2: -14.1372\n",
      "[15]\ttraining's r2: -13.0332\tvalid_1's r2: -12.3049\n",
      "[16]\ttraining's r2: -11.4022\tvalid_1's r2: -10.7593\n",
      "[17]\ttraining's r2: -10.042\tvalid_1's r2: -9.48663\n",
      "[18]\ttraining's r2: -8.92007\tvalid_1's r2: -8.46136\n",
      "[19]\ttraining's r2: -7.96108\tvalid_1's r2: -7.54062\n",
      "[20]\ttraining's r2: -7.12303\tvalid_1's r2: -6.74333\n",
      "[21]\ttraining's r2: -6.40136\tvalid_1's r2: -6.07897\n",
      "[22]\ttraining's r2: -5.7995\tvalid_1's r2: -5.50425\n",
      "[23]\ttraining's r2: -5.25935\tvalid_1's r2: -5.00306\n",
      "[24]\ttraining's r2: -4.79774\tvalid_1's r2: -4.58127\n",
      "[25]\ttraining's r2: -4.38925\tvalid_1's r2: -4.19664\n",
      "[26]\ttraining's r2: -4.02637\tvalid_1's r2: -3.84659\n",
      "[27]\ttraining's r2: -3.70321\tvalid_1's r2: -3.54886\n",
      "[28]\ttraining's r2: -3.40004\tvalid_1's r2: -3.26368\n",
      "[29]\ttraining's r2: -3.14975\tvalid_1's r2: -3.03293\n",
      "[30]\ttraining's r2: -2.92029\tvalid_1's r2: -2.82412\n",
      "[31]\ttraining's r2: -2.70621\tvalid_1's r2: -2.62967\n",
      "[32]\ttraining's r2: -2.52377\tvalid_1's r2: -2.45933\n",
      "[33]\ttraining's r2: -2.35\tvalid_1's r2: -2.29413\n",
      "[34]\ttraining's r2: -2.19271\tvalid_1's r2: -2.14617\n",
      "[35]\ttraining's r2: -2.04602\tvalid_1's r2: -2.00592\n",
      "[36]\ttraining's r2: -1.92294\tvalid_1's r2: -1.89443\n",
      "[37]\ttraining's r2: -1.79688\tvalid_1's r2: -1.77794\n",
      "[38]\ttraining's r2: -1.67688\tvalid_1's r2: -1.66523\n",
      "[39]\ttraining's r2: -1.57064\tvalid_1's r2: -1.56708\n",
      "[40]\ttraining's r2: -1.4752\tvalid_1's r2: -1.4762\n",
      "[41]\ttraining's r2: -1.38134\tvalid_1's r2: -1.38702\n",
      "[42]\ttraining's r2: -1.29663\tvalid_1's r2: -1.30707\n",
      "[43]\ttraining's r2: -1.2204\tvalid_1's r2: -1.23448\n",
      "[44]\ttraining's r2: -1.1428\tvalid_1's r2: -1.16372\n",
      "[45]\ttraining's r2: -1.07513\tvalid_1's r2: -1.0999\n",
      "[46]\ttraining's r2: -1.00988\tvalid_1's r2: -1.03616\n",
      "[47]\ttraining's r2: -0.952628\tvalid_1's r2: -0.984208\n",
      "[48]\ttraining's r2: -0.895429\tvalid_1's r2: -0.929209\n",
      "[49]\ttraining's r2: -0.842311\tvalid_1's r2: -0.878877\n",
      "[50]\ttraining's r2: -0.795001\tvalid_1's r2: -0.831751\n",
      "[51]\ttraining's r2: -0.748387\tvalid_1's r2: -0.789033\n",
      "[52]\ttraining's r2: -0.702692\tvalid_1's r2: -0.744539\n",
      "[53]\ttraining's r2: -0.660741\tvalid_1's r2: -0.704919\n",
      "[54]\ttraining's r2: -0.622187\tvalid_1's r2: -0.671153\n",
      "[55]\ttraining's r2: -0.585308\tvalid_1's r2: -0.639024\n",
      "[56]\ttraining's r2: -0.548858\tvalid_1's r2: -0.6039\n",
      "[57]\ttraining's r2: -0.517057\tvalid_1's r2: -0.575455\n",
      "[58]\ttraining's r2: -0.484217\tvalid_1's r2: -0.543981\n",
      "[59]\ttraining's r2: -0.451837\tvalid_1's r2: -0.513536\n",
      "[60]\ttraining's r2: -0.422251\tvalid_1's r2: -0.48632\n",
      "[61]\ttraining's r2: -0.392445\tvalid_1's r2: -0.457291\n",
      "[62]\ttraining's r2: -0.367653\tvalid_1's r2: -0.43435\n",
      "[63]\ttraining's r2: -0.340406\tvalid_1's r2: -0.409399\n",
      "[64]\ttraining's r2: -0.314907\tvalid_1's r2: -0.386159\n",
      "[65]\ttraining's r2: -0.291861\tvalid_1's r2: -0.365326\n",
      "[66]\ttraining's r2: -0.269004\tvalid_1's r2: -0.34525\n",
      "[67]\ttraining's r2: -0.246178\tvalid_1's r2: -0.32517\n",
      "[68]\ttraining's r2: -0.226097\tvalid_1's r2: -0.307552\n",
      "[69]\ttraining's r2: -0.206648\tvalid_1's r2: -0.290479\n",
      "[70]\ttraining's r2: -0.18772\tvalid_1's r2: -0.274591\n",
      "[71]\ttraining's r2: -0.169512\tvalid_1's r2: -0.259064\n",
      "[72]\ttraining's r2: -0.153661\tvalid_1's r2: -0.243915\n",
      "[73]\ttraining's r2: -0.136805\tvalid_1's r2: -0.228255\n",
      "[74]\ttraining's r2: -0.120326\tvalid_1's r2: -0.212161\n",
      "[75]\ttraining's r2: -0.10504\tvalid_1's r2: -0.19878\n",
      "[76]\ttraining's r2: -0.0899929\tvalid_1's r2: -0.185873\n",
      "[77]\ttraining's r2: -0.0745726\tvalid_1's r2: -0.171232\n",
      "[78]\ttraining's r2: -0.060082\tvalid_1's r2: -0.15801\n",
      "[79]\ttraining's r2: -0.0459947\tvalid_1's r2: -0.144608\n",
      "[80]\ttraining's r2: -0.0320235\tvalid_1's r2: -0.133209\n",
      "[81]\ttraining's r2: -0.0192701\tvalid_1's r2: -0.122599\n",
      "[82]\ttraining's r2: -0.00671775\tvalid_1's r2: -0.111661\n",
      "[83]\ttraining's r2: 0.00427924\tvalid_1's r2: -0.103349\n",
      "[84]\ttraining's r2: 0.0162863\tvalid_1's r2: -0.0922891\n",
      "[85]\ttraining's r2: 0.0278951\tvalid_1's r2: -0.0828606\n",
      "[86]\ttraining's r2: 0.0383679\tvalid_1's r2: -0.0737745\n",
      "[87]\ttraining's r2: 0.0491085\tvalid_1's r2: -0.0652455\n",
      "[88]\ttraining's r2: 0.0599517\tvalid_1's r2: -0.0542241\n",
      "[89]\ttraining's r2: 0.0698859\tvalid_1's r2: -0.0472144\n",
      "[90]\ttraining's r2: 0.078582\tvalid_1's r2: -0.0410517\n",
      "[91]\ttraining's r2: 0.088097\tvalid_1's r2: -0.0339552\n",
      "[92]\ttraining's r2: 0.0967575\tvalid_1's r2: -0.0262462\n",
      "[93]\ttraining's r2: 0.105568\tvalid_1's r2: -0.0200577\n",
      "[94]\ttraining's r2: 0.114159\tvalid_1's r2: -0.0125337\n",
      "[95]\ttraining's r2: 0.122762\tvalid_1's r2: -0.00481555\n",
      "[96]\ttraining's r2: 0.131933\tvalid_1's r2: 0.00272767\n",
      "[97]\ttraining's r2: 0.139828\tvalid_1's r2: 0.00955967\n",
      "[98]\ttraining's r2: 0.148007\tvalid_1's r2: 0.0174106\n",
      "[99]\ttraining's r2: 0.155864\tvalid_1's r2: 0.0236305\n",
      "[100]\ttraining's r2: 0.163105\tvalid_1's r2: 0.0290004\n",
      "[101]\ttraining's r2: 0.170102\tvalid_1's r2: 0.0349539\n",
      "[102]\ttraining's r2: 0.177198\tvalid_1's r2: 0.0404649\n",
      "[103]\ttraining's r2: 0.183611\tvalid_1's r2: 0.0456662\n",
      "[104]\ttraining's r2: 0.190885\tvalid_1's r2: 0.0509865\n",
      "[105]\ttraining's r2: 0.196582\tvalid_1's r2: 0.0560467\n",
      "[106]\ttraining's r2: 0.203142\tvalid_1's r2: 0.0615733\n",
      "[107]\ttraining's r2: 0.208515\tvalid_1's r2: 0.0646572\n",
      "[108]\ttraining's r2: 0.214931\tvalid_1's r2: 0.0694838\n",
      "[109]\ttraining's r2: 0.220696\tvalid_1's r2: 0.0742727\n",
      "[110]\ttraining's r2: 0.226551\tvalid_1's r2: 0.0790082\n",
      "[111]\ttraining's r2: 0.232517\tvalid_1's r2: 0.084297\n",
      "[112]\ttraining's r2: 0.238188\tvalid_1's r2: 0.0881518\n",
      "[113]\ttraining's r2: 0.244131\tvalid_1's r2: 0.0934287\n",
      "[114]\ttraining's r2: 0.249331\tvalid_1's r2: 0.0974678\n",
      "[115]\ttraining's r2: 0.253228\tvalid_1's r2: 0.0995907\n",
      "[116]\ttraining's r2: 0.258303\tvalid_1's r2: 0.102806\n",
      "[117]\ttraining's r2: 0.263114\tvalid_1's r2: 0.106716\n",
      "[118]\ttraining's r2: 0.268104\tvalid_1's r2: 0.109914\n",
      "[119]\ttraining's r2: 0.272043\tvalid_1's r2: 0.111479\n",
      "[120]\ttraining's r2: 0.276574\tvalid_1's r2: 0.114994\n",
      "[121]\ttraining's r2: 0.281231\tvalid_1's r2: 0.118717\n",
      "[122]\ttraining's r2: 0.285021\tvalid_1's r2: 0.121242\n",
      "[123]\ttraining's r2: 0.289349\tvalid_1's r2: 0.125157\n",
      "[124]\ttraining's r2: 0.292722\tvalid_1's r2: 0.126272\n",
      "[125]\ttraining's r2: 0.295946\tvalid_1's r2: 0.12728\n",
      "[126]\ttraining's r2: 0.300069\tvalid_1's r2: 0.129504\n",
      "[127]\ttraining's r2: 0.304379\tvalid_1's r2: 0.133301\n",
      "[128]\ttraining's r2: 0.307946\tvalid_1's r2: 0.135161\n",
      "[129]\ttraining's r2: 0.312131\tvalid_1's r2: 0.139259\n",
      "[130]\ttraining's r2: 0.315835\tvalid_1's r2: 0.140494\n",
      "[131]\ttraining's r2: 0.319523\tvalid_1's r2: 0.142937\n",
      "[132]\ttraining's r2: 0.323241\tvalid_1's r2: 0.145063\n",
      "[133]\ttraining's r2: 0.325993\tvalid_1's r2: 0.14625\n",
      "[134]\ttraining's r2: 0.329953\tvalid_1's r2: 0.149509\n",
      "[135]\ttraining's r2: 0.333242\tvalid_1's r2: 0.151056\n",
      "[136]\ttraining's r2: 0.336222\tvalid_1's r2: 0.152392\n",
      "[137]\ttraining's r2: 0.338745\tvalid_1's r2: 0.153186\n",
      "[138]\ttraining's r2: 0.341148\tvalid_1's r2: 0.154079\n",
      "[139]\ttraining's r2: 0.344097\tvalid_1's r2: 0.154854\n",
      "[140]\ttraining's r2: 0.347054\tvalid_1's r2: 0.157112\n",
      "[141]\ttraining's r2: 0.349495\tvalid_1's r2: 0.158156\n",
      "[142]\ttraining's r2: 0.351975\tvalid_1's r2: 0.159412\n",
      "[143]\ttraining's r2: 0.354519\tvalid_1's r2: 0.159745\n",
      "[144]\ttraining's r2: 0.357148\tvalid_1's r2: 0.160396\n",
      "[145]\ttraining's r2: 0.359456\tvalid_1's r2: 0.161204\n",
      "[146]\ttraining's r2: 0.362085\tvalid_1's r2: 0.161812\n",
      "[147]\ttraining's r2: 0.364768\tvalid_1's r2: 0.16274\n",
      "[148]\ttraining's r2: 0.367766\tvalid_1's r2: 0.163887\n",
      "[149]\ttraining's r2: 0.370005\tvalid_1's r2: 0.16387\n",
      "[150]\ttraining's r2: 0.372242\tvalid_1's r2: 0.165272\n",
      "[151]\ttraining's r2: 0.374735\tvalid_1's r2: 0.166098\n",
      "[152]\ttraining's r2: 0.377403\tvalid_1's r2: 0.167804\n",
      "[153]\ttraining's r2: 0.379921\tvalid_1's r2: 0.168172\n",
      "[154]\ttraining's r2: 0.382254\tvalid_1's r2: 0.168781\n",
      "[155]\ttraining's r2: 0.384725\tvalid_1's r2: 0.169433\n",
      "[156]\ttraining's r2: 0.387786\tvalid_1's r2: 0.170988\n",
      "[157]\ttraining's r2: 0.389902\tvalid_1's r2: 0.171289\n",
      "[158]\ttraining's r2: 0.392078\tvalid_1's r2: 0.17142\n",
      "[159]\ttraining's r2: 0.394869\tvalid_1's r2: 0.172185\n",
      "[160]\ttraining's r2: 0.396861\tvalid_1's r2: 0.173905\n",
      "[161]\ttraining's r2: 0.399093\tvalid_1's r2: 0.174635\n",
      "[162]\ttraining's r2: 0.401273\tvalid_1's r2: 0.175563\n",
      "[163]\ttraining's r2: 0.403044\tvalid_1's r2: 0.176538\n",
      "[164]\ttraining's r2: 0.40527\tvalid_1's r2: 0.176885\n",
      "[165]\ttraining's r2: 0.407434\tvalid_1's r2: 0.177407\n",
      "[166]\ttraining's r2: 0.409195\tvalid_1's r2: 0.177859\n",
      "[167]\ttraining's r2: 0.411005\tvalid_1's r2: 0.178304\n",
      "[168]\ttraining's r2: 0.413477\tvalid_1's r2: 0.179197\n",
      "[169]\ttraining's r2: 0.416088\tvalid_1's r2: 0.181213\n",
      "[170]\ttraining's r2: 0.418314\tvalid_1's r2: 0.181861\n",
      "[171]\ttraining's r2: 0.420538\tvalid_1's r2: 0.183049\n",
      "[172]\ttraining's r2: 0.422678\tvalid_1's r2: 0.184059\n",
      "[173]\ttraining's r2: 0.424778\tvalid_1's r2: 0.185158\n",
      "[174]\ttraining's r2: 0.427015\tvalid_1's r2: 0.185941\n",
      "[175]\ttraining's r2: 0.428982\tvalid_1's r2: 0.186813\n",
      "[176]\ttraining's r2: 0.431152\tvalid_1's r2: 0.187446\n",
      "[177]\ttraining's r2: 0.433259\tvalid_1's r2: 0.187804\n",
      "[178]\ttraining's r2: 0.435171\tvalid_1's r2: 0.188201\n",
      "[179]\ttraining's r2: 0.43767\tvalid_1's r2: 0.188937\n",
      "[180]\ttraining's r2: 0.4397\tvalid_1's r2: 0.189594\n",
      "[181]\ttraining's r2: 0.441416\tvalid_1's r2: 0.190152\n",
      "[182]\ttraining's r2: 0.443335\tvalid_1's r2: 0.190771\n",
      "[183]\ttraining's r2: 0.444942\tvalid_1's r2: 0.190916\n",
      "[184]\ttraining's r2: 0.446483\tvalid_1's r2: 0.19135\n",
      "[185]\ttraining's r2: 0.448311\tvalid_1's r2: 0.192238\n",
      "[186]\ttraining's r2: 0.449991\tvalid_1's r2: 0.192491\n",
      "[187]\ttraining's r2: 0.452015\tvalid_1's r2: 0.193393\n",
      "[188]\ttraining's r2: 0.453721\tvalid_1's r2: 0.193871\n",
      "[189]\ttraining's r2: 0.455602\tvalid_1's r2: 0.193821\n",
      "[190]\ttraining's r2: 0.457235\tvalid_1's r2: 0.194364\n",
      "[191]\ttraining's r2: 0.459031\tvalid_1's r2: 0.194721\n",
      "[192]\ttraining's r2: 0.460814\tvalid_1's r2: 0.19507\n",
      "[193]\ttraining's r2: 0.462473\tvalid_1's r2: 0.196145\n",
      "[194]\ttraining's r2: 0.46406\tvalid_1's r2: 0.196845\n",
      "[195]\ttraining's r2: 0.465719\tvalid_1's r2: 0.197287\n",
      "[196]\ttraining's r2: 0.467265\tvalid_1's r2: 0.197275\n",
      "[197]\ttraining's r2: 0.46946\tvalid_1's r2: 0.198697\n",
      "[198]\ttraining's r2: 0.471137\tvalid_1's r2: 0.199418\n",
      "[199]\ttraining's r2: 0.472722\tvalid_1's r2: 0.199481\n",
      "[200]\ttraining's r2: 0.474461\tvalid_1's r2: 0.199859\n",
      "[201]\ttraining's r2: 0.476245\tvalid_1's r2: 0.200333\n",
      "[202]\ttraining's r2: 0.478215\tvalid_1's r2: 0.201522\n",
      "[203]\ttraining's r2: 0.480114\tvalid_1's r2: 0.20128\n",
      "[204]\ttraining's r2: 0.481942\tvalid_1's r2: 0.200934\n",
      "[205]\ttraining's r2: 0.483758\tvalid_1's r2: 0.201728\n",
      "[206]\ttraining's r2: 0.485217\tvalid_1's r2: 0.202016\n",
      "[207]\ttraining's r2: 0.486843\tvalid_1's r2: 0.20211\n",
      "[208]\ttraining's r2: 0.488662\tvalid_1's r2: 0.202634\n",
      "[209]\ttraining's r2: 0.48986\tvalid_1's r2: 0.203009\n",
      "[210]\ttraining's r2: 0.49137\tvalid_1's r2: 0.203717\n",
      "[211]\ttraining's r2: 0.493075\tvalid_1's r2: 0.20382\n",
      "[212]\ttraining's r2: 0.494662\tvalid_1's r2: 0.20466\n",
      "[213]\ttraining's r2: 0.496124\tvalid_1's r2: 0.204997\n",
      "[214]\ttraining's r2: 0.497444\tvalid_1's r2: 0.205127\n",
      "[215]\ttraining's r2: 0.499061\tvalid_1's r2: 0.206056\n",
      "[216]\ttraining's r2: 0.500485\tvalid_1's r2: 0.206068\n",
      "[217]\ttraining's r2: 0.501856\tvalid_1's r2: 0.206523\n",
      "[218]\ttraining's r2: 0.503501\tvalid_1's r2: 0.207186\n",
      "[219]\ttraining's r2: 0.504703\tvalid_1's r2: 0.206549\n",
      "[220]\ttraining's r2: 0.506376\tvalid_1's r2: 0.206699\n",
      "[221]\ttraining's r2: 0.507755\tvalid_1's r2: 0.207388\n",
      "[222]\ttraining's r2: 0.508768\tvalid_1's r2: 0.207319\n",
      "[223]\ttraining's r2: 0.510228\tvalid_1's r2: 0.207972\n",
      "[224]\ttraining's r2: 0.511604\tvalid_1's r2: 0.207691\n",
      "[225]\ttraining's r2: 0.513046\tvalid_1's r2: 0.207424\n",
      "[226]\ttraining's r2: 0.514531\tvalid_1's r2: 0.207256\n",
      "[227]\ttraining's r2: 0.516022\tvalid_1's r2: 0.207917\n",
      "[228]\ttraining's r2: 0.517781\tvalid_1's r2: 0.207757\n",
      "[229]\ttraining's r2: 0.519451\tvalid_1's r2: 0.207956\n",
      "[230]\ttraining's r2: 0.520812\tvalid_1's r2: 0.208122\n",
      "[231]\ttraining's r2: 0.522215\tvalid_1's r2: 0.208331\n",
      "[232]\ttraining's r2: 0.523863\tvalid_1's r2: 0.207984\n",
      "[233]\ttraining's r2: 0.525314\tvalid_1's r2: 0.207992\n",
      "[234]\ttraining's r2: 0.526845\tvalid_1's r2: 0.207914\n",
      "[235]\ttraining's r2: 0.528315\tvalid_1's r2: 0.208707\n",
      "[236]\ttraining's r2: 0.529479\tvalid_1's r2: 0.208369\n",
      "[237]\ttraining's r2: 0.530815\tvalid_1's r2: 0.20857\n",
      "[238]\ttraining's r2: 0.532366\tvalid_1's r2: 0.209099\n",
      "[239]\ttraining's r2: 0.533747\tvalid_1's r2: 0.209556\n",
      "[240]\ttraining's r2: 0.535199\tvalid_1's r2: 0.209766\n",
      "[241]\ttraining's r2: 0.53651\tvalid_1's r2: 0.211039\n",
      "[242]\ttraining's r2: 0.537995\tvalid_1's r2: 0.210972\n",
      "[243]\ttraining's r2: 0.539361\tvalid_1's r2: 0.211088\n",
      "[244]\ttraining's r2: 0.541009\tvalid_1's r2: 0.210958\n",
      "[245]\ttraining's r2: 0.54239\tvalid_1's r2: 0.211113\n",
      "[246]\ttraining's r2: 0.543729\tvalid_1's r2: 0.212029\n",
      "[247]\ttraining's r2: 0.545279\tvalid_1's r2: 0.211902\n",
      "[248]\ttraining's r2: 0.546584\tvalid_1's r2: 0.211955\n",
      "[249]\ttraining's r2: 0.547758\tvalid_1's r2: 0.212729\n",
      "[250]\ttraining's r2: 0.548856\tvalid_1's r2: 0.212236\n",
      "[251]\ttraining's r2: 0.550319\tvalid_1's r2: 0.212531\n",
      "[252]\ttraining's r2: 0.551417\tvalid_1's r2: 0.212857\n",
      "[253]\ttraining's r2: 0.552623\tvalid_1's r2: 0.212894\n",
      "[254]\ttraining's r2: 0.554003\tvalid_1's r2: 0.212723\n",
      "[255]\ttraining's r2: 0.555473\tvalid_1's r2: 0.213355\n",
      "[256]\ttraining's r2: 0.556192\tvalid_1's r2: 0.212534\n",
      "[257]\ttraining's r2: 0.557463\tvalid_1's r2: 0.212607\n",
      "[258]\ttraining's r2: 0.558704\tvalid_1's r2: 0.212803\n",
      "[259]\ttraining's r2: 0.559868\tvalid_1's r2: 0.213514\n",
      "[260]\ttraining's r2: 0.560988\tvalid_1's r2: 0.213437\n",
      "[261]\ttraining's r2: 0.561989\tvalid_1's r2: 0.213021\n",
      "[262]\ttraining's r2: 0.563069\tvalid_1's r2: 0.213099\n",
      "[263]\ttraining's r2: 0.564152\tvalid_1's r2: 0.213455\n",
      "[264]\ttraining's r2: 0.565627\tvalid_1's r2: 0.214189\n",
      "[265]\ttraining's r2: 0.566683\tvalid_1's r2: 0.214849\n",
      "[266]\ttraining's r2: 0.568061\tvalid_1's r2: 0.214808\n",
      "[267]\ttraining's r2: 0.569306\tvalid_1's r2: 0.215213\n",
      "[268]\ttraining's r2: 0.570498\tvalid_1's r2: 0.215678\n",
      "[269]\ttraining's r2: 0.571609\tvalid_1's r2: 0.21585\n",
      "[270]\ttraining's r2: 0.572837\tvalid_1's r2: 0.215931\n",
      "[271]\ttraining's r2: 0.574165\tvalid_1's r2: 0.216857\n",
      "[272]\ttraining's r2: 0.575341\tvalid_1's r2: 0.217139\n",
      "[273]\ttraining's r2: 0.57652\tvalid_1's r2: 0.217575\n",
      "[274]\ttraining's r2: 0.577506\tvalid_1's r2: 0.217626\n",
      "[275]\ttraining's r2: 0.578976\tvalid_1's r2: 0.218472\n",
      "[276]\ttraining's r2: 0.579901\tvalid_1's r2: 0.21804\n",
      "[277]\ttraining's r2: 0.58107\tvalid_1's r2: 0.218299\n",
      "[278]\ttraining's r2: 0.582242\tvalid_1's r2: 0.218949\n",
      "[279]\ttraining's r2: 0.583315\tvalid_1's r2: 0.219853\n",
      "[280]\ttraining's r2: 0.584299\tvalid_1's r2: 0.219953\n",
      "[281]\ttraining's r2: 0.585434\tvalid_1's r2: 0.22035\n",
      "[282]\ttraining's r2: 0.586415\tvalid_1's r2: 0.220343\n",
      "[283]\ttraining's r2: 0.587396\tvalid_1's r2: 0.220344\n",
      "[284]\ttraining's r2: 0.588321\tvalid_1's r2: 0.220458\n",
      "[285]\ttraining's r2: 0.5892\tvalid_1's r2: 0.220615\n",
      "[286]\ttraining's r2: 0.590455\tvalid_1's r2: 0.221249\n",
      "[287]\ttraining's r2: 0.591891\tvalid_1's r2: 0.221903\n",
      "[288]\ttraining's r2: 0.592879\tvalid_1's r2: 0.221622\n",
      "[289]\ttraining's r2: 0.593788\tvalid_1's r2: 0.221591\n",
      "[290]\ttraining's r2: 0.594865\tvalid_1's r2: 0.221387\n",
      "[291]\ttraining's r2: 0.595882\tvalid_1's r2: 0.221058\n",
      "[292]\ttraining's r2: 0.59675\tvalid_1's r2: 0.221544\n",
      "[293]\ttraining's r2: 0.597658\tvalid_1's r2: 0.221004\n",
      "[294]\ttraining's r2: 0.598943\tvalid_1's r2: 0.221304\n",
      "[295]\ttraining's r2: 0.600097\tvalid_1's r2: 0.221777\n",
      "[296]\ttraining's r2: 0.600938\tvalid_1's r2: 0.222136\n",
      "[297]\ttraining's r2: 0.602023\tvalid_1's r2: 0.222639\n",
      "[298]\ttraining's r2: 0.603088\tvalid_1's r2: 0.222772\n",
      "[299]\ttraining's r2: 0.604132\tvalid_1's r2: 0.22328\n",
      "[300]\ttraining's r2: 0.60518\tvalid_1's r2: 0.223845\n",
      "[301]\ttraining's r2: 0.606299\tvalid_1's r2: 0.224175\n",
      "[302]\ttraining's r2: 0.607218\tvalid_1's r2: 0.224095\n",
      "[303]\ttraining's r2: 0.608117\tvalid_1's r2: 0.223881\n",
      "[304]\ttraining's r2: 0.609113\tvalid_1's r2: 0.224168\n",
      "[305]\ttraining's r2: 0.609976\tvalid_1's r2: 0.224131\n",
      "[306]\ttraining's r2: 0.611136\tvalid_1's r2: 0.224433\n",
      "[307]\ttraining's r2: 0.611976\tvalid_1's r2: 0.224145\n",
      "[308]\ttraining's r2: 0.612931\tvalid_1's r2: 0.22414\n",
      "[309]\ttraining's r2: 0.613877\tvalid_1's r2: 0.223898\n",
      "[310]\ttraining's r2: 0.614979\tvalid_1's r2: 0.224663\n",
      "[311]\ttraining's r2: 0.615758\tvalid_1's r2: 0.225187\n",
      "[312]\ttraining's r2: 0.616806\tvalid_1's r2: 0.22578\n",
      "[313]\ttraining's r2: 0.617695\tvalid_1's r2: 0.226206\n",
      "[314]\ttraining's r2: 0.618477\tvalid_1's r2: 0.226336\n",
      "[315]\ttraining's r2: 0.619437\tvalid_1's r2: 0.226224\n",
      "[316]\ttraining's r2: 0.620495\tvalid_1's r2: 0.226823\n",
      "[317]\ttraining's r2: 0.62135\tvalid_1's r2: 0.227189\n",
      "[318]\ttraining's r2: 0.622278\tvalid_1's r2: 0.227231\n",
      "[319]\ttraining's r2: 0.623193\tvalid_1's r2: 0.227008\n",
      "[320]\ttraining's r2: 0.623883\tvalid_1's r2: 0.226793\n",
      "[321]\ttraining's r2: 0.62474\tvalid_1's r2: 0.227152\n",
      "[322]\ttraining's r2: 0.62567\tvalid_1's r2: 0.227173\n",
      "[323]\ttraining's r2: 0.626755\tvalid_1's r2: 0.227466\n",
      "[324]\ttraining's r2: 0.627664\tvalid_1's r2: 0.227699\n",
      "[325]\ttraining's r2: 0.628449\tvalid_1's r2: 0.227679\n",
      "[326]\ttraining's r2: 0.629206\tvalid_1's r2: 0.227538\n",
      "[327]\ttraining's r2: 0.630018\tvalid_1's r2: 0.227982\n",
      "[328]\ttraining's r2: 0.631088\tvalid_1's r2: 0.228142\n",
      "[329]\ttraining's r2: 0.632095\tvalid_1's r2: 0.228017\n",
      "[330]\ttraining's r2: 0.632955\tvalid_1's r2: 0.227582\n",
      "[331]\ttraining's r2: 0.633734\tvalid_1's r2: 0.228314\n",
      "[332]\ttraining's r2: 0.634663\tvalid_1's r2: 0.228389\n",
      "[333]\ttraining's r2: 0.635361\tvalid_1's r2: 0.228742\n",
      "[334]\ttraining's r2: 0.63627\tvalid_1's r2: 0.22846\n",
      "[335]\ttraining's r2: 0.636919\tvalid_1's r2: 0.228554\n",
      "[336]\ttraining's r2: 0.637992\tvalid_1's r2: 0.228474\n",
      "[337]\ttraining's r2: 0.63898\tvalid_1's r2: 0.22876\n",
      "[338]\ttraining's r2: 0.639862\tvalid_1's r2: 0.229017\n",
      "[339]\ttraining's r2: 0.640644\tvalid_1's r2: 0.228971\n",
      "[340]\ttraining's r2: 0.641448\tvalid_1's r2: 0.229344\n",
      "[341]\ttraining's r2: 0.642271\tvalid_1's r2: 0.229284\n",
      "[342]\ttraining's r2: 0.643096\tvalid_1's r2: 0.229588\n",
      "[343]\ttraining's r2: 0.644276\tvalid_1's r2: 0.230325\n",
      "[344]\ttraining's r2: 0.645058\tvalid_1's r2: 0.230441\n",
      "[345]\ttraining's r2: 0.645771\tvalid_1's r2: 0.231157\n",
      "[346]\ttraining's r2: 0.646451\tvalid_1's r2: 0.231445\n",
      "[347]\ttraining's r2: 0.647192\tvalid_1's r2: 0.231552\n",
      "[348]\ttraining's r2: 0.648044\tvalid_1's r2: 0.231098\n",
      "[349]\ttraining's r2: 0.648848\tvalid_1's r2: 0.231176\n",
      "[350]\ttraining's r2: 0.649625\tvalid_1's r2: 0.231238\n",
      "[351]\ttraining's r2: 0.650553\tvalid_1's r2: 0.231693\n",
      "[352]\ttraining's r2: 0.651596\tvalid_1's r2: 0.231606\n",
      "[353]\ttraining's r2: 0.652413\tvalid_1's r2: 0.231354\n",
      "[354]\ttraining's r2: 0.653096\tvalid_1's r2: 0.231227\n",
      "[355]\ttraining's r2: 0.65363\tvalid_1's r2: 0.23163\n",
      "[356]\ttraining's r2: 0.654543\tvalid_1's r2: 0.231804\n",
      "[357]\ttraining's r2: 0.655296\tvalid_1's r2: 0.231859\n",
      "[358]\ttraining's r2: 0.656198\tvalid_1's r2: 0.232009\n",
      "[359]\ttraining's r2: 0.656909\tvalid_1's r2: 0.231713\n",
      "[360]\ttraining's r2: 0.657735\tvalid_1's r2: 0.231569\n",
      "[361]\ttraining's r2: 0.658532\tvalid_1's r2: 0.231495\n",
      "[362]\ttraining's r2: 0.659419\tvalid_1's r2: 0.231594\n",
      "[363]\ttraining's r2: 0.660237\tvalid_1's r2: 0.232006\n",
      "[364]\ttraining's r2: 0.660907\tvalid_1's r2: 0.231917\n",
      "[365]\ttraining's r2: 0.661753\tvalid_1's r2: 0.232552\n",
      "[366]\ttraining's r2: 0.662622\tvalid_1's r2: 0.232858\n",
      "[367]\ttraining's r2: 0.663324\tvalid_1's r2: 0.232755\n",
      "[368]\ttraining's r2: 0.664212\tvalid_1's r2: 0.232368\n",
      "[369]\ttraining's r2: 0.664984\tvalid_1's r2: 0.232132\n",
      "[370]\ttraining's r2: 0.665807\tvalid_1's r2: 0.232333\n",
      "[371]\ttraining's r2: 0.666573\tvalid_1's r2: 0.232606\n",
      "[372]\ttraining's r2: 0.667281\tvalid_1's r2: 0.232794\n",
      "[373]\ttraining's r2: 0.6678\tvalid_1's r2: 0.232091\n",
      "[374]\ttraining's r2: 0.668757\tvalid_1's r2: 0.23214\n",
      "[375]\ttraining's r2: 0.669333\tvalid_1's r2: 0.232354\n",
      "[376]\ttraining's r2: 0.670016\tvalid_1's r2: 0.232103\n",
      "[377]\ttraining's r2: 0.670714\tvalid_1's r2: 0.232302\n",
      "[378]\ttraining's r2: 0.671353\tvalid_1's r2: 0.231956\n",
      "[379]\ttraining's r2: 0.672152\tvalid_1's r2: 0.23257\n",
      "[380]\ttraining's r2: 0.672919\tvalid_1's r2: 0.232577\n",
      "[381]\ttraining's r2: 0.673525\tvalid_1's r2: 0.232653\n",
      "[382]\ttraining's r2: 0.674333\tvalid_1's r2: 0.233146\n",
      "[383]\ttraining's r2: 0.675096\tvalid_1's r2: 0.232966\n",
      "[384]\ttraining's r2: 0.676043\tvalid_1's r2: 0.232542\n",
      "[385]\ttraining's r2: 0.67699\tvalid_1's r2: 0.232651\n",
      "[386]\ttraining's r2: 0.677595\tvalid_1's r2: 0.232424\n",
      "[387]\ttraining's r2: 0.678461\tvalid_1's r2: 0.232664\n",
      "[388]\ttraining's r2: 0.679113\tvalid_1's r2: 0.232687\n",
      "[389]\ttraining's r2: 0.67986\tvalid_1's r2: 0.233013\n",
      "[390]\ttraining's r2: 0.680716\tvalid_1's r2: 0.233233\n",
      "[391]\ttraining's r2: 0.681569\tvalid_1's r2: 0.233394\n",
      "[392]\ttraining's r2: 0.682229\tvalid_1's r2: 0.233574\n",
      "[393]\ttraining's r2: 0.682899\tvalid_1's r2: 0.233606\n",
      "[394]\ttraining's r2: 0.683501\tvalid_1's r2: 0.233608\n",
      "[395]\ttraining's r2: 0.684252\tvalid_1's r2: 0.233796\n",
      "[396]\ttraining's r2: 0.684813\tvalid_1's r2: 0.233809\n",
      "[397]\ttraining's r2: 0.685587\tvalid_1's r2: 0.234006\n",
      "[398]\ttraining's r2: 0.686451\tvalid_1's r2: 0.23408\n",
      "[399]\ttraining's r2: 0.687121\tvalid_1's r2: 0.233939\n",
      "[400]\ttraining's r2: 0.687895\tvalid_1's r2: 0.23402\n",
      "[401]\ttraining's r2: 0.68841\tvalid_1's r2: 0.234016\n",
      "[402]\ttraining's r2: 0.689127\tvalid_1's r2: 0.234221\n",
      "[403]\ttraining's r2: 0.689705\tvalid_1's r2: 0.234394\n",
      "[404]\ttraining's r2: 0.690567\tvalid_1's r2: 0.234546\n",
      "[405]\ttraining's r2: 0.691296\tvalid_1's r2: 0.234718\n",
      "[406]\ttraining's r2: 0.692031\tvalid_1's r2: 0.234481\n",
      "[407]\ttraining's r2: 0.692751\tvalid_1's r2: 0.234518\n",
      "[408]\ttraining's r2: 0.693525\tvalid_1's r2: 0.235147\n",
      "[409]\ttraining's r2: 0.694132\tvalid_1's r2: 0.23531\n",
      "[410]\ttraining's r2: 0.69472\tvalid_1's r2: 0.235275\n",
      "[411]\ttraining's r2: 0.695721\tvalid_1's r2: 0.235745\n",
      "[412]\ttraining's r2: 0.696226\tvalid_1's r2: 0.235949\n",
      "[413]\ttraining's r2: 0.696871\tvalid_1's r2: 0.235827\n",
      "[414]\ttraining's r2: 0.697616\tvalid_1's r2: 0.236199\n",
      "[415]\ttraining's r2: 0.698369\tvalid_1's r2: 0.23651\n",
      "[416]\ttraining's r2: 0.699048\tvalid_1's r2: 0.236607\n",
      "[417]\ttraining's r2: 0.699737\tvalid_1's r2: 0.236818\n",
      "[418]\ttraining's r2: 0.700441\tvalid_1's r2: 0.23706\n",
      "[419]\ttraining's r2: 0.701269\tvalid_1's r2: 0.236732\n",
      "[420]\ttraining's r2: 0.701878\tvalid_1's r2: 0.237051\n",
      "[421]\ttraining's r2: 0.702525\tvalid_1's r2: 0.237448\n",
      "[422]\ttraining's r2: 0.703235\tvalid_1's r2: 0.23756\n",
      "[423]\ttraining's r2: 0.703743\tvalid_1's r2: 0.23795\n",
      "[424]\ttraining's r2: 0.704444\tvalid_1's r2: 0.237894\n",
      "[425]\ttraining's r2: 0.705124\tvalid_1's r2: 0.238385\n",
      "[426]\ttraining's r2: 0.705779\tvalid_1's r2: 0.238538\n",
      "[427]\ttraining's r2: 0.706328\tvalid_1's r2: 0.238464\n",
      "[428]\ttraining's r2: 0.706967\tvalid_1's r2: 0.238777\n",
      "[429]\ttraining's r2: 0.707682\tvalid_1's r2: 0.238834\n",
      "[430]\ttraining's r2: 0.708306\tvalid_1's r2: 0.239096\n",
      "[431]\ttraining's r2: 0.708894\tvalid_1's r2: 0.238937\n",
      "[432]\ttraining's r2: 0.709471\tvalid_1's r2: 0.239045\n",
      "[433]\ttraining's r2: 0.710227\tvalid_1's r2: 0.23954\n",
      "[434]\ttraining's r2: 0.710679\tvalid_1's r2: 0.239238\n",
      "[435]\ttraining's r2: 0.711253\tvalid_1's r2: 0.239846\n",
      "[436]\ttraining's r2: 0.711848\tvalid_1's r2: 0.239488\n",
      "[437]\ttraining's r2: 0.71252\tvalid_1's r2: 0.23888\n",
      "[438]\ttraining's r2: 0.713134\tvalid_1's r2: 0.238876\n",
      "[439]\ttraining's r2: 0.713708\tvalid_1's r2: 0.238567\n",
      "[440]\ttraining's r2: 0.714439\tvalid_1's r2: 0.238946\n",
      "[441]\ttraining's r2: 0.715183\tvalid_1's r2: 0.239407\n",
      "[442]\ttraining's r2: 0.715896\tvalid_1's r2: 0.239197\n",
      "[443]\ttraining's r2: 0.716412\tvalid_1's r2: 0.239252\n",
      "[444]\ttraining's r2: 0.716948\tvalid_1's r2: 0.239165\n",
      "[445]\ttraining's r2: 0.717475\tvalid_1's r2: 0.239153\n",
      "[446]\ttraining's r2: 0.718052\tvalid_1's r2: 0.239178\n",
      "[447]\ttraining's r2: 0.718715\tvalid_1's r2: 0.239293\n",
      "[448]\ttraining's r2: 0.719321\tvalid_1's r2: 0.239419\n",
      "[449]\ttraining's r2: 0.719875\tvalid_1's r2: 0.239557\n",
      "[450]\ttraining's r2: 0.720424\tvalid_1's r2: 0.239464\n",
      "[451]\ttraining's r2: 0.720981\tvalid_1's r2: 0.239614\n",
      "[452]\ttraining's r2: 0.721593\tvalid_1's r2: 0.239546\n",
      "[453]\ttraining's r2: 0.722146\tvalid_1's r2: 0.239739\n",
      "[454]\ttraining's r2: 0.722666\tvalid_1's r2: 0.239601\n",
      "[455]\ttraining's r2: 0.72316\tvalid_1's r2: 0.239713\n",
      "[456]\ttraining's r2: 0.723633\tvalid_1's r2: 0.239825\n",
      "[457]\ttraining's r2: 0.724205\tvalid_1's r2: 0.239508\n",
      "[458]\ttraining's r2: 0.724795\tvalid_1's r2: 0.239887\n",
      "[459]\ttraining's r2: 0.725358\tvalid_1's r2: 0.239848\n",
      "[460]\ttraining's r2: 0.725847\tvalid_1's r2: 0.240006\n",
      "[461]\ttraining's r2: 0.726255\tvalid_1's r2: 0.240137\n",
      "[462]\ttraining's r2: 0.726728\tvalid_1's r2: 0.239873\n",
      "[463]\ttraining's r2: 0.727231\tvalid_1's r2: 0.240025\n",
      "[464]\ttraining's r2: 0.727889\tvalid_1's r2: 0.240082\n",
      "[465]\ttraining's r2: 0.72842\tvalid_1's r2: 0.239681\n",
      "[466]\ttraining's r2: 0.728913\tvalid_1's r2: 0.240133\n",
      "[467]\ttraining's r2: 0.729372\tvalid_1's r2: 0.240085\n",
      "[468]\ttraining's r2: 0.730039\tvalid_1's r2: 0.240318\n",
      "[469]\ttraining's r2: 0.730547\tvalid_1's r2: 0.240851\n",
      "[470]\ttraining's r2: 0.731154\tvalid_1's r2: 0.240846\n",
      "[471]\ttraining's r2: 0.731701\tvalid_1's r2: 0.240584\n",
      "[472]\ttraining's r2: 0.73235\tvalid_1's r2: 0.24069\n",
      "[473]\ttraining's r2: 0.732808\tvalid_1's r2: 0.240886\n",
      "[474]\ttraining's r2: 0.733336\tvalid_1's r2: 0.240899\n",
      "[475]\ttraining's r2: 0.733939\tvalid_1's r2: 0.241356\n",
      "[476]\ttraining's r2: 0.734544\tvalid_1's r2: 0.241295\n",
      "[477]\ttraining's r2: 0.73505\tvalid_1's r2: 0.241364\n",
      "[478]\ttraining's r2: 0.73552\tvalid_1's r2: 0.240946\n",
      "[479]\ttraining's r2: 0.736031\tvalid_1's r2: 0.24119\n",
      "[480]\ttraining's r2: 0.736463\tvalid_1's r2: 0.241268\n",
      "[481]\ttraining's r2: 0.73707\tvalid_1's r2: 0.241134\n",
      "[482]\ttraining's r2: 0.737545\tvalid_1's r2: 0.240754\n",
      "[483]\ttraining's r2: 0.738039\tvalid_1's r2: 0.24065\n",
      "[484]\ttraining's r2: 0.738543\tvalid_1's r2: 0.240678\n",
      "[485]\ttraining's r2: 0.738979\tvalid_1's r2: 0.241224\n",
      "[486]\ttraining's r2: 0.739431\tvalid_1's r2: 0.24114\n",
      "[487]\ttraining's r2: 0.739928\tvalid_1's r2: 0.240933\n",
      "[488]\ttraining's r2: 0.74055\tvalid_1's r2: 0.241313\n",
      "[489]\ttraining's r2: 0.741024\tvalid_1's r2: 0.241388\n",
      "[490]\ttraining's r2: 0.74156\tvalid_1's r2: 0.241546\n",
      "[491]\ttraining's r2: 0.742115\tvalid_1's r2: 0.241983\n",
      "[492]\ttraining's r2: 0.742589\tvalid_1's r2: 0.241919\n",
      "[493]\ttraining's r2: 0.743079\tvalid_1's r2: 0.241391\n",
      "[494]\ttraining's r2: 0.743574\tvalid_1's r2: 0.241518\n",
      "[495]\ttraining's r2: 0.744148\tvalid_1's r2: 0.241429\n",
      "[496]\ttraining's r2: 0.744705\tvalid_1's r2: 0.241603\n",
      "[497]\ttraining's r2: 0.745282\tvalid_1's r2: 0.241592\n",
      "[498]\ttraining's r2: 0.745832\tvalid_1's r2: 0.241503\n",
      "[499]\ttraining's r2: 0.746334\tvalid_1's r2: 0.241282\n",
      "[500]\ttraining's r2: 0.746703\tvalid_1's r2: 0.241389\n",
      "[501]\ttraining's r2: 0.747133\tvalid_1's r2: 0.241406\n",
      "[502]\ttraining's r2: 0.747734\tvalid_1's r2: 0.241666\n",
      "[503]\ttraining's r2: 0.748282\tvalid_1's r2: 0.241325\n",
      "[504]\ttraining's r2: 0.748782\tvalid_1's r2: 0.24121\n",
      "[505]\ttraining's r2: 0.74924\tvalid_1's r2: 0.241198\n",
      "[506]\ttraining's r2: 0.749658\tvalid_1's r2: 0.241135\n",
      "[507]\ttraining's r2: 0.750143\tvalid_1's r2: 0.241261\n",
      "[508]\ttraining's r2: 0.750519\tvalid_1's r2: 0.241251\n",
      "[509]\ttraining's r2: 0.750943\tvalid_1's r2: 0.24124\n",
      "[510]\ttraining's r2: 0.751481\tvalid_1's r2: 0.241795\n",
      "[511]\ttraining's r2: 0.751944\tvalid_1's r2: 0.242378\n",
      "[512]\ttraining's r2: 0.752493\tvalid_1's r2: 0.241997\n",
      "[513]\ttraining's r2: 0.752979\tvalid_1's r2: 0.242274\n",
      "[514]\ttraining's r2: 0.753353\tvalid_1's r2: 0.24272\n",
      "[515]\ttraining's r2: 0.753862\tvalid_1's r2: 0.242524\n",
      "[516]\ttraining's r2: 0.754217\tvalid_1's r2: 0.242818\n",
      "[517]\ttraining's r2: 0.754778\tvalid_1's r2: 0.243084\n",
      "[518]\ttraining's r2: 0.755151\tvalid_1's r2: 0.243272\n",
      "[519]\ttraining's r2: 0.755596\tvalid_1's r2: 0.243052\n",
      "[520]\ttraining's r2: 0.756022\tvalid_1's r2: 0.243071\n",
      "[521]\ttraining's r2: 0.756476\tvalid_1's r2: 0.243106\n",
      "[522]\ttraining's r2: 0.756852\tvalid_1's r2: 0.242929\n",
      "[523]\ttraining's r2: 0.757491\tvalid_1's r2: 0.243625\n",
      "[524]\ttraining's r2: 0.757889\tvalid_1's r2: 0.243286\n",
      "[525]\ttraining's r2: 0.758429\tvalid_1's r2: 0.243536\n",
      "[526]\ttraining's r2: 0.758781\tvalid_1's r2: 0.243629\n",
      "[527]\ttraining's r2: 0.75925\tvalid_1's r2: 0.243388\n",
      "[528]\ttraining's r2: 0.759784\tvalid_1's r2: 0.243572\n",
      "[529]\ttraining's r2: 0.760346\tvalid_1's r2: 0.243625\n",
      "[530]\ttraining's r2: 0.760837\tvalid_1's r2: 0.243673\n",
      "[531]\ttraining's r2: 0.761258\tvalid_1's r2: 0.243573\n",
      "[532]\ttraining's r2: 0.761723\tvalid_1's r2: 0.243924\n",
      "[533]\ttraining's r2: 0.76213\tvalid_1's r2: 0.243951\n",
      "[534]\ttraining's r2: 0.762598\tvalid_1's r2: 0.243797\n",
      "[535]\ttraining's r2: 0.762939\tvalid_1's r2: 0.24375\n",
      "[536]\ttraining's r2: 0.763468\tvalid_1's r2: 0.244257\n",
      "[537]\ttraining's r2: 0.763859\tvalid_1's r2: 0.244521\n",
      "[538]\ttraining's r2: 0.764225\tvalid_1's r2: 0.244223\n",
      "[539]\ttraining's r2: 0.764655\tvalid_1's r2: 0.244479\n",
      "[540]\ttraining's r2: 0.765043\tvalid_1's r2: 0.24434\n",
      "[541]\ttraining's r2: 0.765439\tvalid_1's r2: 0.244773\n",
      "[542]\ttraining's r2: 0.765748\tvalid_1's r2: 0.244436\n",
      "[543]\ttraining's r2: 0.766049\tvalid_1's r2: 0.244384\n",
      "[544]\ttraining's r2: 0.766428\tvalid_1's r2: 0.244455\n",
      "[545]\ttraining's r2: 0.766854\tvalid_1's r2: 0.244568\n",
      "[546]\ttraining's r2: 0.767175\tvalid_1's r2: 0.244375\n",
      "[547]\ttraining's r2: 0.767553\tvalid_1's r2: 0.244327\n",
      "[548]\ttraining's r2: 0.767978\tvalid_1's r2: 0.244743\n",
      "[549]\ttraining's r2: 0.768414\tvalid_1's r2: 0.244544\n",
      "[550]\ttraining's r2: 0.768742\tvalid_1's r2: 0.244354\n",
      "[551]\ttraining's r2: 0.769199\tvalid_1's r2: 0.244362\n",
      "[552]\ttraining's r2: 0.769694\tvalid_1's r2: 0.244281\n",
      "[553]\ttraining's r2: 0.770202\tvalid_1's r2: 0.244265\n",
      "[554]\ttraining's r2: 0.770635\tvalid_1's r2: 0.244573\n",
      "[555]\ttraining's r2: 0.771114\tvalid_1's r2: 0.244317\n",
      "[556]\ttraining's r2: 0.771544\tvalid_1's r2: 0.244371\n",
      "[557]\ttraining's r2: 0.771988\tvalid_1's r2: 0.24445\n",
      "[558]\ttraining's r2: 0.772492\tvalid_1's r2: 0.245004\n",
      "[559]\ttraining's r2: 0.773006\tvalid_1's r2: 0.24489\n",
      "[560]\ttraining's r2: 0.773332\tvalid_1's r2: 0.24462\n",
      "[561]\ttraining's r2: 0.773874\tvalid_1's r2: 0.244845\n",
      "[562]\ttraining's r2: 0.774293\tvalid_1's r2: 0.244862\n",
      "[563]\ttraining's r2: 0.774714\tvalid_1's r2: 0.244985\n",
      "[564]\ttraining's r2: 0.775212\tvalid_1's r2: 0.244895\n",
      "[565]\ttraining's r2: 0.775667\tvalid_1's r2: 0.244952\n",
      "[566]\ttraining's r2: 0.776103\tvalid_1's r2: 0.24493\n",
      "[567]\ttraining's r2: 0.776528\tvalid_1's r2: 0.24489\n",
      "[568]\ttraining's r2: 0.77685\tvalid_1's r2: 0.244563\n",
      "[569]\ttraining's r2: 0.777354\tvalid_1's r2: 0.245119\n",
      "[570]\ttraining's r2: 0.777614\tvalid_1's r2: 0.244828\n",
      "[571]\ttraining's r2: 0.778015\tvalid_1's r2: 0.24497\n",
      "[572]\ttraining's r2: 0.778303\tvalid_1's r2: 0.244894\n",
      "[573]\ttraining's r2: 0.778667\tvalid_1's r2: 0.244805\n",
      "[574]\ttraining's r2: 0.779089\tvalid_1's r2: 0.244643\n",
      "[575]\ttraining's r2: 0.779386\tvalid_1's r2: 0.244936\n",
      "[576]\ttraining's r2: 0.779891\tvalid_1's r2: 0.244888\n",
      "[577]\ttraining's r2: 0.780322\tvalid_1's r2: 0.24498\n",
      "[578]\ttraining's r2: 0.780687\tvalid_1's r2: 0.244466\n",
      "[579]\ttraining's r2: 0.781044\tvalid_1's r2: 0.244775\n",
      "[580]\ttraining's r2: 0.781479\tvalid_1's r2: 0.244494\n",
      "[581]\ttraining's r2: 0.781784\tvalid_1's r2: 0.244391\n",
      "[582]\ttraining's r2: 0.782202\tvalid_1's r2: 0.244119\n",
      "[583]\ttraining's r2: 0.782633\tvalid_1's r2: 0.243887\n",
      "[584]\ttraining's r2: 0.783039\tvalid_1's r2: 0.24341\n",
      "[585]\ttraining's r2: 0.783416\tvalid_1's r2: 0.243107\n",
      "[586]\ttraining's r2: 0.783835\tvalid_1's r2: 0.242779\n",
      "[587]\ttraining's r2: 0.784206\tvalid_1's r2: 0.242556\n",
      "[588]\ttraining's r2: 0.784495\tvalid_1's r2: 0.242603\n",
      "[589]\ttraining's r2: 0.784922\tvalid_1's r2: 0.243118\n",
      "[590]\ttraining's r2: 0.785294\tvalid_1's r2: 0.243243\n",
      "[591]\ttraining's r2: 0.785622\tvalid_1's r2: 0.243521\n",
      "[592]\ttraining's r2: 0.785947\tvalid_1's r2: 0.243852\n",
      "[593]\ttraining's r2: 0.786416\tvalid_1's r2: 0.244009\n",
      "[594]\ttraining's r2: 0.786678\tvalid_1's r2: 0.243965\n",
      "[595]\ttraining's r2: 0.787075\tvalid_1's r2: 0.243611\n",
      "[596]\ttraining's r2: 0.787492\tvalid_1's r2: 0.243744\n",
      "[597]\ttraining's r2: 0.787923\tvalid_1's r2: 0.243704\n",
      "[598]\ttraining's r2: 0.78839\tvalid_1's r2: 0.243853\n",
      "[599]\ttraining's r2: 0.788826\tvalid_1's r2: 0.243747\n",
      "[600]\ttraining's r2: 0.789143\tvalid_1's r2: 0.243736\n",
      "[601]\ttraining's r2: 0.789464\tvalid_1's r2: 0.243491\n",
      "[602]\ttraining's r2: 0.789846\tvalid_1's r2: 0.243326\n",
      "[603]\ttraining's r2: 0.790191\tvalid_1's r2: 0.243082\n",
      "[604]\ttraining's r2: 0.790607\tvalid_1's r2: 0.243194\n",
      "[605]\ttraining's r2: 0.790929\tvalid_1's r2: 0.243431\n",
      "[606]\ttraining's r2: 0.791219\tvalid_1's r2: 0.243604\n",
      "[607]\ttraining's r2: 0.791602\tvalid_1's r2: 0.243761\n",
      "[608]\ttraining's r2: 0.792147\tvalid_1's r2: 0.243562\n",
      "[609]\ttraining's r2: 0.792469\tvalid_1's r2: 0.24327\n",
      "[610]\ttraining's r2: 0.792793\tvalid_1's r2: 0.243558\n",
      "[611]\ttraining's r2: 0.793187\tvalid_1's r2: 0.243862\n",
      "[612]\ttraining's r2: 0.793624\tvalid_1's r2: 0.243968\n",
      "[613]\ttraining's r2: 0.794\tvalid_1's r2: 0.243881\n",
      "[614]\ttraining's r2: 0.7943\tvalid_1's r2: 0.24396\n",
      "[615]\ttraining's r2: 0.794725\tvalid_1's r2: 0.243957\n",
      "[616]\ttraining's r2: 0.795057\tvalid_1's r2: 0.244218\n",
      "[617]\ttraining's r2: 0.795283\tvalid_1's r2: 0.244181\n",
      "[618]\ttraining's r2: 0.795703\tvalid_1's r2: 0.244259\n",
      "[619]\ttraining's r2: 0.796025\tvalid_1's r2: 0.244714\n",
      "[620]\ttraining's r2: 0.796353\tvalid_1's r2: 0.244785\n",
      "[621]\ttraining's r2: 0.796728\tvalid_1's r2: 0.244339\n",
      "[622]\ttraining's r2: 0.797074\tvalid_1's r2: 0.244311\n",
      "[623]\ttraining's r2: 0.797458\tvalid_1's r2: 0.24423\n",
      "[624]\ttraining's r2: 0.797811\tvalid_1's r2: 0.244431\n",
      "[625]\ttraining's r2: 0.798118\tvalid_1's r2: 0.244843\n",
      "[626]\ttraining's r2: 0.798488\tvalid_1's r2: 0.244893\n",
      "[627]\ttraining's r2: 0.79882\tvalid_1's r2: 0.244711\n",
      "[628]\ttraining's r2: 0.799197\tvalid_1's r2: 0.244455\n",
      "[629]\ttraining's r2: 0.799514\tvalid_1's r2: 0.244207\n",
      "[630]\ttraining's r2: 0.799881\tvalid_1's r2: 0.244197\n",
      "[631]\ttraining's r2: 0.800283\tvalid_1's r2: 0.243893\n",
      "[632]\ttraining's r2: 0.800687\tvalid_1's r2: 0.244166\n",
      "[633]\ttraining's r2: 0.801151\tvalid_1's r2: 0.244392\n",
      "[634]\ttraining's r2: 0.801451\tvalid_1's r2: 0.244511\n",
      "[635]\ttraining's r2: 0.801876\tvalid_1's r2: 0.244868\n",
      "[636]\ttraining's r2: 0.802201\tvalid_1's r2: 0.244668\n",
      "[637]\ttraining's r2: 0.8026\tvalid_1's r2: 0.245039\n",
      "[638]\ttraining's r2: 0.802839\tvalid_1's r2: 0.244672\n",
      "[639]\ttraining's r2: 0.803167\tvalid_1's r2: 0.244562\n",
      "[640]\ttraining's r2: 0.803525\tvalid_1's r2: 0.244592\n",
      "[641]\ttraining's r2: 0.803945\tvalid_1's r2: 0.24482\n",
      "[642]\ttraining's r2: 0.804311\tvalid_1's r2: 0.24494\n",
      "[643]\ttraining's r2: 0.804673\tvalid_1's r2: 0.244677\n",
      "[644]\ttraining's r2: 0.805023\tvalid_1's r2: 0.244615\n",
      "[645]\ttraining's r2: 0.805327\tvalid_1's r2: 0.244777\n",
      "[646]\ttraining's r2: 0.805667\tvalid_1's r2: 0.244906\n",
      "[647]\ttraining's r2: 0.805891\tvalid_1's r2: 0.244544\n",
      "[648]\ttraining's r2: 0.806258\tvalid_1's r2: 0.244254\n",
      "[649]\ttraining's r2: 0.806535\tvalid_1's r2: 0.2439\n",
      "[650]\ttraining's r2: 0.80681\tvalid_1's r2: 0.24387\n",
      "[651]\ttraining's r2: 0.807166\tvalid_1's r2: 0.244012\n",
      "[652]\ttraining's r2: 0.807566\tvalid_1's r2: 0.244267\n",
      "[653]\ttraining's r2: 0.80794\tvalid_1's r2: 0.244528\n",
      "[654]\ttraining's r2: 0.808267\tvalid_1's r2: 0.244615\n",
      "[655]\ttraining's r2: 0.808577\tvalid_1's r2: 0.24478\n",
      "[656]\ttraining's r2: 0.808934\tvalid_1's r2: 0.244503\n",
      "[657]\ttraining's r2: 0.809298\tvalid_1's r2: 0.244304\n",
      "[658]\ttraining's r2: 0.809568\tvalid_1's r2: 0.244273\n",
      "[659]\ttraining's r2: 0.809894\tvalid_1's r2: 0.24404\n",
      "[660]\ttraining's r2: 0.810122\tvalid_1's r2: 0.243973\n",
      "[661]\ttraining's r2: 0.810451\tvalid_1's r2: 0.243883\n",
      "[662]\ttraining's r2: 0.810721\tvalid_1's r2: 0.244035\n",
      "[663]\ttraining's r2: 0.811017\tvalid_1's r2: 0.244066\n",
      "[664]\ttraining's r2: 0.811315\tvalid_1's r2: 0.24418\n",
      "[665]\ttraining's r2: 0.811627\tvalid_1's r2: 0.244198\n",
      "[666]\ttraining's r2: 0.811947\tvalid_1's r2: 0.244371\n",
      "[667]\ttraining's r2: 0.81228\tvalid_1's r2: 0.244189\n",
      "[668]\ttraining's r2: 0.812574\tvalid_1's r2: 0.244332\n",
      "[669]\ttraining's r2: 0.812958\tvalid_1's r2: 0.244301\n",
      "Early stopping, best iteration is:\n",
      "[569]\ttraining's r2: 0.777354\tvalid_1's r2: 0.245119\n",
      "[1]\ttraining's r2: -3439.06\tvalid_1's r2: -3335.19\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[2]\ttraining's r2: -812.588\tvalid_1's r2: -784.416\n",
      "[3]\ttraining's r2: -348.871\tvalid_1's r2: -336.764\n",
      "[4]\ttraining's r2: -193.017\tvalid_1's r2: -186.302\n",
      "[5]\ttraining's r2: -122.809\tvalid_1's r2: -118.652\n",
      "[6]\ttraining's r2: -85.1977\tvalid_1's r2: -82.4882\n",
      "[7]\ttraining's r2: -62.2275\tvalid_1's r2: -60.2432\n",
      "[8]\ttraining's r2: -47.2201\tvalid_1's r2: -45.7748\n",
      "[9]\ttraining's r2: -36.6311\tvalid_1's r2: -35.6238\n",
      "[10]\ttraining's r2: -29.7459\tvalid_1's r2: -28.9003\n",
      "[11]\ttraining's r2: -24.4939\tvalid_1's r2: -23.8433\n",
      "[12]\ttraining's r2: -20.4692\tvalid_1's r2: -19.9196\n",
      "[13]\ttraining's r2: -17.3773\tvalid_1's r2: -16.9818\n",
      "[14]\ttraining's r2: -14.9664\tvalid_1's r2: -14.661\n",
      "[15]\ttraining's r2: -12.9449\tvalid_1's r2: -12.7371\n",
      "[16]\ttraining's r2: -11.3218\tvalid_1's r2: -11.1578\n",
      "[17]\ttraining's r2: -9.97482\tvalid_1's r2: -9.85061\n",
      "[18]\ttraining's r2: -8.85421\tvalid_1's r2: -8.77525\n",
      "[19]\ttraining's r2: -7.89486\tvalid_1's r2: -7.84626\n",
      "[20]\ttraining's r2: -7.07343\tvalid_1's r2: -7.03894\n",
      "[21]\ttraining's r2: -6.35647\tvalid_1's r2: -6.33263\n",
      "[22]\ttraining's r2: -5.75703\tvalid_1's r2: -5.74231\n",
      "[23]\ttraining's r2: -5.20784\tvalid_1's r2: -5.20825\n",
      "[24]\ttraining's r2: -4.72256\tvalid_1's r2: -4.74174\n",
      "[25]\ttraining's r2: -4.32063\tvalid_1's r2: -4.34607\n",
      "[26]\ttraining's r2: -3.95778\tvalid_1's r2: -3.99977\n",
      "[27]\ttraining's r2: -3.62693\tvalid_1's r2: -3.66836\n",
      "[28]\ttraining's r2: -3.32764\tvalid_1's r2: -3.37616\n",
      "[29]\ttraining's r2: -3.06755\tvalid_1's r2: -3.12103\n",
      "[30]\ttraining's r2: -2.83234\tvalid_1's r2: -2.89823\n",
      "[31]\ttraining's r2: -2.62266\tvalid_1's r2: -2.69471\n",
      "[32]\ttraining's r2: -2.43936\tvalid_1's r2: -2.51951\n",
      "[33]\ttraining's r2: -2.26481\tvalid_1's r2: -2.34753\n",
      "[34]\ttraining's r2: -2.11045\tvalid_1's r2: -2.19883\n",
      "[35]\ttraining's r2: -1.96653\tvalid_1's r2: -2.05998\n",
      "[36]\ttraining's r2: -1.83724\tvalid_1's r2: -1.93464\n",
      "[37]\ttraining's r2: -1.71508\tvalid_1's r2: -1.81596\n",
      "[38]\ttraining's r2: -1.60325\tvalid_1's r2: -1.7105\n",
      "[39]\ttraining's r2: -1.50002\tvalid_1's r2: -1.60779\n",
      "[40]\ttraining's r2: -1.40432\tvalid_1's r2: -1.51203\n",
      "[41]\ttraining's r2: -1.31362\tvalid_1's r2: -1.42195\n",
      "[42]\ttraining's r2: -1.23156\tvalid_1's r2: -1.33861\n",
      "[43]\ttraining's r2: -1.15618\tvalid_1's r2: -1.26719\n",
      "[44]\ttraining's r2: -1.08173\tvalid_1's r2: -1.19622\n",
      "[45]\ttraining's r2: -1.01387\tvalid_1's r2: -1.12622\n",
      "[46]\ttraining's r2: -0.952715\tvalid_1's r2: -1.06415\n",
      "[47]\ttraining's r2: -0.898943\tvalid_1's r2: -1.01058\n",
      "[48]\ttraining's r2: -0.844767\tvalid_1's r2: -0.957922\n",
      "[49]\ttraining's r2: -0.793264\tvalid_1's r2: -0.90775\n",
      "[50]\ttraining's r2: -0.745249\tvalid_1's r2: -0.860969\n",
      "[51]\ttraining's r2: -0.701316\tvalid_1's r2: -0.819037\n",
      "[52]\ttraining's r2: -0.658232\tvalid_1's r2: -0.776389\n",
      "[53]\ttraining's r2: -0.620161\tvalid_1's r2: -0.738533\n",
      "[54]\ttraining's r2: -0.581449\tvalid_1's r2: -0.699808\n",
      "[55]\ttraining's r2: -0.542255\tvalid_1's r2: -0.66117\n",
      "[56]\ttraining's r2: -0.507063\tvalid_1's r2: -0.627848\n",
      "[57]\ttraining's r2: -0.471867\tvalid_1's r2: -0.595449\n",
      "[58]\ttraining's r2: -0.441633\tvalid_1's r2: -0.56671\n",
      "[59]\ttraining's r2: -0.411008\tvalid_1's r2: -0.536923\n",
      "[60]\ttraining's r2: -0.382755\tvalid_1's r2: -0.509605\n",
      "[61]\ttraining's r2: -0.354565\tvalid_1's r2: -0.484125\n",
      "[62]\ttraining's r2: -0.328572\tvalid_1's r2: -0.459339\n",
      "[63]\ttraining's r2: -0.30366\tvalid_1's r2: -0.436574\n",
      "[64]\ttraining's r2: -0.279344\tvalid_1's r2: -0.412954\n",
      "[65]\ttraining's r2: -0.256692\tvalid_1's r2: -0.392654\n",
      "[66]\ttraining's r2: -0.234985\tvalid_1's r2: -0.372824\n",
      "[67]\ttraining's r2: -0.21318\tvalid_1's r2: -0.351535\n",
      "[68]\ttraining's r2: -0.192583\tvalid_1's r2: -0.333769\n",
      "[69]\ttraining's r2: -0.173222\tvalid_1's r2: -0.315206\n",
      "[70]\ttraining's r2: -0.154664\tvalid_1's r2: -0.298513\n",
      "[71]\ttraining's r2: -0.135435\tvalid_1's r2: -0.279805\n",
      "[72]\ttraining's r2: -0.120215\tvalid_1's r2: -0.267492\n",
      "[73]\ttraining's r2: -0.104027\tvalid_1's r2: -0.253344\n",
      "[74]\ttraining's r2: -0.0885282\tvalid_1's r2: -0.239087\n",
      "[75]\ttraining's r2: -0.0748987\tvalid_1's r2: -0.226883\n",
      "[76]\ttraining's r2: -0.0608539\tvalid_1's r2: -0.215248\n",
      "[77]\ttraining's r2: -0.0452144\tvalid_1's r2: -0.200632\n",
      "[78]\ttraining's r2: -0.0312226\tvalid_1's r2: -0.18863\n",
      "[79]\ttraining's r2: -0.0172508\tvalid_1's r2: -0.17539\n",
      "[80]\ttraining's r2: -0.00472738\tvalid_1's r2: -0.164686\n",
      "[81]\ttraining's r2: 0.0075219\tvalid_1's r2: -0.152927\n",
      "[82]\ttraining's r2: 0.0186772\tvalid_1's r2: -0.142892\n",
      "[83]\ttraining's r2: 0.0296604\tvalid_1's r2: -0.134649\n",
      "[84]\ttraining's r2: 0.0413832\tvalid_1's r2: -0.124229\n",
      "[85]\ttraining's r2: 0.0536154\tvalid_1's r2: -0.112823\n",
      "[86]\ttraining's r2: 0.0653288\tvalid_1's r2: -0.101374\n",
      "[87]\ttraining's r2: 0.0757688\tvalid_1's r2: -0.0921344\n",
      "[88]\ttraining's r2: 0.0846554\tvalid_1's r2: -0.0846665\n",
      "[89]\ttraining's r2: 0.095228\tvalid_1's r2: -0.0759568\n",
      "[90]\ttraining's r2: 0.104494\tvalid_1's r2: -0.0681989\n",
      "[91]\ttraining's r2: 0.113247\tvalid_1's r2: -0.0597008\n",
      "[92]\ttraining's r2: 0.122029\tvalid_1's r2: -0.0514973\n",
      "[93]\ttraining's r2: 0.131142\tvalid_1's r2: -0.0432108\n",
      "[94]\ttraining's r2: 0.139624\tvalid_1's r2: -0.0353458\n",
      "[95]\ttraining's r2: 0.147934\tvalid_1's r2: -0.0286849\n",
      "[96]\ttraining's r2: 0.156899\tvalid_1's r2: -0.0214236\n",
      "[97]\ttraining's r2: 0.16414\tvalid_1's r2: -0.0150928\n",
      "[98]\ttraining's r2: 0.172234\tvalid_1's r2: -0.00797216\n",
      "[99]\ttraining's r2: 0.180199\tvalid_1's r2: -0.000224865\n",
      "[100]\ttraining's r2: 0.187886\tvalid_1's r2: 0.00574882\n",
      "[101]\ttraining's r2: 0.194498\tvalid_1's r2: 0.0107924\n",
      "[102]\ttraining's r2: 0.201073\tvalid_1's r2: 0.0169811\n",
      "[103]\ttraining's r2: 0.207883\tvalid_1's r2: 0.0223865\n",
      "[104]\ttraining's r2: 0.214654\tvalid_1's r2: 0.0275767\n",
      "[105]\ttraining's r2: 0.220508\tvalid_1's r2: 0.0305848\n",
      "[106]\ttraining's r2: 0.22701\tvalid_1's r2: 0.0362584\n",
      "[107]\ttraining's r2: 0.231798\tvalid_1's r2: 0.0398469\n",
      "[108]\ttraining's r2: 0.238245\tvalid_1's r2: 0.0456445\n",
      "[109]\ttraining's r2: 0.24377\tvalid_1's r2: 0.0503516\n",
      "[110]\ttraining's r2: 0.249346\tvalid_1's r2: 0.0551085\n",
      "[111]\ttraining's r2: 0.254542\tvalid_1's r2: 0.0588972\n",
      "[112]\ttraining's r2: 0.259423\tvalid_1's r2: 0.0626232\n",
      "[113]\ttraining's r2: 0.264676\tvalid_1's r2: 0.0661813\n",
      "[114]\ttraining's r2: 0.270179\tvalid_1's r2: 0.0700777\n",
      "[115]\ttraining's r2: 0.274583\tvalid_1's r2: 0.0737173\n",
      "[116]\ttraining's r2: 0.278404\tvalid_1's r2: 0.0760303\n",
      "[117]\ttraining's r2: 0.282861\tvalid_1's r2: 0.0785261\n",
      "[118]\ttraining's r2: 0.287087\tvalid_1's r2: 0.0813933\n",
      "[119]\ttraining's r2: 0.291931\tvalid_1's r2: 0.0854025\n",
      "[120]\ttraining's r2: 0.296899\tvalid_1's r2: 0.0893883\n",
      "[121]\ttraining's r2: 0.300083\tvalid_1's r2: 0.0903485\n",
      "[122]\ttraining's r2: 0.304599\tvalid_1's r2: 0.0939016\n",
      "[123]\ttraining's r2: 0.308193\tvalid_1's r2: 0.0959654\n",
      "[124]\ttraining's r2: 0.311539\tvalid_1's r2: 0.0967719\n",
      "[125]\ttraining's r2: 0.315087\tvalid_1's r2: 0.0983441\n",
      "[126]\ttraining's r2: 0.318134\tvalid_1's r2: 0.0991924\n",
      "[127]\ttraining's r2: 0.321729\tvalid_1's r2: 0.101088\n",
      "[128]\ttraining's r2: 0.32544\tvalid_1's r2: 0.103038\n",
      "[129]\ttraining's r2: 0.329249\tvalid_1's r2: 0.106715\n",
      "[130]\ttraining's r2: 0.33316\tvalid_1's r2: 0.109163\n",
      "[131]\ttraining's r2: 0.336276\tvalid_1's r2: 0.1106\n",
      "[132]\ttraining's r2: 0.339516\tvalid_1's r2: 0.112398\n",
      "[133]\ttraining's r2: 0.343261\tvalid_1's r2: 0.115561\n",
      "[134]\ttraining's r2: 0.346279\tvalid_1's r2: 0.117173\n",
      "[135]\ttraining's r2: 0.349425\tvalid_1's r2: 0.118847\n",
      "[136]\ttraining's r2: 0.352674\tvalid_1's r2: 0.121685\n",
      "[137]\ttraining's r2: 0.355662\tvalid_1's r2: 0.123474\n",
      "[138]\ttraining's r2: 0.358749\tvalid_1's r2: 0.125142\n",
      "[139]\ttraining's r2: 0.362076\tvalid_1's r2: 0.126841\n",
      "[140]\ttraining's r2: 0.364864\tvalid_1's r2: 0.128642\n",
      "[141]\ttraining's r2: 0.367517\tvalid_1's r2: 0.129665\n",
      "[142]\ttraining's r2: 0.370979\tvalid_1's r2: 0.132051\n",
      "[143]\ttraining's r2: 0.374106\tvalid_1's r2: 0.13481\n",
      "[144]\ttraining's r2: 0.376454\tvalid_1's r2: 0.134684\n",
      "[145]\ttraining's r2: 0.378958\tvalid_1's r2: 0.135259\n",
      "[146]\ttraining's r2: 0.381086\tvalid_1's r2: 0.135733\n",
      "[147]\ttraining's r2: 0.383885\tvalid_1's r2: 0.137418\n",
      "[148]\ttraining's r2: 0.38642\tvalid_1's r2: 0.139342\n",
      "[149]\ttraining's r2: 0.388641\tvalid_1's r2: 0.139782\n",
      "[150]\ttraining's r2: 0.391973\tvalid_1's r2: 0.142201\n",
      "[151]\ttraining's r2: 0.394127\tvalid_1's r2: 0.142521\n",
      "[152]\ttraining's r2: 0.396101\tvalid_1's r2: 0.143256\n",
      "[153]\ttraining's r2: 0.398663\tvalid_1's r2: 0.145076\n",
      "[154]\ttraining's r2: 0.401153\tvalid_1's r2: 0.146102\n",
      "[155]\ttraining's r2: 0.403561\tvalid_1's r2: 0.146819\n",
      "[156]\ttraining's r2: 0.406439\tvalid_1's r2: 0.149173\n",
      "[157]\ttraining's r2: 0.408345\tvalid_1's r2: 0.149755\n",
      "[158]\ttraining's r2: 0.410957\tvalid_1's r2: 0.150553\n",
      "[159]\ttraining's r2: 0.41326\tvalid_1's r2: 0.151513\n",
      "[160]\ttraining's r2: 0.415093\tvalid_1's r2: 0.152296\n",
      "[161]\ttraining's r2: 0.417408\tvalid_1's r2: 0.154096\n",
      "[162]\ttraining's r2: 0.419628\tvalid_1's r2: 0.154092\n",
      "[163]\ttraining's r2: 0.421776\tvalid_1's r2: 0.154836\n",
      "[164]\ttraining's r2: 0.424053\tvalid_1's r2: 0.155012\n",
      "[165]\ttraining's r2: 0.426015\tvalid_1's r2: 0.155884\n",
      "[166]\ttraining's r2: 0.42813\tvalid_1's r2: 0.157401\n",
      "[167]\ttraining's r2: 0.430398\tvalid_1's r2: 0.157938\n",
      "[168]\ttraining's r2: 0.432667\tvalid_1's r2: 0.158507\n",
      "[169]\ttraining's r2: 0.434873\tvalid_1's r2: 0.159479\n",
      "[170]\ttraining's r2: 0.437252\tvalid_1's r2: 0.161343\n",
      "[171]\ttraining's r2: 0.439242\tvalid_1's r2: 0.161502\n",
      "[172]\ttraining's r2: 0.44083\tvalid_1's r2: 0.162494\n",
      "[173]\ttraining's r2: 0.442885\tvalid_1's r2: 0.163212\n",
      "[174]\ttraining's r2: 0.444764\tvalid_1's r2: 0.163996\n",
      "[175]\ttraining's r2: 0.447108\tvalid_1's r2: 0.165072\n",
      "[176]\ttraining's r2: 0.448971\tvalid_1's r2: 0.16538\n",
      "[177]\ttraining's r2: 0.451336\tvalid_1's r2: 0.167383\n",
      "[178]\ttraining's r2: 0.452739\tvalid_1's r2: 0.168541\n",
      "[179]\ttraining's r2: 0.454523\tvalid_1's r2: 0.16825\n",
      "[180]\ttraining's r2: 0.456535\tvalid_1's r2: 0.168855\n",
      "[181]\ttraining's r2: 0.458551\tvalid_1's r2: 0.170332\n",
      "[182]\ttraining's r2: 0.460853\tvalid_1's r2: 0.171177\n",
      "[183]\ttraining's r2: 0.462406\tvalid_1's r2: 0.172015\n",
      "[184]\ttraining's r2: 0.464213\tvalid_1's r2: 0.172388\n",
      "[185]\ttraining's r2: 0.466285\tvalid_1's r2: 0.172671\n",
      "[186]\ttraining's r2: 0.467999\tvalid_1's r2: 0.17292\n",
      "[187]\ttraining's r2: 0.470082\tvalid_1's r2: 0.173458\n",
      "[188]\ttraining's r2: 0.471751\tvalid_1's r2: 0.172727\n",
      "[189]\ttraining's r2: 0.473599\tvalid_1's r2: 0.172755\n",
      "[190]\ttraining's r2: 0.475369\tvalid_1's r2: 0.173622\n",
      "[191]\ttraining's r2: 0.476941\tvalid_1's r2: 0.174324\n",
      "[192]\ttraining's r2: 0.478605\tvalid_1's r2: 0.175451\n",
      "[193]\ttraining's r2: 0.480423\tvalid_1's r2: 0.17567\n",
      "[194]\ttraining's r2: 0.482086\tvalid_1's r2: 0.176525\n",
      "[195]\ttraining's r2: 0.483938\tvalid_1's r2: 0.177357\n",
      "[196]\ttraining's r2: 0.485493\tvalid_1's r2: 0.177737\n",
      "[197]\ttraining's r2: 0.486973\tvalid_1's r2: 0.17821\n",
      "[198]\ttraining's r2: 0.488605\tvalid_1's r2: 0.178946\n",
      "[199]\ttraining's r2: 0.4902\tvalid_1's r2: 0.180045\n",
      "[200]\ttraining's r2: 0.492101\tvalid_1's r2: 0.180535\n",
      "[201]\ttraining's r2: 0.493901\tvalid_1's r2: 0.180287\n",
      "[202]\ttraining's r2: 0.49571\tvalid_1's r2: 0.180902\n",
      "[203]\ttraining's r2: 0.497237\tvalid_1's r2: 0.181126\n",
      "[204]\ttraining's r2: 0.498899\tvalid_1's r2: 0.18178\n",
      "[205]\ttraining's r2: 0.500391\tvalid_1's r2: 0.182083\n",
      "[206]\ttraining's r2: 0.502226\tvalid_1's r2: 0.183424\n",
      "[207]\ttraining's r2: 0.503437\tvalid_1's r2: 0.183811\n",
      "[208]\ttraining's r2: 0.50473\tvalid_1's r2: 0.18411\n",
      "[209]\ttraining's r2: 0.506383\tvalid_1's r2: 0.185253\n",
      "[210]\ttraining's r2: 0.508137\tvalid_1's r2: 0.18668\n",
      "[211]\ttraining's r2: 0.509768\tvalid_1's r2: 0.186768\n",
      "[212]\ttraining's r2: 0.511363\tvalid_1's r2: 0.187913\n",
      "[213]\ttraining's r2: 0.512589\tvalid_1's r2: 0.188459\n",
      "[214]\ttraining's r2: 0.514148\tvalid_1's r2: 0.188747\n",
      "[215]\ttraining's r2: 0.515475\tvalid_1's r2: 0.188709\n",
      "[216]\ttraining's r2: 0.516903\tvalid_1's r2: 0.18857\n",
      "[217]\ttraining's r2: 0.518123\tvalid_1's r2: 0.1889\n",
      "[218]\ttraining's r2: 0.519179\tvalid_1's r2: 0.189176\n",
      "[219]\ttraining's r2: 0.520336\tvalid_1's r2: 0.18949\n",
      "[220]\ttraining's r2: 0.521633\tvalid_1's r2: 0.189502\n",
      "[221]\ttraining's r2: 0.523036\tvalid_1's r2: 0.189598\n",
      "[222]\ttraining's r2: 0.524492\tvalid_1's r2: 0.189843\n",
      "[223]\ttraining's r2: 0.525788\tvalid_1's r2: 0.190069\n",
      "[224]\ttraining's r2: 0.52689\tvalid_1's r2: 0.190299\n",
      "[225]\ttraining's r2: 0.528112\tvalid_1's r2: 0.191223\n",
      "[226]\ttraining's r2: 0.529352\tvalid_1's r2: 0.191833\n",
      "[227]\ttraining's r2: 0.530822\tvalid_1's r2: 0.1926\n",
      "[228]\ttraining's r2: 0.532109\tvalid_1's r2: 0.193238\n",
      "[229]\ttraining's r2: 0.533442\tvalid_1's r2: 0.193907\n",
      "[230]\ttraining's r2: 0.534834\tvalid_1's r2: 0.193905\n",
      "[231]\ttraining's r2: 0.536148\tvalid_1's r2: 0.194704\n",
      "[232]\ttraining's r2: 0.537065\tvalid_1's r2: 0.194653\n",
      "[233]\ttraining's r2: 0.538289\tvalid_1's r2: 0.195078\n",
      "[234]\ttraining's r2: 0.539588\tvalid_1's r2: 0.195427\n",
      "[235]\ttraining's r2: 0.540739\tvalid_1's r2: 0.196267\n",
      "[236]\ttraining's r2: 0.542129\tvalid_1's r2: 0.196631\n",
      "[237]\ttraining's r2: 0.543536\tvalid_1's r2: 0.196911\n",
      "[238]\ttraining's r2: 0.545261\tvalid_1's r2: 0.197296\n",
      "[239]\ttraining's r2: 0.546587\tvalid_1's r2: 0.198157\n",
      "[240]\ttraining's r2: 0.547484\tvalid_1's r2: 0.197521\n",
      "[241]\ttraining's r2: 0.548517\tvalid_1's r2: 0.197444\n",
      "[242]\ttraining's r2: 0.549688\tvalid_1's r2: 0.197497\n",
      "[243]\ttraining's r2: 0.551041\tvalid_1's r2: 0.197663\n",
      "[244]\ttraining's r2: 0.552255\tvalid_1's r2: 0.198433\n",
      "[245]\ttraining's r2: 0.553821\tvalid_1's r2: 0.19843\n",
      "[246]\ttraining's r2: 0.554901\tvalid_1's r2: 0.198977\n",
      "[247]\ttraining's r2: 0.556196\tvalid_1's r2: 0.199084\n",
      "[248]\ttraining's r2: 0.557265\tvalid_1's r2: 0.198868\n",
      "[249]\ttraining's r2: 0.558293\tvalid_1's r2: 0.198628\n",
      "[250]\ttraining's r2: 0.559683\tvalid_1's r2: 0.199048\n",
      "[251]\ttraining's r2: 0.560818\tvalid_1's r2: 0.199404\n",
      "[252]\ttraining's r2: 0.56202\tvalid_1's r2: 0.199576\n",
      "[253]\ttraining's r2: 0.563299\tvalid_1's r2: 0.200003\n",
      "[254]\ttraining's r2: 0.564666\tvalid_1's r2: 0.200692\n",
      "[255]\ttraining's r2: 0.565916\tvalid_1's r2: 0.200911\n",
      "[256]\ttraining's r2: 0.566992\tvalid_1's r2: 0.20121\n",
      "[257]\ttraining's r2: 0.568252\tvalid_1's r2: 0.201978\n",
      "[258]\ttraining's r2: 0.569364\tvalid_1's r2: 0.201881\n",
      "[259]\ttraining's r2: 0.57078\tvalid_1's r2: 0.202012\n",
      "[260]\ttraining's r2: 0.571802\tvalid_1's r2: 0.202369\n",
      "[261]\ttraining's r2: 0.57296\tvalid_1's r2: 0.202596\n",
      "[262]\ttraining's r2: 0.573931\tvalid_1's r2: 0.202557\n",
      "[263]\ttraining's r2: 0.57488\tvalid_1's r2: 0.202343\n",
      "[264]\ttraining's r2: 0.576191\tvalid_1's r2: 0.202656\n",
      "[265]\ttraining's r2: 0.577321\tvalid_1's r2: 0.203086\n",
      "[266]\ttraining's r2: 0.578271\tvalid_1's r2: 0.203092\n",
      "[267]\ttraining's r2: 0.579077\tvalid_1's r2: 0.202759\n",
      "[268]\ttraining's r2: 0.580073\tvalid_1's r2: 0.202992\n",
      "[269]\ttraining's r2: 0.581327\tvalid_1's r2: 0.203303\n",
      "[270]\ttraining's r2: 0.582569\tvalid_1's r2: 0.203392\n",
      "[271]\ttraining's r2: 0.58372\tvalid_1's r2: 0.203276\n",
      "[272]\ttraining's r2: 0.584798\tvalid_1's r2: 0.203745\n",
      "[273]\ttraining's r2: 0.585749\tvalid_1's r2: 0.203869\n",
      "[274]\ttraining's r2: 0.586785\tvalid_1's r2: 0.204735\n",
      "[275]\ttraining's r2: 0.587682\tvalid_1's r2: 0.204721\n",
      "[276]\ttraining's r2: 0.588737\tvalid_1's r2: 0.204581\n",
      "[277]\ttraining's r2: 0.589715\tvalid_1's r2: 0.204688\n",
      "[278]\ttraining's r2: 0.590809\tvalid_1's r2: 0.204949\n",
      "[279]\ttraining's r2: 0.591838\tvalid_1's r2: 0.205425\n",
      "[280]\ttraining's r2: 0.592917\tvalid_1's r2: 0.205777\n",
      "[281]\ttraining's r2: 0.593948\tvalid_1's r2: 0.205988\n",
      "[282]\ttraining's r2: 0.595043\tvalid_1's r2: 0.206473\n",
      "[283]\ttraining's r2: 0.596387\tvalid_1's r2: 0.206123\n",
      "[284]\ttraining's r2: 0.597532\tvalid_1's r2: 0.206759\n",
      "[285]\ttraining's r2: 0.598672\tvalid_1's r2: 0.207571\n",
      "[286]\ttraining's r2: 0.599566\tvalid_1's r2: 0.20785\n",
      "[287]\ttraining's r2: 0.600664\tvalid_1's r2: 0.207339\n",
      "[288]\ttraining's r2: 0.601934\tvalid_1's r2: 0.207776\n",
      "[289]\ttraining's r2: 0.602821\tvalid_1's r2: 0.208091\n",
      "[290]\ttraining's r2: 0.603848\tvalid_1's r2: 0.208392\n",
      "[291]\ttraining's r2: 0.60463\tvalid_1's r2: 0.208466\n",
      "[292]\ttraining's r2: 0.605621\tvalid_1's r2: 0.208382\n",
      "[293]\ttraining's r2: 0.606475\tvalid_1's r2: 0.207989\n",
      "[294]\ttraining's r2: 0.607582\tvalid_1's r2: 0.208155\n",
      "[295]\ttraining's r2: 0.608366\tvalid_1's r2: 0.208907\n",
      "[296]\ttraining's r2: 0.609508\tvalid_1's r2: 0.209618\n",
      "[297]\ttraining's r2: 0.610356\tvalid_1's r2: 0.209573\n",
      "[298]\ttraining's r2: 0.611275\tvalid_1's r2: 0.209935\n",
      "[299]\ttraining's r2: 0.612389\tvalid_1's r2: 0.210369\n",
      "[300]\ttraining's r2: 0.613254\tvalid_1's r2: 0.210678\n",
      "[301]\ttraining's r2: 0.614095\tvalid_1's r2: 0.210753\n",
      "[302]\ttraining's r2: 0.614646\tvalid_1's r2: 0.210318\n",
      "[303]\ttraining's r2: 0.615587\tvalid_1's r2: 0.210566\n",
      "[304]\ttraining's r2: 0.616574\tvalid_1's r2: 0.210623\n",
      "[305]\ttraining's r2: 0.617416\tvalid_1's r2: 0.210649\n",
      "[306]\ttraining's r2: 0.618367\tvalid_1's r2: 0.210677\n",
      "[307]\ttraining's r2: 0.619209\tvalid_1's r2: 0.210986\n",
      "[308]\ttraining's r2: 0.62025\tvalid_1's r2: 0.21131\n",
      "[309]\ttraining's r2: 0.621078\tvalid_1's r2: 0.211437\n",
      "[310]\ttraining's r2: 0.62195\tvalid_1's r2: 0.211002\n",
      "[311]\ttraining's r2: 0.622954\tvalid_1's r2: 0.210958\n",
      "[312]\ttraining's r2: 0.623988\tvalid_1's r2: 0.210539\n",
      "[313]\ttraining's r2: 0.625035\tvalid_1's r2: 0.210417\n",
      "[314]\ttraining's r2: 0.626012\tvalid_1's r2: 0.210113\n",
      "[315]\ttraining's r2: 0.626968\tvalid_1's r2: 0.210746\n",
      "[316]\ttraining's r2: 0.627985\tvalid_1's r2: 0.210829\n",
      "[317]\ttraining's r2: 0.629225\tvalid_1's r2: 0.21108\n",
      "[318]\ttraining's r2: 0.630131\tvalid_1's r2: 0.211327\n",
      "[319]\ttraining's r2: 0.631129\tvalid_1's r2: 0.211925\n",
      "[320]\ttraining's r2: 0.63186\tvalid_1's r2: 0.211199\n",
      "[321]\ttraining's r2: 0.632809\tvalid_1's r2: 0.211452\n",
      "[322]\ttraining's r2: 0.633711\tvalid_1's r2: 0.211197\n",
      "[323]\ttraining's r2: 0.63457\tvalid_1's r2: 0.211706\n",
      "[324]\ttraining's r2: 0.635443\tvalid_1's r2: 0.211794\n",
      "[325]\ttraining's r2: 0.636663\tvalid_1's r2: 0.212063\n",
      "[326]\ttraining's r2: 0.637496\tvalid_1's r2: 0.211858\n",
      "[327]\ttraining's r2: 0.638406\tvalid_1's r2: 0.212624\n",
      "[328]\ttraining's r2: 0.639276\tvalid_1's r2: 0.212613\n",
      "[329]\ttraining's r2: 0.639924\tvalid_1's r2: 0.212503\n",
      "[330]\ttraining's r2: 0.64076\tvalid_1's r2: 0.212718\n",
      "[331]\ttraining's r2: 0.641488\tvalid_1's r2: 0.212565\n",
      "[332]\ttraining's r2: 0.642563\tvalid_1's r2: 0.21304\n",
      "[333]\ttraining's r2: 0.643237\tvalid_1's r2: 0.212917\n",
      "[334]\ttraining's r2: 0.644141\tvalid_1's r2: 0.213241\n",
      "[335]\ttraining's r2: 0.644939\tvalid_1's r2: 0.213557\n",
      "[336]\ttraining's r2: 0.645661\tvalid_1's r2: 0.21365\n",
      "[337]\ttraining's r2: 0.646549\tvalid_1's r2: 0.213907\n",
      "[338]\ttraining's r2: 0.647409\tvalid_1's r2: 0.214441\n",
      "[339]\ttraining's r2: 0.648053\tvalid_1's r2: 0.214058\n",
      "[340]\ttraining's r2: 0.648932\tvalid_1's r2: 0.21484\n",
      "[341]\ttraining's r2: 0.649794\tvalid_1's r2: 0.215841\n",
      "[342]\ttraining's r2: 0.650473\tvalid_1's r2: 0.21519\n",
      "[343]\ttraining's r2: 0.651312\tvalid_1's r2: 0.21526\n",
      "[344]\ttraining's r2: 0.652204\tvalid_1's r2: 0.215458\n",
      "[345]\ttraining's r2: 0.652897\tvalid_1's r2: 0.215359\n",
      "[346]\ttraining's r2: 0.653598\tvalid_1's r2: 0.215436\n",
      "[347]\ttraining's r2: 0.654451\tvalid_1's r2: 0.215996\n",
      "[348]\ttraining's r2: 0.655336\tvalid_1's r2: 0.215814\n",
      "[349]\ttraining's r2: 0.656137\tvalid_1's r2: 0.216045\n",
      "[350]\ttraining's r2: 0.656878\tvalid_1's r2: 0.216503\n",
      "[351]\ttraining's r2: 0.657751\tvalid_1's r2: 0.216623\n",
      "[352]\ttraining's r2: 0.658664\tvalid_1's r2: 0.217174\n",
      "[353]\ttraining's r2: 0.659429\tvalid_1's r2: 0.217267\n",
      "[354]\ttraining's r2: 0.660177\tvalid_1's r2: 0.217777\n",
      "[355]\ttraining's r2: 0.660948\tvalid_1's r2: 0.218417\n",
      "[356]\ttraining's r2: 0.661743\tvalid_1's r2: 0.218705\n",
      "[357]\ttraining's r2: 0.662605\tvalid_1's r2: 0.219038\n",
      "[358]\ttraining's r2: 0.663394\tvalid_1's r2: 0.219227\n",
      "[359]\ttraining's r2: 0.664236\tvalid_1's r2: 0.219162\n",
      "[360]\ttraining's r2: 0.665237\tvalid_1's r2: 0.219601\n",
      "[361]\ttraining's r2: 0.666041\tvalid_1's r2: 0.219893\n",
      "[362]\ttraining's r2: 0.66673\tvalid_1's r2: 0.219776\n",
      "[363]\ttraining's r2: 0.667569\tvalid_1's r2: 0.2199\n",
      "[364]\ttraining's r2: 0.668225\tvalid_1's r2: 0.219133\n",
      "[365]\ttraining's r2: 0.669059\tvalid_1's r2: 0.218619\n",
      "[366]\ttraining's r2: 0.669862\tvalid_1's r2: 0.218632\n",
      "[367]\ttraining's r2: 0.670516\tvalid_1's r2: 0.218507\n",
      "[368]\ttraining's r2: 0.671274\tvalid_1's r2: 0.218411\n",
      "[369]\ttraining's r2: 0.671911\tvalid_1's r2: 0.218793\n",
      "[370]\ttraining's r2: 0.67245\tvalid_1's r2: 0.219122\n",
      "[371]\ttraining's r2: 0.673123\tvalid_1's r2: 0.218894\n",
      "[372]\ttraining's r2: 0.674121\tvalid_1's r2: 0.2195\n",
      "[373]\ttraining's r2: 0.674874\tvalid_1's r2: 0.219878\n",
      "[374]\ttraining's r2: 0.675786\tvalid_1's r2: 0.21965\n",
      "[375]\ttraining's r2: 0.67637\tvalid_1's r2: 0.219153\n",
      "[376]\ttraining's r2: 0.677122\tvalid_1's r2: 0.219275\n",
      "[377]\ttraining's r2: 0.677893\tvalid_1's r2: 0.219097\n",
      "[378]\ttraining's r2: 0.678708\tvalid_1's r2: 0.219367\n",
      "[379]\ttraining's r2: 0.679554\tvalid_1's r2: 0.21954\n",
      "[380]\ttraining's r2: 0.680327\tvalid_1's r2: 0.219479\n",
      "[381]\ttraining's r2: 0.681025\tvalid_1's r2: 0.219683\n",
      "[382]\ttraining's r2: 0.681658\tvalid_1's r2: 0.219727\n",
      "[383]\ttraining's r2: 0.682287\tvalid_1's r2: 0.219415\n",
      "[384]\ttraining's r2: 0.682954\tvalid_1's r2: 0.219301\n",
      "[385]\ttraining's r2: 0.683759\tvalid_1's r2: 0.219997\n",
      "[386]\ttraining's r2: 0.684453\tvalid_1's r2: 0.220526\n",
      "[387]\ttraining's r2: 0.685085\tvalid_1's r2: 0.220279\n",
      "[388]\ttraining's r2: 0.685756\tvalid_1's r2: 0.220416\n",
      "[389]\ttraining's r2: 0.686562\tvalid_1's r2: 0.220702\n",
      "[390]\ttraining's r2: 0.68714\tvalid_1's r2: 0.220838\n",
      "[391]\ttraining's r2: 0.687855\tvalid_1's r2: 0.221201\n",
      "[392]\ttraining's r2: 0.688541\tvalid_1's r2: 0.221317\n",
      "[393]\ttraining's r2: 0.689246\tvalid_1's r2: 0.221467\n",
      "[394]\ttraining's r2: 0.689922\tvalid_1's r2: 0.220917\n",
      "[395]\ttraining's r2: 0.690594\tvalid_1's r2: 0.22075\n",
      "[396]\ttraining's r2: 0.691288\tvalid_1's r2: 0.221191\n",
      "[397]\ttraining's r2: 0.69188\tvalid_1's r2: 0.220893\n",
      "[398]\ttraining's r2: 0.692394\tvalid_1's r2: 0.221027\n",
      "[399]\ttraining's r2: 0.69328\tvalid_1's r2: 0.220944\n",
      "[400]\ttraining's r2: 0.694091\tvalid_1's r2: 0.221071\n",
      "[401]\ttraining's r2: 0.694759\tvalid_1's r2: 0.221096\n",
      "[402]\ttraining's r2: 0.695327\tvalid_1's r2: 0.220823\n",
      "[403]\ttraining's r2: 0.695957\tvalid_1's r2: 0.221286\n",
      "[404]\ttraining's r2: 0.69663\tvalid_1's r2: 0.221192\n",
      "[405]\ttraining's r2: 0.697239\tvalid_1's r2: 0.221328\n",
      "[406]\ttraining's r2: 0.697884\tvalid_1's r2: 0.22152\n",
      "[407]\ttraining's r2: 0.698558\tvalid_1's r2: 0.221617\n",
      "[408]\ttraining's r2: 0.699072\tvalid_1's r2: 0.221637\n",
      "[409]\ttraining's r2: 0.699626\tvalid_1's r2: 0.22166\n",
      "[410]\ttraining's r2: 0.700195\tvalid_1's r2: 0.221693\n",
      "[411]\ttraining's r2: 0.700937\tvalid_1's r2: 0.222062\n",
      "[412]\ttraining's r2: 0.701447\tvalid_1's r2: 0.221779\n",
      "[413]\ttraining's r2: 0.70211\tvalid_1's r2: 0.221907\n",
      "[414]\ttraining's r2: 0.702853\tvalid_1's r2: 0.222313\n",
      "[415]\ttraining's r2: 0.703447\tvalid_1's r2: 0.222625\n",
      "[416]\ttraining's r2: 0.704122\tvalid_1's r2: 0.223078\n",
      "[417]\ttraining's r2: 0.70479\tvalid_1's r2: 0.222935\n",
      "[418]\ttraining's r2: 0.705269\tvalid_1's r2: 0.222958\n",
      "[419]\ttraining's r2: 0.706001\tvalid_1's r2: 0.223632\n",
      "[420]\ttraining's r2: 0.706791\tvalid_1's r2: 0.223151\n",
      "[421]\ttraining's r2: 0.707336\tvalid_1's r2: 0.223038\n",
      "[422]\ttraining's r2: 0.708082\tvalid_1's r2: 0.223422\n",
      "[423]\ttraining's r2: 0.708733\tvalid_1's r2: 0.223435\n",
      "[424]\ttraining's r2: 0.709509\tvalid_1's r2: 0.223604\n",
      "[425]\ttraining's r2: 0.710048\tvalid_1's r2: 0.223983\n",
      "[426]\ttraining's r2: 0.710668\tvalid_1's r2: 0.224057\n",
      "[427]\ttraining's r2: 0.711076\tvalid_1's r2: 0.224548\n",
      "[428]\ttraining's r2: 0.711573\tvalid_1's r2: 0.224155\n",
      "[429]\ttraining's r2: 0.712237\tvalid_1's r2: 0.223886\n",
      "[430]\ttraining's r2: 0.712866\tvalid_1's r2: 0.224231\n",
      "[431]\ttraining's r2: 0.713408\tvalid_1's r2: 0.224423\n",
      "[432]\ttraining's r2: 0.713963\tvalid_1's r2: 0.224253\n",
      "[433]\ttraining's r2: 0.714491\tvalid_1's r2: 0.22405\n",
      "[434]\ttraining's r2: 0.715123\tvalid_1's r2: 0.224367\n",
      "[435]\ttraining's r2: 0.715691\tvalid_1's r2: 0.224654\n",
      "[436]\ttraining's r2: 0.716381\tvalid_1's r2: 0.22456\n",
      "[437]\ttraining's r2: 0.717025\tvalid_1's r2: 0.224584\n",
      "[438]\ttraining's r2: 0.717718\tvalid_1's r2: 0.224585\n",
      "[439]\ttraining's r2: 0.718187\tvalid_1's r2: 0.224647\n",
      "[440]\ttraining's r2: 0.718834\tvalid_1's r2: 0.224727\n",
      "[441]\ttraining's r2: 0.719466\tvalid_1's r2: 0.224716\n",
      "[442]\ttraining's r2: 0.720046\tvalid_1's r2: 0.225121\n",
      "[443]\ttraining's r2: 0.720438\tvalid_1's r2: 0.225078\n",
      "[444]\ttraining's r2: 0.721051\tvalid_1's r2: 0.225242\n",
      "[445]\ttraining's r2: 0.72152\tvalid_1's r2: 0.224894\n",
      "[446]\ttraining's r2: 0.722142\tvalid_1's r2: 0.224823\n",
      "[447]\ttraining's r2: 0.722582\tvalid_1's r2: 0.224594\n",
      "[448]\ttraining's r2: 0.723108\tvalid_1's r2: 0.224665\n",
      "[449]\ttraining's r2: 0.723707\tvalid_1's r2: 0.224485\n",
      "[450]\ttraining's r2: 0.724268\tvalid_1's r2: 0.224448\n",
      "[451]\ttraining's r2: 0.724927\tvalid_1's r2: 0.224518\n",
      "[452]\ttraining's r2: 0.72543\tvalid_1's r2: 0.224623\n",
      "[453]\ttraining's r2: 0.726037\tvalid_1's r2: 0.224736\n",
      "[454]\ttraining's r2: 0.726652\tvalid_1's r2: 0.224604\n",
      "[455]\ttraining's r2: 0.727048\tvalid_1's r2: 0.224378\n",
      "[456]\ttraining's r2: 0.727605\tvalid_1's r2: 0.224768\n",
      "[457]\ttraining's r2: 0.728078\tvalid_1's r2: 0.224735\n",
      "[458]\ttraining's r2: 0.728633\tvalid_1's r2: 0.225044\n",
      "[459]\ttraining's r2: 0.729157\tvalid_1's r2: 0.225143\n",
      "[460]\ttraining's r2: 0.729667\tvalid_1's r2: 0.225107\n",
      "[461]\ttraining's r2: 0.730244\tvalid_1's r2: 0.225574\n",
      "[462]\ttraining's r2: 0.730745\tvalid_1's r2: 0.225582\n",
      "[463]\ttraining's r2: 0.731323\tvalid_1's r2: 0.225667\n",
      "[464]\ttraining's r2: 0.731856\tvalid_1's r2: 0.225557\n",
      "[465]\ttraining's r2: 0.732319\tvalid_1's r2: 0.225659\n",
      "[466]\ttraining's r2: 0.732852\tvalid_1's r2: 0.225797\n",
      "[467]\ttraining's r2: 0.733336\tvalid_1's r2: 0.226068\n",
      "[468]\ttraining's r2: 0.733773\tvalid_1's r2: 0.226211\n",
      "[469]\ttraining's r2: 0.734351\tvalid_1's r2: 0.226174\n",
      "[470]\ttraining's r2: 0.734833\tvalid_1's r2: 0.225735\n",
      "[471]\ttraining's r2: 0.735389\tvalid_1's r2: 0.225863\n",
      "[472]\ttraining's r2: 0.736087\tvalid_1's r2: 0.22625\n",
      "[473]\ttraining's r2: 0.736683\tvalid_1's r2: 0.226434\n",
      "[474]\ttraining's r2: 0.736994\tvalid_1's r2: 0.226113\n",
      "[475]\ttraining's r2: 0.737453\tvalid_1's r2: 0.225941\n",
      "[476]\ttraining's r2: 0.738052\tvalid_1's r2: 0.22607\n",
      "[477]\ttraining's r2: 0.738581\tvalid_1's r2: 0.226088\n",
      "[478]\ttraining's r2: 0.73932\tvalid_1's r2: 0.226322\n",
      "[479]\ttraining's r2: 0.739786\tvalid_1's r2: 0.226175\n",
      "[480]\ttraining's r2: 0.740378\tvalid_1's r2: 0.225735\n",
      "[481]\ttraining's r2: 0.740776\tvalid_1's r2: 0.225477\n",
      "[482]\ttraining's r2: 0.741354\tvalid_1's r2: 0.225302\n",
      "[483]\ttraining's r2: 0.741758\tvalid_1's r2: 0.225169\n",
      "[484]\ttraining's r2: 0.742286\tvalid_1's r2: 0.224944\n",
      "[485]\ttraining's r2: 0.742665\tvalid_1's r2: 0.224906\n",
      "[486]\ttraining's r2: 0.743145\tvalid_1's r2: 0.224232\n",
      "[487]\ttraining's r2: 0.743731\tvalid_1's r2: 0.224188\n",
      "[488]\ttraining's r2: 0.74429\tvalid_1's r2: 0.223939\n",
      "[489]\ttraining's r2: 0.744802\tvalid_1's r2: 0.223672\n",
      "[490]\ttraining's r2: 0.745365\tvalid_1's r2: 0.223112\n",
      "[491]\ttraining's r2: 0.745831\tvalid_1's r2: 0.223217\n",
      "[492]\ttraining's r2: 0.746252\tvalid_1's r2: 0.223259\n",
      "[493]\ttraining's r2: 0.74664\tvalid_1's r2: 0.22333\n",
      "[494]\ttraining's r2: 0.747118\tvalid_1's r2: 0.223496\n",
      "[495]\ttraining's r2: 0.747561\tvalid_1's r2: 0.223587\n",
      "[496]\ttraining's r2: 0.748094\tvalid_1's r2: 0.223288\n",
      "[497]\ttraining's r2: 0.748618\tvalid_1's r2: 0.223556\n",
      "[498]\ttraining's r2: 0.749115\tvalid_1's r2: 0.223893\n",
      "[499]\ttraining's r2: 0.749608\tvalid_1's r2: 0.223727\n",
      "[500]\ttraining's r2: 0.750173\tvalid_1's r2: 0.223611\n",
      "[501]\ttraining's r2: 0.750576\tvalid_1's r2: 0.223533\n",
      "[502]\ttraining's r2: 0.751045\tvalid_1's r2: 0.223531\n",
      "[503]\ttraining's r2: 0.75153\tvalid_1's r2: 0.22318\n",
      "[504]\ttraining's r2: 0.752036\tvalid_1's r2: 0.223106\n",
      "[505]\ttraining's r2: 0.752586\tvalid_1's r2: 0.222863\n",
      "[506]\ttraining's r2: 0.753061\tvalid_1's r2: 0.222816\n",
      "[507]\ttraining's r2: 0.75355\tvalid_1's r2: 0.223153\n",
      "[508]\ttraining's r2: 0.75401\tvalid_1's r2: 0.223414\n",
      "[509]\ttraining's r2: 0.754538\tvalid_1's r2: 0.223519\n",
      "[510]\ttraining's r2: 0.755085\tvalid_1's r2: 0.223354\n",
      "[511]\ttraining's r2: 0.755557\tvalid_1's r2: 0.223513\n",
      "[512]\ttraining's r2: 0.756053\tvalid_1's r2: 0.223862\n",
      "[513]\ttraining's r2: 0.756649\tvalid_1's r2: 0.223991\n",
      "[514]\ttraining's r2: 0.757103\tvalid_1's r2: 0.223933\n",
      "[515]\ttraining's r2: 0.757469\tvalid_1's r2: 0.223836\n",
      "[516]\ttraining's r2: 0.758054\tvalid_1's r2: 0.224117\n",
      "[517]\ttraining's r2: 0.758516\tvalid_1's r2: 0.223963\n",
      "[518]\ttraining's r2: 0.758882\tvalid_1's r2: 0.223691\n",
      "[519]\ttraining's r2: 0.759329\tvalid_1's r2: 0.223874\n",
      "[520]\ttraining's r2: 0.759758\tvalid_1's r2: 0.223838\n",
      "[521]\ttraining's r2: 0.760189\tvalid_1's r2: 0.224186\n",
      "[522]\ttraining's r2: 0.760561\tvalid_1's r2: 0.224193\n",
      "[523]\ttraining's r2: 0.761067\tvalid_1's r2: 0.224219\n",
      "[524]\ttraining's r2: 0.761589\tvalid_1's r2: 0.224117\n",
      "[525]\ttraining's r2: 0.761956\tvalid_1's r2: 0.223883\n",
      "[526]\ttraining's r2: 0.762417\tvalid_1's r2: 0.223792\n",
      "[527]\ttraining's r2: 0.762886\tvalid_1's r2: 0.224089\n",
      "[528]\ttraining's r2: 0.763403\tvalid_1's r2: 0.22417\n",
      "[529]\ttraining's r2: 0.763878\tvalid_1's r2: 0.224509\n",
      "[530]\ttraining's r2: 0.76438\tvalid_1's r2: 0.22467\n",
      "[531]\ttraining's r2: 0.764683\tvalid_1's r2: 0.224602\n",
      "[532]\ttraining's r2: 0.76526\tvalid_1's r2: 0.224696\n",
      "[533]\ttraining's r2: 0.765675\tvalid_1's r2: 0.224813\n",
      "[534]\ttraining's r2: 0.766063\tvalid_1's r2: 0.224679\n",
      "[535]\ttraining's r2: 0.766657\tvalid_1's r2: 0.224436\n",
      "[536]\ttraining's r2: 0.767106\tvalid_1's r2: 0.22461\n",
      "[537]\ttraining's r2: 0.767606\tvalid_1's r2: 0.224654\n",
      "[538]\ttraining's r2: 0.768076\tvalid_1's r2: 0.224843\n",
      "[539]\ttraining's r2: 0.768515\tvalid_1's r2: 0.225075\n",
      "[540]\ttraining's r2: 0.768998\tvalid_1's r2: 0.225277\n",
      "[541]\ttraining's r2: 0.769313\tvalid_1's r2: 0.224942\n",
      "[542]\ttraining's r2: 0.769692\tvalid_1's r2: 0.225085\n",
      "[543]\ttraining's r2: 0.770127\tvalid_1's r2: 0.22529\n",
      "[544]\ttraining's r2: 0.770643\tvalid_1's r2: 0.225505\n",
      "[545]\ttraining's r2: 0.771174\tvalid_1's r2: 0.225676\n",
      "[546]\ttraining's r2: 0.771654\tvalid_1's r2: 0.225916\n",
      "[547]\ttraining's r2: 0.77199\tvalid_1's r2: 0.225841\n",
      "[548]\ttraining's r2: 0.772451\tvalid_1's r2: 0.225935\n",
      "[549]\ttraining's r2: 0.772863\tvalid_1's r2: 0.225979\n",
      "[550]\ttraining's r2: 0.773284\tvalid_1's r2: 0.226023\n",
      "[551]\ttraining's r2: 0.773681\tvalid_1's r2: 0.225982\n",
      "[552]\ttraining's r2: 0.774132\tvalid_1's r2: 0.226532\n",
      "[553]\ttraining's r2: 0.774654\tvalid_1's r2: 0.226251\n",
      "[554]\ttraining's r2: 0.775041\tvalid_1's r2: 0.226244\n",
      "[555]\ttraining's r2: 0.775511\tvalid_1's r2: 0.226016\n",
      "[556]\ttraining's r2: 0.77592\tvalid_1's r2: 0.226195\n",
      "[557]\ttraining's r2: 0.776323\tvalid_1's r2: 0.226612\n",
      "[558]\ttraining's r2: 0.776794\tvalid_1's r2: 0.226419\n",
      "[559]\ttraining's r2: 0.777221\tvalid_1's r2: 0.22613\n",
      "[560]\ttraining's r2: 0.777734\tvalid_1's r2: 0.226223\n",
      "[561]\ttraining's r2: 0.778148\tvalid_1's r2: 0.226047\n",
      "[562]\ttraining's r2: 0.77846\tvalid_1's r2: 0.225953\n",
      "[563]\ttraining's r2: 0.778817\tvalid_1's r2: 0.226173\n",
      "[564]\ttraining's r2: 0.779177\tvalid_1's r2: 0.226487\n",
      "[565]\ttraining's r2: 0.779606\tvalid_1's r2: 0.226417\n",
      "[566]\ttraining's r2: 0.78008\tvalid_1's r2: 0.226513\n",
      "[567]\ttraining's r2: 0.780386\tvalid_1's r2: 0.22623\n",
      "[568]\ttraining's r2: 0.780795\tvalid_1's r2: 0.226402\n",
      "[569]\ttraining's r2: 0.781201\tvalid_1's r2: 0.226128\n",
      "[570]\ttraining's r2: 0.781559\tvalid_1's r2: 0.226067\n",
      "[571]\ttraining's r2: 0.781863\tvalid_1's r2: 0.225993\n",
      "[572]\ttraining's r2: 0.782177\tvalid_1's r2: 0.225838\n",
      "[573]\ttraining's r2: 0.78261\tvalid_1's r2: 0.226032\n",
      "[574]\ttraining's r2: 0.783058\tvalid_1's r2: 0.226174\n",
      "[575]\ttraining's r2: 0.783546\tvalid_1's r2: 0.226197\n",
      "[576]\ttraining's r2: 0.783982\tvalid_1's r2: 0.226236\n",
      "[577]\ttraining's r2: 0.78432\tvalid_1's r2: 0.226232\n",
      "[578]\ttraining's r2: 0.784708\tvalid_1's r2: 0.226142\n",
      "[579]\ttraining's r2: 0.785061\tvalid_1's r2: 0.225916\n",
      "[580]\ttraining's r2: 0.785488\tvalid_1's r2: 0.226197\n",
      "[581]\ttraining's r2: 0.785825\tvalid_1's r2: 0.226382\n",
      "[582]\ttraining's r2: 0.786224\tvalid_1's r2: 0.226146\n",
      "[583]\ttraining's r2: 0.786611\tvalid_1's r2: 0.226117\n",
      "[584]\ttraining's r2: 0.78696\tvalid_1's r2: 0.226316\n",
      "[585]\ttraining's r2: 0.78726\tvalid_1's r2: 0.226232\n",
      "[586]\ttraining's r2: 0.787642\tvalid_1's r2: 0.225751\n",
      "[587]\ttraining's r2: 0.788066\tvalid_1's r2: 0.225662\n",
      "[588]\ttraining's r2: 0.788422\tvalid_1's r2: 0.225537\n",
      "[589]\ttraining's r2: 0.788755\tvalid_1's r2: 0.225631\n",
      "[590]\ttraining's r2: 0.789159\tvalid_1's r2: 0.225323\n",
      "[591]\ttraining's r2: 0.789529\tvalid_1's r2: 0.225868\n",
      "[592]\ttraining's r2: 0.789866\tvalid_1's r2: 0.225908\n",
      "[593]\ttraining's r2: 0.790282\tvalid_1's r2: 0.22596\n",
      "[594]\ttraining's r2: 0.790756\tvalid_1's r2: 0.226026\n",
      "[595]\ttraining's r2: 0.791147\tvalid_1's r2: 0.226102\n",
      "[596]\ttraining's r2: 0.791604\tvalid_1's r2: 0.226365\n",
      "[597]\ttraining's r2: 0.792029\tvalid_1's r2: 0.226464\n",
      "[598]\ttraining's r2: 0.792316\tvalid_1's r2: 0.226575\n",
      "[599]\ttraining's r2: 0.792677\tvalid_1's r2: 0.226954\n",
      "[600]\ttraining's r2: 0.793\tvalid_1's r2: 0.227031\n",
      "[601]\ttraining's r2: 0.793512\tvalid_1's r2: 0.227247\n",
      "[602]\ttraining's r2: 0.793893\tvalid_1's r2: 0.227232\n",
      "[603]\ttraining's r2: 0.794249\tvalid_1's r2: 0.227298\n",
      "[604]\ttraining's r2: 0.794604\tvalid_1's r2: 0.22723\n",
      "[605]\ttraining's r2: 0.794965\tvalid_1's r2: 0.227183\n",
      "[606]\ttraining's r2: 0.795408\tvalid_1's r2: 0.22724\n",
      "[607]\ttraining's r2: 0.795805\tvalid_1's r2: 0.227389\n",
      "[608]\ttraining's r2: 0.796226\tvalid_1's r2: 0.227029\n",
      "[609]\ttraining's r2: 0.796563\tvalid_1's r2: 0.227021\n",
      "[610]\ttraining's r2: 0.796968\tvalid_1's r2: 0.227227\n",
      "[611]\ttraining's r2: 0.797297\tvalid_1's r2: 0.227097\n",
      "[612]\ttraining's r2: 0.797664\tvalid_1's r2: 0.227006\n",
      "[613]\ttraining's r2: 0.798015\tvalid_1's r2: 0.226953\n",
      "[614]\ttraining's r2: 0.79851\tvalid_1's r2: 0.226634\n",
      "[615]\ttraining's r2: 0.798842\tvalid_1's r2: 0.226733\n",
      "[616]\ttraining's r2: 0.799207\tvalid_1's r2: 0.226788\n",
      "[617]\ttraining's r2: 0.799625\tvalid_1's r2: 0.226927\n",
      "[618]\ttraining's r2: 0.799911\tvalid_1's r2: 0.226387\n",
      "[619]\ttraining's r2: 0.800206\tvalid_1's r2: 0.226527\n",
      "[620]\ttraining's r2: 0.800483\tvalid_1's r2: 0.226553\n",
      "[621]\ttraining's r2: 0.800868\tvalid_1's r2: 0.226559\n",
      "[622]\ttraining's r2: 0.801217\tvalid_1's r2: 0.226834\n",
      "[623]\ttraining's r2: 0.801553\tvalid_1's r2: 0.226889\n",
      "[624]\ttraining's r2: 0.801938\tvalid_1's r2: 0.227096\n",
      "[625]\ttraining's r2: 0.802217\tvalid_1's r2: 0.226648\n",
      "[626]\ttraining's r2: 0.802506\tvalid_1's r2: 0.226219\n",
      "[627]\ttraining's r2: 0.802883\tvalid_1's r2: 0.226191\n",
      "[628]\ttraining's r2: 0.803219\tvalid_1's r2: 0.226594\n",
      "[629]\ttraining's r2: 0.803609\tvalid_1's r2: 0.22673\n",
      "[630]\ttraining's r2: 0.803914\tvalid_1's r2: 0.226614\n",
      "[631]\ttraining's r2: 0.804272\tvalid_1's r2: 0.226836\n",
      "[632]\ttraining's r2: 0.804462\tvalid_1's r2: 0.2268\n",
      "[633]\ttraining's r2: 0.804781\tvalid_1's r2: 0.227049\n",
      "[634]\ttraining's r2: 0.805106\tvalid_1's r2: 0.226929\n",
      "[635]\ttraining's r2: 0.805462\tvalid_1's r2: 0.22706\n",
      "[636]\ttraining's r2: 0.805808\tvalid_1's r2: 0.227094\n",
      "[637]\ttraining's r2: 0.806205\tvalid_1's r2: 0.227169\n",
      "[638]\ttraining's r2: 0.806521\tvalid_1's r2: 0.226996\n",
      "[639]\ttraining's r2: 0.806868\tvalid_1's r2: 0.226933\n",
      "[640]\ttraining's r2: 0.807209\tvalid_1's r2: 0.226951\n",
      "[641]\ttraining's r2: 0.807566\tvalid_1's r2: 0.227002\n",
      "[642]\ttraining's r2: 0.807909\tvalid_1's r2: 0.227148\n",
      "[643]\ttraining's r2: 0.808282\tvalid_1's r2: 0.227234\n",
      "[644]\ttraining's r2: 0.808593\tvalid_1's r2: 0.227029\n",
      "[645]\ttraining's r2: 0.809009\tvalid_1's r2: 0.226977\n",
      "[646]\ttraining's r2: 0.809333\tvalid_1's r2: 0.227046\n",
      "[647]\ttraining's r2: 0.809649\tvalid_1's r2: 0.226963\n",
      "[648]\ttraining's r2: 0.809975\tvalid_1's r2: 0.22701\n",
      "[649]\ttraining's r2: 0.810299\tvalid_1's r2: 0.227442\n",
      "[650]\ttraining's r2: 0.810654\tvalid_1's r2: 0.227195\n",
      "[651]\ttraining's r2: 0.811022\tvalid_1's r2: 0.227378\n",
      "[652]\ttraining's r2: 0.811317\tvalid_1's r2: 0.227316\n",
      "[653]\ttraining's r2: 0.811628\tvalid_1's r2: 0.227648\n",
      "[654]\ttraining's r2: 0.811936\tvalid_1's r2: 0.227953\n",
      "[655]\ttraining's r2: 0.812318\tvalid_1's r2: 0.228052\n",
      "[656]\ttraining's r2: 0.812663\tvalid_1's r2: 0.228111\n",
      "[657]\ttraining's r2: 0.813037\tvalid_1's r2: 0.228452\n",
      "[658]\ttraining's r2: 0.813341\tvalid_1's r2: 0.228504\n",
      "[659]\ttraining's r2: 0.813666\tvalid_1's r2: 0.228869\n",
      "[660]\ttraining's r2: 0.81392\tvalid_1's r2: 0.228663\n",
      "[661]\ttraining's r2: 0.814337\tvalid_1's r2: 0.228839\n",
      "[662]\ttraining's r2: 0.8146\tvalid_1's r2: 0.228652\n",
      "[663]\ttraining's r2: 0.814934\tvalid_1's r2: 0.228771\n",
      "[664]\ttraining's r2: 0.815204\tvalid_1's r2: 0.228778\n",
      "[665]\ttraining's r2: 0.81553\tvalid_1's r2: 0.228802\n",
      "[666]\ttraining's r2: 0.815863\tvalid_1's r2: 0.228731\n",
      "[667]\ttraining's r2: 0.81617\tvalid_1's r2: 0.228889\n",
      "[668]\ttraining's r2: 0.816487\tvalid_1's r2: 0.228687\n",
      "[669]\ttraining's r2: 0.81676\tvalid_1's r2: 0.228628\n",
      "[670]\ttraining's r2: 0.817151\tvalid_1's r2: 0.228648\n",
      "[671]\ttraining's r2: 0.817547\tvalid_1's r2: 0.228505\n",
      "[672]\ttraining's r2: 0.817803\tvalid_1's r2: 0.228573\n",
      "[673]\ttraining's r2: 0.818151\tvalid_1's r2: 0.228488\n",
      "[674]\ttraining's r2: 0.818438\tvalid_1's r2: 0.228221\n",
      "[675]\ttraining's r2: 0.818701\tvalid_1's r2: 0.227955\n",
      "[676]\ttraining's r2: 0.818993\tvalid_1's r2: 0.228199\n",
      "[677]\ttraining's r2: 0.81926\tvalid_1's r2: 0.228152\n",
      "[678]\ttraining's r2: 0.819626\tvalid_1's r2: 0.228528\n",
      "[679]\ttraining's r2: 0.819947\tvalid_1's r2: 0.228645\n",
      "[680]\ttraining's r2: 0.820179\tvalid_1's r2: 0.228325\n",
      "[681]\ttraining's r2: 0.820461\tvalid_1's r2: 0.228137\n",
      "[682]\ttraining's r2: 0.820731\tvalid_1's r2: 0.227958\n",
      "[683]\ttraining's r2: 0.820938\tvalid_1's r2: 0.227634\n",
      "[684]\ttraining's r2: 0.821257\tvalid_1's r2: 0.22731\n",
      "[685]\ttraining's r2: 0.821597\tvalid_1's r2: 0.227268\n",
      "[686]\ttraining's r2: 0.821867\tvalid_1's r2: 0.227543\n",
      "[687]\ttraining's r2: 0.822127\tvalid_1's r2: 0.227497\n",
      "[688]\ttraining's r2: 0.822345\tvalid_1's r2: 0.227425\n",
      "[689]\ttraining's r2: 0.822599\tvalid_1's r2: 0.227452\n",
      "[690]\ttraining's r2: 0.822891\tvalid_1's r2: 0.227553\n",
      "[691]\ttraining's r2: 0.823159\tvalid_1's r2: 0.227406\n",
      "[692]\ttraining's r2: 0.823468\tvalid_1's r2: 0.227383\n",
      "[693]\ttraining's r2: 0.82373\tvalid_1's r2: 0.227182\n",
      "[694]\ttraining's r2: 0.823912\tvalid_1's r2: 0.227165\n",
      "[695]\ttraining's r2: 0.824172\tvalid_1's r2: 0.227133\n",
      "[696]\ttraining's r2: 0.824434\tvalid_1's r2: 0.226914\n",
      "[697]\ttraining's r2: 0.82478\tvalid_1's r2: 0.22682\n",
      "[698]\ttraining's r2: 0.825065\tvalid_1's r2: 0.227045\n",
      "[699]\ttraining's r2: 0.82535\tvalid_1's r2: 0.227412\n",
      "[700]\ttraining's r2: 0.825674\tvalid_1's r2: 0.22734\n",
      "[701]\ttraining's r2: 0.825916\tvalid_1's r2: 0.227379\n",
      "[702]\ttraining's r2: 0.826155\tvalid_1's r2: 0.227318\n",
      "[703]\ttraining's r2: 0.82651\tvalid_1's r2: 0.226941\n",
      "[704]\ttraining's r2: 0.82685\tvalid_1's r2: 0.227106\n",
      "[705]\ttraining's r2: 0.827022\tvalid_1's r2: 0.226895\n",
      "[706]\ttraining's r2: 0.827225\tvalid_1's r2: 0.226817\n",
      "[707]\ttraining's r2: 0.827477\tvalid_1's r2: 0.226732\n",
      "[708]\ttraining's r2: 0.827725\tvalid_1's r2: 0.226669\n",
      "[709]\ttraining's r2: 0.827985\tvalid_1's r2: 0.226597\n",
      "[710]\ttraining's r2: 0.828263\tvalid_1's r2: 0.226554\n",
      "[711]\ttraining's r2: 0.828595\tvalid_1's r2: 0.226702\n",
      "[712]\ttraining's r2: 0.828824\tvalid_1's r2: 0.226665\n",
      "[713]\ttraining's r2: 0.829142\tvalid_1's r2: 0.226916\n",
      "[714]\ttraining's r2: 0.8295\tvalid_1's r2: 0.226855\n",
      "[715]\ttraining's r2: 0.829681\tvalid_1's r2: 0.22675\n",
      "[716]\ttraining's r2: 0.830033\tvalid_1's r2: 0.227063\n",
      "[717]\ttraining's r2: 0.830319\tvalid_1's r2: 0.227238\n",
      "[718]\ttraining's r2: 0.830629\tvalid_1's r2: 0.227295\n",
      "[719]\ttraining's r2: 0.830865\tvalid_1's r2: 0.227286\n",
      "[720]\ttraining's r2: 0.831155\tvalid_1's r2: 0.227135\n",
      "[721]\ttraining's r2: 0.831406\tvalid_1's r2: 0.226888\n",
      "[722]\ttraining's r2: 0.831681\tvalid_1's r2: 0.226889\n",
      "[723]\ttraining's r2: 0.831924\tvalid_1's r2: 0.226916\n",
      "[724]\ttraining's r2: 0.832229\tvalid_1's r2: 0.226826\n",
      "[725]\ttraining's r2: 0.832508\tvalid_1's r2: 0.226644\n",
      "[726]\ttraining's r2: 0.83278\tvalid_1's r2: 0.226555\n",
      "[727]\ttraining's r2: 0.833047\tvalid_1's r2: 0.226495\n",
      "[728]\ttraining's r2: 0.833397\tvalid_1's r2: 0.226626\n",
      "[729]\ttraining's r2: 0.833638\tvalid_1's r2: 0.226619\n",
      "[730]\ttraining's r2: 0.833892\tvalid_1's r2: 0.226724\n",
      "[731]\ttraining's r2: 0.83417\tvalid_1's r2: 0.226393\n",
      "[732]\ttraining's r2: 0.834442\tvalid_1's r2: 0.226163\n",
      "[733]\ttraining's r2: 0.834661\tvalid_1's r2: 0.226073\n",
      "[734]\ttraining's r2: 0.834971\tvalid_1's r2: 0.226192\n",
      "[735]\ttraining's r2: 0.835175\tvalid_1's r2: 0.226178\n",
      "[736]\ttraining's r2: 0.835394\tvalid_1's r2: 0.226408\n",
      "[737]\ttraining's r2: 0.835571\tvalid_1's r2: 0.226102\n",
      "[738]\ttraining's r2: 0.835846\tvalid_1's r2: 0.225827\n",
      "[739]\ttraining's r2: 0.836163\tvalid_1's r2: 0.225807\n",
      "[740]\ttraining's r2: 0.836422\tvalid_1's r2: 0.225574\n",
      "[741]\ttraining's r2: 0.836736\tvalid_1's r2: 0.225235\n",
      "[742]\ttraining's r2: 0.836927\tvalid_1's r2: 0.225468\n",
      "[743]\ttraining's r2: 0.837143\tvalid_1's r2: 0.225415\n",
      "[744]\ttraining's r2: 0.837398\tvalid_1's r2: 0.225285\n",
      "[745]\ttraining's r2: 0.837655\tvalid_1's r2: 0.225462\n",
      "[746]\ttraining's r2: 0.837883\tvalid_1's r2: 0.225355\n",
      "[747]\ttraining's r2: 0.838234\tvalid_1's r2: 0.225371\n",
      "[748]\ttraining's r2: 0.838471\tvalid_1's r2: 0.225415\n",
      "[749]\ttraining's r2: 0.838767\tvalid_1's r2: 0.225466\n",
      "[750]\ttraining's r2: 0.838993\tvalid_1's r2: 0.225745\n",
      "[751]\ttraining's r2: 0.839251\tvalid_1's r2: 0.225673\n",
      "[752]\ttraining's r2: 0.83955\tvalid_1's r2: 0.225859\n",
      "[753]\ttraining's r2: 0.839785\tvalid_1's r2: 0.2258\n",
      "[754]\ttraining's r2: 0.840059\tvalid_1's r2: 0.225699\n",
      "[755]\ttraining's r2: 0.840388\tvalid_1's r2: 0.226074\n",
      "[756]\ttraining's r2: 0.84062\tvalid_1's r2: 0.226385\n",
      "[757]\ttraining's r2: 0.840852\tvalid_1's r2: 0.226236\n",
      "[758]\ttraining's r2: 0.841099\tvalid_1's r2: 0.226267\n",
      "[759]\ttraining's r2: 0.841388\tvalid_1's r2: 0.226154\n",
      "[760]\ttraining's r2: 0.841665\tvalid_1's r2: 0.226344\n",
      "[761]\ttraining's r2: 0.841903\tvalid_1's r2: 0.226615\n",
      "[762]\ttraining's r2: 0.842151\tvalid_1's r2: 0.226795\n",
      "[763]\ttraining's r2: 0.842364\tvalid_1's r2: 0.226903\n",
      "[764]\ttraining's r2: 0.84255\tvalid_1's r2: 0.226744\n",
      "[765]\ttraining's r2: 0.842775\tvalid_1's r2: 0.22672\n",
      "[766]\ttraining's r2: 0.84304\tvalid_1's r2: 0.226618\n",
      "[767]\ttraining's r2: 0.843303\tvalid_1's r2: 0.226698\n",
      "Early stopping, best iteration is:\n",
      "[667]\ttraining's r2: 0.81617\tvalid_1's r2: 0.228889\n",
      "[1]\ttraining's r2: -3384.88\tvalid_1's r2: -4474.8\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[2]\ttraining's r2: -800.049\tvalid_1's r2: -1061.49\n",
      "[3]\ttraining's r2: -341.309\tvalid_1's r2: -454.462\n",
      "[4]\ttraining's r2: -187.762\tvalid_1's r2: -253.205\n",
      "[5]\ttraining's r2: -118.936\tvalid_1's r2: -161.83\n",
      "[6]\ttraining's r2: -81.5859\tvalid_1's r2: -112.096\n",
      "[7]\ttraining's r2: -59.0889\tvalid_1's r2: -81.9653\n",
      "[8]\ttraining's r2: -44.5978\tvalid_1's r2: -62.4425\n",
      "[9]\ttraining's r2: -34.6563\tvalid_1's r2: -48.9829\n",
      "[10]\ttraining's r2: -27.9664\tvalid_1's r2: -39.8227\n",
      "[11]\ttraining's r2: -22.9408\tvalid_1's r2: -33.006\n",
      "[12]\ttraining's r2: -19.1902\tvalid_1's r2: -27.8579\n",
      "[13]\ttraining's r2: -16.2186\tvalid_1's r2: -23.7825\n",
      "[14]\ttraining's r2: -13.8432\tvalid_1's r2: -20.5056\n",
      "[15]\ttraining's r2: -11.9714\tvalid_1's r2: -17.9153\n",
      "[16]\ttraining's r2: -10.4121\tvalid_1's r2: -15.7554\n",
      "[17]\ttraining's r2: -9.13901\tvalid_1's r2: -14.0016\n",
      "[18]\ttraining's r2: -8.10374\tvalid_1's r2: -12.5677\n",
      "[19]\ttraining's r2: -7.19876\tvalid_1's r2: -11.3064\n",
      "[20]\ttraining's r2: -6.41954\tvalid_1's r2: -10.2267\n",
      "[21]\ttraining's r2: -5.76638\tvalid_1's r2: -9.30263\n",
      "[22]\ttraining's r2: -5.20451\tvalid_1's r2: -8.51878\n",
      "[23]\ttraining's r2: -4.70849\tvalid_1's r2: -7.81781\n",
      "[24]\ttraining's r2: -4.27011\tvalid_1's r2: -7.1964\n",
      "[25]\ttraining's r2: -3.90096\tvalid_1's r2: -6.66824\n",
      "[26]\ttraining's r2: -3.55553\tvalid_1's r2: -6.17645\n",
      "[27]\ttraining's r2: -3.2483\tvalid_1's r2: -5.73968\n",
      "[28]\ttraining's r2: -2.95982\tvalid_1's r2: -5.33003\n",
      "[29]\ttraining's r2: -2.7228\tvalid_1's r2: -5.0027\n",
      "[30]\ttraining's r2: -2.50728\tvalid_1's r2: -4.69959\n",
      "[31]\ttraining's r2: -2.3093\tvalid_1's r2: -4.42115\n",
      "[32]\ttraining's r2: -2.14003\tvalid_1's r2: -4.17507\n",
      "[33]\ttraining's r2: -1.97712\tvalid_1's r2: -3.93634\n",
      "[34]\ttraining's r2: -1.83778\tvalid_1's r2: -3.73051\n",
      "[35]\ttraining's r2: -1.70288\tvalid_1's r2: -3.53507\n",
      "[36]\ttraining's r2: -1.58529\tvalid_1's r2: -3.35961\n",
      "[37]\ttraining's r2: -1.47397\tvalid_1's r2: -3.19752\n",
      "[38]\ttraining's r2: -1.37057\tvalid_1's r2: -3.04199\n",
      "[39]\ttraining's r2: -1.2708\tvalid_1's r2: -2.90008\n",
      "[40]\ttraining's r2: -1.17618\tvalid_1's r2: -2.75762\n",
      "[41]\ttraining's r2: -1.09061\tvalid_1's r2: -2.62859\n",
      "[42]\ttraining's r2: -1.01657\tvalid_1's r2: -2.52519\n",
      "[43]\ttraining's r2: -0.950912\tvalid_1's r2: -2.43397\n",
      "[44]\ttraining's r2: -0.885783\tvalid_1's r2: -2.33995\n",
      "[45]\ttraining's r2: -0.824465\tvalid_1's r2: -2.24772\n",
      "[46]\ttraining's r2: -0.765893\tvalid_1's r2: -2.1661\n",
      "[47]\ttraining's r2: -0.71131\tvalid_1's r2: -2.08606\n",
      "[48]\ttraining's r2: -0.661472\tvalid_1's r2: -2.01965\n",
      "[49]\ttraining's r2: -0.612886\tvalid_1's r2: -1.94762\n",
      "[50]\ttraining's r2: -0.571036\tvalid_1's r2: -1.88651\n",
      "[51]\ttraining's r2: -0.52937\tvalid_1's r2: -1.82501\n",
      "[52]\ttraining's r2: -0.487998\tvalid_1's r2: -1.76255\n",
      "[53]\ttraining's r2: -0.45349\tvalid_1's r2: -1.71196\n",
      "[54]\ttraining's r2: -0.417559\tvalid_1's r2: -1.65793\n",
      "[55]\ttraining's r2: -0.383641\tvalid_1's r2: -1.60953\n",
      "[56]\ttraining's r2: -0.350461\tvalid_1's r2: -1.55967\n",
      "[57]\ttraining's r2: -0.318072\tvalid_1's r2: -1.51261\n",
      "[58]\ttraining's r2: -0.288885\tvalid_1's r2: -1.47168\n",
      "[59]\ttraining's r2: -0.261674\tvalid_1's r2: -1.42822\n",
      "[60]\ttraining's r2: -0.235052\tvalid_1's r2: -1.38885\n",
      "[61]\ttraining's r2: -0.208503\tvalid_1's r2: -1.34937\n",
      "[62]\ttraining's r2: -0.183452\tvalid_1's r2: -1.3117\n",
      "[63]\ttraining's r2: -0.159496\tvalid_1's r2: -1.27481\n",
      "[64]\ttraining's r2: -0.138679\tvalid_1's r2: -1.24477\n",
      "[65]\ttraining's r2: -0.115991\tvalid_1's r2: -1.20987\n",
      "[66]\ttraining's r2: -0.0966787\tvalid_1's r2: -1.17877\n",
      "[67]\ttraining's r2: -0.0773847\tvalid_1's r2: -1.15185\n",
      "[68]\ttraining's r2: -0.0571377\tvalid_1's r2: -1.12259\n",
      "[69]\ttraining's r2: -0.0388777\tvalid_1's r2: -1.09564\n",
      "[70]\ttraining's r2: -0.0209256\tvalid_1's r2: -1.06899\n",
      "[71]\ttraining's r2: -0.00428913\tvalid_1's r2: -1.04327\n",
      "[72]\ttraining's r2: 0.0125392\tvalid_1's r2: -1.01869\n",
      "[73]\ttraining's r2: 0.0262277\tvalid_1's r2: -0.99893\n",
      "[74]\ttraining's r2: 0.0413935\tvalid_1's r2: -0.975622\n",
      "[75]\ttraining's r2: 0.0545103\tvalid_1's r2: -0.953974\n",
      "[76]\ttraining's r2: 0.0664728\tvalid_1's r2: -0.93564\n",
      "[77]\ttraining's r2: 0.0793673\tvalid_1's r2: -0.91572\n",
      "[78]\ttraining's r2: 0.0913749\tvalid_1's r2: -0.898091\n",
      "[79]\ttraining's r2: 0.103612\tvalid_1's r2: -0.880033\n",
      "[80]\ttraining's r2: 0.115643\tvalid_1's r2: -0.862032\n",
      "[81]\ttraining's r2: 0.126023\tvalid_1's r2: -0.847136\n",
      "[82]\ttraining's r2: 0.13728\tvalid_1's r2: -0.828525\n",
      "[83]\ttraining's r2: 0.147429\tvalid_1's r2: -0.814822\n",
      "[84]\ttraining's r2: 0.15843\tvalid_1's r2: -0.79845\n",
      "[85]\ttraining's r2: 0.168583\tvalid_1's r2: -0.782192\n",
      "[86]\ttraining's r2: 0.177911\tvalid_1's r2: -0.768089\n",
      "[87]\ttraining's r2: 0.186618\tvalid_1's r2: -0.755781\n",
      "[88]\ttraining's r2: 0.195976\tvalid_1's r2: -0.741865\n",
      "[89]\ttraining's r2: 0.205246\tvalid_1's r2: -0.727483\n",
      "[90]\ttraining's r2: 0.214518\tvalid_1's r2: -0.713743\n",
      "[91]\ttraining's r2: 0.222634\tvalid_1's r2: -0.700275\n",
      "[92]\ttraining's r2: 0.231472\tvalid_1's r2: -0.687201\n",
      "[93]\ttraining's r2: 0.239739\tvalid_1's r2: -0.674796\n",
      "[94]\ttraining's r2: 0.248094\tvalid_1's r2: -0.662603\n",
      "[95]\ttraining's r2: 0.255725\tvalid_1's r2: -0.650798\n",
      "[96]\ttraining's r2: 0.264118\tvalid_1's r2: -0.638343\n",
      "[97]\ttraining's r2: 0.27087\tvalid_1's r2: -0.628021\n",
      "[98]\ttraining's r2: 0.277977\tvalid_1's r2: -0.618018\n",
      "[99]\ttraining's r2: 0.284536\tvalid_1's r2: -0.607629\n",
      "[100]\ttraining's r2: 0.291378\tvalid_1's r2: -0.596485\n",
      "[101]\ttraining's r2: 0.297804\tvalid_1's r2: -0.587653\n",
      "[102]\ttraining's r2: 0.303359\tvalid_1's r2: -0.57998\n",
      "[103]\ttraining's r2: 0.309869\tvalid_1's r2: -0.570107\n",
      "[104]\ttraining's r2: 0.316239\tvalid_1's r2: -0.560987\n",
      "[105]\ttraining's r2: 0.320701\tvalid_1's r2: -0.55546\n",
      "[106]\ttraining's r2: 0.326733\tvalid_1's r2: -0.545956\n",
      "[107]\ttraining's r2: 0.331716\tvalid_1's r2: -0.538944\n",
      "[108]\ttraining's r2: 0.337593\tvalid_1's r2: -0.529598\n",
      "[109]\ttraining's r2: 0.342926\tvalid_1's r2: -0.521695\n",
      "[110]\ttraining's r2: 0.348344\tvalid_1's r2: -0.513022\n",
      "[111]\ttraining's r2: 0.353543\tvalid_1's r2: -0.505196\n",
      "[112]\ttraining's r2: 0.358636\tvalid_1's r2: -0.497931\n",
      "[113]\ttraining's r2: 0.363012\tvalid_1's r2: -0.492339\n",
      "[114]\ttraining's r2: 0.367887\tvalid_1's r2: -0.484836\n",
      "[115]\ttraining's r2: 0.372459\tvalid_1's r2: -0.478015\n",
      "[116]\ttraining's r2: 0.376635\tvalid_1's r2: -0.471976\n",
      "[117]\ttraining's r2: 0.381199\tvalid_1's r2: -0.465766\n",
      "[118]\ttraining's r2: 0.385239\tvalid_1's r2: -0.461244\n",
      "[119]\ttraining's r2: 0.389271\tvalid_1's r2: -0.4556\n",
      "[120]\ttraining's r2: 0.393674\tvalid_1's r2: -0.449817\n",
      "[121]\ttraining's r2: 0.397769\tvalid_1's r2: -0.444536\n",
      "[122]\ttraining's r2: 0.401855\tvalid_1's r2: -0.439118\n",
      "[123]\ttraining's r2: 0.405654\tvalid_1's r2: -0.434634\n",
      "[124]\ttraining's r2: 0.409617\tvalid_1's r2: -0.429387\n",
      "[125]\ttraining's r2: 0.413066\tvalid_1's r2: -0.425384\n",
      "[126]\ttraining's r2: 0.417006\tvalid_1's r2: -0.419783\n",
      "[127]\ttraining's r2: 0.420387\tvalid_1's r2: -0.415469\n",
      "[128]\ttraining's r2: 0.423636\tvalid_1's r2: -0.411715\n",
      "[129]\ttraining's r2: 0.426978\tvalid_1's r2: -0.408062\n",
      "[130]\ttraining's r2: 0.429807\tvalid_1's r2: -0.405999\n",
      "[131]\ttraining's r2: 0.433352\tvalid_1's r2: -0.401106\n",
      "[132]\ttraining's r2: 0.436284\tvalid_1's r2: -0.399136\n",
      "[133]\ttraining's r2: 0.439018\tvalid_1's r2: -0.395915\n",
      "[134]\ttraining's r2: 0.441338\tvalid_1's r2: -0.392708\n",
      "[135]\ttraining's r2: 0.444653\tvalid_1's r2: -0.388953\n",
      "[136]\ttraining's r2: 0.448026\tvalid_1's r2: -0.384358\n",
      "[137]\ttraining's r2: 0.450856\tvalid_1's r2: -0.381473\n",
      "[138]\ttraining's r2: 0.45398\tvalid_1's r2: -0.378815\n",
      "[139]\ttraining's r2: 0.456155\tvalid_1's r2: -0.377386\n",
      "[140]\ttraining's r2: 0.458513\tvalid_1's r2: -0.374847\n",
      "[141]\ttraining's r2: 0.460733\tvalid_1's r2: -0.372798\n",
      "[142]\ttraining's r2: 0.4634\tvalid_1's r2: -0.368562\n",
      "[143]\ttraining's r2: 0.465952\tvalid_1's r2: -0.365684\n",
      "[144]\ttraining's r2: 0.468286\tvalid_1's r2: -0.362794\n",
      "[145]\ttraining's r2: 0.471168\tvalid_1's r2: -0.358508\n",
      "[146]\ttraining's r2: 0.474034\tvalid_1's r2: -0.353984\n",
      "[147]\ttraining's r2: 0.476138\tvalid_1's r2: -0.352276\n",
      "[148]\ttraining's r2: 0.478546\tvalid_1's r2: -0.35111\n",
      "[149]\ttraining's r2: 0.480767\tvalid_1's r2: -0.349578\n",
      "[150]\ttraining's r2: 0.483129\tvalid_1's r2: -0.346701\n",
      "[151]\ttraining's r2: 0.485782\tvalid_1's r2: -0.343744\n",
      "[152]\ttraining's r2: 0.487688\tvalid_1's r2: -0.34219\n",
      "[153]\ttraining's r2: 0.489835\tvalid_1's r2: -0.339994\n",
      "[154]\ttraining's r2: 0.491739\tvalid_1's r2: -0.337914\n",
      "[155]\ttraining's r2: 0.493702\tvalid_1's r2: -0.335429\n",
      "[156]\ttraining's r2: 0.495946\tvalid_1's r2: -0.33358\n",
      "[157]\ttraining's r2: 0.498078\tvalid_1's r2: -0.332028\n",
      "[158]\ttraining's r2: 0.499492\tvalid_1's r2: -0.332288\n",
      "[159]\ttraining's r2: 0.501755\tvalid_1's r2: -0.329927\n",
      "[160]\ttraining's r2: 0.503968\tvalid_1's r2: -0.326799\n",
      "[161]\ttraining's r2: 0.505986\tvalid_1's r2: -0.324602\n",
      "[162]\ttraining's r2: 0.507705\tvalid_1's r2: -0.324301\n",
      "[163]\ttraining's r2: 0.509753\tvalid_1's r2: -0.322385\n",
      "[164]\ttraining's r2: 0.511408\tvalid_1's r2: -0.32188\n",
      "[165]\ttraining's r2: 0.51321\tvalid_1's r2: -0.320562\n",
      "[166]\ttraining's r2: 0.51523\tvalid_1's r2: -0.317616\n",
      "[167]\ttraining's r2: 0.517131\tvalid_1's r2: -0.317107\n",
      "[168]\ttraining's r2: 0.519512\tvalid_1's r2: -0.314154\n",
      "[169]\ttraining's r2: 0.521229\tvalid_1's r2: -0.311938\n",
      "[170]\ttraining's r2: 0.523067\tvalid_1's r2: -0.309683\n",
      "[171]\ttraining's r2: 0.525079\tvalid_1's r2: -0.307307\n",
      "[172]\ttraining's r2: 0.526786\tvalid_1's r2: -0.305685\n",
      "[173]\ttraining's r2: 0.528465\tvalid_1's r2: -0.305024\n",
      "[174]\ttraining's r2: 0.529872\tvalid_1's r2: -0.304565\n",
      "[175]\ttraining's r2: 0.53178\tvalid_1's r2: -0.301756\n",
      "[176]\ttraining's r2: 0.533905\tvalid_1's r2: -0.299036\n",
      "[177]\ttraining's r2: 0.535777\tvalid_1's r2: -0.297002\n",
      "[178]\ttraining's r2: 0.537363\tvalid_1's r2: -0.296589\n",
      "[179]\ttraining's r2: 0.539136\tvalid_1's r2: -0.295952\n",
      "[180]\ttraining's r2: 0.540573\tvalid_1's r2: -0.294919\n",
      "[181]\ttraining's r2: 0.542393\tvalid_1's r2: -0.2931\n",
      "[182]\ttraining's r2: 0.544319\tvalid_1's r2: -0.291335\n",
      "[183]\ttraining's r2: 0.546222\tvalid_1's r2: -0.289967\n",
      "[184]\ttraining's r2: 0.5481\tvalid_1's r2: -0.288128\n",
      "[185]\ttraining's r2: 0.549704\tvalid_1's r2: -0.287817\n",
      "[186]\ttraining's r2: 0.551653\tvalid_1's r2: -0.28619\n",
      "[187]\ttraining's r2: 0.553242\tvalid_1's r2: -0.285135\n",
      "[188]\ttraining's r2: 0.554739\tvalid_1's r2: -0.284272\n",
      "[189]\ttraining's r2: 0.55633\tvalid_1's r2: -0.283214\n",
      "[190]\ttraining's r2: 0.557748\tvalid_1's r2: -0.282226\n",
      "[191]\ttraining's r2: 0.559322\tvalid_1's r2: -0.280836\n",
      "[192]\ttraining's r2: 0.560572\tvalid_1's r2: -0.28009\n",
      "[193]\ttraining's r2: 0.562026\tvalid_1's r2: -0.279371\n",
      "[194]\ttraining's r2: 0.563524\tvalid_1's r2: -0.277795\n",
      "[195]\ttraining's r2: 0.564713\tvalid_1's r2: -0.276654\n",
      "[196]\ttraining's r2: 0.565768\tvalid_1's r2: -0.276317\n",
      "[197]\ttraining's r2: 0.567374\tvalid_1's r2: -0.27561\n",
      "[198]\ttraining's r2: 0.568769\tvalid_1's r2: -0.275149\n",
      "[199]\ttraining's r2: 0.570245\tvalid_1's r2: -0.274521\n",
      "[200]\ttraining's r2: 0.571469\tvalid_1's r2: -0.274248\n",
      "[201]\ttraining's r2: 0.573048\tvalid_1's r2: -0.273247\n",
      "[202]\ttraining's r2: 0.574412\tvalid_1's r2: -0.273116\n",
      "[203]\ttraining's r2: 0.576031\tvalid_1's r2: -0.272074\n",
      "[204]\ttraining's r2: 0.577433\tvalid_1's r2: -0.271222\n",
      "[205]\ttraining's r2: 0.578634\tvalid_1's r2: -0.269162\n",
      "[206]\ttraining's r2: 0.579716\tvalid_1's r2: -0.269325\n",
      "[207]\ttraining's r2: 0.580983\tvalid_1's r2: -0.268831\n",
      "[208]\ttraining's r2: 0.581918\tvalid_1's r2: -0.268228\n",
      "[209]\ttraining's r2: 0.583244\tvalid_1's r2: -0.267326\n",
      "[210]\ttraining's r2: 0.584583\tvalid_1's r2: -0.266195\n",
      "[211]\ttraining's r2: 0.58589\tvalid_1's r2: -0.264978\n",
      "[212]\ttraining's r2: 0.587264\tvalid_1's r2: -0.264111\n",
      "[213]\ttraining's r2: 0.588316\tvalid_1's r2: -0.263939\n",
      "[214]\ttraining's r2: 0.589548\tvalid_1's r2: -0.263369\n",
      "[215]\ttraining's r2: 0.590823\tvalid_1's r2: -0.262732\n",
      "[216]\ttraining's r2: 0.592017\tvalid_1's r2: -0.262642\n",
      "[217]\ttraining's r2: 0.593254\tvalid_1's r2: -0.26247\n",
      "[218]\ttraining's r2: 0.594528\tvalid_1's r2: -0.261901\n",
      "[219]\ttraining's r2: 0.595829\tvalid_1's r2: -0.261468\n",
      "[220]\ttraining's r2: 0.596893\tvalid_1's r2: -0.261595\n",
      "[221]\ttraining's r2: 0.598163\tvalid_1's r2: -0.260155\n",
      "[222]\ttraining's r2: 0.599413\tvalid_1's r2: -0.258417\n",
      "[223]\ttraining's r2: 0.600841\tvalid_1's r2: -0.257334\n",
      "[224]\ttraining's r2: 0.601842\tvalid_1's r2: -0.25673\n",
      "[225]\ttraining's r2: 0.603045\tvalid_1's r2: -0.256238\n",
      "[226]\ttraining's r2: 0.604095\tvalid_1's r2: -0.256031\n",
      "[227]\ttraining's r2: 0.605118\tvalid_1's r2: -0.256181\n",
      "[228]\ttraining's r2: 0.606532\tvalid_1's r2: -0.25481\n",
      "[229]\ttraining's r2: 0.607911\tvalid_1's r2: -0.253916\n",
      "[230]\ttraining's r2: 0.609056\tvalid_1's r2: -0.253094\n",
      "[231]\ttraining's r2: 0.610163\tvalid_1's r2: -0.252563\n",
      "[232]\ttraining's r2: 0.611378\tvalid_1's r2: -0.251528\n",
      "[233]\ttraining's r2: 0.612621\tvalid_1's r2: -0.250862\n",
      "[234]\ttraining's r2: 0.613679\tvalid_1's r2: -0.250307\n",
      "[235]\ttraining's r2: 0.61478\tvalid_1's r2: -0.248935\n",
      "[236]\ttraining's r2: 0.615896\tvalid_1's r2: -0.247873\n",
      "[237]\ttraining's r2: 0.617139\tvalid_1's r2: -0.246969\n",
      "[238]\ttraining's r2: 0.618141\tvalid_1's r2: -0.246311\n",
      "[239]\ttraining's r2: 0.619152\tvalid_1's r2: -0.2459\n",
      "[240]\ttraining's r2: 0.620428\tvalid_1's r2: -0.245229\n",
      "[241]\ttraining's r2: 0.621291\tvalid_1's r2: -0.245558\n",
      "[242]\ttraining's r2: 0.622606\tvalid_1's r2: -0.244847\n",
      "[243]\ttraining's r2: 0.623845\tvalid_1's r2: -0.244351\n",
      "[244]\ttraining's r2: 0.624994\tvalid_1's r2: -0.243885\n",
      "[245]\ttraining's r2: 0.626054\tvalid_1's r2: -0.243247\n",
      "[246]\ttraining's r2: 0.627041\tvalid_1's r2: -0.242433\n",
      "[247]\ttraining's r2: 0.628361\tvalid_1's r2: -0.241458\n",
      "[248]\ttraining's r2: 0.629377\tvalid_1's r2: -0.240252\n",
      "[249]\ttraining's r2: 0.630253\tvalid_1's r2: -0.240201\n",
      "[250]\ttraining's r2: 0.631394\tvalid_1's r2: -0.239942\n",
      "[251]\ttraining's r2: 0.632449\tvalid_1's r2: -0.239962\n",
      "[252]\ttraining's r2: 0.633412\tvalid_1's r2: -0.239749\n",
      "[253]\ttraining's r2: 0.634382\tvalid_1's r2: -0.239572\n",
      "[254]\ttraining's r2: 0.635303\tvalid_1's r2: -0.239241\n",
      "[255]\ttraining's r2: 0.636225\tvalid_1's r2: -0.238776\n",
      "[256]\ttraining's r2: 0.637457\tvalid_1's r2: -0.237764\n",
      "[257]\ttraining's r2: 0.638591\tvalid_1's r2: -0.236574\n",
      "[258]\ttraining's r2: 0.63968\tvalid_1's r2: -0.235692\n",
      "[259]\ttraining's r2: 0.640668\tvalid_1's r2: -0.234306\n",
      "[260]\ttraining's r2: 0.641757\tvalid_1's r2: -0.233531\n",
      "[261]\ttraining's r2: 0.642919\tvalid_1's r2: -0.232739\n",
      "[262]\ttraining's r2: 0.643793\tvalid_1's r2: -0.233013\n",
      "[263]\ttraining's r2: 0.644912\tvalid_1's r2: -0.232266\n",
      "[264]\ttraining's r2: 0.645987\tvalid_1's r2: -0.231297\n",
      "[265]\ttraining's r2: 0.647056\tvalid_1's r2: -0.23023\n",
      "[266]\ttraining's r2: 0.647877\tvalid_1's r2: -0.230269\n",
      "[267]\ttraining's r2: 0.648937\tvalid_1's r2: -0.229579\n",
      "[268]\ttraining's r2: 0.649805\tvalid_1's r2: -0.229705\n",
      "[269]\ttraining's r2: 0.65075\tvalid_1's r2: -0.22984\n",
      "[270]\ttraining's r2: 0.651643\tvalid_1's r2: -0.229483\n",
      "[271]\ttraining's r2: 0.652503\tvalid_1's r2: -0.229308\n",
      "[272]\ttraining's r2: 0.653541\tvalid_1's r2: -0.228179\n",
      "[273]\ttraining's r2: 0.654591\tvalid_1's r2: -0.227564\n",
      "[274]\ttraining's r2: 0.655494\tvalid_1's r2: -0.227068\n",
      "[275]\ttraining's r2: 0.656416\tvalid_1's r2: -0.226384\n",
      "[276]\ttraining's r2: 0.657303\tvalid_1's r2: -0.225561\n",
      "[277]\ttraining's r2: 0.658164\tvalid_1's r2: -0.225986\n",
      "[278]\ttraining's r2: 0.659421\tvalid_1's r2: -0.225127\n",
      "[279]\ttraining's r2: 0.660511\tvalid_1's r2: -0.224486\n",
      "[280]\ttraining's r2: 0.661378\tvalid_1's r2: -0.223794\n",
      "[281]\ttraining's r2: 0.662497\tvalid_1's r2: -0.223427\n",
      "[282]\ttraining's r2: 0.663402\tvalid_1's r2: -0.223652\n",
      "[283]\ttraining's r2: 0.664292\tvalid_1's r2: -0.223132\n",
      "[284]\ttraining's r2: 0.665391\tvalid_1's r2: -0.222743\n",
      "[285]\ttraining's r2: 0.666313\tvalid_1's r2: -0.222159\n",
      "[286]\ttraining's r2: 0.667236\tvalid_1's r2: -0.221792\n",
      "[287]\ttraining's r2: 0.668156\tvalid_1's r2: -0.221097\n",
      "[288]\ttraining's r2: 0.669227\tvalid_1's r2: -0.220346\n",
      "[289]\ttraining's r2: 0.67\tvalid_1's r2: -0.219911\n",
      "[290]\ttraining's r2: 0.670885\tvalid_1's r2: -0.219102\n",
      "[291]\ttraining's r2: 0.671765\tvalid_1's r2: -0.218097\n",
      "[292]\ttraining's r2: 0.67264\tvalid_1's r2: -0.217711\n",
      "[293]\ttraining's r2: 0.67339\tvalid_1's r2: -0.217547\n",
      "[294]\ttraining's r2: 0.674343\tvalid_1's r2: -0.21668\n",
      "[295]\ttraining's r2: 0.675317\tvalid_1's r2: -0.216273\n",
      "[296]\ttraining's r2: 0.676108\tvalid_1's r2: -0.21604\n",
      "[297]\ttraining's r2: 0.676855\tvalid_1's r2: -0.215873\n",
      "[298]\ttraining's r2: 0.677538\tvalid_1's r2: -0.21625\n",
      "[299]\ttraining's r2: 0.678493\tvalid_1's r2: -0.216303\n",
      "[300]\ttraining's r2: 0.679343\tvalid_1's r2: -0.215615\n",
      "[301]\ttraining's r2: 0.680147\tvalid_1's r2: -0.215734\n",
      "[302]\ttraining's r2: 0.681082\tvalid_1's r2: -0.215152\n",
      "[303]\ttraining's r2: 0.681989\tvalid_1's r2: -0.215176\n",
      "[304]\ttraining's r2: 0.682894\tvalid_1's r2: -0.215033\n",
      "[305]\ttraining's r2: 0.683698\tvalid_1's r2: -0.214594\n",
      "[306]\ttraining's r2: 0.684357\tvalid_1's r2: -0.214498\n",
      "[307]\ttraining's r2: 0.685315\tvalid_1's r2: -0.214385\n",
      "[308]\ttraining's r2: 0.68595\tvalid_1's r2: -0.214684\n",
      "[309]\ttraining's r2: 0.686786\tvalid_1's r2: -0.214136\n",
      "[310]\ttraining's r2: 0.687556\tvalid_1's r2: -0.214087\n",
      "[311]\ttraining's r2: 0.688246\tvalid_1's r2: -0.214197\n",
      "[312]\ttraining's r2: 0.688995\tvalid_1's r2: -0.213704\n",
      "[313]\ttraining's r2: 0.689936\tvalid_1's r2: -0.21329\n",
      "[314]\ttraining's r2: 0.6907\tvalid_1's r2: -0.212514\n",
      "[315]\ttraining's r2: 0.691547\tvalid_1's r2: -0.211824\n",
      "[316]\ttraining's r2: 0.692454\tvalid_1's r2: -0.211198\n",
      "[317]\ttraining's r2: 0.693206\tvalid_1's r2: -0.210628\n",
      "[318]\ttraining's r2: 0.693923\tvalid_1's r2: -0.210598\n",
      "[319]\ttraining's r2: 0.694875\tvalid_1's r2: -0.210296\n",
      "[320]\ttraining's r2: 0.695783\tvalid_1's r2: -0.209973\n",
      "[321]\ttraining's r2: 0.69655\tvalid_1's r2: -0.209771\n",
      "[322]\ttraining's r2: 0.697354\tvalid_1's r2: -0.20935\n",
      "[323]\ttraining's r2: 0.698138\tvalid_1's r2: -0.209322\n",
      "[324]\ttraining's r2: 0.699065\tvalid_1's r2: -0.208708\n",
      "[325]\ttraining's r2: 0.699716\tvalid_1's r2: -0.208899\n",
      "[326]\ttraining's r2: 0.700547\tvalid_1's r2: -0.208683\n",
      "[327]\ttraining's r2: 0.70127\tvalid_1's r2: -0.208215\n",
      "[328]\ttraining's r2: 0.702133\tvalid_1's r2: -0.207584\n",
      "[329]\ttraining's r2: 0.70266\tvalid_1's r2: -0.207596\n",
      "[330]\ttraining's r2: 0.703381\tvalid_1's r2: -0.207089\n",
      "[331]\ttraining's r2: 0.70407\tvalid_1's r2: -0.207304\n",
      "[332]\ttraining's r2: 0.7047\tvalid_1's r2: -0.207283\n",
      "[333]\ttraining's r2: 0.705509\tvalid_1's r2: -0.207215\n",
      "[334]\ttraining's r2: 0.7063\tvalid_1's r2: -0.207247\n",
      "[335]\ttraining's r2: 0.707241\tvalid_1's r2: -0.206739\n",
      "[336]\ttraining's r2: 0.70797\tvalid_1's r2: -0.206649\n",
      "[337]\ttraining's r2: 0.708618\tvalid_1's r2: -0.206233\n",
      "[338]\ttraining's r2: 0.709366\tvalid_1's r2: -0.206525\n",
      "[339]\ttraining's r2: 0.710047\tvalid_1's r2: -0.206178\n",
      "[340]\ttraining's r2: 0.710832\tvalid_1's r2: -0.205648\n",
      "[341]\ttraining's r2: 0.711478\tvalid_1's r2: -0.205581\n",
      "[342]\ttraining's r2: 0.712267\tvalid_1's r2: -0.206024\n",
      "[343]\ttraining's r2: 0.712891\tvalid_1's r2: -0.205797\n",
      "[344]\ttraining's r2: 0.71351\tvalid_1's r2: -0.205951\n",
      "[345]\ttraining's r2: 0.71428\tvalid_1's r2: -0.206183\n",
      "[346]\ttraining's r2: 0.714979\tvalid_1's r2: -0.206724\n",
      "[347]\ttraining's r2: 0.715689\tvalid_1's r2: -0.20666\n",
      "[348]\ttraining's r2: 0.716269\tvalid_1's r2: -0.206778\n",
      "[349]\ttraining's r2: 0.7169\tvalid_1's r2: -0.206834\n",
      "[350]\ttraining's r2: 0.717597\tvalid_1's r2: -0.206168\n",
      "[351]\ttraining's r2: 0.718294\tvalid_1's r2: -0.205786\n",
      "[352]\ttraining's r2: 0.718949\tvalid_1's r2: -0.205173\n",
      "[353]\ttraining's r2: 0.719503\tvalid_1's r2: -0.20515\n",
      "[354]\ttraining's r2: 0.720248\tvalid_1's r2: -0.205353\n",
      "[355]\ttraining's r2: 0.720872\tvalid_1's r2: -0.205084\n",
      "[356]\ttraining's r2: 0.721495\tvalid_1's r2: -0.205058\n",
      "[357]\ttraining's r2: 0.722046\tvalid_1's r2: -0.205392\n",
      "[358]\ttraining's r2: 0.722629\tvalid_1's r2: -0.205417\n",
      "[359]\ttraining's r2: 0.72322\tvalid_1's r2: -0.205291\n",
      "[360]\ttraining's r2: 0.723895\tvalid_1's r2: -0.205021\n",
      "[361]\ttraining's r2: 0.724589\tvalid_1's r2: -0.204546\n",
      "[362]\ttraining's r2: 0.725225\tvalid_1's r2: -0.20463\n",
      "[363]\ttraining's r2: 0.725712\tvalid_1's r2: -0.204509\n",
      "[364]\ttraining's r2: 0.72645\tvalid_1's r2: -0.20431\n",
      "[365]\ttraining's r2: 0.727055\tvalid_1's r2: -0.204245\n",
      "[366]\ttraining's r2: 0.727714\tvalid_1's r2: -0.204111\n",
      "[367]\ttraining's r2: 0.72823\tvalid_1's r2: -0.204391\n",
      "[368]\ttraining's r2: 0.728861\tvalid_1's r2: -0.204254\n",
      "[369]\ttraining's r2: 0.729404\tvalid_1's r2: -0.204066\n",
      "[370]\ttraining's r2: 0.730117\tvalid_1's r2: -0.203974\n",
      "[371]\ttraining's r2: 0.730645\tvalid_1's r2: -0.203945\n",
      "[372]\ttraining's r2: 0.731296\tvalid_1's r2: -0.203577\n",
      "[373]\ttraining's r2: 0.731939\tvalid_1's r2: -0.203052\n",
      "[374]\ttraining's r2: 0.732562\tvalid_1's r2: -0.202824\n",
      "[375]\ttraining's r2: 0.733317\tvalid_1's r2: -0.202534\n",
      "[376]\ttraining's r2: 0.733883\tvalid_1's r2: -0.202729\n",
      "[377]\ttraining's r2: 0.734515\tvalid_1's r2: -0.202115\n",
      "[378]\ttraining's r2: 0.735084\tvalid_1's r2: -0.201984\n",
      "[379]\ttraining's r2: 0.735684\tvalid_1's r2: -0.2013\n",
      "[380]\ttraining's r2: 0.736221\tvalid_1's r2: -0.201203\n",
      "[381]\ttraining's r2: 0.736771\tvalid_1's r2: -0.201127\n",
      "[382]\ttraining's r2: 0.737491\tvalid_1's r2: -0.200742\n",
      "[383]\ttraining's r2: 0.738079\tvalid_1's r2: -0.200444\n",
      "[384]\ttraining's r2: 0.738748\tvalid_1's r2: -0.200404\n",
      "[385]\ttraining's r2: 0.739428\tvalid_1's r2: -0.199246\n",
      "[386]\ttraining's r2: 0.740121\tvalid_1's r2: -0.198915\n",
      "[387]\ttraining's r2: 0.740807\tvalid_1's r2: -0.198735\n",
      "[388]\ttraining's r2: 0.741478\tvalid_1's r2: -0.198736\n",
      "[389]\ttraining's r2: 0.741992\tvalid_1's r2: -0.199235\n",
      "[390]\ttraining's r2: 0.742506\tvalid_1's r2: -0.198795\n",
      "[391]\ttraining's r2: 0.743113\tvalid_1's r2: -0.198574\n",
      "[392]\ttraining's r2: 0.743557\tvalid_1's r2: -0.19902\n",
      "[393]\ttraining's r2: 0.744085\tvalid_1's r2: -0.198643\n",
      "[394]\ttraining's r2: 0.7447\tvalid_1's r2: -0.198528\n",
      "[395]\ttraining's r2: 0.745384\tvalid_1's r2: -0.197962\n",
      "[396]\ttraining's r2: 0.746029\tvalid_1's r2: -0.197887\n",
      "[397]\ttraining's r2: 0.74668\tvalid_1's r2: -0.198295\n",
      "[398]\ttraining's r2: 0.747257\tvalid_1's r2: -0.197786\n",
      "[399]\ttraining's r2: 0.74778\tvalid_1's r2: -0.197809\n",
      "[400]\ttraining's r2: 0.748302\tvalid_1's r2: -0.197836\n",
      "[401]\ttraining's r2: 0.748893\tvalid_1's r2: -0.197579\n",
      "[402]\ttraining's r2: 0.749452\tvalid_1's r2: -0.196906\n",
      "[403]\ttraining's r2: 0.75015\tvalid_1's r2: -0.196205\n",
      "[404]\ttraining's r2: 0.750716\tvalid_1's r2: -0.195978\n",
      "[405]\ttraining's r2: 0.751244\tvalid_1's r2: -0.195951\n",
      "[406]\ttraining's r2: 0.751873\tvalid_1's r2: -0.195808\n",
      "[407]\ttraining's r2: 0.752496\tvalid_1's r2: -0.195421\n",
      "[408]\ttraining's r2: 0.753204\tvalid_1's r2: -0.194576\n",
      "[409]\ttraining's r2: 0.753761\tvalid_1's r2: -0.194113\n",
      "[410]\ttraining's r2: 0.754534\tvalid_1's r2: -0.193548\n",
      "[411]\ttraining's r2: 0.755005\tvalid_1's r2: -0.193606\n",
      "[412]\ttraining's r2: 0.755579\tvalid_1's r2: -0.193201\n",
      "[413]\ttraining's r2: 0.756198\tvalid_1's r2: -0.19313\n",
      "[414]\ttraining's r2: 0.756811\tvalid_1's r2: -0.193131\n",
      "[415]\ttraining's r2: 0.757317\tvalid_1's r2: -0.193054\n",
      "[416]\ttraining's r2: 0.757823\tvalid_1's r2: -0.192763\n",
      "[417]\ttraining's r2: 0.758375\tvalid_1's r2: -0.191977\n",
      "[418]\ttraining's r2: 0.759042\tvalid_1's r2: -0.191542\n",
      "[419]\ttraining's r2: 0.759625\tvalid_1's r2: -0.191769\n",
      "[420]\ttraining's r2: 0.760075\tvalid_1's r2: -0.191492\n",
      "[421]\ttraining's r2: 0.760694\tvalid_1's r2: -0.190564\n",
      "[422]\ttraining's r2: 0.761258\tvalid_1's r2: -0.190254\n",
      "[423]\ttraining's r2: 0.761772\tvalid_1's r2: -0.190105\n",
      "[424]\ttraining's r2: 0.762317\tvalid_1's r2: -0.190127\n",
      "[425]\ttraining's r2: 0.762777\tvalid_1's r2: -0.189698\n",
      "[426]\ttraining's r2: 0.76326\tvalid_1's r2: -0.190055\n",
      "[427]\ttraining's r2: 0.763744\tvalid_1's r2: -0.190129\n",
      "[428]\ttraining's r2: 0.764235\tvalid_1's r2: -0.190136\n",
      "[429]\ttraining's r2: 0.764874\tvalid_1's r2: -0.189838\n",
      "[430]\ttraining's r2: 0.765506\tvalid_1's r2: -0.189972\n",
      "[431]\ttraining's r2: 0.765936\tvalid_1's r2: -0.189639\n",
      "[432]\ttraining's r2: 0.766468\tvalid_1's r2: -0.189051\n",
      "[433]\ttraining's r2: 0.767082\tvalid_1's r2: -0.188906\n",
      "[434]\ttraining's r2: 0.767636\tvalid_1's r2: -0.189284\n",
      "[435]\ttraining's r2: 0.768232\tvalid_1's r2: -0.188266\n",
      "[436]\ttraining's r2: 0.76876\tvalid_1's r2: -0.187714\n",
      "[437]\ttraining's r2: 0.769311\tvalid_1's r2: -0.187416\n",
      "[438]\ttraining's r2: 0.769831\tvalid_1's r2: -0.186964\n",
      "[439]\ttraining's r2: 0.770305\tvalid_1's r2: -0.186675\n",
      "[440]\ttraining's r2: 0.7708\tvalid_1's r2: -0.186321\n",
      "[441]\ttraining's r2: 0.771301\tvalid_1's r2: -0.185831\n",
      "[442]\ttraining's r2: 0.771859\tvalid_1's r2: -0.18591\n",
      "[443]\ttraining's r2: 0.772317\tvalid_1's r2: -0.185871\n",
      "[444]\ttraining's r2: 0.772809\tvalid_1's r2: -0.185733\n",
      "[445]\ttraining's r2: 0.773361\tvalid_1's r2: -0.185397\n",
      "[446]\ttraining's r2: 0.773746\tvalid_1's r2: -0.185908\n",
      "[447]\ttraining's r2: 0.774132\tvalid_1's r2: -0.185964\n",
      "[448]\ttraining's r2: 0.774664\tvalid_1's r2: -0.185487\n",
      "[449]\ttraining's r2: 0.775165\tvalid_1's r2: -0.185278\n",
      "[450]\ttraining's r2: 0.775678\tvalid_1's r2: -0.185199\n",
      "[451]\ttraining's r2: 0.776173\tvalid_1's r2: -0.184979\n",
      "[452]\ttraining's r2: 0.776632\tvalid_1's r2: -0.184975\n",
      "[453]\ttraining's r2: 0.777261\tvalid_1's r2: -0.184484\n",
      "[454]\ttraining's r2: 0.777862\tvalid_1's r2: -0.183652\n",
      "[455]\ttraining's r2: 0.778292\tvalid_1's r2: -0.183887\n",
      "[456]\ttraining's r2: 0.778791\tvalid_1's r2: -0.184156\n",
      "[457]\ttraining's r2: 0.779229\tvalid_1's r2: -0.184271\n",
      "[458]\ttraining's r2: 0.77968\tvalid_1's r2: -0.184922\n",
      "[459]\ttraining's r2: 0.78007\tvalid_1's r2: -0.185108\n",
      "[460]\ttraining's r2: 0.780641\tvalid_1's r2: -0.185234\n",
      "[461]\ttraining's r2: 0.781148\tvalid_1's r2: -0.185222\n",
      "[462]\ttraining's r2: 0.78166\tvalid_1's r2: -0.185149\n",
      "[463]\ttraining's r2: 0.782111\tvalid_1's r2: -0.184769\n",
      "[464]\ttraining's r2: 0.782607\tvalid_1's r2: -0.184386\n",
      "[465]\ttraining's r2: 0.783159\tvalid_1's r2: -0.184258\n",
      "[466]\ttraining's r2: 0.783673\tvalid_1's r2: -0.183481\n",
      "[467]\ttraining's r2: 0.784215\tvalid_1's r2: -0.182704\n",
      "[468]\ttraining's r2: 0.78466\tvalid_1's r2: -0.183144\n",
      "[469]\ttraining's r2: 0.785137\tvalid_1's r2: -0.182742\n",
      "[470]\ttraining's r2: 0.785592\tvalid_1's r2: -0.183176\n",
      "[471]\ttraining's r2: 0.786029\tvalid_1's r2: -0.182976\n",
      "[472]\ttraining's r2: 0.786586\tvalid_1's r2: -0.182473\n",
      "[473]\ttraining's r2: 0.787062\tvalid_1's r2: -0.182153\n",
      "[474]\ttraining's r2: 0.787574\tvalid_1's r2: -0.181634\n",
      "[475]\ttraining's r2: 0.788071\tvalid_1's r2: -0.181326\n",
      "[476]\ttraining's r2: 0.788533\tvalid_1's r2: -0.181411\n",
      "[477]\ttraining's r2: 0.789037\tvalid_1's r2: -0.180453\n",
      "[478]\ttraining's r2: 0.789434\tvalid_1's r2: -0.180209\n",
      "[479]\ttraining's r2: 0.78977\tvalid_1's r2: -0.180401\n",
      "[480]\ttraining's r2: 0.790325\tvalid_1's r2: -0.179975\n",
      "[481]\ttraining's r2: 0.79076\tvalid_1's r2: -0.179856\n",
      "[482]\ttraining's r2: 0.791188\tvalid_1's r2: -0.179498\n",
      "[483]\ttraining's r2: 0.791533\tvalid_1's r2: -0.17995\n",
      "[484]\ttraining's r2: 0.791975\tvalid_1's r2: -0.180323\n",
      "[485]\ttraining's r2: 0.792407\tvalid_1's r2: -0.180427\n",
      "[486]\ttraining's r2: 0.792857\tvalid_1's r2: -0.179849\n",
      "[487]\ttraining's r2: 0.793273\tvalid_1's r2: -0.179613\n",
      "[488]\ttraining's r2: 0.793671\tvalid_1's r2: -0.179367\n",
      "[489]\ttraining's r2: 0.794125\tvalid_1's r2: -0.179092\n",
      "[490]\ttraining's r2: 0.794514\tvalid_1's r2: -0.179\n",
      "[491]\ttraining's r2: 0.794887\tvalid_1's r2: -0.179149\n",
      "[492]\ttraining's r2: 0.795412\tvalid_1's r2: -0.179185\n",
      "[493]\ttraining's r2: 0.79583\tvalid_1's r2: -0.179578\n",
      "[494]\ttraining's r2: 0.79631\tvalid_1's r2: -0.179496\n",
      "[495]\ttraining's r2: 0.796831\tvalid_1's r2: -0.179243\n",
      "[496]\ttraining's r2: 0.797188\tvalid_1's r2: -0.179445\n",
      "[497]\ttraining's r2: 0.797596\tvalid_1's r2: -0.179769\n",
      "[498]\ttraining's r2: 0.797988\tvalid_1's r2: -0.179405\n",
      "[499]\ttraining's r2: 0.798448\tvalid_1's r2: -0.178493\n",
      "[500]\ttraining's r2: 0.798935\tvalid_1's r2: -0.177805\n",
      "[501]\ttraining's r2: 0.799365\tvalid_1's r2: -0.177603\n",
      "[502]\ttraining's r2: 0.799832\tvalid_1's r2: -0.178071\n",
      "[503]\ttraining's r2: 0.800234\tvalid_1's r2: -0.177972\n",
      "[504]\ttraining's r2: 0.800589\tvalid_1's r2: -0.177775\n",
      "[505]\ttraining's r2: 0.800972\tvalid_1's r2: -0.177625\n",
      "[506]\ttraining's r2: 0.801367\tvalid_1's r2: -0.177807\n",
      "[507]\ttraining's r2: 0.801825\tvalid_1's r2: -0.177064\n",
      "[508]\ttraining's r2: 0.802198\tvalid_1's r2: -0.176814\n",
      "[509]\ttraining's r2: 0.802647\tvalid_1's r2: -0.1765\n",
      "[510]\ttraining's r2: 0.803071\tvalid_1's r2: -0.176124\n",
      "[511]\ttraining's r2: 0.803415\tvalid_1's r2: -0.175906\n",
      "[512]\ttraining's r2: 0.803879\tvalid_1's r2: -0.175734\n",
      "[513]\ttraining's r2: 0.804363\tvalid_1's r2: -0.175333\n",
      "[514]\ttraining's r2: 0.804749\tvalid_1's r2: -0.175228\n",
      "[515]\ttraining's r2: 0.805169\tvalid_1's r2: -0.175152\n",
      "[516]\ttraining's r2: 0.805596\tvalid_1's r2: -0.174712\n",
      "[517]\ttraining's r2: 0.806048\tvalid_1's r2: -0.174608\n",
      "[518]\ttraining's r2: 0.806491\tvalid_1's r2: -0.175019\n",
      "[519]\ttraining's r2: 0.806944\tvalid_1's r2: -0.174855\n",
      "[520]\ttraining's r2: 0.807384\tvalid_1's r2: -0.174823\n",
      "[521]\ttraining's r2: 0.807775\tvalid_1's r2: -0.17468\n",
      "[522]\ttraining's r2: 0.80809\tvalid_1's r2: -0.175066\n",
      "[523]\ttraining's r2: 0.808425\tvalid_1's r2: -0.175147\n",
      "[524]\ttraining's r2: 0.808787\tvalid_1's r2: -0.175225\n",
      "[525]\ttraining's r2: 0.809175\tvalid_1's r2: -0.175444\n",
      "[526]\ttraining's r2: 0.809506\tvalid_1's r2: -0.175352\n",
      "[527]\ttraining's r2: 0.80992\tvalid_1's r2: -0.17518\n",
      "[528]\ttraining's r2: 0.810387\tvalid_1's r2: -0.175594\n",
      "[529]\ttraining's r2: 0.810758\tvalid_1's r2: -0.175713\n",
      "[530]\ttraining's r2: 0.811218\tvalid_1's r2: -0.175511\n",
      "[531]\ttraining's r2: 0.811706\tvalid_1's r2: -0.175659\n",
      "[532]\ttraining's r2: 0.812095\tvalid_1's r2: -0.175432\n",
      "[533]\ttraining's r2: 0.812519\tvalid_1's r2: -0.175766\n",
      "[534]\ttraining's r2: 0.812955\tvalid_1's r2: -0.175155\n",
      "[535]\ttraining's r2: 0.813277\tvalid_1's r2: -0.175977\n",
      "[536]\ttraining's r2: 0.813698\tvalid_1's r2: -0.175332\n",
      "[537]\ttraining's r2: 0.814095\tvalid_1's r2: -0.175019\n",
      "[538]\ttraining's r2: 0.814444\tvalid_1's r2: -0.175587\n",
      "[539]\ttraining's r2: 0.814831\tvalid_1's r2: -0.175866\n",
      "[540]\ttraining's r2: 0.815193\tvalid_1's r2: -0.176358\n",
      "[541]\ttraining's r2: 0.815558\tvalid_1's r2: -0.176444\n",
      "[542]\ttraining's r2: 0.815913\tvalid_1's r2: -0.176244\n",
      "[543]\ttraining's r2: 0.816256\tvalid_1's r2: -0.176173\n",
      "[544]\ttraining's r2: 0.816622\tvalid_1's r2: -0.176099\n",
      "[545]\ttraining's r2: 0.816997\tvalid_1's r2: -0.175595\n",
      "[546]\ttraining's r2: 0.817332\tvalid_1's r2: -0.175353\n",
      "[547]\ttraining's r2: 0.817691\tvalid_1's r2: -0.17533\n",
      "[548]\ttraining's r2: 0.81809\tvalid_1's r2: -0.174887\n",
      "[549]\ttraining's r2: 0.818526\tvalid_1's r2: -0.174791\n",
      "[550]\ttraining's r2: 0.818823\tvalid_1's r2: -0.175164\n",
      "[551]\ttraining's r2: 0.819139\tvalid_1's r2: -0.175112\n",
      "[552]\ttraining's r2: 0.819573\tvalid_1's r2: -0.174975\n",
      "[553]\ttraining's r2: 0.819976\tvalid_1's r2: -0.174999\n",
      "[554]\ttraining's r2: 0.820373\tvalid_1's r2: -0.175092\n",
      "[555]\ttraining's r2: 0.820745\tvalid_1's r2: -0.174795\n",
      "[556]\ttraining's r2: 0.821\tvalid_1's r2: -0.174953\n",
      "[557]\ttraining's r2: 0.821353\tvalid_1's r2: -0.174815\n",
      "[558]\ttraining's r2: 0.821692\tvalid_1's r2: -0.175463\n",
      "[559]\ttraining's r2: 0.822056\tvalid_1's r2: -0.17529\n",
      "[560]\ttraining's r2: 0.822458\tvalid_1's r2: -0.175167\n",
      "[561]\ttraining's r2: 0.822782\tvalid_1's r2: -0.175273\n",
      "[562]\ttraining's r2: 0.823204\tvalid_1's r2: -0.175048\n",
      "[563]\ttraining's r2: 0.823646\tvalid_1's r2: -0.174859\n",
      "[564]\ttraining's r2: 0.824005\tvalid_1's r2: -0.174896\n",
      "[565]\ttraining's r2: 0.824278\tvalid_1's r2: -0.174809\n",
      "[566]\ttraining's r2: 0.824595\tvalid_1's r2: -0.174888\n",
      "[567]\ttraining's r2: 0.824982\tvalid_1's r2: -0.174681\n",
      "[568]\ttraining's r2: 0.82533\tvalid_1's r2: -0.174503\n",
      "[569]\ttraining's r2: 0.825748\tvalid_1's r2: -0.174386\n",
      "[570]\ttraining's r2: 0.826091\tvalid_1's r2: -0.174018\n",
      "[571]\ttraining's r2: 0.826442\tvalid_1's r2: -0.173821\n",
      "[572]\ttraining's r2: 0.826744\tvalid_1's r2: -0.173766\n",
      "[573]\ttraining's r2: 0.827092\tvalid_1's r2: -0.173945\n",
      "[574]\ttraining's r2: 0.827427\tvalid_1's r2: -0.174064\n",
      "[575]\ttraining's r2: 0.827775\tvalid_1's r2: -0.173873\n",
      "[576]\ttraining's r2: 0.828076\tvalid_1's r2: -0.173706\n",
      "[577]\ttraining's r2: 0.8284\tvalid_1's r2: -0.17362\n",
      "[578]\ttraining's r2: 0.828653\tvalid_1's r2: -0.173774\n",
      "[579]\ttraining's r2: 0.829\tvalid_1's r2: -0.173607\n",
      "[580]\ttraining's r2: 0.829294\tvalid_1's r2: -0.173447\n",
      "[581]\ttraining's r2: 0.829662\tvalid_1's r2: -0.172989\n",
      "[582]\ttraining's r2: 0.830025\tvalid_1's r2: -0.172601\n",
      "[583]\ttraining's r2: 0.83039\tvalid_1's r2: -0.172404\n",
      "[584]\ttraining's r2: 0.830722\tvalid_1's r2: -0.171863\n",
      "[585]\ttraining's r2: 0.831055\tvalid_1's r2: -0.171752\n",
      "[586]\ttraining's r2: 0.831392\tvalid_1's r2: -0.171509\n",
      "[587]\ttraining's r2: 0.831652\tvalid_1's r2: -0.171588\n",
      "[588]\ttraining's r2: 0.831938\tvalid_1's r2: -0.171045\n",
      "[589]\ttraining's r2: 0.832233\tvalid_1's r2: -0.171253\n",
      "[590]\ttraining's r2: 0.832517\tvalid_1's r2: -0.1714\n",
      "[591]\ttraining's r2: 0.832804\tvalid_1's r2: -0.171046\n",
      "[592]\ttraining's r2: 0.833138\tvalid_1's r2: -0.170861\n",
      "[593]\ttraining's r2: 0.833543\tvalid_1's r2: -0.170686\n",
      "[594]\ttraining's r2: 0.833902\tvalid_1's r2: -0.170549\n",
      "[595]\ttraining's r2: 0.834239\tvalid_1's r2: -0.17013\n",
      "[596]\ttraining's r2: 0.834584\tvalid_1's r2: -0.170276\n",
      "[597]\ttraining's r2: 0.834908\tvalid_1's r2: -0.170231\n",
      "[598]\ttraining's r2: 0.835202\tvalid_1's r2: -0.170314\n",
      "[599]\ttraining's r2: 0.835528\tvalid_1's r2: -0.170226\n",
      "[600]\ttraining's r2: 0.835862\tvalid_1's r2: -0.169755\n",
      "[601]\ttraining's r2: 0.836158\tvalid_1's r2: -0.169503\n",
      "[602]\ttraining's r2: 0.836457\tvalid_1's r2: -0.169352\n",
      "[603]\ttraining's r2: 0.836741\tvalid_1's r2: -0.169308\n",
      "[604]\ttraining's r2: 0.837103\tvalid_1's r2: -0.169304\n",
      "[605]\ttraining's r2: 0.837377\tvalid_1's r2: -0.169408\n",
      "[606]\ttraining's r2: 0.837666\tvalid_1's r2: -0.169044\n",
      "[607]\ttraining's r2: 0.837976\tvalid_1's r2: -0.16894\n",
      "[608]\ttraining's r2: 0.838358\tvalid_1's r2: -0.168704\n",
      "[609]\ttraining's r2: 0.838719\tvalid_1's r2: -0.16825\n",
      "[610]\ttraining's r2: 0.838967\tvalid_1's r2: -0.168306\n",
      "[611]\ttraining's r2: 0.839276\tvalid_1's r2: -0.168334\n",
      "[612]\ttraining's r2: 0.839561\tvalid_1's r2: -0.168081\n",
      "[613]\ttraining's r2: 0.839863\tvalid_1's r2: -0.167895\n",
      "[614]\ttraining's r2: 0.840191\tvalid_1's r2: -0.167452\n",
      "[615]\ttraining's r2: 0.840471\tvalid_1's r2: -0.167428\n",
      "[616]\ttraining's r2: 0.840735\tvalid_1's r2: -0.167722\n",
      "[617]\ttraining's r2: 0.841069\tvalid_1's r2: -0.16744\n",
      "[618]\ttraining's r2: 0.841408\tvalid_1's r2: -0.167126\n",
      "[619]\ttraining's r2: 0.841755\tvalid_1's r2: -0.166605\n",
      "[620]\ttraining's r2: 0.842055\tvalid_1's r2: -0.166334\n",
      "[621]\ttraining's r2: 0.842341\tvalid_1's r2: -0.166089\n",
      "[622]\ttraining's r2: 0.842723\tvalid_1's r2: -0.165938\n",
      "[623]\ttraining's r2: 0.843031\tvalid_1's r2: -0.165412\n",
      "[624]\ttraining's r2: 0.843279\tvalid_1's r2: -0.165713\n",
      "[625]\ttraining's r2: 0.843604\tvalid_1's r2: -0.16566\n",
      "[626]\ttraining's r2: 0.843931\tvalid_1's r2: -0.16537\n",
      "[627]\ttraining's r2: 0.844287\tvalid_1's r2: -0.165266\n",
      "[628]\ttraining's r2: 0.844582\tvalid_1's r2: -0.165242\n",
      "[629]\ttraining's r2: 0.844867\tvalid_1's r2: -0.165052\n",
      "[630]\ttraining's r2: 0.845148\tvalid_1's r2: -0.165055\n",
      "[631]\ttraining's r2: 0.845448\tvalid_1's r2: -0.165064\n",
      "[632]\ttraining's r2: 0.845745\tvalid_1's r2: -0.165038\n",
      "[633]\ttraining's r2: 0.84607\tvalid_1's r2: -0.165094\n",
      "[634]\ttraining's r2: 0.846414\tvalid_1's r2: -0.164838\n",
      "[635]\ttraining's r2: 0.846745\tvalid_1's r2: -0.164322\n",
      "[636]\ttraining's r2: 0.846943\tvalid_1's r2: -0.164512\n",
      "[637]\ttraining's r2: 0.847225\tvalid_1's r2: -0.163987\n",
      "[638]\ttraining's r2: 0.84747\tvalid_1's r2: -0.16415\n",
      "[639]\ttraining's r2: 0.847796\tvalid_1's r2: -0.163874\n",
      "[640]\ttraining's r2: 0.848033\tvalid_1's r2: -0.164055\n",
      "[641]\ttraining's r2: 0.848297\tvalid_1's r2: -0.164291\n",
      "[642]\ttraining's r2: 0.8486\tvalid_1's r2: -0.164421\n",
      "[643]\ttraining's r2: 0.848918\tvalid_1's r2: -0.164573\n",
      "[644]\ttraining's r2: 0.849207\tvalid_1's r2: -0.164431\n",
      "[645]\ttraining's r2: 0.849543\tvalid_1's r2: -0.163994\n",
      "[646]\ttraining's r2: 0.849782\tvalid_1's r2: -0.164276\n",
      "[647]\ttraining's r2: 0.85004\tvalid_1's r2: -0.164274\n",
      "[648]\ttraining's r2: 0.850275\tvalid_1's r2: -0.164402\n",
      "[649]\ttraining's r2: 0.85059\tvalid_1's r2: -0.164436\n",
      "[650]\ttraining's r2: 0.850892\tvalid_1's r2: -0.164326\n",
      "[651]\ttraining's r2: 0.851203\tvalid_1's r2: -0.164299\n",
      "[652]\ttraining's r2: 0.851507\tvalid_1's r2: -0.163973\n",
      "[653]\ttraining's r2: 0.851759\tvalid_1's r2: -0.164145\n",
      "[654]\ttraining's r2: 0.851987\tvalid_1's r2: -0.164129\n",
      "[655]\ttraining's r2: 0.852268\tvalid_1's r2: -0.164036\n",
      "[656]\ttraining's r2: 0.85249\tvalid_1's r2: -0.163784\n",
      "[657]\ttraining's r2: 0.852743\tvalid_1's r2: -0.164121\n",
      "[658]\ttraining's r2: 0.85299\tvalid_1's r2: -0.164162\n",
      "[659]\ttraining's r2: 0.853278\tvalid_1's r2: -0.163745\n",
      "[660]\ttraining's r2: 0.85358\tvalid_1's r2: -0.163994\n",
      "[661]\ttraining's r2: 0.853841\tvalid_1's r2: -0.164154\n",
      "[662]\ttraining's r2: 0.854111\tvalid_1's r2: -0.164499\n",
      "[663]\ttraining's r2: 0.854387\tvalid_1's r2: -0.164122\n",
      "[664]\ttraining's r2: 0.854606\tvalid_1's r2: -0.164451\n",
      "[665]\ttraining's r2: 0.854909\tvalid_1's r2: -0.16458\n",
      "[666]\ttraining's r2: 0.855138\tvalid_1's r2: -0.164746\n",
      "[667]\ttraining's r2: 0.855364\tvalid_1's r2: -0.165132\n",
      "[668]\ttraining's r2: 0.85563\tvalid_1's r2: -0.165286\n",
      "[669]\ttraining's r2: 0.855928\tvalid_1's r2: -0.165301\n",
      "[670]\ttraining's r2: 0.85618\tvalid_1's r2: -0.16531\n",
      "[671]\ttraining's r2: 0.856495\tvalid_1's r2: -0.165546\n",
      "[672]\ttraining's r2: 0.856776\tvalid_1's r2: -0.16552\n",
      "[673]\ttraining's r2: 0.856995\tvalid_1's r2: -0.165562\n",
      "[674]\ttraining's r2: 0.857247\tvalid_1's r2: -0.165682\n",
      "[675]\ttraining's r2: 0.857519\tvalid_1's r2: -0.165316\n",
      "[676]\ttraining's r2: 0.857803\tvalid_1's r2: -0.165139\n",
      "[677]\ttraining's r2: 0.858049\tvalid_1's r2: -0.164907\n",
      "[678]\ttraining's r2: 0.858293\tvalid_1's r2: -0.164595\n",
      "[679]\ttraining's r2: 0.85858\tvalid_1's r2: -0.164349\n",
      "[680]\ttraining's r2: 0.858857\tvalid_1's r2: -0.164407\n",
      "[681]\ttraining's r2: 0.859032\tvalid_1's r2: -0.164732\n",
      "[682]\ttraining's r2: 0.859278\tvalid_1's r2: -0.164464\n",
      "[683]\ttraining's r2: 0.859479\tvalid_1's r2: -0.164873\n",
      "[684]\ttraining's r2: 0.85972\tvalid_1's r2: -0.164759\n",
      "[685]\ttraining's r2: 0.859927\tvalid_1's r2: -0.164996\n",
      "[686]\ttraining's r2: 0.86016\tvalid_1's r2: -0.164674\n",
      "[687]\ttraining's r2: 0.860376\tvalid_1's r2: -0.164781\n",
      "[688]\ttraining's r2: 0.860575\tvalid_1's r2: -0.165093\n",
      "[689]\ttraining's r2: 0.860817\tvalid_1's r2: -0.16515\n",
      "[690]\ttraining's r2: 0.861025\tvalid_1's r2: -0.165399\n",
      "[691]\ttraining's r2: 0.861273\tvalid_1's r2: -0.16537\n",
      "[692]\ttraining's r2: 0.861521\tvalid_1's r2: -0.165087\n",
      "[693]\ttraining's r2: 0.861796\tvalid_1's r2: -0.165029\n",
      "[694]\ttraining's r2: 0.862008\tvalid_1's r2: -0.164786\n",
      "[695]\ttraining's r2: 0.862345\tvalid_1's r2: -0.164175\n",
      "[696]\ttraining's r2: 0.862655\tvalid_1's r2: -0.163616\n",
      "[697]\ttraining's r2: 0.862897\tvalid_1's r2: -0.163384\n",
      "[698]\ttraining's r2: 0.86316\tvalid_1's r2: -0.163782\n",
      "[699]\ttraining's r2: 0.863446\tvalid_1's r2: -0.163234\n",
      "[700]\ttraining's r2: 0.863724\tvalid_1's r2: -0.163299\n",
      "[701]\ttraining's r2: 0.863944\tvalid_1's r2: -0.163557\n",
      "[702]\ttraining's r2: 0.864199\tvalid_1's r2: -0.163256\n",
      "[703]\ttraining's r2: 0.864425\tvalid_1's r2: -0.163294\n",
      "[704]\ttraining's r2: 0.864686\tvalid_1's r2: -0.163487\n",
      "[705]\ttraining's r2: 0.864907\tvalid_1's r2: -0.163242\n",
      "[706]\ttraining's r2: 0.865115\tvalid_1's r2: -0.163343\n",
      "[707]\ttraining's r2: 0.865332\tvalid_1's r2: -0.162956\n",
      "[708]\ttraining's r2: 0.865639\tvalid_1's r2: -0.162887\n",
      "[709]\ttraining's r2: 0.865877\tvalid_1's r2: -0.16291\n",
      "[710]\ttraining's r2: 0.866142\tvalid_1's r2: -0.162646\n",
      "[711]\ttraining's r2: 0.86644\tvalid_1's r2: -0.16246\n",
      "[712]\ttraining's r2: 0.866686\tvalid_1's r2: -0.162252\n",
      "[713]\ttraining's r2: 0.866977\tvalid_1's r2: -0.162061\n",
      "[714]\ttraining's r2: 0.867206\tvalid_1's r2: -0.161807\n",
      "[715]\ttraining's r2: 0.867454\tvalid_1's r2: -0.161623\n",
      "[716]\ttraining's r2: 0.867701\tvalid_1's r2: -0.161654\n",
      "[717]\ttraining's r2: 0.867956\tvalid_1's r2: -0.16178\n",
      "[718]\ttraining's r2: 0.868146\tvalid_1's r2: -0.161972\n",
      "[719]\ttraining's r2: 0.868376\tvalid_1's r2: -0.161826\n",
      "[720]\ttraining's r2: 0.868637\tvalid_1's r2: -0.161458\n",
      "[721]\ttraining's r2: 0.868871\tvalid_1's r2: -0.161632\n",
      "[722]\ttraining's r2: 0.869121\tvalid_1's r2: -0.161666\n",
      "[723]\ttraining's r2: 0.86935\tvalid_1's r2: -0.161776\n",
      "[724]\ttraining's r2: 0.869561\tvalid_1's r2: -0.161784\n",
      "[725]\ttraining's r2: 0.869783\tvalid_1's r2: -0.161493\n",
      "[726]\ttraining's r2: 0.870024\tvalid_1's r2: -0.16169\n",
      "[727]\ttraining's r2: 0.870232\tvalid_1's r2: -0.161968\n",
      "[728]\ttraining's r2: 0.87046\tvalid_1's r2: -0.161597\n",
      "[729]\ttraining's r2: 0.870677\tvalid_1's r2: -0.1614\n",
      "[730]\ttraining's r2: 0.870882\tvalid_1's r2: -0.16162\n",
      "[731]\ttraining's r2: 0.871079\tvalid_1's r2: -0.161924\n",
      "[732]\ttraining's r2: 0.871319\tvalid_1's r2: -0.161924\n",
      "[733]\ttraining's r2: 0.871529\tvalid_1's r2: -0.161834\n",
      "[734]\ttraining's r2: 0.871787\tvalid_1's r2: -0.161557\n",
      "[735]\ttraining's r2: 0.871979\tvalid_1's r2: -0.161491\n",
      "[736]\ttraining's r2: 0.8722\tvalid_1's r2: -0.16147\n",
      "[737]\ttraining's r2: 0.872473\tvalid_1's r2: -0.161205\n",
      "[738]\ttraining's r2: 0.872721\tvalid_1's r2: -0.160872\n",
      "[739]\ttraining's r2: 0.872959\tvalid_1's r2: -0.16087\n",
      "[740]\ttraining's r2: 0.873211\tvalid_1's r2: -0.160483\n",
      "[741]\ttraining's r2: 0.873395\tvalid_1's r2: -0.160754\n",
      "[742]\ttraining's r2: 0.873603\tvalid_1's r2: -0.160564\n",
      "[743]\ttraining's r2: 0.873817\tvalid_1's r2: -0.160589\n",
      "[744]\ttraining's r2: 0.874047\tvalid_1's r2: -0.160972\n",
      "[745]\ttraining's r2: 0.874271\tvalid_1's r2: -0.161118\n",
      "[746]\ttraining's r2: 0.874536\tvalid_1's r2: -0.160846\n",
      "[747]\ttraining's r2: 0.87478\tvalid_1's r2: -0.16075\n",
      "[748]\ttraining's r2: 0.874983\tvalid_1's r2: -0.161044\n",
      "[749]\ttraining's r2: 0.875207\tvalid_1's r2: -0.160939\n",
      "[750]\ttraining's r2: 0.875474\tvalid_1's r2: -0.160699\n",
      "[751]\ttraining's r2: 0.875662\tvalid_1's r2: -0.161084\n",
      "[752]\ttraining's r2: 0.87588\tvalid_1's r2: -0.160875\n",
      "[753]\ttraining's r2: 0.876138\tvalid_1's r2: -0.160499\n",
      "[754]\ttraining's r2: 0.876339\tvalid_1's r2: -0.160307\n",
      "[755]\ttraining's r2: 0.87653\tvalid_1's r2: -0.160347\n",
      "[756]\ttraining's r2: 0.876745\tvalid_1's r2: -0.160491\n",
      "[757]\ttraining's r2: 0.876966\tvalid_1's r2: -0.160333\n",
      "[758]\ttraining's r2: 0.877184\tvalid_1's r2: -0.160097\n",
      "[759]\ttraining's r2: 0.877387\tvalid_1's r2: -0.160041\n",
      "[760]\ttraining's r2: 0.877595\tvalid_1's r2: -0.16028\n",
      "[761]\ttraining's r2: 0.877843\tvalid_1's r2: -0.160361\n",
      "[762]\ttraining's r2: 0.878023\tvalid_1's r2: -0.16056\n",
      "[763]\ttraining's r2: 0.878242\tvalid_1's r2: -0.160551\n",
      "[764]\ttraining's r2: 0.878432\tvalid_1's r2: -0.160192\n",
      "[765]\ttraining's r2: 0.878634\tvalid_1's r2: -0.160086\n",
      "[766]\ttraining's r2: 0.878833\tvalid_1's r2: -0.160106\n",
      "[767]\ttraining's r2: 0.879061\tvalid_1's r2: -0.159907\n",
      "[768]\ttraining's r2: 0.879267\tvalid_1's r2: -0.16003\n",
      "[769]\ttraining's r2: 0.879485\tvalid_1's r2: -0.160178\n",
      "[770]\ttraining's r2: 0.879726\tvalid_1's r2: -0.159843\n",
      "[771]\ttraining's r2: 0.87995\tvalid_1's r2: -0.159402\n",
      "[772]\ttraining's r2: 0.880147\tvalid_1's r2: -0.159712\n",
      "[773]\ttraining's r2: 0.880314\tvalid_1's r2: -0.159864\n",
      "[774]\ttraining's r2: 0.880567\tvalid_1's r2: -0.159327\n",
      "[775]\ttraining's r2: 0.880746\tvalid_1's r2: -0.159216\n",
      "[776]\ttraining's r2: 0.880952\tvalid_1's r2: -0.159148\n",
      "[777]\ttraining's r2: 0.881172\tvalid_1's r2: -0.159052\n",
      "[778]\ttraining's r2: 0.881347\tvalid_1's r2: -0.158862\n",
      "[779]\ttraining's r2: 0.881552\tvalid_1's r2: -0.158484\n",
      "[780]\ttraining's r2: 0.881751\tvalid_1's r2: -0.158474\n",
      "[781]\ttraining's r2: 0.881926\tvalid_1's r2: -0.158864\n",
      "[782]\ttraining's r2: 0.882155\tvalid_1's r2: -0.158914\n",
      "[783]\ttraining's r2: 0.882355\tvalid_1's r2: -0.15842\n",
      "[784]\ttraining's r2: 0.882617\tvalid_1's r2: -0.158057\n",
      "[785]\ttraining's r2: 0.882825\tvalid_1's r2: -0.15798\n",
      "[786]\ttraining's r2: 0.883038\tvalid_1's r2: -0.157873\n",
      "[787]\ttraining's r2: 0.883281\tvalid_1's r2: -0.157476\n",
      "[788]\ttraining's r2: 0.883532\tvalid_1's r2: -0.157381\n",
      "[789]\ttraining's r2: 0.883644\tvalid_1's r2: -0.157954\n",
      "[790]\ttraining's r2: 0.88385\tvalid_1's r2: -0.157793\n",
      "[791]\ttraining's r2: 0.884071\tvalid_1's r2: -0.157812\n",
      "[792]\ttraining's r2: 0.884254\tvalid_1's r2: -0.158072\n",
      "[793]\ttraining's r2: 0.884461\tvalid_1's r2: -0.157655\n",
      "[794]\ttraining's r2: 0.884661\tvalid_1's r2: -0.157608\n",
      "[795]\ttraining's r2: 0.884833\tvalid_1's r2: -0.157669\n",
      "[796]\ttraining's r2: 0.885003\tvalid_1's r2: -0.158002\n",
      "[797]\ttraining's r2: 0.885193\tvalid_1's r2: -0.157938\n",
      "[798]\ttraining's r2: 0.885388\tvalid_1's r2: -0.157957\n",
      "[799]\ttraining's r2: 0.885589\tvalid_1's r2: -0.158299\n",
      "[800]\ttraining's r2: 0.885769\tvalid_1's r2: -0.158221\n",
      "[801]\ttraining's r2: 0.885987\tvalid_1's r2: -0.158133\n",
      "[802]\ttraining's r2: 0.886217\tvalid_1's r2: -0.157877\n",
      "[803]\ttraining's r2: 0.886432\tvalid_1's r2: -0.15759\n",
      "[804]\ttraining's r2: 0.886583\tvalid_1's r2: -0.157631\n",
      "[805]\ttraining's r2: 0.886784\tvalid_1's r2: -0.157266\n",
      "[806]\ttraining's r2: 0.886982\tvalid_1's r2: -0.157397\n",
      "[807]\ttraining's r2: 0.887158\tvalid_1's r2: -0.157485\n",
      "[808]\ttraining's r2: 0.88738\tvalid_1's r2: -0.157321\n",
      "[809]\ttraining's r2: 0.88757\tvalid_1's r2: -0.157287\n",
      "[810]\ttraining's r2: 0.887715\tvalid_1's r2: -0.157357\n",
      "[811]\ttraining's r2: 0.887918\tvalid_1's r2: -0.157354\n",
      "[812]\ttraining's r2: 0.888074\tvalid_1's r2: -0.157441\n",
      "[813]\ttraining's r2: 0.8883\tvalid_1's r2: -0.157811\n",
      "[814]\ttraining's r2: 0.888454\tvalid_1's r2: -0.157887\n",
      "[815]\ttraining's r2: 0.888641\tvalid_1's r2: -0.157959\n",
      "[816]\ttraining's r2: 0.888845\tvalid_1's r2: -0.157676\n",
      "[817]\ttraining's r2: 0.889028\tvalid_1's r2: -0.157227\n",
      "[818]\ttraining's r2: 0.889206\tvalid_1's r2: -0.156807\n",
      "[819]\ttraining's r2: 0.889368\tvalid_1's r2: -0.156937\n",
      "[820]\ttraining's r2: 0.889567\tvalid_1's r2: -0.156595\n",
      "[821]\ttraining's r2: 0.889747\tvalid_1's r2: -0.156703\n",
      "[822]\ttraining's r2: 0.889931\tvalid_1's r2: -0.156814\n",
      "[823]\ttraining's r2: 0.890149\tvalid_1's r2: -0.156788\n",
      "[824]\ttraining's r2: 0.890331\tvalid_1's r2: -0.156435\n",
      "[825]\ttraining's r2: 0.890531\tvalid_1's r2: -0.156529\n",
      "[826]\ttraining's r2: 0.890726\tvalid_1's r2: -0.156633\n",
      "[827]\ttraining's r2: 0.890864\tvalid_1's r2: -0.156887\n",
      "[828]\ttraining's r2: 0.891032\tvalid_1's r2: -0.156862\n",
      "[829]\ttraining's r2: 0.891185\tvalid_1's r2: -0.157046\n",
      "[830]\ttraining's r2: 0.891351\tvalid_1's r2: -0.156804\n",
      "[831]\ttraining's r2: 0.891536\tvalid_1's r2: -0.156634\n",
      "[832]\ttraining's r2: 0.891774\tvalid_1's r2: -0.156448\n",
      "[833]\ttraining's r2: 0.891972\tvalid_1's r2: -0.156647\n",
      "[834]\ttraining's r2: 0.892178\tvalid_1's r2: -0.156183\n",
      "[835]\ttraining's r2: 0.892372\tvalid_1's r2: -0.156189\n",
      "[836]\ttraining's r2: 0.892566\tvalid_1's r2: -0.156146\n",
      "[837]\ttraining's r2: 0.892733\tvalid_1's r2: -0.156156\n",
      "[838]\ttraining's r2: 0.892898\tvalid_1's r2: -0.155661\n",
      "[839]\ttraining's r2: 0.893052\tvalid_1's r2: -0.156078\n",
      "[840]\ttraining's r2: 0.893251\tvalid_1's r2: -0.156085\n",
      "[841]\ttraining's r2: 0.893467\tvalid_1's r2: -0.155959\n",
      "[842]\ttraining's r2: 0.893591\tvalid_1's r2: -0.155936\n",
      "[843]\ttraining's r2: 0.893765\tvalid_1's r2: -0.155988\n",
      "[844]\ttraining's r2: 0.89396\tvalid_1's r2: -0.155895\n",
      "[845]\ttraining's r2: 0.894116\tvalid_1's r2: -0.155832\n",
      "[846]\ttraining's r2: 0.894312\tvalid_1's r2: -0.155739\n",
      "[847]\ttraining's r2: 0.894453\tvalid_1's r2: -0.15551\n",
      "[848]\ttraining's r2: 0.894632\tvalid_1's r2: -0.155202\n",
      "[849]\ttraining's r2: 0.89479\tvalid_1's r2: -0.155228\n",
      "[850]\ttraining's r2: 0.894944\tvalid_1's r2: -0.155518\n",
      "[851]\ttraining's r2: 0.895113\tvalid_1's r2: -0.155922\n",
      "[852]\ttraining's r2: 0.895269\tvalid_1's r2: -0.155796\n",
      "[853]\ttraining's r2: 0.895429\tvalid_1's r2: -0.155772\n",
      "[854]\ttraining's r2: 0.895622\tvalid_1's r2: -0.155457\n",
      "[855]\ttraining's r2: 0.895796\tvalid_1's r2: -0.155597\n",
      "[856]\ttraining's r2: 0.895957\tvalid_1's r2: -0.155679\n",
      "[857]\ttraining's r2: 0.896125\tvalid_1's r2: -0.155784\n",
      "[858]\ttraining's r2: 0.8963\tvalid_1's r2: -0.155773\n",
      "[859]\ttraining's r2: 0.896487\tvalid_1's r2: -0.155783\n",
      "[860]\ttraining's r2: 0.896664\tvalid_1's r2: -0.155473\n",
      "[861]\ttraining's r2: 0.896813\tvalid_1's r2: -0.155695\n",
      "[862]\ttraining's r2: 0.89694\tvalid_1's r2: -0.155875\n",
      "[863]\ttraining's r2: 0.897117\tvalid_1's r2: -0.155806\n",
      "[864]\ttraining's r2: 0.897279\tvalid_1's r2: -0.156071\n",
      "[865]\ttraining's r2: 0.897399\tvalid_1's r2: -0.156248\n",
      "[866]\ttraining's r2: 0.897576\tvalid_1's r2: -0.156359\n",
      "[867]\ttraining's r2: 0.897708\tvalid_1's r2: -0.156624\n",
      "[868]\ttraining's r2: 0.89787\tvalid_1's r2: -0.156933\n",
      "[869]\ttraining's r2: 0.898087\tvalid_1's r2: -0.157059\n",
      "[870]\ttraining's r2: 0.898234\tvalid_1's r2: -0.156813\n",
      "[871]\ttraining's r2: 0.89842\tvalid_1's r2: -0.156848\n",
      "[872]\ttraining's r2: 0.898593\tvalid_1's r2: -0.156376\n",
      "[873]\ttraining's r2: 0.898765\tvalid_1's r2: -0.156373\n",
      "[874]\ttraining's r2: 0.898894\tvalid_1's r2: -0.156585\n",
      "[875]\ttraining's r2: 0.899055\tvalid_1's r2: -0.156553\n",
      "[876]\ttraining's r2: 0.899239\tvalid_1's r2: -0.156247\n",
      "[877]\ttraining's r2: 0.899406\tvalid_1's r2: -0.156003\n",
      "[878]\ttraining's r2: 0.899593\tvalid_1's r2: -0.156086\n",
      "[879]\ttraining's r2: 0.899756\tvalid_1's r2: -0.155868\n",
      "[880]\ttraining's r2: 0.899918\tvalid_1's r2: -0.155669\n",
      "[881]\ttraining's r2: 0.900055\tvalid_1's r2: -0.155637\n",
      "[882]\ttraining's r2: 0.900232\tvalid_1's r2: -0.155204\n",
      "[883]\ttraining's r2: 0.900362\tvalid_1's r2: -0.155304\n",
      "[884]\ttraining's r2: 0.900533\tvalid_1's r2: -0.155013\n",
      "[885]\ttraining's r2: 0.900672\tvalid_1's r2: -0.154999\n",
      "[886]\ttraining's r2: 0.900819\tvalid_1's r2: -0.154848\n",
      "[887]\ttraining's r2: 0.900984\tvalid_1's r2: -0.154593\n",
      "[888]\ttraining's r2: 0.901164\tvalid_1's r2: -0.154668\n",
      "[889]\ttraining's r2: 0.901341\tvalid_1's r2: -0.154499\n",
      "[890]\ttraining's r2: 0.901506\tvalid_1's r2: -0.154425\n",
      "[891]\ttraining's r2: 0.901689\tvalid_1's r2: -0.15429\n",
      "[892]\ttraining's r2: 0.901865\tvalid_1's r2: -0.154034\n",
      "[893]\ttraining's r2: 0.902001\tvalid_1's r2: -0.154028\n",
      "[894]\ttraining's r2: 0.902145\tvalid_1's r2: -0.153943\n",
      "[895]\ttraining's r2: 0.902294\tvalid_1's r2: -0.15389\n",
      "[896]\ttraining's r2: 0.902425\tvalid_1's r2: -0.153964\n",
      "[897]\ttraining's r2: 0.902571\tvalid_1's r2: -0.153956\n",
      "[898]\ttraining's r2: 0.902738\tvalid_1's r2: -0.15391\n",
      "[899]\ttraining's r2: 0.902884\tvalid_1's r2: -0.154018\n",
      "[900]\ttraining's r2: 0.902994\tvalid_1's r2: -0.154197\n",
      "[901]\ttraining's r2: 0.90312\tvalid_1's r2: -0.153946\n",
      "[902]\ttraining's r2: 0.903268\tvalid_1's r2: -0.154201\n",
      "[903]\ttraining's r2: 0.903369\tvalid_1's r2: -0.154548\n",
      "[904]\ttraining's r2: 0.903514\tvalid_1's r2: -0.154515\n",
      "[905]\ttraining's r2: 0.903658\tvalid_1's r2: -0.154552\n",
      "[906]\ttraining's r2: 0.903793\tvalid_1's r2: -0.154493\n",
      "[907]\ttraining's r2: 0.903963\tvalid_1's r2: -0.154382\n",
      "[908]\ttraining's r2: 0.904121\tvalid_1's r2: -0.154122\n",
      "[909]\ttraining's r2: 0.904323\tvalid_1's r2: -0.15415\n",
      "[910]\ttraining's r2: 0.904496\tvalid_1's r2: -0.154157\n",
      "[911]\ttraining's r2: 0.904651\tvalid_1's r2: -0.154024\n",
      "[912]\ttraining's r2: 0.90477\tvalid_1's r2: -0.154221\n",
      "[913]\ttraining's r2: 0.90493\tvalid_1's r2: -0.154383\n",
      "[914]\ttraining's r2: 0.905074\tvalid_1's r2: -0.154414\n",
      "[915]\ttraining's r2: 0.90522\tvalid_1's r2: -0.154498\n",
      "[916]\ttraining's r2: 0.905374\tvalid_1's r2: -0.154782\n",
      "[917]\ttraining's r2: 0.905531\tvalid_1's r2: -0.154664\n",
      "[918]\ttraining's r2: 0.905706\tvalid_1's r2: -0.154175\n",
      "[919]\ttraining's r2: 0.905849\tvalid_1's r2: -0.154313\n",
      "[920]\ttraining's r2: 0.905987\tvalid_1's r2: -0.154529\n",
      "[921]\ttraining's r2: 0.906172\tvalid_1's r2: -0.154599\n",
      "[922]\ttraining's r2: 0.906318\tvalid_1's r2: -0.154325\n",
      "[923]\ttraining's r2: 0.906459\tvalid_1's r2: -0.154012\n",
      "[924]\ttraining's r2: 0.906587\tvalid_1's r2: -0.153933\n",
      "[925]\ttraining's r2: 0.906713\tvalid_1's r2: -0.154383\n",
      "[926]\ttraining's r2: 0.906874\tvalid_1's r2: -0.15429\n",
      "[927]\ttraining's r2: 0.907029\tvalid_1's r2: -0.154145\n",
      "[928]\ttraining's r2: 0.907167\tvalid_1's r2: -0.154557\n",
      "[929]\ttraining's r2: 0.907313\tvalid_1's r2: -0.154402\n",
      "[930]\ttraining's r2: 0.90746\tvalid_1's r2: -0.154536\n",
      "[931]\ttraining's r2: 0.907611\tvalid_1's r2: -0.154498\n",
      "[932]\ttraining's r2: 0.907774\tvalid_1's r2: -0.153953\n",
      "[933]\ttraining's r2: 0.90789\tvalid_1's r2: -0.154044\n",
      "[934]\ttraining's r2: 0.907998\tvalid_1's r2: -0.154127\n",
      "[935]\ttraining's r2: 0.908133\tvalid_1's r2: -0.154154\n",
      "[936]\ttraining's r2: 0.908241\tvalid_1's r2: -0.154268\n",
      "[937]\ttraining's r2: 0.9084\tvalid_1's r2: -0.15379\n",
      "[938]\ttraining's r2: 0.908545\tvalid_1's r2: -0.15348\n",
      "[939]\ttraining's r2: 0.908677\tvalid_1's r2: -0.153426\n",
      "[940]\ttraining's r2: 0.908827\tvalid_1's r2: -0.153286\n",
      "[941]\ttraining's r2: 0.908955\tvalid_1's r2: -0.153156\n",
      "[942]\ttraining's r2: 0.909075\tvalid_1's r2: -0.153118\n",
      "[943]\ttraining's r2: 0.9092\tvalid_1's r2: -0.152964\n",
      "[944]\ttraining's r2: 0.909341\tvalid_1's r2: -0.152828\n",
      "[945]\ttraining's r2: 0.909501\tvalid_1's r2: -0.152755\n",
      "[946]\ttraining's r2: 0.909639\tvalid_1's r2: -0.152427\n",
      "[947]\ttraining's r2: 0.909771\tvalid_1's r2: -0.152279\n",
      "[948]\ttraining's r2: 0.909921\tvalid_1's r2: -0.152433\n",
      "[949]\ttraining's r2: 0.91005\tvalid_1's r2: -0.152449\n",
      "[950]\ttraining's r2: 0.910167\tvalid_1's r2: -0.152642\n",
      "[951]\ttraining's r2: 0.910292\tvalid_1's r2: -0.152604\n",
      "[952]\ttraining's r2: 0.910446\tvalid_1's r2: -0.152389\n",
      "[953]\ttraining's r2: 0.910574\tvalid_1's r2: -0.152526\n",
      "[954]\ttraining's r2: 0.910686\tvalid_1's r2: -0.152316\n",
      "[955]\ttraining's r2: 0.910823\tvalid_1's r2: -0.152357\n",
      "[956]\ttraining's r2: 0.910957\tvalid_1's r2: -0.152161\n",
      "[957]\ttraining's r2: 0.911083\tvalid_1's r2: -0.152178\n",
      "[958]\ttraining's r2: 0.911209\tvalid_1's r2: -0.152058\n",
      "[959]\ttraining's r2: 0.911343\tvalid_1's r2: -0.151909\n",
      "[960]\ttraining's r2: 0.911475\tvalid_1's r2: -0.151764\n",
      "[961]\ttraining's r2: 0.911643\tvalid_1's r2: -0.151502\n",
      "[962]\ttraining's r2: 0.911798\tvalid_1's r2: -0.151423\n",
      "[963]\ttraining's r2: 0.911918\tvalid_1's r2: -0.151534\n",
      "[964]\ttraining's r2: 0.91207\tvalid_1's r2: -0.151674\n",
      "[965]\ttraining's r2: 0.912223\tvalid_1's r2: -0.151506\n",
      "[966]\ttraining's r2: 0.912347\tvalid_1's r2: -0.151586\n",
      "[967]\ttraining's r2: 0.912468\tvalid_1's r2: -0.151623\n",
      "[968]\ttraining's r2: 0.912574\tvalid_1's r2: -0.152148\n",
      "[969]\ttraining's r2: 0.9127\tvalid_1's r2: -0.152228\n",
      "[970]\ttraining's r2: 0.912823\tvalid_1's r2: -0.152451\n",
      "[971]\ttraining's r2: 0.912939\tvalid_1's r2: -0.152374\n",
      "[972]\ttraining's r2: 0.913092\tvalid_1's r2: -0.152205\n",
      "[973]\ttraining's r2: 0.913249\tvalid_1's r2: -0.151941\n",
      "[974]\ttraining's r2: 0.913408\tvalid_1's r2: -0.15188\n",
      "[975]\ttraining's r2: 0.913562\tvalid_1's r2: -0.151797\n",
      "[976]\ttraining's r2: 0.913676\tvalid_1's r2: -0.151547\n",
      "[977]\ttraining's r2: 0.913793\tvalid_1's r2: -0.151571\n",
      "[978]\ttraining's r2: 0.913909\tvalid_1's r2: -0.151562\n",
      "[979]\ttraining's r2: 0.914031\tvalid_1's r2: -0.151445\n",
      "[980]\ttraining's r2: 0.914148\tvalid_1's r2: -0.151751\n",
      "[981]\ttraining's r2: 0.914265\tvalid_1's r2: -0.15184\n",
      "[982]\ttraining's r2: 0.91439\tvalid_1's r2: -0.151955\n",
      "[983]\ttraining's r2: 0.914525\tvalid_1's r2: -0.151705\n",
      "[984]\ttraining's r2: 0.914655\tvalid_1's r2: -0.151493\n",
      "[985]\ttraining's r2: 0.914792\tvalid_1's r2: -0.151402\n",
      "[986]\ttraining's r2: 0.914914\tvalid_1's r2: -0.151403\n",
      "[987]\ttraining's r2: 0.915055\tvalid_1's r2: -0.151141\n",
      "[988]\ttraining's r2: 0.915176\tvalid_1's r2: -0.15108\n",
      "[989]\ttraining's r2: 0.9153\tvalid_1's r2: -0.151219\n",
      "[990]\ttraining's r2: 0.91544\tvalid_1's r2: -0.150988\n",
      "[991]\ttraining's r2: 0.915541\tvalid_1's r2: -0.151065\n",
      "[992]\ttraining's r2: 0.915679\tvalid_1's r2: -0.150879\n",
      "[993]\ttraining's r2: 0.915818\tvalid_1's r2: -0.150697\n",
      "[994]\ttraining's r2: 0.91592\tvalid_1's r2: -0.150917\n",
      "[995]\ttraining's r2: 0.916032\tvalid_1's r2: -0.150951\n",
      "[996]\ttraining's r2: 0.916151\tvalid_1's r2: -0.151057\n",
      "[997]\ttraining's r2: 0.916295\tvalid_1's r2: -0.151041\n",
      "[998]\ttraining's r2: 0.916401\tvalid_1's r2: -0.150976\n",
      "[999]\ttraining's r2: 0.916527\tvalid_1's r2: -0.150997\n",
      "[1000]\ttraining's r2: 0.916662\tvalid_1's r2: -0.15112\n",
      "[1001]\ttraining's r2: 0.916749\tvalid_1's r2: -0.151257\n",
      "[1002]\ttraining's r2: 0.916858\tvalid_1's r2: -0.151241\n",
      "[1003]\ttraining's r2: 0.917013\tvalid_1's r2: -0.150911\n",
      "[1004]\ttraining's r2: 0.917149\tvalid_1's r2: -0.150764\n",
      "[1005]\ttraining's r2: 0.917252\tvalid_1's r2: -0.150798\n",
      "[1006]\ttraining's r2: 0.917371\tvalid_1's r2: -0.150713\n",
      "[1007]\ttraining's r2: 0.917503\tvalid_1's r2: -0.150668\n",
      "[1008]\ttraining's r2: 0.917634\tvalid_1's r2: -0.15076\n",
      "[1009]\ttraining's r2: 0.917721\tvalid_1's r2: -0.150755\n",
      "[1010]\ttraining's r2: 0.917837\tvalid_1's r2: -0.150433\n",
      "[1011]\ttraining's r2: 0.917944\tvalid_1's r2: -0.150372\n",
      "[1012]\ttraining's r2: 0.91807\tvalid_1's r2: -0.150119\n",
      "[1013]\ttraining's r2: 0.918204\tvalid_1's r2: -0.150226\n",
      "[1014]\ttraining's r2: 0.918318\tvalid_1's r2: -0.150342\n",
      "[1015]\ttraining's r2: 0.918458\tvalid_1's r2: -0.150219\n",
      "[1016]\ttraining's r2: 0.918553\tvalid_1's r2: -0.149815\n",
      "[1017]\ttraining's r2: 0.918672\tvalid_1's r2: -0.149766\n",
      "[1018]\ttraining's r2: 0.918787\tvalid_1's r2: -0.149618\n",
      "[1019]\ttraining's r2: 0.918935\tvalid_1's r2: -0.149571\n",
      "[1020]\ttraining's r2: 0.919004\tvalid_1's r2: -0.149824\n",
      "[1021]\ttraining's r2: 0.919128\tvalid_1's r2: -0.149772\n",
      "[1022]\ttraining's r2: 0.919249\tvalid_1's r2: -0.14976\n",
      "[1023]\ttraining's r2: 0.919369\tvalid_1's r2: -0.149461\n",
      "[1024]\ttraining's r2: 0.91948\tvalid_1's r2: -0.149655\n",
      "[1025]\ttraining's r2: 0.919575\tvalid_1's r2: -0.149517\n",
      "[1026]\ttraining's r2: 0.919696\tvalid_1's r2: -0.149187\n",
      "[1027]\ttraining's r2: 0.919791\tvalid_1's r2: -0.149543\n",
      "[1028]\ttraining's r2: 0.919906\tvalid_1's r2: -0.149492\n",
      "[1029]\ttraining's r2: 0.920015\tvalid_1's r2: -0.14953\n",
      "[1030]\ttraining's r2: 0.920123\tvalid_1's r2: -0.149524\n",
      "[1031]\ttraining's r2: 0.92025\tvalid_1's r2: -0.14936\n",
      "[1032]\ttraining's r2: 0.920354\tvalid_1's r2: -0.149404\n",
      "[1033]\ttraining's r2: 0.920483\tvalid_1's r2: -0.149325\n",
      "[1034]\ttraining's r2: 0.920584\tvalid_1's r2: -0.149505\n",
      "[1035]\ttraining's r2: 0.920696\tvalid_1's r2: -0.149392\n",
      "[1036]\ttraining's r2: 0.920793\tvalid_1's r2: -0.14961\n",
      "[1037]\ttraining's r2: 0.920906\tvalid_1's r2: -0.149272\n",
      "[1038]\ttraining's r2: 0.921032\tvalid_1's r2: -0.149001\n",
      "[1039]\ttraining's r2: 0.921156\tvalid_1's r2: -0.149007\n",
      "[1040]\ttraining's r2: 0.921245\tvalid_1's r2: -0.149146\n",
      "[1041]\ttraining's r2: 0.921362\tvalid_1's r2: -0.149089\n",
      "[1042]\ttraining's r2: 0.921469\tvalid_1's r2: -0.14899\n",
      "[1043]\ttraining's r2: 0.921611\tvalid_1's r2: -0.148814\n",
      "[1044]\ttraining's r2: 0.921736\tvalid_1's r2: -0.148672\n",
      "[1045]\ttraining's r2: 0.921852\tvalid_1's r2: -0.148761\n",
      "[1046]\ttraining's r2: 0.921954\tvalid_1's r2: -0.148838\n",
      "[1047]\ttraining's r2: 0.922053\tvalid_1's r2: -0.149101\n",
      "[1048]\ttraining's r2: 0.922148\tvalid_1's r2: -0.149268\n",
      "[1049]\ttraining's r2: 0.922286\tvalid_1's r2: -0.149171\n",
      "[1050]\ttraining's r2: 0.922438\tvalid_1's r2: -0.14884\n",
      "[1051]\ttraining's r2: 0.922549\tvalid_1's r2: -0.148923\n",
      "[1052]\ttraining's r2: 0.922655\tvalid_1's r2: -0.148787\n",
      "[1053]\ttraining's r2: 0.922772\tvalid_1's r2: -0.148838\n",
      "[1054]\ttraining's r2: 0.922902\tvalid_1's r2: -0.14866\n",
      "[1055]\ttraining's r2: 0.92303\tvalid_1's r2: -0.148458\n",
      "[1056]\ttraining's r2: 0.923137\tvalid_1's r2: -0.14832\n",
      "[1057]\ttraining's r2: 0.92325\tvalid_1's r2: -0.148338\n",
      "[1058]\ttraining's r2: 0.923346\tvalid_1's r2: -0.14852\n",
      "[1059]\ttraining's r2: 0.92345\tvalid_1's r2: -0.148575\n",
      "[1060]\ttraining's r2: 0.923558\tvalid_1's r2: -0.148813\n",
      "[1061]\ttraining's r2: 0.92368\tvalid_1's r2: -0.148515\n",
      "[1062]\ttraining's r2: 0.923799\tvalid_1's r2: -0.148547\n",
      "[1063]\ttraining's r2: 0.92391\tvalid_1's r2: -0.148729\n",
      "[1064]\ttraining's r2: 0.924013\tvalid_1's r2: -0.148569\n",
      "[1065]\ttraining's r2: 0.924119\tvalid_1's r2: -0.148466\n",
      "[1066]\ttraining's r2: 0.924205\tvalid_1's r2: -0.148389\n",
      "[1067]\ttraining's r2: 0.924295\tvalid_1's r2: -0.148388\n",
      "[1068]\ttraining's r2: 0.924414\tvalid_1's r2: -0.148024\n",
      "[1069]\ttraining's r2: 0.924513\tvalid_1's r2: -0.147557\n",
      "[1070]\ttraining's r2: 0.924621\tvalid_1's r2: -0.147563\n",
      "[1071]\ttraining's r2: 0.924729\tvalid_1's r2: -0.147523\n",
      "[1072]\ttraining's r2: 0.924836\tvalid_1's r2: -0.147668\n",
      "[1073]\ttraining's r2: 0.924931\tvalid_1's r2: -0.147828\n",
      "[1074]\ttraining's r2: 0.925048\tvalid_1's r2: -0.147808\n",
      "[1075]\ttraining's r2: 0.925139\tvalid_1's r2: -0.147733\n",
      "[1076]\ttraining's r2: 0.925242\tvalid_1's r2: -0.147543\n",
      "[1077]\ttraining's r2: 0.925323\tvalid_1's r2: -0.147618\n",
      "[1078]\ttraining's r2: 0.925414\tvalid_1's r2: -0.147518\n",
      "[1079]\ttraining's r2: 0.925547\tvalid_1's r2: -0.147258\n",
      "[1080]\ttraining's r2: 0.92564\tvalid_1's r2: -0.147285\n",
      "[1081]\ttraining's r2: 0.92575\tvalid_1's r2: -0.146761\n",
      "[1082]\ttraining's r2: 0.925842\tvalid_1's r2: -0.146923\n",
      "[1083]\ttraining's r2: 0.925962\tvalid_1's r2: -0.146676\n",
      "[1084]\ttraining's r2: 0.926054\tvalid_1's r2: -0.146786\n",
      "[1085]\ttraining's r2: 0.926162\tvalid_1's r2: -0.146791\n",
      "[1086]\ttraining's r2: 0.926269\tvalid_1's r2: -0.146571\n",
      "[1087]\ttraining's r2: 0.926364\tvalid_1's r2: -0.146213\n",
      "[1088]\ttraining's r2: 0.926464\tvalid_1's r2: -0.145905\n",
      "[1089]\ttraining's r2: 0.926566\tvalid_1's r2: -0.145883\n",
      "[1090]\ttraining's r2: 0.926681\tvalid_1's r2: -0.145697\n",
      "[1091]\ttraining's r2: 0.926779\tvalid_1's r2: -0.145815\n",
      "[1092]\ttraining's r2: 0.926896\tvalid_1's r2: -0.145714\n",
      "[1093]\ttraining's r2: 0.926987\tvalid_1's r2: -0.145923\n",
      "[1094]\ttraining's r2: 0.927067\tvalid_1's r2: -0.145877\n",
      "[1095]\ttraining's r2: 0.927158\tvalid_1's r2: -0.145669\n",
      "[1096]\ttraining's r2: 0.927236\tvalid_1's r2: -0.145749\n",
      "[1097]\ttraining's r2: 0.927342\tvalid_1's r2: -0.145794\n",
      "[1098]\ttraining's r2: 0.92744\tvalid_1's r2: -0.145653\n",
      "[1099]\ttraining's r2: 0.927561\tvalid_1's r2: -0.145488\n",
      "[1100]\ttraining's r2: 0.927658\tvalid_1's r2: -0.145577\n",
      "[1101]\ttraining's r2: 0.927771\tvalid_1's r2: -0.145562\n",
      "[1102]\ttraining's r2: 0.92784\tvalid_1's r2: -0.145467\n",
      "[1103]\ttraining's r2: 0.927933\tvalid_1's r2: -0.14562\n",
      "[1104]\ttraining's r2: 0.92802\tvalid_1's r2: -0.145757\n",
      "[1105]\ttraining's r2: 0.928108\tvalid_1's r2: -0.14585\n",
      "[1106]\ttraining's r2: 0.928211\tvalid_1's r2: -0.145955\n",
      "[1107]\ttraining's r2: 0.928306\tvalid_1's r2: -0.145817\n",
      "[1108]\ttraining's r2: 0.928412\tvalid_1's r2: -0.145789\n",
      "[1109]\ttraining's r2: 0.928489\tvalid_1's r2: -0.145936\n",
      "[1110]\ttraining's r2: 0.92858\tvalid_1's r2: -0.145984\n",
      "[1111]\ttraining's r2: 0.928677\tvalid_1's r2: -0.146231\n",
      "[1112]\ttraining's r2: 0.92878\tvalid_1's r2: -0.146092\n",
      "[1113]\ttraining's r2: 0.928892\tvalid_1's r2: -0.145952\n",
      "[1114]\ttraining's r2: 0.929007\tvalid_1's r2: -0.14575\n",
      "[1115]\ttraining's r2: 0.929094\tvalid_1's r2: -0.145936\n",
      "[1116]\ttraining's r2: 0.929184\tvalid_1's r2: -0.146111\n",
      "[1117]\ttraining's r2: 0.929284\tvalid_1's r2: -0.1463\n",
      "[1118]\ttraining's r2: 0.929361\tvalid_1's r2: -0.146391\n",
      "[1119]\ttraining's r2: 0.929438\tvalid_1's r2: -0.146548\n",
      "[1120]\ttraining's r2: 0.929516\tvalid_1's r2: -0.146646\n",
      "[1121]\ttraining's r2: 0.929597\tvalid_1's r2: -0.146966\n",
      "[1122]\ttraining's r2: 0.929677\tvalid_1's r2: -0.147095\n",
      "[1123]\ttraining's r2: 0.929759\tvalid_1's r2: -0.147162\n",
      "[1124]\ttraining's r2: 0.929864\tvalid_1's r2: -0.147182\n",
      "[1125]\ttraining's r2: 0.92996\tvalid_1's r2: -0.147323\n",
      "[1126]\ttraining's r2: 0.930044\tvalid_1's r2: -0.147502\n",
      "[1127]\ttraining's r2: 0.930145\tvalid_1's r2: -0.147346\n",
      "[1128]\ttraining's r2: 0.930246\tvalid_1's r2: -0.147122\n",
      "[1129]\ttraining's r2: 0.930353\tvalid_1's r2: -0.147001\n",
      "[1130]\ttraining's r2: 0.93043\tvalid_1's r2: -0.147141\n",
      "[1131]\ttraining's r2: 0.930513\tvalid_1's r2: -0.146773\n",
      "[1132]\ttraining's r2: 0.930614\tvalid_1's r2: -0.146614\n",
      "[1133]\ttraining's r2: 0.930704\tvalid_1's r2: -0.146826\n",
      "[1134]\ttraining's r2: 0.930798\tvalid_1's r2: -0.146872\n",
      "[1135]\ttraining's r2: 0.930892\tvalid_1's r2: -0.146715\n",
      "[1136]\ttraining's r2: 0.93097\tvalid_1's r2: -0.146958\n",
      "[1137]\ttraining's r2: 0.931054\tvalid_1's r2: -0.146994\n",
      "[1138]\ttraining's r2: 0.931154\tvalid_1's r2: -0.147137\n",
      "[1139]\ttraining's r2: 0.931226\tvalid_1's r2: -0.147243\n",
      "[1140]\ttraining's r2: 0.931324\tvalid_1's r2: -0.147464\n",
      "[1141]\ttraining's r2: 0.931416\tvalid_1's r2: -0.147408\n",
      "[1142]\ttraining's r2: 0.931521\tvalid_1's r2: -0.147323\n",
      "[1143]\ttraining's r2: 0.931632\tvalid_1's r2: -0.147139\n",
      "[1144]\ttraining's r2: 0.931709\tvalid_1's r2: -0.147306\n",
      "[1145]\ttraining's r2: 0.931813\tvalid_1's r2: -0.147215\n",
      "[1146]\ttraining's r2: 0.931913\tvalid_1's r2: -0.147385\n",
      "[1147]\ttraining's r2: 0.932015\tvalid_1's r2: -0.147405\n",
      "[1148]\ttraining's r2: 0.932116\tvalid_1's r2: -0.147371\n",
      "[1149]\ttraining's r2: 0.932215\tvalid_1's r2: -0.147044\n",
      "[1150]\ttraining's r2: 0.932295\tvalid_1's r2: -0.146973\n",
      "[1151]\ttraining's r2: 0.932382\tvalid_1's r2: -0.146894\n",
      "[1152]\ttraining's r2: 0.932477\tvalid_1's r2: -0.146966\n",
      "[1153]\ttraining's r2: 0.932557\tvalid_1's r2: -0.147029\n",
      "[1154]\ttraining's r2: 0.932643\tvalid_1's r2: -0.146986\n",
      "[1155]\ttraining's r2: 0.932726\tvalid_1's r2: -0.147111\n",
      "[1156]\ttraining's r2: 0.932799\tvalid_1's r2: -0.147177\n",
      "[1157]\ttraining's r2: 0.932891\tvalid_1's r2: -0.147092\n",
      "[1158]\ttraining's r2: 0.932983\tvalid_1's r2: -0.147072\n",
      "[1159]\ttraining's r2: 0.933039\tvalid_1's r2: -0.147299\n",
      "[1160]\ttraining's r2: 0.933136\tvalid_1's r2: -0.147425\n",
      "[1161]\ttraining's r2: 0.933218\tvalid_1's r2: -0.147506\n",
      "[1162]\ttraining's r2: 0.933306\tvalid_1's r2: -0.147564\n",
      "[1163]\ttraining's r2: 0.933399\tvalid_1's r2: -0.147415\n",
      "[1164]\ttraining's r2: 0.933499\tvalid_1's r2: -0.147062\n",
      "[1165]\ttraining's r2: 0.933615\tvalid_1's r2: -0.147174\n",
      "[1166]\ttraining's r2: 0.933694\tvalid_1's r2: -0.146985\n",
      "[1167]\ttraining's r2: 0.933774\tvalid_1's r2: -0.147058\n",
      "[1168]\ttraining's r2: 0.933859\tvalid_1's r2: -0.146978\n",
      "[1169]\ttraining's r2: 0.933948\tvalid_1's r2: -0.146864\n",
      "[1170]\ttraining's r2: 0.934028\tvalid_1's r2: -0.1469\n",
      "[1171]\ttraining's r2: 0.93412\tvalid_1's r2: -0.147043\n",
      "[1172]\ttraining's r2: 0.934214\tvalid_1's r2: -0.146887\n",
      "[1173]\ttraining's r2: 0.934304\tvalid_1's r2: -0.146926\n",
      "[1174]\ttraining's r2: 0.934385\tvalid_1's r2: -0.14688\n",
      "[1175]\ttraining's r2: 0.934467\tvalid_1's r2: -0.146893\n",
      "[1176]\ttraining's r2: 0.934553\tvalid_1's r2: -0.146786\n",
      "[1177]\ttraining's r2: 0.934655\tvalid_1's r2: -0.146918\n",
      "[1178]\ttraining's r2: 0.934753\tvalid_1's r2: -0.146855\n",
      "[1179]\ttraining's r2: 0.93485\tvalid_1's r2: -0.146938\n",
      "[1180]\ttraining's r2: 0.934937\tvalid_1's r2: -0.146513\n",
      "[1181]\ttraining's r2: 0.935032\tvalid_1's r2: -0.146411\n",
      "[1182]\ttraining's r2: 0.935113\tvalid_1's r2: -0.14643\n",
      "[1183]\ttraining's r2: 0.935213\tvalid_1's r2: -0.146228\n",
      "[1184]\ttraining's r2: 0.935309\tvalid_1's r2: -0.146398\n",
      "[1185]\ttraining's r2: 0.935408\tvalid_1's r2: -0.146263\n",
      "[1186]\ttraining's r2: 0.935499\tvalid_1's r2: -0.146501\n",
      "[1187]\ttraining's r2: 0.935597\tvalid_1's r2: -0.146811\n",
      "[1188]\ttraining's r2: 0.93568\tvalid_1's r2: -0.146657\n",
      "[1189]\ttraining's r2: 0.935751\tvalid_1's r2: -0.146615\n",
      "[1190]\ttraining's r2: 0.935829\tvalid_1's r2: -0.146543\n",
      "[1191]\ttraining's r2: 0.935924\tvalid_1's r2: -0.146712\n",
      "[1192]\ttraining's r2: 0.936014\tvalid_1's r2: -0.146516\n",
      "[1193]\ttraining's r2: 0.936084\tvalid_1's r2: -0.146666\n",
      "[1194]\ttraining's r2: 0.936167\tvalid_1's r2: -0.146543\n",
      "[1195]\ttraining's r2: 0.93626\tvalid_1's r2: -0.146492\n",
      "[1196]\ttraining's r2: 0.936364\tvalid_1's r2: -0.146272\n",
      "[1197]\ttraining's r2: 0.936446\tvalid_1's r2: -0.146378\n",
      "[1198]\ttraining's r2: 0.936524\tvalid_1's r2: -0.146306\n",
      "[1199]\ttraining's r2: 0.936594\tvalid_1's r2: -0.146218\n",
      "[1200]\ttraining's r2: 0.936669\tvalid_1's r2: -0.146085\n",
      "[1201]\ttraining's r2: 0.936742\tvalid_1's r2: -0.1461\n",
      "[1202]\ttraining's r2: 0.936814\tvalid_1's r2: -0.145965\n",
      "Early stopping, best iteration is:\n",
      "[1102]\ttraining's r2: 0.92784\tvalid_1's r2: -0.145467\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=6, shuffle=True, random_state=420)\n",
    "\n",
    "models_lgb = {}\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(data_train, target):\n",
    "    X_train = data_train[train_index]\n",
    "    X_val = data_train[test_index]\n",
    "    Y_train = target[train_index]\n",
    "    Y_val = target[test_index]\n",
    "    \n",
    "    models_lgb[i] = lgb.LGBMRegressor(reg_alpha=50, max_depth=-1, learning_rate=0.03,\n",
    "                                    num_leaves=31, colsample_bytree=0.8, min_child_weight=3,\n",
    "                                    boosting_type='gbdt', max_bin=255, n_estimators=3000,\n",
    "                                    subsample_for_bin=50000, objective=None, min_split_gain=0, \n",
    "                                    min_child_samples=10, subsample=0.8, \n",
    "                                    subsample_freq=1, reg_lambda=100, \n",
    "                                    seed=420)\n",
    "    models_lgb[i].fit(X_train, Y_train, eval_metric=r2_eval_lgb,\n",
    "                      eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "                      early_stopping_rounds=100)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "seed = 420\n",
    "test_size = 0.2\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data_train, target, test_size=test_size, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's r2: -4204.48\tvalid_1's r2: -3649.99\n",
      "Train until valid scores didn't improve in 200 rounds.\n",
      "[2]\ttraining's r2: -894.865\tvalid_1's r2: -792.803\n",
      "[3]\ttraining's r2: -380.926\tvalid_1's r2: -335.447\n",
      "[4]\ttraining's r2: -211.157\tvalid_1's r2: -185.001\n",
      "[5]\ttraining's r2: -134.342\tvalid_1's r2: -117.161\n",
      "[6]\ttraining's r2: -92.2056\tvalid_1's r2: -80.1329\n",
      "[7]\ttraining's r2: -66.9705\tvalid_1's r2: -57.8252\n",
      "[8]\ttraining's r2: -50.4145\tvalid_1's r2: -43.2742\n",
      "[9]\ttraining's r2: -39.2959\tvalid_1's r2: -33.569\n",
      "[10]\ttraining's r2: -31.9839\tvalid_1's r2: -27.2799\n",
      "[11]\ttraining's r2: -26.2833\tvalid_1's r2: -22.298\n",
      "[12]\ttraining's r2: -21.9154\tvalid_1's r2: -18.5111\n",
      "[13]\ttraining's r2: -18.5336\tvalid_1's r2: -15.5948\n",
      "[14]\ttraining's r2: -15.8455\tvalid_1's r2: -13.2841\n",
      "[15]\ttraining's r2: -13.7346\tvalid_1's r2: -11.4688\n",
      "[16]\ttraining's r2: -12.0142\tvalid_1's r2: -10.0047\n",
      "[17]\ttraining's r2: -10.5962\tvalid_1's r2: -8.785\n",
      "[18]\ttraining's r2: -9.44114\tvalid_1's r2: -7.79796\n",
      "[19]\ttraining's r2: -8.4293\tvalid_1's r2: -6.91577\n",
      "[20]\ttraining's r2: -7.58384\tvalid_1's r2: -6.19057\n",
      "[21]\ttraining's r2: -6.82514\tvalid_1's r2: -5.52929\n",
      "[22]\ttraining's r2: -6.17564\tvalid_1's r2: -4.96498\n",
      "[23]\ttraining's r2: -5.61211\tvalid_1's r2: -4.49256\n",
      "[24]\ttraining's r2: -5.12051\tvalid_1's r2: -4.07668\n",
      "[25]\ttraining's r2: -4.71227\tvalid_1's r2: -3.73692\n",
      "[26]\ttraining's r2: -4.33164\tvalid_1's r2: -3.41186\n",
      "[27]\ttraining's r2: -3.98352\tvalid_1's r2: -3.12067\n",
      "[28]\ttraining's r2: -3.65299\tvalid_1's r2: -2.84375\n",
      "[29]\ttraining's r2: -3.38015\tvalid_1's r2: -2.6091\n",
      "[30]\ttraining's r2: -3.13065\tvalid_1's r2: -2.39891\n",
      "[31]\ttraining's r2: -2.90214\tvalid_1's r2: -2.20717\n",
      "[32]\ttraining's r2: -2.70496\tvalid_1's r2: -2.04267\n",
      "[33]\ttraining's r2: -2.5221\tvalid_1's r2: -1.89346\n",
      "[34]\ttraining's r2: -2.3585\tvalid_1's r2: -1.75704\n",
      "[35]\ttraining's r2: -2.20867\tvalid_1's r2: -1.63208\n",
      "[36]\ttraining's r2: -2.06853\tvalid_1's r2: -1.51861\n",
      "[37]\ttraining's r2: -1.93946\tvalid_1's r2: -1.40821\n",
      "[38]\ttraining's r2: -1.81773\tvalid_1's r2: -1.30484\n",
      "[39]\ttraining's r2: -1.70729\tvalid_1's r2: -1.21048\n",
      "[40]\ttraining's r2: -1.60587\tvalid_1's r2: -1.12635\n",
      "[41]\ttraining's r2: -1.51278\tvalid_1's r2: -1.0479\n",
      "[42]\ttraining's r2: -1.42673\tvalid_1's r2: -0.976382\n",
      "[43]\ttraining's r2: -1.34138\tvalid_1's r2: -0.911439\n",
      "[44]\ttraining's r2: -1.26353\tvalid_1's r2: -0.845545\n",
      "[45]\ttraining's r2: -1.18998\tvalid_1's r2: -0.785059\n",
      "[46]\ttraining's r2: -1.12223\tvalid_1's r2: -0.728466\n",
      "[47]\ttraining's r2: -1.05886\tvalid_1's r2: -0.67704\n",
      "[48]\ttraining's r2: -1.00139\tvalid_1's r2: -0.629468\n",
      "[49]\ttraining's r2: -0.944629\tvalid_1's r2: -0.582566\n",
      "[50]\ttraining's r2: -0.892878\tvalid_1's r2: -0.542922\n",
      "[51]\ttraining's r2: -0.844108\tvalid_1's r2: -0.506114\n",
      "[52]\ttraining's r2: -0.796329\tvalid_1's r2: -0.467861\n",
      "[53]\ttraining's r2: -0.752685\tvalid_1's r2: -0.433464\n",
      "[54]\ttraining's r2: -0.711113\tvalid_1's r2: -0.401629\n",
      "[55]\ttraining's r2: -0.672695\tvalid_1's r2: -0.37338\n",
      "[56]\ttraining's r2: -0.636694\tvalid_1's r2: -0.34578\n",
      "[57]\ttraining's r2: -0.600563\tvalid_1's r2: -0.319451\n",
      "[58]\ttraining's r2: -0.566656\tvalid_1's r2: -0.293046\n",
      "[59]\ttraining's r2: -0.533655\tvalid_1's r2: -0.267723\n",
      "[60]\ttraining's r2: -0.503819\tvalid_1's r2: -0.246636\n",
      "[61]\ttraining's r2: -0.474959\tvalid_1's r2: -0.224031\n",
      "[62]\ttraining's r2: -0.446366\tvalid_1's r2: -0.201159\n",
      "[63]\ttraining's r2: -0.418082\tvalid_1's r2: -0.177784\n",
      "[64]\ttraining's r2: -0.392013\tvalid_1's r2: -0.158929\n",
      "[65]\ttraining's r2: -0.366731\tvalid_1's r2: -0.137358\n",
      "[66]\ttraining's r2: -0.340955\tvalid_1's r2: -0.117979\n",
      "[67]\ttraining's r2: -0.318768\tvalid_1's r2: -0.101797\n",
      "[68]\ttraining's r2: -0.295986\tvalid_1's r2: -0.0843631\n",
      "[69]\ttraining's r2: -0.273957\tvalid_1's r2: -0.0681647\n",
      "[70]\ttraining's r2: -0.25304\tvalid_1's r2: -0.0524718\n",
      "[71]\ttraining's r2: -0.233535\tvalid_1's r2: -0.0376447\n",
      "[72]\ttraining's r2: -0.213914\tvalid_1's r2: -0.0235489\n",
      "[73]\ttraining's r2: -0.19637\tvalid_1's r2: -0.0104957\n",
      "[74]\ttraining's r2: -0.178277\tvalid_1's r2: 0.00246766\n",
      "[75]\ttraining's r2: -0.163194\tvalid_1's r2: 0.0127492\n",
      "[76]\ttraining's r2: -0.146181\tvalid_1's r2: 0.0252586\n",
      "[77]\ttraining's r2: -0.130105\tvalid_1's r2: 0.0368902\n",
      "[78]\ttraining's r2: -0.114954\tvalid_1's r2: 0.0481282\n",
      "[79]\ttraining's r2: -0.0985834\tvalid_1's r2: 0.0608763\n",
      "[80]\ttraining's r2: -0.0832394\tvalid_1's r2: 0.0718044\n",
      "[81]\ttraining's r2: -0.0695724\tvalid_1's r2: 0.0817633\n",
      "[82]\ttraining's r2: -0.0565534\tvalid_1's r2: 0.0913704\n",
      "[83]\ttraining's r2: -0.0439603\tvalid_1's r2: 0.100865\n",
      "[84]\ttraining's r2: -0.0326732\tvalid_1's r2: 0.109499\n",
      "[85]\ttraining's r2: -0.0220041\tvalid_1's r2: 0.116358\n",
      "[86]\ttraining's r2: -0.00967598\tvalid_1's r2: 0.124312\n",
      "[87]\ttraining's r2: 0.000907375\tvalid_1's r2: 0.132009\n",
      "[88]\ttraining's r2: 0.0121142\tvalid_1's r2: 0.139478\n",
      "[89]\ttraining's r2: 0.0223228\tvalid_1's r2: 0.146798\n",
      "[90]\ttraining's r2: 0.030782\tvalid_1's r2: 0.151839\n",
      "[91]\ttraining's r2: 0.0412055\tvalid_1's r2: 0.159559\n",
      "[92]\ttraining's r2: 0.0509374\tvalid_1's r2: 0.166329\n",
      "[93]\ttraining's r2: 0.0613258\tvalid_1's r2: 0.173198\n",
      "[94]\ttraining's r2: 0.0707623\tvalid_1's r2: 0.179678\n",
      "[95]\ttraining's r2: 0.0801848\tvalid_1's r2: 0.185735\n",
      "[96]\ttraining's r2: 0.0893964\tvalid_1's r2: 0.191854\n",
      "[97]\ttraining's r2: 0.0962837\tvalid_1's r2: 0.195472\n",
      "[98]\ttraining's r2: 0.104206\tvalid_1's r2: 0.200612\n",
      "[99]\ttraining's r2: 0.112207\tvalid_1's r2: 0.206081\n",
      "[100]\ttraining's r2: 0.119011\tvalid_1's r2: 0.210502\n",
      "[101]\ttraining's r2: 0.126587\tvalid_1's r2: 0.215294\n",
      "[102]\ttraining's r2: 0.134141\tvalid_1's r2: 0.220424\n",
      "[103]\ttraining's r2: 0.14149\tvalid_1's r2: 0.224531\n",
      "[104]\ttraining's r2: 0.148977\tvalid_1's r2: 0.229379\n",
      "[105]\ttraining's r2: 0.154879\tvalid_1's r2: 0.232373\n",
      "[106]\ttraining's r2: 0.161339\tvalid_1's r2: 0.235482\n",
      "[107]\ttraining's r2: 0.166475\tvalid_1's r2: 0.237807\n",
      "[108]\ttraining's r2: 0.173475\tvalid_1's r2: 0.242767\n",
      "[109]\ttraining's r2: 0.179534\tvalid_1's r2: 0.246474\n",
      "[110]\ttraining's r2: 0.185346\tvalid_1's r2: 0.249294\n",
      "[111]\ttraining's r2: 0.19063\tvalid_1's r2: 0.252202\n",
      "[112]\ttraining's r2: 0.196542\tvalid_1's r2: 0.255588\n",
      "[113]\ttraining's r2: 0.202741\tvalid_1's r2: 0.259167\n",
      "[114]\ttraining's r2: 0.208163\tvalid_1's r2: 0.263032\n",
      "[115]\ttraining's r2: 0.214008\tvalid_1's r2: 0.266312\n",
      "[116]\ttraining's r2: 0.219285\tvalid_1's r2: 0.269262\n",
      "[117]\ttraining's r2: 0.224665\tvalid_1's r2: 0.272547\n",
      "[118]\ttraining's r2: 0.229494\tvalid_1's r2: 0.275268\n",
      "[119]\ttraining's r2: 0.234794\tvalid_1's r2: 0.278667\n",
      "[120]\ttraining's r2: 0.238366\tvalid_1's r2: 0.280529\n",
      "[121]\ttraining's r2: 0.242925\tvalid_1's r2: 0.282989\n",
      "[122]\ttraining's r2: 0.248255\tvalid_1's r2: 0.286352\n",
      "[123]\ttraining's r2: 0.252152\tvalid_1's r2: 0.287641\n",
      "[124]\ttraining's r2: 0.256974\tvalid_1's r2: 0.290526\n",
      "[125]\ttraining's r2: 0.260667\tvalid_1's r2: 0.291936\n",
      "[126]\ttraining's r2: 0.265021\tvalid_1's r2: 0.294781\n",
      "[127]\ttraining's r2: 0.269797\tvalid_1's r2: 0.297457\n",
      "[128]\ttraining's r2: 0.273438\tvalid_1's r2: 0.299035\n",
      "[129]\ttraining's r2: 0.277335\tvalid_1's r2: 0.30125\n",
      "[130]\ttraining's r2: 0.280985\tvalid_1's r2: 0.303025\n",
      "[131]\ttraining's r2: 0.284527\tvalid_1's r2: 0.305132\n",
      "[132]\ttraining's r2: 0.288848\tvalid_1's r2: 0.307351\n",
      "[133]\ttraining's r2: 0.293268\tvalid_1's r2: 0.309805\n",
      "[134]\ttraining's r2: 0.296702\tvalid_1's r2: 0.311046\n",
      "[135]\ttraining's r2: 0.300088\tvalid_1's r2: 0.312018\n",
      "[136]\ttraining's r2: 0.303456\tvalid_1's r2: 0.313446\n",
      "[137]\ttraining's r2: 0.306493\tvalid_1's r2: 0.31467\n",
      "[138]\ttraining's r2: 0.309752\tvalid_1's r2: 0.315751\n",
      "[139]\ttraining's r2: 0.313579\tvalid_1's r2: 0.317557\n",
      "[140]\ttraining's r2: 0.317345\tvalid_1's r2: 0.320083\n",
      "[141]\ttraining's r2: 0.320315\tvalid_1's r2: 0.321374\n",
      "[142]\ttraining's r2: 0.323209\tvalid_1's r2: 0.321575\n",
      "[143]\ttraining's r2: 0.32685\tvalid_1's r2: 0.322907\n",
      "[144]\ttraining's r2: 0.329386\tvalid_1's r2: 0.323655\n",
      "[145]\ttraining's r2: 0.332583\tvalid_1's r2: 0.324845\n",
      "[146]\ttraining's r2: 0.335406\tvalid_1's r2: 0.325512\n",
      "[147]\ttraining's r2: 0.337742\tvalid_1's r2: 0.325856\n",
      "[148]\ttraining's r2: 0.340628\tvalid_1's r2: 0.326936\n",
      "[149]\ttraining's r2: 0.343214\tvalid_1's r2: 0.327315\n",
      "[150]\ttraining's r2: 0.345742\tvalid_1's r2: 0.32819\n",
      "[151]\ttraining's r2: 0.348791\tvalid_1's r2: 0.330083\n",
      "[152]\ttraining's r2: 0.351856\tvalid_1's r2: 0.330915\n",
      "[153]\ttraining's r2: 0.355103\tvalid_1's r2: 0.332183\n",
      "[154]\ttraining's r2: 0.357429\tvalid_1's r2: 0.333164\n",
      "[155]\ttraining's r2: 0.359922\tvalid_1's r2: 0.334149\n",
      "[156]\ttraining's r2: 0.362884\tvalid_1's r2: 0.335514\n",
      "[157]\ttraining's r2: 0.365186\tvalid_1's r2: 0.336237\n",
      "[158]\ttraining's r2: 0.367903\tvalid_1's r2: 0.33682\n",
      "[159]\ttraining's r2: 0.370411\tvalid_1's r2: 0.337861\n",
      "[160]\ttraining's r2: 0.372925\tvalid_1's r2: 0.339023\n",
      "[161]\ttraining's r2: 0.375154\tvalid_1's r2: 0.340088\n",
      "[162]\ttraining's r2: 0.377679\tvalid_1's r2: 0.341201\n",
      "[163]\ttraining's r2: 0.380211\tvalid_1's r2: 0.342342\n",
      "[164]\ttraining's r2: 0.383026\tvalid_1's r2: 0.343277\n",
      "[165]\ttraining's r2: 0.385178\tvalid_1's r2: 0.343855\n",
      "[166]\ttraining's r2: 0.387355\tvalid_1's r2: 0.344762\n",
      "[167]\ttraining's r2: 0.390006\tvalid_1's r2: 0.345608\n",
      "[168]\ttraining's r2: 0.392346\tvalid_1's r2: 0.345989\n",
      "[169]\ttraining's r2: 0.395039\tvalid_1's r2: 0.346826\n",
      "[170]\ttraining's r2: 0.396973\tvalid_1's r2: 0.346716\n",
      "[171]\ttraining's r2: 0.39912\tvalid_1's r2: 0.346994\n",
      "[172]\ttraining's r2: 0.401631\tvalid_1's r2: 0.348259\n",
      "[173]\ttraining's r2: 0.403994\tvalid_1's r2: 0.348986\n",
      "[174]\ttraining's r2: 0.406366\tvalid_1's r2: 0.349249\n",
      "[175]\ttraining's r2: 0.40804\tvalid_1's r2: 0.349554\n",
      "[176]\ttraining's r2: 0.410125\tvalid_1's r2: 0.349597\n",
      "[177]\ttraining's r2: 0.412345\tvalid_1's r2: 0.350362\n",
      "[178]\ttraining's r2: 0.414434\tvalid_1's r2: 0.351198\n",
      "[179]\ttraining's r2: 0.416523\tvalid_1's r2: 0.351336\n",
      "[180]\ttraining's r2: 0.418305\tvalid_1's r2: 0.351924\n",
      "[181]\ttraining's r2: 0.420216\tvalid_1's r2: 0.352837\n",
      "[182]\ttraining's r2: 0.422048\tvalid_1's r2: 0.352615\n",
      "[183]\ttraining's r2: 0.424364\tvalid_1's r2: 0.352901\n",
      "[184]\ttraining's r2: 0.426511\tvalid_1's r2: 0.353843\n",
      "[185]\ttraining's r2: 0.428698\tvalid_1's r2: 0.354784\n",
      "[186]\ttraining's r2: 0.430555\tvalid_1's r2: 0.355116\n",
      "[187]\ttraining's r2: 0.433019\tvalid_1's r2: 0.355918\n",
      "[188]\ttraining's r2: 0.435896\tvalid_1's r2: 0.357352\n",
      "[189]\ttraining's r2: 0.438325\tvalid_1's r2: 0.357836\n",
      "[190]\ttraining's r2: 0.440212\tvalid_1's r2: 0.358475\n",
      "[191]\ttraining's r2: 0.442149\tvalid_1's r2: 0.358308\n",
      "[192]\ttraining's r2: 0.444216\tvalid_1's r2: 0.35936\n",
      "[193]\ttraining's r2: 0.446052\tvalid_1's r2: 0.359982\n",
      "[194]\ttraining's r2: 0.448193\tvalid_1's r2: 0.361456\n",
      "[195]\ttraining's r2: 0.450159\tvalid_1's r2: 0.361713\n",
      "[196]\ttraining's r2: 0.451863\tvalid_1's r2: 0.362483\n",
      "[197]\ttraining's r2: 0.453771\tvalid_1's r2: 0.362597\n",
      "[198]\ttraining's r2: 0.455664\tvalid_1's r2: 0.363283\n",
      "[199]\ttraining's r2: 0.457791\tvalid_1's r2: 0.36397\n",
      "[200]\ttraining's r2: 0.459334\tvalid_1's r2: 0.364148\n",
      "[201]\ttraining's r2: 0.46104\tvalid_1's r2: 0.364776\n",
      "[202]\ttraining's r2: 0.463025\tvalid_1's r2: 0.365228\n",
      "[203]\ttraining's r2: 0.464654\tvalid_1's r2: 0.365351\n",
      "[204]\ttraining's r2: 0.466116\tvalid_1's r2: 0.365815\n",
      "[205]\ttraining's r2: 0.467709\tvalid_1's r2: 0.366201\n",
      "[206]\ttraining's r2: 0.469387\tvalid_1's r2: 0.366404\n",
      "[207]\ttraining's r2: 0.47095\tvalid_1's r2: 0.367365\n",
      "[208]\ttraining's r2: 0.472561\tvalid_1's r2: 0.367831\n",
      "[209]\ttraining's r2: 0.474181\tvalid_1's r2: 0.368565\n",
      "[210]\ttraining's r2: 0.475931\tvalid_1's r2: 0.369311\n",
      "[211]\ttraining's r2: 0.477607\tvalid_1's r2: 0.370094\n",
      "[212]\ttraining's r2: 0.479363\tvalid_1's r2: 0.369799\n",
      "[213]\ttraining's r2: 0.48135\tvalid_1's r2: 0.37044\n",
      "[214]\ttraining's r2: 0.483088\tvalid_1's r2: 0.370553\n",
      "[215]\ttraining's r2: 0.485041\tvalid_1's r2: 0.370489\n",
      "[216]\ttraining's r2: 0.486557\tvalid_1's r2: 0.370353\n",
      "[217]\ttraining's r2: 0.488066\tvalid_1's r2: 0.370797\n",
      "[218]\ttraining's r2: 0.489908\tvalid_1's r2: 0.371103\n",
      "[219]\ttraining's r2: 0.491302\tvalid_1's r2: 0.37198\n",
      "[220]\ttraining's r2: 0.492589\tvalid_1's r2: 0.372111\n",
      "[221]\ttraining's r2: 0.494057\tvalid_1's r2: 0.372537\n",
      "[222]\ttraining's r2: 0.495777\tvalid_1's r2: 0.372855\n",
      "[223]\ttraining's r2: 0.497626\tvalid_1's r2: 0.373141\n",
      "[224]\ttraining's r2: 0.49921\tvalid_1's r2: 0.373324\n",
      "[225]\ttraining's r2: 0.500648\tvalid_1's r2: 0.373461\n",
      "[226]\ttraining's r2: 0.502197\tvalid_1's r2: 0.373584\n",
      "[227]\ttraining's r2: 0.503623\tvalid_1's r2: 0.373834\n",
      "[228]\ttraining's r2: 0.505413\tvalid_1's r2: 0.374113\n",
      "[229]\ttraining's r2: 0.506746\tvalid_1's r2: 0.374556\n",
      "[230]\ttraining's r2: 0.507932\tvalid_1's r2: 0.37445\n",
      "[231]\ttraining's r2: 0.508979\tvalid_1's r2: 0.374732\n",
      "[232]\ttraining's r2: 0.51054\tvalid_1's r2: 0.375304\n",
      "[233]\ttraining's r2: 0.512034\tvalid_1's r2: 0.375929\n",
      "[234]\ttraining's r2: 0.513438\tvalid_1's r2: 0.376153\n",
      "[235]\ttraining's r2: 0.515439\tvalid_1's r2: 0.377234\n",
      "[236]\ttraining's r2: 0.516929\tvalid_1's r2: 0.377395\n",
      "[237]\ttraining's r2: 0.518419\tvalid_1's r2: 0.378064\n",
      "[238]\ttraining's r2: 0.519582\tvalid_1's r2: 0.377844\n",
      "[239]\ttraining's r2: 0.521172\tvalid_1's r2: 0.378058\n",
      "[240]\ttraining's r2: 0.522607\tvalid_1's r2: 0.37838\n",
      "[241]\ttraining's r2: 0.524012\tvalid_1's r2: 0.378658\n",
      "[242]\ttraining's r2: 0.525114\tvalid_1's r2: 0.379043\n",
      "[243]\ttraining's r2: 0.526483\tvalid_1's r2: 0.379567\n",
      "[244]\ttraining's r2: 0.527922\tvalid_1's r2: 0.379718\n",
      "[245]\ttraining's r2: 0.529052\tvalid_1's r2: 0.37974\n",
      "[246]\ttraining's r2: 0.53047\tvalid_1's r2: 0.379862\n",
      "[247]\ttraining's r2: 0.531853\tvalid_1's r2: 0.37987\n",
      "[248]\ttraining's r2: 0.533571\tvalid_1's r2: 0.379983\n",
      "[249]\ttraining's r2: 0.535062\tvalid_1's r2: 0.380672\n",
      "[250]\ttraining's r2: 0.53631\tvalid_1's r2: 0.380945\n",
      "[251]\ttraining's r2: 0.537501\tvalid_1's r2: 0.381246\n",
      "[252]\ttraining's r2: 0.538798\tvalid_1's r2: 0.38117\n",
      "[253]\ttraining's r2: 0.540413\tvalid_1's r2: 0.381261\n",
      "[254]\ttraining's r2: 0.541802\tvalid_1's r2: 0.3815\n",
      "[255]\ttraining's r2: 0.543065\tvalid_1's r2: 0.381902\n",
      "[256]\ttraining's r2: 0.544443\tvalid_1's r2: 0.381939\n",
      "[257]\ttraining's r2: 0.545628\tvalid_1's r2: 0.381834\n",
      "[258]\ttraining's r2: 0.546645\tvalid_1's r2: 0.381801\n",
      "[259]\ttraining's r2: 0.547889\tvalid_1's r2: 0.38186\n",
      "[260]\ttraining's r2: 0.549375\tvalid_1's r2: 0.382032\n",
      "[261]\ttraining's r2: 0.550904\tvalid_1's r2: 0.382105\n",
      "[262]\ttraining's r2: 0.552161\tvalid_1's r2: 0.38232\n",
      "[263]\ttraining's r2: 0.553335\tvalid_1's r2: 0.382393\n",
      "[264]\ttraining's r2: 0.554656\tvalid_1's r2: 0.38341\n",
      "[265]\ttraining's r2: 0.555881\tvalid_1's r2: 0.383602\n",
      "[266]\ttraining's r2: 0.55701\tvalid_1's r2: 0.383663\n",
      "[267]\ttraining's r2: 0.558281\tvalid_1's r2: 0.383888\n",
      "[268]\ttraining's r2: 0.559285\tvalid_1's r2: 0.383692\n",
      "[269]\ttraining's r2: 0.560438\tvalid_1's r2: 0.383621\n",
      "[270]\ttraining's r2: 0.561755\tvalid_1's r2: 0.383789\n",
      "[271]\ttraining's r2: 0.562997\tvalid_1's r2: 0.384007\n",
      "[272]\ttraining's r2: 0.564185\tvalid_1's r2: 0.38418\n",
      "[273]\ttraining's r2: 0.565645\tvalid_1's r2: 0.384485\n",
      "[274]\ttraining's r2: 0.566635\tvalid_1's r2: 0.384688\n",
      "[275]\ttraining's r2: 0.567947\tvalid_1's r2: 0.385052\n",
      "[276]\ttraining's r2: 0.569242\tvalid_1's r2: 0.385201\n",
      "[277]\ttraining's r2: 0.570491\tvalid_1's r2: 0.385239\n",
      "[278]\ttraining's r2: 0.571801\tvalid_1's r2: 0.385964\n",
      "[279]\ttraining's r2: 0.572666\tvalid_1's r2: 0.385806\n",
      "[280]\ttraining's r2: 0.574007\tvalid_1's r2: 0.386299\n",
      "[281]\ttraining's r2: 0.575116\tvalid_1's r2: 0.386586\n",
      "[282]\ttraining's r2: 0.576264\tvalid_1's r2: 0.386628\n",
      "[283]\ttraining's r2: 0.577197\tvalid_1's r2: 0.38642\n",
      "[284]\ttraining's r2: 0.578274\tvalid_1's r2: 0.386544\n",
      "[285]\ttraining's r2: 0.579413\tvalid_1's r2: 0.386603\n",
      "[286]\ttraining's r2: 0.580739\tvalid_1's r2: 0.386896\n",
      "[287]\ttraining's r2: 0.582012\tvalid_1's r2: 0.386741\n",
      "[288]\ttraining's r2: 0.583327\tvalid_1's r2: 0.386912\n",
      "[289]\ttraining's r2: 0.584448\tvalid_1's r2: 0.387082\n",
      "[290]\ttraining's r2: 0.58574\tvalid_1's r2: 0.387564\n",
      "[291]\ttraining's r2: 0.586976\tvalid_1's r2: 0.387571\n",
      "[292]\ttraining's r2: 0.588036\tvalid_1's r2: 0.387944\n",
      "[293]\ttraining's r2: 0.589065\tvalid_1's r2: 0.388029\n",
      "[294]\ttraining's r2: 0.590123\tvalid_1's r2: 0.388121\n",
      "[295]\ttraining's r2: 0.591315\tvalid_1's r2: 0.388339\n",
      "[296]\ttraining's r2: 0.592549\tvalid_1's r2: 0.38854\n",
      "[297]\ttraining's r2: 0.593516\tvalid_1's r2: 0.388673\n",
      "[298]\ttraining's r2: 0.594437\tvalid_1's r2: 0.388585\n",
      "[299]\ttraining's r2: 0.59534\tvalid_1's r2: 0.388187\n",
      "[300]\ttraining's r2: 0.596242\tvalid_1's r2: 0.388039\n",
      "[301]\ttraining's r2: 0.597333\tvalid_1's r2: 0.387785\n",
      "[302]\ttraining's r2: 0.598466\tvalid_1's r2: 0.38811\n",
      "[303]\ttraining's r2: 0.599447\tvalid_1's r2: 0.388228\n",
      "[304]\ttraining's r2: 0.600437\tvalid_1's r2: 0.388122\n",
      "[305]\ttraining's r2: 0.601527\tvalid_1's r2: 0.388536\n",
      "[306]\ttraining's r2: 0.602725\tvalid_1's r2: 0.388775\n",
      "[307]\ttraining's r2: 0.603582\tvalid_1's r2: 0.38884\n",
      "[308]\ttraining's r2: 0.604799\tvalid_1's r2: 0.389031\n",
      "[309]\ttraining's r2: 0.605801\tvalid_1's r2: 0.389529\n",
      "[310]\ttraining's r2: 0.606613\tvalid_1's r2: 0.389562\n",
      "[311]\ttraining's r2: 0.607533\tvalid_1's r2: 0.389897\n",
      "[312]\ttraining's r2: 0.608625\tvalid_1's r2: 0.38998\n",
      "[313]\ttraining's r2: 0.609589\tvalid_1's r2: 0.390351\n",
      "[314]\ttraining's r2: 0.610499\tvalid_1's r2: 0.390587\n",
      "[315]\ttraining's r2: 0.611676\tvalid_1's r2: 0.390869\n",
      "[316]\ttraining's r2: 0.612725\tvalid_1's r2: 0.391124\n",
      "[317]\ttraining's r2: 0.613876\tvalid_1's r2: 0.391078\n",
      "[318]\ttraining's r2: 0.61475\tvalid_1's r2: 0.391135\n",
      "[319]\ttraining's r2: 0.615666\tvalid_1's r2: 0.391566\n",
      "[320]\ttraining's r2: 0.616557\tvalid_1's r2: 0.391633\n",
      "[321]\ttraining's r2: 0.617451\tvalid_1's r2: 0.391812\n",
      "[322]\ttraining's r2: 0.618527\tvalid_1's r2: 0.392243\n",
      "[323]\ttraining's r2: 0.619524\tvalid_1's r2: 0.392484\n",
      "[324]\ttraining's r2: 0.620703\tvalid_1's r2: 0.392217\n",
      "[325]\ttraining's r2: 0.621584\tvalid_1's r2: 0.392228\n",
      "[326]\ttraining's r2: 0.622714\tvalid_1's r2: 0.392335\n",
      "[327]\ttraining's r2: 0.623503\tvalid_1's r2: 0.39258\n",
      "[328]\ttraining's r2: 0.624329\tvalid_1's r2: 0.392592\n",
      "[329]\ttraining's r2: 0.625545\tvalid_1's r2: 0.392391\n",
      "[330]\ttraining's r2: 0.626361\tvalid_1's r2: 0.392288\n",
      "[331]\ttraining's r2: 0.627414\tvalid_1's r2: 0.392457\n",
      "[332]\ttraining's r2: 0.628532\tvalid_1's r2: 0.392343\n",
      "[333]\ttraining's r2: 0.629575\tvalid_1's r2: 0.392144\n",
      "[334]\ttraining's r2: 0.630446\tvalid_1's r2: 0.392286\n",
      "[335]\ttraining's r2: 0.631497\tvalid_1's r2: 0.392409\n",
      "[336]\ttraining's r2: 0.632639\tvalid_1's r2: 0.392506\n",
      "[337]\ttraining's r2: 0.633759\tvalid_1's r2: 0.392437\n",
      "[338]\ttraining's r2: 0.634616\tvalid_1's r2: 0.392376\n",
      "[339]\ttraining's r2: 0.635711\tvalid_1's r2: 0.392374\n",
      "[340]\ttraining's r2: 0.636685\tvalid_1's r2: 0.392581\n",
      "[341]\ttraining's r2: 0.637463\tvalid_1's r2: 0.392563\n",
      "[342]\ttraining's r2: 0.638249\tvalid_1's r2: 0.392594\n",
      "[343]\ttraining's r2: 0.639067\tvalid_1's r2: 0.392567\n",
      "[344]\ttraining's r2: 0.639803\tvalid_1's r2: 0.392221\n",
      "[345]\ttraining's r2: 0.640765\tvalid_1's r2: 0.39205\n",
      "[346]\ttraining's r2: 0.641851\tvalid_1's r2: 0.39228\n",
      "[347]\ttraining's r2: 0.642785\tvalid_1's r2: 0.392575\n",
      "[348]\ttraining's r2: 0.643512\tvalid_1's r2: 0.392717\n",
      "[349]\ttraining's r2: 0.644323\tvalid_1's r2: 0.392829\n",
      "[350]\ttraining's r2: 0.64493\tvalid_1's r2: 0.392534\n",
      "[351]\ttraining's r2: 0.645711\tvalid_1's r2: 0.392598\n",
      "[352]\ttraining's r2: 0.646264\tvalid_1's r2: 0.392505\n",
      "[353]\ttraining's r2: 0.647032\tvalid_1's r2: 0.392567\n",
      "[354]\ttraining's r2: 0.647981\tvalid_1's r2: 0.392335\n",
      "[355]\ttraining's r2: 0.648921\tvalid_1's r2: 0.392357\n",
      "[356]\ttraining's r2: 0.649723\tvalid_1's r2: 0.392131\n",
      "[357]\ttraining's r2: 0.650755\tvalid_1's r2: 0.391872\n",
      "[358]\ttraining's r2: 0.651735\tvalid_1's r2: 0.392145\n",
      "[359]\ttraining's r2: 0.652485\tvalid_1's r2: 0.392264\n",
      "[360]\ttraining's r2: 0.653222\tvalid_1's r2: 0.392157\n",
      "[361]\ttraining's r2: 0.654161\tvalid_1's r2: 0.39275\n",
      "[362]\ttraining's r2: 0.655189\tvalid_1's r2: 0.39281\n",
      "[363]\ttraining's r2: 0.656069\tvalid_1's r2: 0.392702\n",
      "[364]\ttraining's r2: 0.656954\tvalid_1's r2: 0.392522\n",
      "[365]\ttraining's r2: 0.65786\tvalid_1's r2: 0.392581\n",
      "[366]\ttraining's r2: 0.658773\tvalid_1's r2: 0.392634\n",
      "[367]\ttraining's r2: 0.659623\tvalid_1's r2: 0.392596\n",
      "[368]\ttraining's r2: 0.660414\tvalid_1's r2: 0.392467\n",
      "[369]\ttraining's r2: 0.661254\tvalid_1's r2: 0.392964\n",
      "[370]\ttraining's r2: 0.662105\tvalid_1's r2: 0.393164\n",
      "[371]\ttraining's r2: 0.662992\tvalid_1's r2: 0.393343\n",
      "[372]\ttraining's r2: 0.663784\tvalid_1's r2: 0.393286\n",
      "[373]\ttraining's r2: 0.664375\tvalid_1's r2: 0.393516\n",
      "[374]\ttraining's r2: 0.665099\tvalid_1's r2: 0.393362\n",
      "[375]\ttraining's r2: 0.66586\tvalid_1's r2: 0.393489\n",
      "[376]\ttraining's r2: 0.666717\tvalid_1's r2: 0.393751\n",
      "[377]\ttraining's r2: 0.667431\tvalid_1's r2: 0.393881\n",
      "[378]\ttraining's r2: 0.668154\tvalid_1's r2: 0.393847\n",
      "[379]\ttraining's r2: 0.668766\tvalid_1's r2: 0.393571\n",
      "[380]\ttraining's r2: 0.669541\tvalid_1's r2: 0.393462\n",
      "[381]\ttraining's r2: 0.670356\tvalid_1's r2: 0.393721\n",
      "[382]\ttraining's r2: 0.671368\tvalid_1's r2: 0.39391\n",
      "[383]\ttraining's r2: 0.672066\tvalid_1's r2: 0.393725\n",
      "[384]\ttraining's r2: 0.6729\tvalid_1's r2: 0.393818\n",
      "[385]\ttraining's r2: 0.673508\tvalid_1's r2: 0.393852\n",
      "[386]\ttraining's r2: 0.674367\tvalid_1's r2: 0.394074\n",
      "[387]\ttraining's r2: 0.675038\tvalid_1's r2: 0.394383\n",
      "[388]\ttraining's r2: 0.675875\tvalid_1's r2: 0.394579\n",
      "[389]\ttraining's r2: 0.676535\tvalid_1's r2: 0.394754\n",
      "[390]\ttraining's r2: 0.677411\tvalid_1's r2: 0.394466\n",
      "[391]\ttraining's r2: 0.678289\tvalid_1's r2: 0.394848\n",
      "[392]\ttraining's r2: 0.678997\tvalid_1's r2: 0.394586\n",
      "[393]\ttraining's r2: 0.67984\tvalid_1's r2: 0.394691\n",
      "[394]\ttraining's r2: 0.6804\tvalid_1's r2: 0.394421\n",
      "[395]\ttraining's r2: 0.681116\tvalid_1's r2: 0.394274\n",
      "[396]\ttraining's r2: 0.682035\tvalid_1's r2: 0.394392\n",
      "[397]\ttraining's r2: 0.68276\tvalid_1's r2: 0.394514\n",
      "[398]\ttraining's r2: 0.683388\tvalid_1's r2: 0.394756\n",
      "[399]\ttraining's r2: 0.684212\tvalid_1's r2: 0.39497\n",
      "[400]\ttraining's r2: 0.684862\tvalid_1's r2: 0.394847\n",
      "[401]\ttraining's r2: 0.685681\tvalid_1's r2: 0.395003\n",
      "[402]\ttraining's r2: 0.686502\tvalid_1's r2: 0.39448\n",
      "[403]\ttraining's r2: 0.687163\tvalid_1's r2: 0.394423\n",
      "[404]\ttraining's r2: 0.687813\tvalid_1's r2: 0.394454\n",
      "[405]\ttraining's r2: 0.688739\tvalid_1's r2: 0.394513\n",
      "[406]\ttraining's r2: 0.689466\tvalid_1's r2: 0.394545\n",
      "[407]\ttraining's r2: 0.690368\tvalid_1's r2: 0.39492\n",
      "[408]\ttraining's r2: 0.69099\tvalid_1's r2: 0.394962\n",
      "[409]\ttraining's r2: 0.691645\tvalid_1's r2: 0.394939\n",
      "[410]\ttraining's r2: 0.692282\tvalid_1's r2: 0.395125\n",
      "[411]\ttraining's r2: 0.693143\tvalid_1's r2: 0.394825\n",
      "[412]\ttraining's r2: 0.693839\tvalid_1's r2: 0.395069\n",
      "[413]\ttraining's r2: 0.694462\tvalid_1's r2: 0.395077\n",
      "[414]\ttraining's r2: 0.695124\tvalid_1's r2: 0.3952\n",
      "[415]\ttraining's r2: 0.695828\tvalid_1's r2: 0.395068\n",
      "[416]\ttraining's r2: 0.696449\tvalid_1's r2: 0.395034\n",
      "[417]\ttraining's r2: 0.697197\tvalid_1's r2: 0.395113\n",
      "[418]\ttraining's r2: 0.69785\tvalid_1's r2: 0.394856\n",
      "[419]\ttraining's r2: 0.698782\tvalid_1's r2: 0.394956\n",
      "[420]\ttraining's r2: 0.69938\tvalid_1's r2: 0.395045\n",
      "[421]\ttraining's r2: 0.69999\tvalid_1's r2: 0.395075\n",
      "[422]\ttraining's r2: 0.700733\tvalid_1's r2: 0.395517\n",
      "[423]\ttraining's r2: 0.701342\tvalid_1's r2: 0.395332\n",
      "[424]\ttraining's r2: 0.701906\tvalid_1's r2: 0.395425\n",
      "[425]\ttraining's r2: 0.702545\tvalid_1's r2: 0.39521\n",
      "[426]\ttraining's r2: 0.703033\tvalid_1's r2: 0.395237\n",
      "[427]\ttraining's r2: 0.703738\tvalid_1's r2: 0.395089\n",
      "[428]\ttraining's r2: 0.704253\tvalid_1's r2: 0.395036\n",
      "[429]\ttraining's r2: 0.704741\tvalid_1's r2: 0.394695\n",
      "[430]\ttraining's r2: 0.705364\tvalid_1's r2: 0.394496\n",
      "[431]\ttraining's r2: 0.706048\tvalid_1's r2: 0.394618\n",
      "[432]\ttraining's r2: 0.706822\tvalid_1's r2: 0.394522\n",
      "[433]\ttraining's r2: 0.70751\tvalid_1's r2: 0.394576\n",
      "[434]\ttraining's r2: 0.708273\tvalid_1's r2: 0.394779\n",
      "[435]\ttraining's r2: 0.70891\tvalid_1's r2: 0.395071\n",
      "[436]\ttraining's r2: 0.709598\tvalid_1's r2: 0.395399\n",
      "[437]\ttraining's r2: 0.710279\tvalid_1's r2: 0.39537\n",
      "[438]\ttraining's r2: 0.710862\tvalid_1's r2: 0.395299\n",
      "[439]\ttraining's r2: 0.711527\tvalid_1's r2: 0.395184\n",
      "[440]\ttraining's r2: 0.712079\tvalid_1's r2: 0.395097\n",
      "[441]\ttraining's r2: 0.712722\tvalid_1's r2: 0.39491\n",
      "[442]\ttraining's r2: 0.713352\tvalid_1's r2: 0.395244\n",
      "[443]\ttraining's r2: 0.71398\tvalid_1's r2: 0.395\n",
      "[444]\ttraining's r2: 0.714583\tvalid_1's r2: 0.395179\n",
      "[445]\ttraining's r2: 0.715023\tvalid_1's r2: 0.394827\n",
      "[446]\ttraining's r2: 0.715575\tvalid_1's r2: 0.39491\n",
      "[447]\ttraining's r2: 0.716036\tvalid_1's r2: 0.395014\n",
      "[448]\ttraining's r2: 0.716577\tvalid_1's r2: 0.395098\n",
      "[449]\ttraining's r2: 0.717396\tvalid_1's r2: 0.395251\n",
      "[450]\ttraining's r2: 0.718098\tvalid_1's r2: 0.395509\n",
      "[451]\ttraining's r2: 0.718544\tvalid_1's r2: 0.395509\n",
      "[452]\ttraining's r2: 0.719058\tvalid_1's r2: 0.395091\n",
      "[453]\ttraining's r2: 0.719594\tvalid_1's r2: 0.394954\n",
      "[454]\ttraining's r2: 0.720337\tvalid_1's r2: 0.395042\n",
      "[455]\ttraining's r2: 0.720818\tvalid_1's r2: 0.394784\n",
      "[456]\ttraining's r2: 0.721473\tvalid_1's r2: 0.394674\n",
      "[457]\ttraining's r2: 0.722034\tvalid_1's r2: 0.394992\n",
      "[458]\ttraining's r2: 0.722653\tvalid_1's r2: 0.394769\n",
      "[459]\ttraining's r2: 0.723124\tvalid_1's r2: 0.394831\n",
      "[460]\ttraining's r2: 0.723712\tvalid_1's r2: 0.39492\n",
      "[461]\ttraining's r2: 0.724231\tvalid_1's r2: 0.394744\n",
      "[462]\ttraining's r2: 0.724851\tvalid_1's r2: 0.395142\n",
      "[463]\ttraining's r2: 0.725435\tvalid_1's r2: 0.394871\n",
      "[464]\ttraining's r2: 0.726073\tvalid_1's r2: 0.395031\n",
      "[465]\ttraining's r2: 0.726582\tvalid_1's r2: 0.395203\n",
      "[466]\ttraining's r2: 0.727355\tvalid_1's r2: 0.395239\n",
      "[467]\ttraining's r2: 0.728021\tvalid_1's r2: 0.395127\n",
      "[468]\ttraining's r2: 0.728551\tvalid_1's r2: 0.39484\n",
      "[469]\ttraining's r2: 0.729079\tvalid_1's r2: 0.394701\n",
      "[470]\ttraining's r2: 0.729612\tvalid_1's r2: 0.39451\n",
      "[471]\ttraining's r2: 0.730145\tvalid_1's r2: 0.394325\n",
      "[472]\ttraining's r2: 0.73077\tvalid_1's r2: 0.394253\n",
      "[473]\ttraining's r2: 0.731333\tvalid_1's r2: 0.394424\n",
      "[474]\ttraining's r2: 0.732058\tvalid_1's r2: 0.394458\n",
      "[475]\ttraining's r2: 0.73262\tvalid_1's r2: 0.394463\n",
      "[476]\ttraining's r2: 0.733287\tvalid_1's r2: 0.394762\n",
      "[477]\ttraining's r2: 0.733804\tvalid_1's r2: 0.395117\n",
      "[478]\ttraining's r2: 0.734364\tvalid_1's r2: 0.394988\n",
      "[479]\ttraining's r2: 0.734955\tvalid_1's r2: 0.395012\n",
      "[480]\ttraining's r2: 0.735692\tvalid_1's r2: 0.395482\n",
      "[481]\ttraining's r2: 0.736241\tvalid_1's r2: 0.395248\n",
      "[482]\ttraining's r2: 0.736749\tvalid_1's r2: 0.395261\n",
      "[483]\ttraining's r2: 0.73732\tvalid_1's r2: 0.395231\n",
      "[484]\ttraining's r2: 0.737868\tvalid_1's r2: 0.39523\n",
      "[485]\ttraining's r2: 0.738471\tvalid_1's r2: 0.395032\n",
      "[486]\ttraining's r2: 0.739065\tvalid_1's r2: 0.395094\n",
      "[487]\ttraining's r2: 0.739645\tvalid_1's r2: 0.394847\n",
      "[488]\ttraining's r2: 0.740368\tvalid_1's r2: 0.394587\n",
      "[489]\ttraining's r2: 0.740907\tvalid_1's r2: 0.394571\n",
      "[490]\ttraining's r2: 0.7414\tvalid_1's r2: 0.394587\n",
      "[491]\ttraining's r2: 0.741806\tvalid_1's r2: 0.394381\n",
      "[492]\ttraining's r2: 0.742225\tvalid_1's r2: 0.39462\n",
      "[493]\ttraining's r2: 0.742698\tvalid_1's r2: 0.394656\n",
      "[494]\ttraining's r2: 0.743124\tvalid_1's r2: 0.394654\n",
      "[495]\ttraining's r2: 0.743529\tvalid_1's r2: 0.394434\n",
      "[496]\ttraining's r2: 0.743877\tvalid_1's r2: 0.394099\n",
      "[497]\ttraining's r2: 0.744433\tvalid_1's r2: 0.393968\n",
      "[498]\ttraining's r2: 0.744787\tvalid_1's r2: 0.393602\n",
      "[499]\ttraining's r2: 0.745214\tvalid_1's r2: 0.3936\n",
      "[500]\ttraining's r2: 0.745674\tvalid_1's r2: 0.393809\n",
      "[501]\ttraining's r2: 0.746375\tvalid_1's r2: 0.393872\n",
      "[502]\ttraining's r2: 0.746794\tvalid_1's r2: 0.393824\n",
      "[503]\ttraining's r2: 0.747295\tvalid_1's r2: 0.393834\n",
      "[504]\ttraining's r2: 0.747856\tvalid_1's r2: 0.393948\n",
      "[505]\ttraining's r2: 0.748398\tvalid_1's r2: 0.39391\n",
      "[506]\ttraining's r2: 0.74894\tvalid_1's r2: 0.393892\n",
      "[507]\ttraining's r2: 0.749419\tvalid_1's r2: 0.393901\n",
      "[508]\ttraining's r2: 0.750081\tvalid_1's r2: 0.394258\n",
      "[509]\ttraining's r2: 0.750494\tvalid_1's r2: 0.394195\n",
      "[510]\ttraining's r2: 0.750865\tvalid_1's r2: 0.394165\n",
      "[511]\ttraining's r2: 0.751314\tvalid_1's r2: 0.3944\n",
      "[512]\ttraining's r2: 0.751947\tvalid_1's r2: 0.394203\n",
      "[513]\ttraining's r2: 0.752442\tvalid_1's r2: 0.39387\n",
      "[514]\ttraining's r2: 0.75304\tvalid_1's r2: 0.394036\n",
      "[515]\ttraining's r2: 0.753446\tvalid_1's r2: 0.39384\n",
      "[516]\ttraining's r2: 0.75394\tvalid_1's r2: 0.39372\n",
      "[517]\ttraining's r2: 0.754407\tvalid_1's r2: 0.393565\n",
      "[518]\ttraining's r2: 0.754836\tvalid_1's r2: 0.393572\n",
      "[519]\ttraining's r2: 0.7552\tvalid_1's r2: 0.393396\n",
      "[520]\ttraining's r2: 0.755696\tvalid_1's r2: 0.393391\n",
      "[521]\ttraining's r2: 0.756192\tvalid_1's r2: 0.39325\n",
      "[522]\ttraining's r2: 0.756674\tvalid_1's r2: 0.393285\n",
      "[523]\ttraining's r2: 0.757273\tvalid_1's r2: 0.393511\n",
      "[524]\ttraining's r2: 0.757828\tvalid_1's r2: 0.393582\n",
      "[525]\ttraining's r2: 0.75824\tvalid_1's r2: 0.393528\n",
      "[526]\ttraining's r2: 0.758698\tvalid_1's r2: 0.393479\n",
      "[527]\ttraining's r2: 0.759121\tvalid_1's r2: 0.393903\n",
      "[528]\ttraining's r2: 0.759556\tvalid_1's r2: 0.393772\n",
      "[529]\ttraining's r2: 0.759965\tvalid_1's r2: 0.394091\n",
      "[530]\ttraining's r2: 0.760548\tvalid_1's r2: 0.394155\n",
      "[531]\ttraining's r2: 0.760917\tvalid_1's r2: 0.394186\n",
      "[532]\ttraining's r2: 0.761468\tvalid_1's r2: 0.394103\n",
      "[533]\ttraining's r2: 0.761914\tvalid_1's r2: 0.393958\n",
      "[534]\ttraining's r2: 0.76235\tvalid_1's r2: 0.393724\n",
      "[535]\ttraining's r2: 0.762727\tvalid_1's r2: 0.393697\n",
      "[536]\ttraining's r2: 0.76317\tvalid_1's r2: 0.394102\n",
      "[537]\ttraining's r2: 0.763525\tvalid_1's r2: 0.394146\n",
      "[538]\ttraining's r2: 0.763903\tvalid_1's r2: 0.394003\n",
      "[539]\ttraining's r2: 0.76429\tvalid_1's r2: 0.394081\n",
      "[540]\ttraining's r2: 0.764797\tvalid_1's r2: 0.394136\n",
      "[541]\ttraining's r2: 0.765143\tvalid_1's r2: 0.394267\n",
      "[542]\ttraining's r2: 0.76559\tvalid_1's r2: 0.394018\n",
      "[543]\ttraining's r2: 0.766047\tvalid_1's r2: 0.393643\n",
      "[544]\ttraining's r2: 0.766322\tvalid_1's r2: 0.39348\n",
      "[545]\ttraining's r2: 0.766726\tvalid_1's r2: 0.393328\n",
      "[546]\ttraining's r2: 0.767196\tvalid_1's r2: 0.393201\n",
      "[547]\ttraining's r2: 0.76769\tvalid_1's r2: 0.393273\n",
      "[548]\ttraining's r2: 0.768078\tvalid_1's r2: 0.393003\n",
      "[549]\ttraining's r2: 0.768422\tvalid_1's r2: 0.393113\n",
      "[550]\ttraining's r2: 0.768872\tvalid_1's r2: 0.393026\n",
      "[551]\ttraining's r2: 0.769322\tvalid_1's r2: 0.393215\n",
      "[552]\ttraining's r2: 0.769789\tvalid_1's r2: 0.393077\n",
      "[553]\ttraining's r2: 0.770177\tvalid_1's r2: 0.392939\n",
      "[554]\ttraining's r2: 0.770566\tvalid_1's r2: 0.39256\n",
      "[555]\ttraining's r2: 0.771026\tvalid_1's r2: 0.392695\n",
      "[556]\ttraining's r2: 0.771619\tvalid_1's r2: 0.392702\n",
      "[557]\ttraining's r2: 0.772029\tvalid_1's r2: 0.392659\n",
      "[558]\ttraining's r2: 0.77248\tvalid_1's r2: 0.392786\n",
      "[559]\ttraining's r2: 0.773035\tvalid_1's r2: 0.392884\n",
      "[560]\ttraining's r2: 0.773415\tvalid_1's r2: 0.392888\n",
      "[561]\ttraining's r2: 0.773946\tvalid_1's r2: 0.39291\n",
      "[562]\ttraining's r2: 0.774351\tvalid_1's r2: 0.392998\n",
      "[563]\ttraining's r2: 0.774965\tvalid_1's r2: 0.392879\n",
      "[564]\ttraining's r2: 0.775409\tvalid_1's r2: 0.392698\n",
      "[565]\ttraining's r2: 0.775767\tvalid_1's r2: 0.392988\n",
      "[566]\ttraining's r2: 0.776243\tvalid_1's r2: 0.393209\n",
      "[567]\ttraining's r2: 0.776775\tvalid_1's r2: 0.393124\n",
      "[568]\ttraining's r2: 0.777131\tvalid_1's r2: 0.392832\n",
      "[569]\ttraining's r2: 0.777601\tvalid_1's r2: 0.392779\n",
      "[570]\ttraining's r2: 0.777951\tvalid_1's r2: 0.39288\n",
      "[571]\ttraining's r2: 0.778403\tvalid_1's r2: 0.393044\n",
      "[572]\ttraining's r2: 0.778787\tvalid_1's r2: 0.393001\n",
      "[573]\ttraining's r2: 0.779219\tvalid_1's r2: 0.392798\n",
      "[574]\ttraining's r2: 0.779599\tvalid_1's r2: 0.392683\n",
      "[575]\ttraining's r2: 0.780108\tvalid_1's r2: 0.392891\n",
      "[576]\ttraining's r2: 0.780542\tvalid_1's r2: 0.393184\n",
      "[577]\ttraining's r2: 0.781028\tvalid_1's r2: 0.393171\n",
      "[578]\ttraining's r2: 0.781445\tvalid_1's r2: 0.393405\n",
      "[579]\ttraining's r2: 0.781846\tvalid_1's r2: 0.393492\n",
      "[580]\ttraining's r2: 0.782256\tvalid_1's r2: 0.393487\n",
      "[581]\ttraining's r2: 0.782653\tvalid_1's r2: 0.393279\n",
      "[582]\ttraining's r2: 0.783115\tvalid_1's r2: 0.393244\n",
      "[583]\ttraining's r2: 0.783562\tvalid_1's r2: 0.393364\n",
      "[584]\ttraining's r2: 0.783995\tvalid_1's r2: 0.393417\n",
      "[585]\ttraining's r2: 0.784406\tvalid_1's r2: 0.393322\n",
      "[586]\ttraining's r2: 0.784779\tvalid_1's r2: 0.393258\n",
      "[587]\ttraining's r2: 0.785194\tvalid_1's r2: 0.393214\n",
      "[588]\ttraining's r2: 0.785708\tvalid_1's r2: 0.393228\n",
      "[589]\ttraining's r2: 0.786063\tvalid_1's r2: 0.393235\n",
      "[590]\ttraining's r2: 0.786471\tvalid_1's r2: 0.393399\n",
      "[591]\ttraining's r2: 0.78684\tvalid_1's r2: 0.393405\n",
      "[592]\ttraining's r2: 0.787199\tvalid_1's r2: 0.393341\n",
      "[593]\ttraining's r2: 0.787576\tvalid_1's r2: 0.393263\n",
      "[594]\ttraining's r2: 0.78798\tvalid_1's r2: 0.393334\n",
      "[595]\ttraining's r2: 0.788331\tvalid_1's r2: 0.393362\n",
      "[596]\ttraining's r2: 0.788647\tvalid_1's r2: 0.393101\n",
      "[597]\ttraining's r2: 0.789114\tvalid_1's r2: 0.393336\n",
      "[598]\ttraining's r2: 0.789594\tvalid_1's r2: 0.393403\n",
      "[599]\ttraining's r2: 0.789993\tvalid_1's r2: 0.393608\n",
      "[600]\ttraining's r2: 0.79033\tvalid_1's r2: 0.393484\n",
      "[601]\ttraining's r2: 0.790748\tvalid_1's r2: 0.393394\n",
      "[602]\ttraining's r2: 0.791165\tvalid_1's r2: 0.39335\n",
      "[603]\ttraining's r2: 0.791462\tvalid_1's r2: 0.393266\n",
      "[604]\ttraining's r2: 0.791897\tvalid_1's r2: 0.393345\n",
      "[605]\ttraining's r2: 0.792202\tvalid_1's r2: 0.393407\n",
      "[606]\ttraining's r2: 0.792562\tvalid_1's r2: 0.393459\n",
      "[607]\ttraining's r2: 0.792862\tvalid_1's r2: 0.393312\n",
      "[608]\ttraining's r2: 0.793282\tvalid_1's r2: 0.393429\n",
      "[609]\ttraining's r2: 0.793636\tvalid_1's r2: 0.39313\n",
      "[610]\ttraining's r2: 0.794013\tvalid_1's r2: 0.392941\n",
      "[611]\ttraining's r2: 0.794361\tvalid_1's r2: 0.393055\n",
      "[612]\ttraining's r2: 0.794786\tvalid_1's r2: 0.393065\n",
      "[613]\ttraining's r2: 0.795151\tvalid_1's r2: 0.392903\n",
      "[614]\ttraining's r2: 0.795571\tvalid_1's r2: 0.39289\n",
      "[615]\ttraining's r2: 0.795821\tvalid_1's r2: 0.392788\n",
      "[616]\ttraining's r2: 0.796178\tvalid_1's r2: 0.392987\n",
      "[617]\ttraining's r2: 0.7966\tvalid_1's r2: 0.393011\n",
      "[618]\ttraining's r2: 0.797011\tvalid_1's r2: 0.392968\n",
      "[619]\ttraining's r2: 0.797408\tvalid_1's r2: 0.393244\n",
      "[620]\ttraining's r2: 0.797681\tvalid_1's r2: 0.392891\n",
      "[621]\ttraining's r2: 0.798054\tvalid_1's r2: 0.39286\n",
      "[622]\ttraining's r2: 0.798407\tvalid_1's r2: 0.392987\n",
      "Early stopping, best iteration is:\n",
      "[422]\ttraining's r2: 0.700733\tvalid_1's r2: 0.395517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', colsample_bytree=0.8, learning_rate=0.03,\n",
       "       max_bin=255, max_depth=-1, min_child_samples=10, min_child_weight=3,\n",
       "       min_split_gain=0, n_estimators=3000, nthread=-1, num_leaves=31,\n",
       "       objective='regression', reg_alpha=50, reg_lambda=100, seed=420,\n",
       "       silent=True, subsample=0.8, subsample_for_bin=50000,\n",
       "       subsample_freq=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgb = lgb.LGBMRegressor(reg_alpha=50, max_depth=-1, learning_rate=0.03,\n",
    "                            num_leaves=31, colsample_bytree=0.8, min_child_weight=3,\n",
    "                            boosting_type='gbdt', max_bin=255, n_estimators=3000,\n",
    "                            subsample_for_bin=50000, objective=None, min_split_gain=0, \n",
    "                            min_child_samples=10, subsample=0.8, \n",
    "                            subsample_freq=1, reg_lambda=100, \n",
    "                            seed=420)\n",
    "model_lgb.fit(X_train, Y_train, eval_metric=r2_eval_lgb,\n",
    "                eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "                early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's r2: -3608.42\n",
      "Train until valid scores didn't improve in 200 rounds.\n",
      "[2]\tvalid_0's r2: -809.412\n",
      "[3]\tvalid_0's r2: -344.13\n",
      "[4]\tvalid_0's r2: -190.336\n",
      "[5]\tvalid_0's r2: -121.772\n",
      "[6]\tvalid_0's r2: -83.3772\n",
      "[7]\tvalid_0's r2: -60.8154\n",
      "[8]\tvalid_0's r2: -46.0876\n",
      "[9]\tvalid_0's r2: -35.927\n",
      "[10]\tvalid_0's r2: -29.0301\n",
      "[11]\tvalid_0's r2: -23.8493\n",
      "[12]\tvalid_0's r2: -19.9488\n",
      "[13]\tvalid_0's r2: -16.8934\n",
      "[14]\tvalid_0's r2: -14.486\n",
      "[15]\tvalid_0's r2: -12.5788\n",
      "[16]\tvalid_0's r2: -11.003\n",
      "[17]\tvalid_0's r2: -9.70519\n",
      "[18]\tvalid_0's r2: -8.65242\n",
      "[19]\tvalid_0's r2: -7.72626\n",
      "[20]\tvalid_0's r2: -6.92478\n",
      "[21]\tvalid_0's r2: -6.2327\n",
      "[22]\tvalid_0's r2: -5.64001\n",
      "[23]\tvalid_0's r2: -5.11913\n",
      "[24]\tvalid_0's r2: -4.65911\n",
      "[25]\tvalid_0's r2: -4.26913\n",
      "[26]\tvalid_0's r2: -3.92089\n",
      "[27]\tvalid_0's r2: -3.59667\n",
      "[28]\tvalid_0's r2: -3.30511\n",
      "[29]\tvalid_0's r2: -3.06069\n",
      "[30]\tvalid_0's r2: -2.83595\n",
      "[31]\tvalid_0's r2: -2.6334\n",
      "[32]\tvalid_0's r2: -2.45687\n",
      "[33]\tvalid_0's r2: -2.28835\n",
      "[34]\tvalid_0's r2: -2.13925\n",
      "[35]\tvalid_0's r2: -2.00136\n",
      "[36]\tvalid_0's r2: -1.87339\n",
      "[37]\tvalid_0's r2: -1.75362\n",
      "[38]\tvalid_0's r2: -1.64079\n",
      "[39]\tvalid_0's r2: -1.53927\n",
      "[40]\tvalid_0's r2: -1.44436\n",
      "[41]\tvalid_0's r2: -1.35406\n",
      "[42]\tvalid_0's r2: -1.27347\n",
      "[43]\tvalid_0's r2: -1.19843\n",
      "[44]\tvalid_0's r2: -1.12651\n",
      "[45]\tvalid_0's r2: -1.05717\n",
      "[46]\tvalid_0's r2: -0.993736\n",
      "[47]\tvalid_0's r2: -0.93718\n",
      "[48]\tvalid_0's r2: -0.881525\n",
      "[49]\tvalid_0's r2: -0.829844\n",
      "[50]\tvalid_0's r2: -0.782475\n",
      "[51]\tvalid_0's r2: -0.739568\n",
      "[52]\tvalid_0's r2: -0.695227\n",
      "[53]\tvalid_0's r2: -0.656937\n",
      "[54]\tvalid_0's r2: -0.616848\n",
      "[55]\tvalid_0's r2: -0.581408\n",
      "[56]\tvalid_0's r2: -0.547934\n",
      "[57]\tvalid_0's r2: -0.515534\n",
      "[58]\tvalid_0's r2: -0.484129\n",
      "[59]\tvalid_0's r2: -0.453189\n",
      "[60]\tvalid_0's r2: -0.42409\n",
      "[61]\tvalid_0's r2: -0.397262\n",
      "[62]\tvalid_0's r2: -0.370745\n",
      "[63]\tvalid_0's r2: -0.34648\n",
      "[64]\tvalid_0's r2: -0.321803\n",
      "[65]\tvalid_0's r2: -0.29895\n",
      "[66]\tvalid_0's r2: -0.276613\n",
      "[67]\tvalid_0's r2: -0.256757\n",
      "[68]\tvalid_0's r2: -0.235801\n",
      "[69]\tvalid_0's r2: -0.217751\n",
      "[70]\tvalid_0's r2: -0.199242\n",
      "[71]\tvalid_0's r2: -0.181648\n",
      "[72]\tvalid_0's r2: -0.165842\n",
      "[73]\tvalid_0's r2: -0.150613\n",
      "[74]\tvalid_0's r2: -0.134566\n",
      "[75]\tvalid_0's r2: -0.11996\n",
      "[76]\tvalid_0's r2: -0.106569\n",
      "[77]\tvalid_0's r2: -0.090978\n",
      "[78]\tvalid_0's r2: -0.0768513\n",
      "[79]\tvalid_0's r2: -0.0642119\n",
      "[80]\tvalid_0's r2: -0.0518167\n",
      "[81]\tvalid_0's r2: -0.0393524\n",
      "[82]\tvalid_0's r2: -0.0275853\n",
      "[83]\tvalid_0's r2: -0.0163058\n",
      "[84]\tvalid_0's r2: -0.00488369\n",
      "[85]\tvalid_0's r2: 0.00662834\n",
      "[86]\tvalid_0's r2: 0.0178022\n",
      "[87]\tvalid_0's r2: 0.0282316\n",
      "[88]\tvalid_0's r2: 0.0377355\n",
      "[89]\tvalid_0's r2: 0.0472781\n",
      "[90]\tvalid_0's r2: 0.0558643\n",
      "[91]\tvalid_0's r2: 0.0652324\n",
      "[92]\tvalid_0's r2: 0.0749491\n",
      "[93]\tvalid_0's r2: 0.0834339\n",
      "[94]\tvalid_0's r2: 0.0922205\n",
      "[95]\tvalid_0's r2: 0.100257\n",
      "[96]\tvalid_0's r2: 0.108937\n",
      "[97]\tvalid_0's r2: 0.115222\n",
      "[98]\tvalid_0's r2: 0.122894\n",
      "[99]\tvalid_0's r2: 0.130104\n",
      "[100]\tvalid_0's r2: 0.137219\n",
      "[101]\tvalid_0's r2: 0.143707\n",
      "[102]\tvalid_0's r2: 0.150629\n",
      "[103]\tvalid_0's r2: 0.15765\n",
      "[104]\tvalid_0's r2: 0.163979\n",
      "[105]\tvalid_0's r2: 0.168764\n",
      "[106]\tvalid_0's r2: 0.175053\n",
      "[107]\tvalid_0's r2: 0.179968\n",
      "[108]\tvalid_0's r2: 0.185549\n",
      "[109]\tvalid_0's r2: 0.191233\n",
      "[110]\tvalid_0's r2: 0.196861\n",
      "[111]\tvalid_0's r2: 0.202401\n",
      "[112]\tvalid_0's r2: 0.208183\n",
      "[113]\tvalid_0's r2: 0.213625\n",
      "[114]\tvalid_0's r2: 0.218588\n",
      "[115]\tvalid_0's r2: 0.222558\n",
      "[116]\tvalid_0's r2: 0.226969\n",
      "[117]\tvalid_0's r2: 0.232136\n",
      "[118]\tvalid_0's r2: 0.236916\n",
      "[119]\tvalid_0's r2: 0.241712\n",
      "[120]\tvalid_0's r2: 0.246391\n",
      "[121]\tvalid_0's r2: 0.250278\n",
      "[122]\tvalid_0's r2: 0.254324\n",
      "[123]\tvalid_0's r2: 0.25859\n",
      "[124]\tvalid_0's r2: 0.261722\n",
      "[125]\tvalid_0's r2: 0.264905\n",
      "[126]\tvalid_0's r2: 0.268339\n",
      "[127]\tvalid_0's r2: 0.271876\n",
      "[128]\tvalid_0's r2: 0.274955\n",
      "[129]\tvalid_0's r2: 0.277852\n",
      "[130]\tvalid_0's r2: 0.281844\n",
      "[131]\tvalid_0's r2: 0.2851\n",
      "[132]\tvalid_0's r2: 0.288276\n",
      "[133]\tvalid_0's r2: 0.291031\n",
      "[134]\tvalid_0's r2: 0.294422\n",
      "[135]\tvalid_0's r2: 0.29792\n",
      "[136]\tvalid_0's r2: 0.300366\n",
      "[137]\tvalid_0's r2: 0.303696\n",
      "[138]\tvalid_0's r2: 0.306137\n",
      "[139]\tvalid_0's r2: 0.308556\n",
      "[140]\tvalid_0's r2: 0.31245\n",
      "[141]\tvalid_0's r2: 0.315156\n",
      "[142]\tvalid_0's r2: 0.317665\n",
      "[143]\tvalid_0's r2: 0.320911\n",
      "[144]\tvalid_0's r2: 0.32386\n",
      "[145]\tvalid_0's r2: 0.326425\n",
      "[146]\tvalid_0's r2: 0.328833\n",
      "[147]\tvalid_0's r2: 0.331796\n",
      "[148]\tvalid_0's r2: 0.334257\n",
      "[149]\tvalid_0's r2: 0.336495\n",
      "[150]\tvalid_0's r2: 0.339284\n",
      "[151]\tvalid_0's r2: 0.3416\n",
      "[152]\tvalid_0's r2: 0.343825\n",
      "[153]\tvalid_0's r2: 0.346327\n",
      "[154]\tvalid_0's r2: 0.349147\n",
      "[155]\tvalid_0's r2: 0.351567\n",
      "[156]\tvalid_0's r2: 0.354177\n",
      "[157]\tvalid_0's r2: 0.356717\n",
      "[158]\tvalid_0's r2: 0.35899\n",
      "[159]\tvalid_0's r2: 0.361418\n",
      "[160]\tvalid_0's r2: 0.36359\n",
      "[161]\tvalid_0's r2: 0.366357\n",
      "[162]\tvalid_0's r2: 0.368784\n",
      "[163]\tvalid_0's r2: 0.3713\n",
      "[164]\tvalid_0's r2: 0.372986\n",
      "[165]\tvalid_0's r2: 0.375145\n",
      "[166]\tvalid_0's r2: 0.377012\n",
      "[167]\tvalid_0's r2: 0.3793\n",
      "[168]\tvalid_0's r2: 0.38181\n",
      "[169]\tvalid_0's r2: 0.383951\n",
      "[170]\tvalid_0's r2: 0.386137\n",
      "[171]\tvalid_0's r2: 0.388276\n",
      "[172]\tvalid_0's r2: 0.390289\n",
      "[173]\tvalid_0's r2: 0.392347\n",
      "[174]\tvalid_0's r2: 0.39419\n",
      "[175]\tvalid_0's r2: 0.39583\n",
      "[176]\tvalid_0's r2: 0.397856\n",
      "[177]\tvalid_0's r2: 0.399792\n",
      "[178]\tvalid_0's r2: 0.401784\n",
      "[179]\tvalid_0's r2: 0.403889\n",
      "[180]\tvalid_0's r2: 0.406191\n",
      "[181]\tvalid_0's r2: 0.40833\n",
      "[182]\tvalid_0's r2: 0.410687\n",
      "[183]\tvalid_0's r2: 0.412463\n",
      "[184]\tvalid_0's r2: 0.413961\n",
      "[185]\tvalid_0's r2: 0.415734\n",
      "[186]\tvalid_0's r2: 0.417995\n",
      "[187]\tvalid_0's r2: 0.419726\n",
      "[188]\tvalid_0's r2: 0.421349\n",
      "[189]\tvalid_0's r2: 0.423012\n",
      "[190]\tvalid_0's r2: 0.424632\n",
      "[191]\tvalid_0's r2: 0.426258\n",
      "[192]\tvalid_0's r2: 0.427712\n",
      "[193]\tvalid_0's r2: 0.429487\n",
      "[194]\tvalid_0's r2: 0.431324\n",
      "[195]\tvalid_0's r2: 0.433048\n",
      "[196]\tvalid_0's r2: 0.434341\n",
      "[197]\tvalid_0's r2: 0.435992\n",
      "[198]\tvalid_0's r2: 0.437603\n",
      "[199]\tvalid_0's r2: 0.439275\n",
      "[200]\tvalid_0's r2: 0.440748\n",
      "[201]\tvalid_0's r2: 0.442208\n",
      "[202]\tvalid_0's r2: 0.444034\n",
      "[203]\tvalid_0's r2: 0.445584\n",
      "[204]\tvalid_0's r2: 0.447076\n",
      "[205]\tvalid_0's r2: 0.448889\n",
      "[206]\tvalid_0's r2: 0.450791\n",
      "[207]\tvalid_0's r2: 0.452428\n",
      "[208]\tvalid_0's r2: 0.45405\n",
      "[209]\tvalid_0's r2: 0.455352\n",
      "[210]\tvalid_0's r2: 0.456918\n",
      "[211]\tvalid_0's r2: 0.458589\n",
      "[212]\tvalid_0's r2: 0.460048\n",
      "[213]\tvalid_0's r2: 0.461512\n",
      "[214]\tvalid_0's r2: 0.462855\n",
      "[215]\tvalid_0's r2: 0.464299\n",
      "[216]\tvalid_0's r2: 0.466125\n",
      "[217]\tvalid_0's r2: 0.467467\n",
      "[218]\tvalid_0's r2: 0.468968\n",
      "[219]\tvalid_0's r2: 0.470332\n",
      "[220]\tvalid_0's r2: 0.471379\n",
      "[221]\tvalid_0's r2: 0.472839\n",
      "[222]\tvalid_0's r2: 0.474192\n",
      "[223]\tvalid_0's r2: 0.475459\n",
      "[224]\tvalid_0's r2: 0.4768\n",
      "[225]\tvalid_0's r2: 0.478552\n",
      "[226]\tvalid_0's r2: 0.480195\n",
      "[227]\tvalid_0's r2: 0.481688\n",
      "[228]\tvalid_0's r2: 0.482838\n",
      "[229]\tvalid_0's r2: 0.484171\n",
      "[230]\tvalid_0's r2: 0.485625\n",
      "[231]\tvalid_0's r2: 0.486713\n",
      "[232]\tvalid_0's r2: 0.488346\n",
      "[233]\tvalid_0's r2: 0.48965\n",
      "[234]\tvalid_0's r2: 0.491085\n",
      "[235]\tvalid_0's r2: 0.492321\n",
      "[236]\tvalid_0's r2: 0.493272\n",
      "[237]\tvalid_0's r2: 0.494338\n",
      "[238]\tvalid_0's r2: 0.495472\n",
      "[239]\tvalid_0's r2: 0.49658\n",
      "[240]\tvalid_0's r2: 0.497696\n",
      "[241]\tvalid_0's r2: 0.499166\n",
      "[242]\tvalid_0's r2: 0.500125\n",
      "[243]\tvalid_0's r2: 0.501587\n",
      "[244]\tvalid_0's r2: 0.503043\n",
      "[245]\tvalid_0's r2: 0.504329\n",
      "[246]\tvalid_0's r2: 0.505555\n",
      "[247]\tvalid_0's r2: 0.507117\n",
      "[248]\tvalid_0's r2: 0.508252\n",
      "[249]\tvalid_0's r2: 0.509282\n",
      "[250]\tvalid_0's r2: 0.510776\n",
      "[251]\tvalid_0's r2: 0.512132\n",
      "[252]\tvalid_0's r2: 0.513372\n",
      "[253]\tvalid_0's r2: 0.514814\n",
      "[254]\tvalid_0's r2: 0.515928\n",
      "[255]\tvalid_0's r2: 0.517253\n",
      "[256]\tvalid_0's r2: 0.518453\n",
      "[257]\tvalid_0's r2: 0.519705\n",
      "[258]\tvalid_0's r2: 0.521218\n",
      "[259]\tvalid_0's r2: 0.522568\n",
      "[260]\tvalid_0's r2: 0.523792\n",
      "[261]\tvalid_0's r2: 0.524916\n",
      "[262]\tvalid_0's r2: 0.525858\n",
      "[263]\tvalid_0's r2: 0.527148\n",
      "[264]\tvalid_0's r2: 0.528432\n",
      "[265]\tvalid_0's r2: 0.530025\n",
      "[266]\tvalid_0's r2: 0.531096\n",
      "[267]\tvalid_0's r2: 0.532397\n",
      "[268]\tvalid_0's r2: 0.533739\n",
      "[269]\tvalid_0's r2: 0.534706\n",
      "[270]\tvalid_0's r2: 0.535888\n",
      "[271]\tvalid_0's r2: 0.537016\n",
      "[272]\tvalid_0's r2: 0.538231\n",
      "[273]\tvalid_0's r2: 0.539292\n",
      "[274]\tvalid_0's r2: 0.540301\n",
      "[275]\tvalid_0's r2: 0.541351\n",
      "[276]\tvalid_0's r2: 0.542323\n",
      "[277]\tvalid_0's r2: 0.543508\n",
      "[278]\tvalid_0's r2: 0.544581\n",
      "[279]\tvalid_0's r2: 0.545843\n",
      "[280]\tvalid_0's r2: 0.547073\n",
      "[281]\tvalid_0's r2: 0.547916\n",
      "[282]\tvalid_0's r2: 0.549275\n",
      "[283]\tvalid_0's r2: 0.550348\n",
      "[284]\tvalid_0's r2: 0.551319\n",
      "[285]\tvalid_0's r2: 0.552673\n",
      "[286]\tvalid_0's r2: 0.55376\n",
      "[287]\tvalid_0's r2: 0.554777\n",
      "[288]\tvalid_0's r2: 0.555758\n",
      "[289]\tvalid_0's r2: 0.556755\n",
      "[290]\tvalid_0's r2: 0.557928\n",
      "[291]\tvalid_0's r2: 0.558985\n",
      "[292]\tvalid_0's r2: 0.560032\n",
      "[293]\tvalid_0's r2: 0.561\n",
      "[294]\tvalid_0's r2: 0.56184\n",
      "[295]\tvalid_0's r2: 0.56306\n",
      "[296]\tvalid_0's r2: 0.563908\n",
      "[297]\tvalid_0's r2: 0.564955\n",
      "[298]\tvalid_0's r2: 0.56601\n",
      "[299]\tvalid_0's r2: 0.567194\n",
      "[300]\tvalid_0's r2: 0.568179\n",
      "[301]\tvalid_0's r2: 0.56918\n",
      "[302]\tvalid_0's r2: 0.570193\n",
      "[303]\tvalid_0's r2: 0.571211\n",
      "[304]\tvalid_0's r2: 0.572099\n",
      "[305]\tvalid_0's r2: 0.573367\n",
      "[306]\tvalid_0's r2: 0.57441\n",
      "[307]\tvalid_0's r2: 0.575475\n",
      "[308]\tvalid_0's r2: 0.576517\n",
      "[309]\tvalid_0's r2: 0.57761\n",
      "[310]\tvalid_0's r2: 0.578405\n",
      "[311]\tvalid_0's r2: 0.579497\n",
      "[312]\tvalid_0's r2: 0.580293\n",
      "[313]\tvalid_0's r2: 0.581198\n",
      "[314]\tvalid_0's r2: 0.582062\n",
      "[315]\tvalid_0's r2: 0.583074\n",
      "[316]\tvalid_0's r2: 0.583924\n",
      "[317]\tvalid_0's r2: 0.584786\n",
      "[318]\tvalid_0's r2: 0.585631\n",
      "[319]\tvalid_0's r2: 0.586776\n",
      "[320]\tvalid_0's r2: 0.587513\n",
      "[321]\tvalid_0's r2: 0.588519\n",
      "[322]\tvalid_0's r2: 0.589423\n",
      "[323]\tvalid_0's r2: 0.590478\n",
      "[324]\tvalid_0's r2: 0.591439\n",
      "[325]\tvalid_0's r2: 0.59241\n",
      "[326]\tvalid_0's r2: 0.59353\n",
      "[327]\tvalid_0's r2: 0.594326\n",
      "[328]\tvalid_0's r2: 0.595304\n",
      "[329]\tvalid_0's r2: 0.59615\n",
      "[330]\tvalid_0's r2: 0.597186\n",
      "[331]\tvalid_0's r2: 0.598071\n",
      "[332]\tvalid_0's r2: 0.599128\n",
      "[333]\tvalid_0's r2: 0.600037\n",
      "[334]\tvalid_0's r2: 0.600912\n",
      "[335]\tvalid_0's r2: 0.601704\n",
      "[336]\tvalid_0's r2: 0.602601\n",
      "[337]\tvalid_0's r2: 0.603631\n",
      "[338]\tvalid_0's r2: 0.604437\n",
      "[339]\tvalid_0's r2: 0.60547\n",
      "[340]\tvalid_0's r2: 0.60631\n",
      "[341]\tvalid_0's r2: 0.606807\n",
      "[342]\tvalid_0's r2: 0.607839\n",
      "[343]\tvalid_0's r2: 0.608734\n",
      "[344]\tvalid_0's r2: 0.609807\n",
      "[345]\tvalid_0's r2: 0.610593\n",
      "[346]\tvalid_0's r2: 0.611373\n",
      "[347]\tvalid_0's r2: 0.612268\n",
      "[348]\tvalid_0's r2: 0.613036\n",
      "[349]\tvalid_0's r2: 0.613765\n",
      "[350]\tvalid_0's r2: 0.614655\n",
      "[351]\tvalid_0's r2: 0.615539\n",
      "[352]\tvalid_0's r2: 0.616473\n",
      "[353]\tvalid_0's r2: 0.617334\n",
      "[354]\tvalid_0's r2: 0.618256\n",
      "[355]\tvalid_0's r2: 0.618828\n",
      "[356]\tvalid_0's r2: 0.61962\n",
      "[357]\tvalid_0's r2: 0.62041\n",
      "[358]\tvalid_0's r2: 0.621202\n",
      "[359]\tvalid_0's r2: 0.622084\n",
      "[360]\tvalid_0's r2: 0.62288\n",
      "[361]\tvalid_0's r2: 0.62387\n",
      "[362]\tvalid_0's r2: 0.624659\n",
      "[363]\tvalid_0's r2: 0.625385\n",
      "[364]\tvalid_0's r2: 0.626074\n",
      "[365]\tvalid_0's r2: 0.627028\n",
      "[366]\tvalid_0's r2: 0.627756\n",
      "[367]\tvalid_0's r2: 0.628411\n",
      "[368]\tvalid_0's r2: 0.629273\n",
      "[369]\tvalid_0's r2: 0.630141\n",
      "[370]\tvalid_0's r2: 0.631006\n",
      "[371]\tvalid_0's r2: 0.631768\n",
      "[372]\tvalid_0's r2: 0.632358\n",
      "[373]\tvalid_0's r2: 0.633135\n",
      "[374]\tvalid_0's r2: 0.63388\n",
      "[375]\tvalid_0's r2: 0.634608\n",
      "[376]\tvalid_0's r2: 0.635662\n",
      "[377]\tvalid_0's r2: 0.636515\n",
      "[378]\tvalid_0's r2: 0.637315\n",
      "[379]\tvalid_0's r2: 0.638117\n",
      "[380]\tvalid_0's r2: 0.638805\n",
      "[381]\tvalid_0's r2: 0.639624\n",
      "[382]\tvalid_0's r2: 0.640691\n",
      "[383]\tvalid_0's r2: 0.641442\n",
      "[384]\tvalid_0's r2: 0.642113\n",
      "[385]\tvalid_0's r2: 0.642926\n",
      "[386]\tvalid_0's r2: 0.643655\n",
      "[387]\tvalid_0's r2: 0.644187\n",
      "[388]\tvalid_0's r2: 0.644857\n",
      "[389]\tvalid_0's r2: 0.645575\n",
      "[390]\tvalid_0's r2: 0.646263\n",
      "[391]\tvalid_0's r2: 0.646733\n",
      "[392]\tvalid_0's r2: 0.647329\n",
      "[393]\tvalid_0's r2: 0.647869\n",
      "[394]\tvalid_0's r2: 0.648514\n",
      "[395]\tvalid_0's r2: 0.649168\n",
      "[396]\tvalid_0's r2: 0.649784\n",
      "[397]\tvalid_0's r2: 0.650306\n",
      "[398]\tvalid_0's r2: 0.650975\n",
      "[399]\tvalid_0's r2: 0.651616\n",
      "[400]\tvalid_0's r2: 0.652271\n",
      "[401]\tvalid_0's r2: 0.652967\n",
      "[402]\tvalid_0's r2: 0.653638\n",
      "[403]\tvalid_0's r2: 0.654322\n",
      "[404]\tvalid_0's r2: 0.655066\n",
      "[405]\tvalid_0's r2: 0.655823\n",
      "[406]\tvalid_0's r2: 0.656455\n",
      "[407]\tvalid_0's r2: 0.657218\n",
      "[408]\tvalid_0's r2: 0.657836\n",
      "[409]\tvalid_0's r2: 0.658488\n",
      "[410]\tvalid_0's r2: 0.659212\n",
      "[411]\tvalid_0's r2: 0.659749\n",
      "[412]\tvalid_0's r2: 0.660269\n",
      "[413]\tvalid_0's r2: 0.661043\n",
      "[414]\tvalid_0's r2: 0.661639\n",
      "[415]\tvalid_0's r2: 0.662427\n",
      "[416]\tvalid_0's r2: 0.663078\n",
      "[417]\tvalid_0's r2: 0.663652\n",
      "[418]\tvalid_0's r2: 0.664213\n",
      "[419]\tvalid_0's r2: 0.664845\n",
      "[420]\tvalid_0's r2: 0.665384\n",
      "[421]\tvalid_0's r2: 0.666107\n",
      "[422]\tvalid_0's r2: 0.666777\n",
      "[423]\tvalid_0's r2: 0.667382\n",
      "[424]\tvalid_0's r2: 0.668085\n",
      "[425]\tvalid_0's r2: 0.668729\n",
      "[426]\tvalid_0's r2: 0.669395\n",
      "[427]\tvalid_0's r2: 0.670116\n",
      "[428]\tvalid_0's r2: 0.670738\n",
      "[429]\tvalid_0's r2: 0.67146\n",
      "[430]\tvalid_0's r2: 0.672179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', colsample_bytree=0.8, learning_rate=0.03,\n",
       "       max_bin=255, max_depth=-1, min_child_samples=10, min_child_weight=3,\n",
       "       min_split_gain=0, n_estimators=430, nthread=-1, num_leaves=31,\n",
       "       objective='regression', reg_alpha=50, reg_lambda=100, seed=420,\n",
       "       silent=True, subsample=0.8, subsample_for_bin=50000,\n",
       "       subsample_freq=1)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgb = lgb.LGBMRegressor(reg_alpha=50, max_depth=-1, learning_rate=0.03,\n",
    "                            num_leaves=31, colsample_bytree=0.8, min_child_weight=3,\n",
    "                            boosting_type='gbdt', max_bin=255, n_estimators=430,\n",
    "                            subsample_for_bin=50000, objective=None, min_split_gain=0, \n",
    "                            min_child_samples=10, subsample=0.8, \n",
    "                            subsample_freq=1, reg_lambda=100, \n",
    "                            seed=420)\n",
    "model_lgb.fit(data_train, target, eval_metric=r2_eval_lgb,\n",
    "                eval_set=[(X_train, Y_train)],\n",
    "                early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5.1 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def r2_eval_xgb(Y_pred, Y_true):\n",
    "    return 'r2', -r2_score(Y_true.get_label(), Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feature_importances_(booster):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        feature_importances_ : array of shape = [n_features]\n",
    "\n",
    "        \"\"\"\n",
    "        b = booster\n",
    "        fs = b.get_fscore()\n",
    "        all_features = [fs.get(f, 0.) for f in b.feature_names]\n",
    "        all_features = np.array(all_features, dtype=np.float32)\n",
    "        return all_features / all_features.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "583e9496bde1e1bf354edb49801f5f1c1de252d4",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.1.1 Model K-Fold Validation for initial exploration and performance checking¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=6, shuffle=True, random_state=420)\n",
    "\n",
    "models_xgb = {}\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(data_train, target):\n",
    "    X_train = data_train[train_index]\n",
    "    X_val = data_train[test_index]\n",
    "    Y_train = target[train_index]\n",
    "    Y_val = target[test_index]\n",
    "    \n",
    "    models_xgb[i] = XGBRegressor(max_depth=10, learning_rate=0.03, n_estimators=3000,\n",
    "                                      subsample=0.8, colsample_bytree=0.8, \n",
    "                                      seed=420)\n",
    "    models_xgb[i].fit(X_train, Y_train, eval_metric=r2_eval_xgb,\n",
    "                      eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "                     early_stopping_rounds=100)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(data.columns.values, feature_importances_(models_xgb[0].booster())), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.1.2 Grid Search Parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] reg_lambda=100, reg_alpha=50 ....................................\n",
      "[CV] ........... reg_lambda=100, reg_alpha=50, score=0.623537 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n",
      "[CV] reg_lambda=100, reg_alpha=50 ....................................\n",
      "[CV] ........... reg_lambda=100, reg_alpha=50, score=0.588549 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    2.9s remaining:    0.0s\n",
      "[CV] reg_lambda=100, reg_alpha=50 ....................................\n",
      "[CV] ........... reg_lambda=100, reg_alpha=50, score=0.473603 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    4.4s remaining:    0.0s\n",
      "[CV] reg_lambda=500, reg_alpha=50 ....................................\n",
      "[CV] ........... reg_lambda=500, reg_alpha=50, score=0.623320 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    5.8s remaining:    0.0s\n",
      "[CV] reg_lambda=500, reg_alpha=50 ....................................\n",
      "[CV] ........... reg_lambda=500, reg_alpha=50, score=0.585233 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    7.3s remaining:    0.0s\n",
      "[CV] reg_lambda=500, reg_alpha=50 ....................................\n",
      "[CV] ........... reg_lambda=500, reg_alpha=50, score=0.475276 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    8.7s remaining:    0.0s\n",
      "[CV] reg_lambda=1000, reg_alpha=50 ...................................\n",
      "[CV] .......... reg_lambda=1000, reg_alpha=50, score=0.620525 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   10.2s remaining:    0.0s\n",
      "[CV] reg_lambda=1000, reg_alpha=50 ...................................\n",
      "[CV] .......... reg_lambda=1000, reg_alpha=50, score=0.581218 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   11.6s remaining:    0.0s\n",
      "[CV] reg_lambda=1000, reg_alpha=50 ...................................\n",
      "[CV] .......... reg_lambda=1000, reg_alpha=50, score=0.473268 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   13.0s remaining:    0.0s\n",
      "[CV] reg_lambda=100, reg_alpha=500 ...................................\n",
      "[CV] .......... reg_lambda=100, reg_alpha=500, score=0.621573 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   14.0s remaining:    0.0s\n",
      "[CV] reg_lambda=100, reg_alpha=500 ...................................\n",
      "[CV] .......... reg_lambda=100, reg_alpha=500, score=0.577974 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:   15.0s remaining:    0.0s\n",
      "[CV] reg_lambda=100, reg_alpha=500 ...................................\n",
      "[CV] .......... reg_lambda=100, reg_alpha=500, score=0.473211 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:   16.0s remaining:    0.0s\n",
      "[CV] reg_lambda=500, reg_alpha=500 ...................................\n",
      "[CV] .......... reg_lambda=500, reg_alpha=500, score=0.618179 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:   17.4s remaining:    0.0s\n",
      "[CV] reg_lambda=500, reg_alpha=500 ...................................\n",
      "[CV] .......... reg_lambda=500, reg_alpha=500, score=0.576395 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:   18.7s remaining:    0.0s\n",
      "[CV] reg_lambda=500, reg_alpha=500 ...................................\n",
      "[CV] .......... reg_lambda=500, reg_alpha=500, score=0.470514 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   20.1s remaining:    0.0s\n",
      "[CV] reg_lambda=1000, reg_alpha=500 ..................................\n",
      "[CV] ......... reg_lambda=1000, reg_alpha=500, score=0.610732 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:   21.6s remaining:    0.0s\n",
      "[CV] reg_lambda=1000, reg_alpha=500 ..................................\n",
      "[CV] ......... reg_lambda=1000, reg_alpha=500, score=0.568723 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:   23.0s remaining:    0.0s\n",
      "[CV] reg_lambda=1000, reg_alpha=500 ..................................\n",
      "[CV] ......... reg_lambda=1000, reg_alpha=500, score=0.466863 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:   24.4s remaining:    0.0s\n",
      "[CV] reg_lambda=100, reg_alpha=1000 ..................................\n",
      "[CV] ......... reg_lambda=100, reg_alpha=1000, score=0.590092 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:   25.3s remaining:    0.0s\n",
      "[CV] reg_lambda=100, reg_alpha=1000 ..................................\n",
      "[CV] ......... reg_lambda=100, reg_alpha=1000, score=0.549481 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   26.2s remaining:    0.0s\n",
      "[CV] reg_lambda=100, reg_alpha=1000 ..................................\n",
      "[CV] ......... reg_lambda=100, reg_alpha=1000, score=0.453053 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:   27.2s remaining:    0.0s\n",
      "[CV] reg_lambda=500, reg_alpha=1000 ..................................\n",
      "[CV] ......... reg_lambda=500, reg_alpha=1000, score=0.593399 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:   28.4s remaining:    0.0s\n",
      "[CV] reg_lambda=500, reg_alpha=1000 ..................................\n",
      "[CV] ......... reg_lambda=500, reg_alpha=1000, score=0.551136 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:   29.7s remaining:    0.0s\n",
      "[CV] reg_lambda=500, reg_alpha=1000 ..................................\n",
      "[CV] ......... reg_lambda=500, reg_alpha=1000, score=0.455336 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   31.1s remaining:    0.0s\n",
      "[CV] reg_lambda=1000, reg_alpha=1000 .................................\n",
      "[CV] ........ reg_lambda=1000, reg_alpha=1000, score=0.587807 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:   32.5s remaining:    0.0s\n",
      "[CV] reg_lambda=1000, reg_alpha=1000 .................................\n",
      "[CV] ........ reg_lambda=1000, reg_alpha=1000, score=0.546435 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:   34.0s remaining:    0.0s\n",
      "[CV] reg_lambda=1000, reg_alpha=1000 .................................\n",
      "[CV] ........ reg_lambda=1000, reg_alpha=1000, score=0.452487 -   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:   35.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:   35.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=[(array([   0,    2, ..., 4205, 4207]), array([   1,    7, ..., 4206, 4208])), (array([   0,    1, ..., 4207, 4208]), array([   4,    6, ..., 4202, 4205])), (array([   1,    4, ..., 4206, 4208]), array([   0,    2, ..., 4204, 4207]))],\n",
       "       error_score='raise',\n",
       "       estimator=XGBRegressor(base_score=100.66931812782134, colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=5, missing=None, n_estimators=200,\n",
       "       nthread=-1, objective='reg:linear', reg_alpha=50, reg_lambda=100,\n",
       "       scale_pos_weight=1, seed=420, silent=True, subsample=1),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'reg_lambda': [100, 500, 1000], 'reg_alpha': [50, 500, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='r2', verbose=50)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model = XGBRegressor(subsample=1, colsample_bytree=0.8, min_child_weight=5,\n",
    "                          reg_alpha=50, reg_lambda=100,\n",
    "                          n_estimators=200, base_score=target.mean(), seed=420)\n",
    "grid_params = {\n",
    "    #'min_child_weight': [1,3,5,7],\n",
    "    #'subsample': [0.6,0.7,0.8,0.9,1],\n",
    "    #'colsample_bytree': [0.6,0.7,0.8,0.9,1],\n",
    "    'reg_alpha': [50, 500,1000],#[0,1,10,50,100],\n",
    "    'reg_lambda': [100, 500,1000]#[0,1,10,50,100],\n",
    "    #'max_depth':[4,6,8,10],\n",
    "    #'learning_rate':[0.1,0.05,0.03,0.01,0.005,0.001],\n",
    "}\n",
    "grid_cv = list(KFold(n_splits=3, shuffle=True, random_state=420).split(data_train, target))\n",
    "\n",
    "grid = GridSearchCV(grid_model, grid_params, scoring='r2',\n",
    "                    cv=grid_cv, verbose=50)\n",
    "grid.fit(data_train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reg_alpha': 50, 'reg_lambda': 100}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.1.3 Find Early Stopping Round with More Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "1389467a21df5e49d7fc27ea2e579a39e3d7bbb9",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "seed = 420\n",
    "test_size = 0.2\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data_train, target, test_size=test_size, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_execution_state": "busy",
    "_uuid": "de11a09bc5b54875c85e297eeacd5e43db45fff9",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:12.6163\tvalidation_1-rmse:12.0573\tvalidation_0-r2:-0.026125\tvalidation_1-r2:-0.030071\n",
      "Multiple eval metrics have been passed: 'validation_1-r2' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-r2 hasn't improved in 200 rounds.\n",
      "[1]\tvalidation_0-rmse:12.4546\tvalidation_1-rmse:11.878\tvalidation_0-r2:-0.050927\tvalidation_1-r2:-0.058689\n",
      "[2]\tvalidation_0-rmse:12.2986\tvalidation_1-rmse:11.7049\tvalidation_0-r2:-0.074546\tvalidation_1-r2:-0.085927\n",
      "[3]\tvalidation_0-rmse:12.1512\tvalidation_1-rmse:11.5418\tvalidation_0-r2:-0.0966\tvalidation_1-r2:-0.111225\n",
      "[4]\tvalidation_0-rmse:12.0089\tvalidation_1-rmse:11.3828\tvalidation_0-r2:-0.117631\tvalidation_1-r2:-0.135553\n",
      "[5]\tvalidation_0-rmse:11.8697\tvalidation_1-rmse:11.2275\tvalidation_0-r2:-0.137972\tvalidation_1-r2:-0.158974\n",
      "[6]\tvalidation_0-rmse:11.7358\tvalidation_1-rmse:11.0773\tvalidation_0-r2:-0.157315\tvalidation_1-r2:-0.18132\n",
      "[7]\tvalidation_0-rmse:11.6088\tvalidation_1-rmse:10.9351\tvalidation_0-r2:-0.175445\tvalidation_1-r2:-0.202209\n",
      "[8]\tvalidation_0-rmse:11.4868\tvalidation_1-rmse:10.7975\tvalidation_0-r2:-0.192697\tvalidation_1-r2:-0.222158\n",
      "[9]\tvalidation_0-rmse:11.3694\tvalidation_1-rmse:10.6653\tvalidation_0-r2:-0.209115\tvalidation_1-r2:-0.241088\n",
      "[10]\tvalidation_0-rmse:11.2501\tvalidation_1-rmse:10.53\tvalidation_0-r2:-0.225623\tvalidation_1-r2:-0.260221\n",
      "[11]\tvalidation_0-rmse:11.1364\tvalidation_1-rmse:10.403\tvalidation_0-r2:-0.241189\tvalidation_1-r2:-0.277967\n",
      "[12]\tvalidation_0-rmse:11.0305\tvalidation_1-rmse:10.2824\tvalidation_0-r2:-0.255549\tvalidation_1-r2:-0.294612\n",
      "[13]\tvalidation_0-rmse:10.9349\tvalidation_1-rmse:10.1758\tvalidation_0-r2:-0.268398\tvalidation_1-r2:-0.309154\n",
      "[14]\tvalidation_0-rmse:10.8432\tvalidation_1-rmse:10.0745\tvalidation_0-r2:-0.280623\tvalidation_1-r2:-0.322849\n",
      "[15]\tvalidation_0-rmse:10.7487\tvalidation_1-rmse:9.96865\tvalidation_0-r2:-0.293113\tvalidation_1-r2:-0.336998\n",
      "[16]\tvalidation_0-rmse:10.6582\tvalidation_1-rmse:9.8653\tvalidation_0-r2:-0.304959\tvalidation_1-r2:-0.350673\n",
      "[17]\tvalidation_0-rmse:10.5727\tvalidation_1-rmse:9.76717\tvalidation_0-r2:-0.316072\tvalidation_1-r2:-0.363526\n",
      "[18]\tvalidation_0-rmse:10.4892\tvalidation_1-rmse:9.67158\tvalidation_0-r2:-0.326823\tvalidation_1-r2:-0.375923\n",
      "[19]\tvalidation_0-rmse:10.4088\tvalidation_1-rmse:9.57891\tvalidation_0-r2:-0.337108\tvalidation_1-r2:-0.387826\n",
      "[20]\tvalidation_0-rmse:10.3276\tvalidation_1-rmse:9.48639\tvalidation_0-r2:-0.347408\tvalidation_1-r2:-0.399595\n",
      "[21]\tvalidation_0-rmse:10.2537\tvalidation_1-rmse:9.40304\tvalidation_0-r2:-0.356719\tvalidation_1-r2:-0.410099\n",
      "[22]\tvalidation_0-rmse:10.1832\tvalidation_1-rmse:9.32104\tvalidation_0-r2:-0.365534\tvalidation_1-r2:-0.420342\n",
      "[23]\tvalidation_0-rmse:10.1151\tvalidation_1-rmse:9.24498\tvalidation_0-r2:-0.373985\tvalidation_1-r2:-0.429764\n",
      "[24]\tvalidation_0-rmse:10.0498\tvalidation_1-rmse:9.1692\tvalidation_0-r2:-0.382043\tvalidation_1-r2:-0.439074\n",
      "[25]\tvalidation_0-rmse:9.98744\tvalidation_1-rmse:9.09756\tvalidation_0-r2:-0.38969\tvalidation_1-r2:-0.447805\n",
      "[26]\tvalidation_0-rmse:9.92764\tvalidation_1-rmse:9.02902\tvalidation_0-r2:-0.396977\tvalidation_1-r2:-0.456093\n",
      "[27]\tvalidation_0-rmse:9.87027\tvalidation_1-rmse:8.96199\tvalidation_0-r2:-0.403927\tvalidation_1-r2:-0.46414\n",
      "[28]\tvalidation_0-rmse:9.81523\tvalidation_1-rmse:8.89777\tvalidation_0-r2:-0.410556\tvalidation_1-r2:-0.471792\n",
      "[29]\tvalidation_0-rmse:9.76243\tvalidation_1-rmse:8.83676\tvalidation_0-r2:-0.416881\tvalidation_1-r2:-0.479011\n",
      "[30]\tvalidation_0-rmse:9.71173\tvalidation_1-rmse:8.77765\tvalidation_0-r2:-0.422922\tvalidation_1-r2:-0.485958\n",
      "[31]\tvalidation_0-rmse:9.66313\tvalidation_1-rmse:8.721\tvalidation_0-r2:-0.428683\tvalidation_1-r2:-0.492571\n",
      "[32]\tvalidation_0-rmse:9.61621\tvalidation_1-rmse:8.66888\tvalidation_0-r2:-0.434218\tvalidation_1-r2:-0.498618\n",
      "[33]\tvalidation_0-rmse:9.57189\tvalidation_1-rmse:8.61722\tvalidation_0-r2:-0.439421\tvalidation_1-r2:-0.504576\n",
      "[34]\tvalidation_0-rmse:9.52888\tvalidation_1-rmse:8.5667\tvalidation_0-r2:-0.444448\tvalidation_1-r2:-0.510368\n",
      "[35]\tvalidation_0-rmse:9.4877\tvalidation_1-rmse:8.51949\tvalidation_0-r2:-0.449238\tvalidation_1-r2:-0.515749\n",
      "[36]\tvalidation_0-rmse:9.44808\tvalidation_1-rmse:8.47351\tvalidation_0-r2:-0.453828\tvalidation_1-r2:-0.520963\n",
      "[37]\tvalidation_0-rmse:9.41019\tvalidation_1-rmse:8.42903\tvalidation_0-r2:-0.4582\tvalidation_1-r2:-0.525978\n",
      "[38]\tvalidation_0-rmse:9.37417\tvalidation_1-rmse:8.38714\tvalidation_0-r2:-0.462341\tvalidation_1-r2:-0.530678\n",
      "[39]\tvalidation_0-rmse:9.33896\tvalidation_1-rmse:8.34673\tvalidation_0-r2:-0.466372\tvalidation_1-r2:-0.53519\n",
      "[40]\tvalidation_0-rmse:9.3052\tvalidation_1-rmse:8.30759\tvalidation_0-r2:-0.470223\tvalidation_1-r2:-0.539539\n",
      "[41]\tvalidation_0-rmse:9.27257\tvalidation_1-rmse:8.2723\tvalidation_0-r2:-0.473932\tvalidation_1-r2:-0.543442\n",
      "[42]\tvalidation_0-rmse:9.24139\tvalidation_1-rmse:8.23661\tvalidation_0-r2:-0.477464\tvalidation_1-r2:-0.547374\n",
      "[43]\tvalidation_0-rmse:9.21167\tvalidation_1-rmse:8.2026\tvalidation_0-r2:-0.48082\tvalidation_1-r2:-0.551104\n",
      "[44]\tvalidation_0-rmse:9.18258\tvalidation_1-rmse:8.1713\tvalidation_0-r2:-0.484094\tvalidation_1-r2:-0.554523\n",
      "[45]\tvalidation_0-rmse:9.15497\tvalidation_1-rmse:8.14036\tvalidation_0-r2:-0.487191\tvalidation_1-r2:-0.557891\n",
      "[46]\tvalidation_0-rmse:9.13077\tvalidation_1-rmse:8.1125\tvalidation_0-r2:-0.489898\tvalidation_1-r2:-0.560911\n",
      "[47]\tvalidation_0-rmse:9.10508\tvalidation_1-rmse:8.08351\tvalidation_0-r2:-0.492765\tvalidation_1-r2:-0.564045\n",
      "[48]\tvalidation_0-rmse:9.08046\tvalidation_1-rmse:8.05514\tvalidation_0-r2:-0.495504\tvalidation_1-r2:-0.567099\n",
      "[49]\tvalidation_0-rmse:9.05669\tvalidation_1-rmse:8.02872\tvalidation_0-r2:-0.498142\tvalidation_1-r2:-0.569934\n",
      "[50]\tvalidation_0-rmse:9.03407\tvalidation_1-rmse:8.00351\tvalidation_0-r2:-0.500646\tvalidation_1-r2:-0.572631\n",
      "[51]\tvalidation_0-rmse:9.01239\tvalidation_1-rmse:7.97872\tvalidation_0-r2:-0.50304\tvalidation_1-r2:-0.575273\n",
      "[52]\tvalidation_0-rmse:8.98977\tvalidation_1-rmse:7.9541\tvalidation_0-r2:-0.505531\tvalidation_1-r2:-0.577891\n",
      "[53]\tvalidation_0-rmse:8.96951\tvalidation_1-rmse:7.93299\tvalidation_0-r2:-0.507757\tvalidation_1-r2:-0.580128\n",
      "[54]\tvalidation_0-rmse:8.95219\tvalidation_1-rmse:7.9153\tvalidation_0-r2:-0.509657\tvalidation_1-r2:-0.581999\n",
      "[55]\tvalidation_0-rmse:8.93379\tvalidation_1-rmse:7.89439\tvalidation_0-r2:-0.51167\tvalidation_1-r2:-0.584204\n",
      "[56]\tvalidation_0-rmse:8.91573\tvalidation_1-rmse:7.87417\tvalidation_0-r2:-0.513642\tvalidation_1-r2:-0.586331\n",
      "[57]\tvalidation_0-rmse:8.89835\tvalidation_1-rmse:7.85397\tvalidation_0-r2:-0.515537\tvalidation_1-r2:-0.588451\n",
      "[58]\tvalidation_0-rmse:8.88296\tvalidation_1-rmse:7.83969\tvalidation_0-r2:-0.517212\tvalidation_1-r2:-0.589946\n",
      "[59]\tvalidation_0-rmse:8.86657\tvalidation_1-rmse:7.82095\tvalidation_0-r2:-0.518991\tvalidation_1-r2:-0.591904\n",
      "[60]\tvalidation_0-rmse:8.85057\tvalidation_1-rmse:7.80294\tvalidation_0-r2:-0.520726\tvalidation_1-r2:-0.593782\n",
      "[61]\tvalidation_0-rmse:8.83596\tvalidation_1-rmse:7.78676\tvalidation_0-r2:-0.522307\tvalidation_1-r2:-0.595465\n",
      "[62]\tvalidation_0-rmse:8.82169\tvalidation_1-rmse:7.77089\tvalidation_0-r2:-0.523848\tvalidation_1-r2:-0.597112\n",
      "[63]\tvalidation_0-rmse:8.80792\tvalidation_1-rmse:7.75647\tvalidation_0-r2:-0.525334\tvalidation_1-r2:-0.598606\n",
      "[64]\tvalidation_0-rmse:8.79429\tvalidation_1-rmse:7.74298\tvalidation_0-r2:-0.526802\tvalidation_1-r2:-0.600001\n",
      "[65]\tvalidation_0-rmse:8.78285\tvalidation_1-rmse:7.73092\tvalidation_0-r2:-0.528032\tvalidation_1-r2:-0.601246\n",
      "[66]\tvalidation_0-rmse:8.77024\tvalidation_1-rmse:7.71731\tvalidation_0-r2:-0.529386\tvalidation_1-r2:-0.602649\n",
      "[67]\tvalidation_0-rmse:8.75787\tvalidation_1-rmse:7.70369\tvalidation_0-r2:-0.530713\tvalidation_1-r2:-0.60405\n",
      "[68]\tvalidation_0-rmse:8.74608\tvalidation_1-rmse:7.69128\tvalidation_0-r2:-0.531975\tvalidation_1-r2:-0.605325\n",
      "[69]\tvalidation_0-rmse:8.73483\tvalidation_1-rmse:7.67843\tvalidation_0-r2:-0.533179\tvalidation_1-r2:-0.606642\n",
      "[70]\tvalidation_0-rmse:8.72352\tvalidation_1-rmse:7.66636\tvalidation_0-r2:-0.534387\tvalidation_1-r2:-0.607878\n",
      "[71]\tvalidation_0-rmse:8.71255\tvalidation_1-rmse:7.65643\tvalidation_0-r2:-0.535557\tvalidation_1-r2:-0.608893\n",
      "[72]\tvalidation_0-rmse:8.70194\tvalidation_1-rmse:7.64666\tvalidation_0-r2:-0.536687\tvalidation_1-r2:-0.60989\n",
      "[73]\tvalidation_0-rmse:8.69189\tvalidation_1-rmse:7.63604\tvalidation_0-r2:-0.537757\tvalidation_1-r2:-0.610974\n",
      "[74]\tvalidation_0-rmse:8.68219\tvalidation_1-rmse:7.62654\tvalidation_0-r2:-0.538788\tvalidation_1-r2:-0.611941\n",
      "[75]\tvalidation_0-rmse:8.67282\tvalidation_1-rmse:7.6171\tvalidation_0-r2:-0.539783\tvalidation_1-r2:-0.612901\n",
      "[76]\tvalidation_0-rmse:8.66386\tvalidation_1-rmse:7.60938\tvalidation_0-r2:-0.540734\tvalidation_1-r2:-0.613686\n",
      "[77]\tvalidation_0-rmse:8.65572\tvalidation_1-rmse:7.6033\tvalidation_0-r2:-0.541596\tvalidation_1-r2:-0.614303\n",
      "[78]\tvalidation_0-rmse:8.64669\tvalidation_1-rmse:7.59506\tvalidation_0-r2:-0.542552\tvalidation_1-r2:-0.615138\n",
      "[79]\tvalidation_0-rmse:8.63934\tvalidation_1-rmse:7.58915\tvalidation_0-r2:-0.54333\tvalidation_1-r2:-0.615737\n",
      "[80]\tvalidation_0-rmse:8.63089\tvalidation_1-rmse:7.58118\tvalidation_0-r2:-0.544222\tvalidation_1-r2:-0.616543\n",
      "[81]\tvalidation_0-rmse:8.62264\tvalidation_1-rmse:7.5731\tvalidation_0-r2:-0.545093\tvalidation_1-r2:-0.61736\n",
      "[82]\tvalidation_0-rmse:8.61497\tvalidation_1-rmse:7.56582\tvalidation_0-r2:-0.545902\tvalidation_1-r2:-0.618096\n",
      "[83]\tvalidation_0-rmse:8.60795\tvalidation_1-rmse:7.55931\tvalidation_0-r2:-0.546642\tvalidation_1-r2:-0.618753\n",
      "[84]\tvalidation_0-rmse:8.60069\tvalidation_1-rmse:7.55316\tvalidation_0-r2:-0.547407\tvalidation_1-r2:-0.619373\n",
      "[85]\tvalidation_0-rmse:8.59347\tvalidation_1-rmse:7.54687\tvalidation_0-r2:-0.548166\tvalidation_1-r2:-0.620006\n",
      "[86]\tvalidation_0-rmse:8.58667\tvalidation_1-rmse:7.54192\tvalidation_0-r2:-0.548881\tvalidation_1-r2:-0.620505\n",
      "[87]\tvalidation_0-rmse:8.57994\tvalidation_1-rmse:7.53586\tvalidation_0-r2:-0.549588\tvalidation_1-r2:-0.621114\n",
      "[88]\tvalidation_0-rmse:8.57362\tvalidation_1-rmse:7.53165\tvalidation_0-r2:-0.550251\tvalidation_1-r2:-0.621537\n",
      "[89]\tvalidation_0-rmse:8.5673\tvalidation_1-rmse:7.52647\tvalidation_0-r2:-0.550914\tvalidation_1-r2:-0.622058\n",
      "[90]\tvalidation_0-rmse:8.56129\tvalidation_1-rmse:7.52195\tvalidation_0-r2:-0.551544\tvalidation_1-r2:-0.622512\n",
      "[91]\tvalidation_0-rmse:8.55522\tvalidation_1-rmse:7.51805\tvalidation_0-r2:-0.552179\tvalidation_1-r2:-0.622903\n",
      "[92]\tvalidation_0-rmse:8.54973\tvalidation_1-rmse:7.51267\tvalidation_0-r2:-0.552754\tvalidation_1-r2:-0.623443\n",
      "[93]\tvalidation_0-rmse:8.54432\tvalidation_1-rmse:7.5084\tvalidation_0-r2:-0.55332\tvalidation_1-r2:-0.62387\n",
      "[94]\tvalidation_0-rmse:8.53875\tvalidation_1-rmse:7.50516\tvalidation_0-r2:-0.553902\tvalidation_1-r2:-0.624195\n",
      "[95]\tvalidation_0-rmse:8.53339\tvalidation_1-rmse:7.50111\tvalidation_0-r2:-0.554462\tvalidation_1-r2:-0.6246\n",
      "[96]\tvalidation_0-rmse:8.52824\tvalidation_1-rmse:7.49811\tvalidation_0-r2:-0.555\tvalidation_1-r2:-0.624901\n",
      "[97]\tvalidation_0-rmse:8.52349\tvalidation_1-rmse:7.49527\tvalidation_0-r2:-0.555495\tvalidation_1-r2:-0.625185\n",
      "[98]\tvalidation_0-rmse:8.51867\tvalidation_1-rmse:7.49288\tvalidation_0-r2:-0.555998\tvalidation_1-r2:-0.625424\n",
      "[99]\tvalidation_0-rmse:8.51424\tvalidation_1-rmse:7.49054\tvalidation_0-r2:-0.55646\tvalidation_1-r2:-0.625657\n",
      "[100]\tvalidation_0-rmse:8.50987\tvalidation_1-rmse:7.48734\tvalidation_0-r2:-0.556914\tvalidation_1-r2:-0.625977\n",
      "[101]\tvalidation_0-rmse:8.50582\tvalidation_1-rmse:7.48499\tvalidation_0-r2:-0.557336\tvalidation_1-r2:-0.626212\n",
      "[102]\tvalidation_0-rmse:8.50117\tvalidation_1-rmse:7.48211\tvalidation_0-r2:-0.55782\tvalidation_1-r2:-0.6265\n",
      "[103]\tvalidation_0-rmse:8.49696\tvalidation_1-rmse:7.47944\tvalidation_0-r2:-0.558258\tvalidation_1-r2:-0.626766\n",
      "[104]\tvalidation_0-rmse:8.49309\tvalidation_1-rmse:7.47697\tvalidation_0-r2:-0.55866\tvalidation_1-r2:-0.627012\n",
      "[105]\tvalidation_0-rmse:8.48946\tvalidation_1-rmse:7.47409\tvalidation_0-r2:-0.559038\tvalidation_1-r2:-0.6273\n",
      "[106]\tvalidation_0-rmse:8.48555\tvalidation_1-rmse:7.47132\tvalidation_0-r2:-0.559444\tvalidation_1-r2:-0.627576\n",
      "[107]\tvalidation_0-rmse:8.48191\tvalidation_1-rmse:7.46897\tvalidation_0-r2:-0.559821\tvalidation_1-r2:-0.62781\n",
      "[108]\tvalidation_0-rmse:8.47839\tvalidation_1-rmse:7.46696\tvalidation_0-r2:-0.560187\tvalidation_1-r2:-0.628011\n",
      "[109]\tvalidation_0-rmse:8.47488\tvalidation_1-rmse:7.46558\tvalidation_0-r2:-0.560551\tvalidation_1-r2:-0.628148\n",
      "[110]\tvalidation_0-rmse:8.4716\tvalidation_1-rmse:7.46328\tvalidation_0-r2:-0.560891\tvalidation_1-r2:-0.628377\n",
      "[111]\tvalidation_0-rmse:8.46844\tvalidation_1-rmse:7.46117\tvalidation_0-r2:-0.561218\tvalidation_1-r2:-0.628587\n",
      "[112]\tvalidation_0-rmse:8.46512\tvalidation_1-rmse:7.45989\tvalidation_0-r2:-0.561562\tvalidation_1-r2:-0.628714\n",
      "[113]\tvalidation_0-rmse:8.46207\tvalidation_1-rmse:7.4577\tvalidation_0-r2:-0.561878\tvalidation_1-r2:-0.628933\n",
      "[114]\tvalidation_0-rmse:8.45916\tvalidation_1-rmse:7.45628\tvalidation_0-r2:-0.562179\tvalidation_1-r2:-0.629074\n",
      "[115]\tvalidation_0-rmse:8.45608\tvalidation_1-rmse:7.45432\tvalidation_0-r2:-0.562499\tvalidation_1-r2:-0.629269\n",
      "[116]\tvalidation_0-rmse:8.45305\tvalidation_1-rmse:7.45196\tvalidation_0-r2:-0.562811\tvalidation_1-r2:-0.629504\n",
      "[117]\tvalidation_0-rmse:8.44978\tvalidation_1-rmse:7.45064\tvalidation_0-r2:-0.56315\tvalidation_1-r2:-0.629635\n",
      "[118]\tvalidation_0-rmse:8.44698\tvalidation_1-rmse:7.44834\tvalidation_0-r2:-0.56344\tvalidation_1-r2:-0.629863\n",
      "[119]\tvalidation_0-rmse:8.44417\tvalidation_1-rmse:7.44716\tvalidation_0-r2:-0.56373\tvalidation_1-r2:-0.629981\n",
      "[120]\tvalidation_0-rmse:8.44129\tvalidation_1-rmse:7.4461\tvalidation_0-r2:-0.564027\tvalidation_1-r2:-0.630086\n",
      "[121]\tvalidation_0-rmse:8.4382\tvalidation_1-rmse:7.44423\tvalidation_0-r2:-0.564346\tvalidation_1-r2:-0.630272\n",
      "[122]\tvalidation_0-rmse:8.43565\tvalidation_1-rmse:7.44261\tvalidation_0-r2:-0.56461\tvalidation_1-r2:-0.630433\n",
      "[123]\tvalidation_0-rmse:8.43299\tvalidation_1-rmse:7.44145\tvalidation_0-r2:-0.564884\tvalidation_1-r2:-0.630548\n",
      "[124]\tvalidation_0-rmse:8.43062\tvalidation_1-rmse:7.4402\tvalidation_0-r2:-0.565129\tvalidation_1-r2:-0.630673\n",
      "[125]\tvalidation_0-rmse:8.42799\tvalidation_1-rmse:7.43947\tvalidation_0-r2:-0.5654\tvalidation_1-r2:-0.630745\n",
      "[126]\tvalidation_0-rmse:8.42535\tvalidation_1-rmse:7.4385\tvalidation_0-r2:-0.565672\tvalidation_1-r2:-0.630841\n",
      "[127]\tvalidation_0-rmse:8.42311\tvalidation_1-rmse:7.4374\tvalidation_0-r2:-0.565903\tvalidation_1-r2:-0.63095\n",
      "[128]\tvalidation_0-rmse:8.42045\tvalidation_1-rmse:7.43658\tvalidation_0-r2:-0.566178\tvalidation_1-r2:-0.631032\n",
      "[129]\tvalidation_0-rmse:8.41834\tvalidation_1-rmse:7.43567\tvalidation_0-r2:-0.566395\tvalidation_1-r2:-0.631122\n",
      "[130]\tvalidation_0-rmse:8.41623\tvalidation_1-rmse:7.43419\tvalidation_0-r2:-0.566612\tvalidation_1-r2:-0.631269\n",
      "[131]\tvalidation_0-rmse:8.41419\tvalidation_1-rmse:7.43341\tvalidation_0-r2:-0.566822\tvalidation_1-r2:-0.631346\n",
      "[132]\tvalidation_0-rmse:8.41207\tvalidation_1-rmse:7.43272\tvalidation_0-r2:-0.56704\tvalidation_1-r2:-0.631415\n",
      "[133]\tvalidation_0-rmse:8.41005\tvalidation_1-rmse:7.43138\tvalidation_0-r2:-0.567249\tvalidation_1-r2:-0.631547\n",
      "[134]\tvalidation_0-rmse:8.40816\tvalidation_1-rmse:7.43059\tvalidation_0-r2:-0.567443\tvalidation_1-r2:-0.631626\n",
      "[135]\tvalidation_0-rmse:8.40625\tvalidation_1-rmse:7.42947\tvalidation_0-r2:-0.567639\tvalidation_1-r2:-0.631737\n",
      "[136]\tvalidation_0-rmse:8.40434\tvalidation_1-rmse:7.42809\tvalidation_0-r2:-0.567836\tvalidation_1-r2:-0.631874\n",
      "[137]\tvalidation_0-rmse:8.40201\tvalidation_1-rmse:7.42739\tvalidation_0-r2:-0.568075\tvalidation_1-r2:-0.631943\n",
      "[138]\tvalidation_0-rmse:8.40023\tvalidation_1-rmse:7.42631\tvalidation_0-r2:-0.568258\tvalidation_1-r2:-0.63205\n",
      "[139]\tvalidation_0-rmse:8.39843\tvalidation_1-rmse:7.42517\tvalidation_0-r2:-0.568443\tvalidation_1-r2:-0.632163\n",
      "[140]\tvalidation_0-rmse:8.39667\tvalidation_1-rmse:7.42416\tvalidation_0-r2:-0.568625\tvalidation_1-r2:-0.632263\n",
      "[141]\tvalidation_0-rmse:8.39385\tvalidation_1-rmse:7.42257\tvalidation_0-r2:-0.568914\tvalidation_1-r2:-0.632421\n",
      "[142]\tvalidation_0-rmse:8.39224\tvalidation_1-rmse:7.42159\tvalidation_0-r2:-0.56908\tvalidation_1-r2:-0.632518\n",
      "[143]\tvalidation_0-rmse:8.38947\tvalidation_1-rmse:7.41965\tvalidation_0-r2:-0.569363\tvalidation_1-r2:-0.63271\n",
      "[144]\tvalidation_0-rmse:8.38718\tvalidation_1-rmse:7.41857\tvalidation_0-r2:-0.569599\tvalidation_1-r2:-0.632816\n",
      "[145]\tvalidation_0-rmse:8.38568\tvalidation_1-rmse:7.41814\tvalidation_0-r2:-0.569753\tvalidation_1-r2:-0.632859\n",
      "[146]\tvalidation_0-rmse:8.38411\tvalidation_1-rmse:7.41727\tvalidation_0-r2:-0.569914\tvalidation_1-r2:-0.632945\n",
      "[147]\tvalidation_0-rmse:8.38252\tvalidation_1-rmse:7.41669\tvalidation_0-r2:-0.570076\tvalidation_1-r2:-0.633003\n",
      "[148]\tvalidation_0-rmse:8.38048\tvalidation_1-rmse:7.41695\tvalidation_0-r2:-0.570286\tvalidation_1-r2:-0.632977\n",
      "[149]\tvalidation_0-rmse:8.37898\tvalidation_1-rmse:7.41594\tvalidation_0-r2:-0.57044\tvalidation_1-r2:-0.633076\n",
      "[150]\tvalidation_0-rmse:8.37735\tvalidation_1-rmse:7.41536\tvalidation_0-r2:-0.570607\tvalidation_1-r2:-0.633134\n",
      "[151]\tvalidation_0-rmse:8.37541\tvalidation_1-rmse:7.41569\tvalidation_0-r2:-0.570806\tvalidation_1-r2:-0.633102\n",
      "[152]\tvalidation_0-rmse:8.37305\tvalidation_1-rmse:7.41418\tvalidation_0-r2:-0.571048\tvalidation_1-r2:-0.633251\n",
      "[153]\tvalidation_0-rmse:8.37121\tvalidation_1-rmse:7.41443\tvalidation_0-r2:-0.571236\tvalidation_1-r2:-0.633226\n",
      "[154]\tvalidation_0-rmse:8.36982\tvalidation_1-rmse:7.4138\tvalidation_0-r2:-0.571378\tvalidation_1-r2:-0.633289\n",
      "[155]\tvalidation_0-rmse:8.36803\tvalidation_1-rmse:7.41358\tvalidation_0-r2:-0.571561\tvalidation_1-r2:-0.63331\n",
      "[156]\tvalidation_0-rmse:8.36631\tvalidation_1-rmse:7.41348\tvalidation_0-r2:-0.571738\tvalidation_1-r2:-0.63332\n",
      "[157]\tvalidation_0-rmse:8.36458\tvalidation_1-rmse:7.41391\tvalidation_0-r2:-0.571915\tvalidation_1-r2:-0.633277\n",
      "[158]\tvalidation_0-rmse:8.36326\tvalidation_1-rmse:7.41285\tvalidation_0-r2:-0.57205\tvalidation_1-r2:-0.633383\n",
      "[159]\tvalidation_0-rmse:8.36201\tvalidation_1-rmse:7.41275\tvalidation_0-r2:-0.572178\tvalidation_1-r2:-0.633393\n",
      "[160]\tvalidation_0-rmse:8.36007\tvalidation_1-rmse:7.41192\tvalidation_0-r2:-0.572377\tvalidation_1-r2:-0.633475\n",
      "[161]\tvalidation_0-rmse:8.35812\tvalidation_1-rmse:7.41176\tvalidation_0-r2:-0.572576\tvalidation_1-r2:-0.63349\n",
      "[162]\tvalidation_0-rmse:8.35647\tvalidation_1-rmse:7.41239\tvalidation_0-r2:-0.572744\tvalidation_1-r2:-0.633428\n",
      "[163]\tvalidation_0-rmse:8.35524\tvalidation_1-rmse:7.41169\tvalidation_0-r2:-0.572871\tvalidation_1-r2:-0.633497\n",
      "[164]\tvalidation_0-rmse:8.35407\tvalidation_1-rmse:7.41139\tvalidation_0-r2:-0.57299\tvalidation_1-r2:-0.633527\n",
      "[165]\tvalidation_0-rmse:8.35247\tvalidation_1-rmse:7.41183\tvalidation_0-r2:-0.573154\tvalidation_1-r2:-0.633484\n",
      "[166]\tvalidation_0-rmse:8.35131\tvalidation_1-rmse:7.4111\tvalidation_0-r2:-0.573272\tvalidation_1-r2:-0.633555\n",
      "[167]\tvalidation_0-rmse:8.34909\tvalidation_1-rmse:7.40995\tvalidation_0-r2:-0.573499\tvalidation_1-r2:-0.633669\n",
      "[168]\tvalidation_0-rmse:8.34689\tvalidation_1-rmse:7.4089\tvalidation_0-r2:-0.573724\tvalidation_1-r2:-0.633773\n",
      "[169]\tvalidation_0-rmse:8.34579\tvalidation_1-rmse:7.40864\tvalidation_0-r2:-0.573836\tvalidation_1-r2:-0.633799\n",
      "[170]\tvalidation_0-rmse:8.34401\tvalidation_1-rmse:7.40859\tvalidation_0-r2:-0.574018\tvalidation_1-r2:-0.633804\n",
      "[171]\tvalidation_0-rmse:8.34223\tvalidation_1-rmse:7.40812\tvalidation_0-r2:-0.5742\tvalidation_1-r2:-0.63385\n",
      "[172]\tvalidation_0-rmse:8.33965\tvalidation_1-rmse:7.40847\tvalidation_0-r2:-0.574463\tvalidation_1-r2:-0.633816\n",
      "[173]\tvalidation_0-rmse:8.33715\tvalidation_1-rmse:7.40747\tvalidation_0-r2:-0.574718\tvalidation_1-r2:-0.633915\n",
      "[174]\tvalidation_0-rmse:8.33606\tvalidation_1-rmse:7.40666\tvalidation_0-r2:-0.57483\tvalidation_1-r2:-0.633995\n",
      "[175]\tvalidation_0-rmse:8.33388\tvalidation_1-rmse:7.40602\tvalidation_0-r2:-0.575052\tvalidation_1-r2:-0.634057\n",
      "[176]\tvalidation_0-rmse:8.33252\tvalidation_1-rmse:7.40587\tvalidation_0-r2:-0.57519\tvalidation_1-r2:-0.634073\n",
      "[177]\tvalidation_0-rmse:8.33051\tvalidation_1-rmse:7.40597\tvalidation_0-r2:-0.575395\tvalidation_1-r2:-0.634062\n",
      "[178]\tvalidation_0-rmse:8.32816\tvalidation_1-rmse:7.40669\tvalidation_0-r2:-0.575635\tvalidation_1-r2:-0.633992\n",
      "[179]\tvalidation_0-rmse:8.3262\tvalidation_1-rmse:7.40661\tvalidation_0-r2:-0.575834\tvalidation_1-r2:-0.633999\n",
      "[180]\tvalidation_0-rmse:8.32445\tvalidation_1-rmse:7.40633\tvalidation_0-r2:-0.576012\tvalidation_1-r2:-0.634027\n",
      "[181]\tvalidation_0-rmse:8.32239\tvalidation_1-rmse:7.40535\tvalidation_0-r2:-0.576223\tvalidation_1-r2:-0.634124\n",
      "[182]\tvalidation_0-rmse:8.32053\tvalidation_1-rmse:7.40533\tvalidation_0-r2:-0.576412\tvalidation_1-r2:-0.634126\n",
      "[183]\tvalidation_0-rmse:8.31842\tvalidation_1-rmse:7.40545\tvalidation_0-r2:-0.576627\tvalidation_1-r2:-0.634114\n",
      "[184]\tvalidation_0-rmse:8.31674\tvalidation_1-rmse:7.40523\tvalidation_0-r2:-0.576797\tvalidation_1-r2:-0.634136\n",
      "[185]\tvalidation_0-rmse:8.31467\tvalidation_1-rmse:7.40484\tvalidation_0-r2:-0.577009\tvalidation_1-r2:-0.634175\n",
      "[186]\tvalidation_0-rmse:8.31216\tvalidation_1-rmse:7.40397\tvalidation_0-r2:-0.577264\tvalidation_1-r2:-0.63426\n",
      "[187]\tvalidation_0-rmse:8.31077\tvalidation_1-rmse:7.40372\tvalidation_0-r2:-0.577405\tvalidation_1-r2:-0.634285\n",
      "[188]\tvalidation_0-rmse:8.30913\tvalidation_1-rmse:7.40367\tvalidation_0-r2:-0.577572\tvalidation_1-r2:-0.63429\n",
      "[189]\tvalidation_0-rmse:8.30784\tvalidation_1-rmse:7.40314\tvalidation_0-r2:-0.577703\tvalidation_1-r2:-0.634343\n",
      "[190]\tvalidation_0-rmse:8.30636\tvalidation_1-rmse:7.4032\tvalidation_0-r2:-0.577854\tvalidation_1-r2:-0.634336\n",
      "[191]\tvalidation_0-rmse:8.3044\tvalidation_1-rmse:7.40248\tvalidation_0-r2:-0.578052\tvalidation_1-r2:-0.634408\n",
      "[192]\tvalidation_0-rmse:8.30309\tvalidation_1-rmse:7.40226\tvalidation_0-r2:-0.578186\tvalidation_1-r2:-0.63443\n",
      "[193]\tvalidation_0-rmse:8.30095\tvalidation_1-rmse:7.40136\tvalidation_0-r2:-0.578403\tvalidation_1-r2:-0.634518\n",
      "[194]\tvalidation_0-rmse:8.29939\tvalidation_1-rmse:7.40135\tvalidation_0-r2:-0.578562\tvalidation_1-r2:-0.634519\n",
      "[195]\tvalidation_0-rmse:8.29767\tvalidation_1-rmse:7.40124\tvalidation_0-r2:-0.578737\tvalidation_1-r2:-0.63453\n",
      "[196]\tvalidation_0-rmse:8.29565\tvalidation_1-rmse:7.40103\tvalidation_0-r2:-0.578941\tvalidation_1-r2:-0.63455\n",
      "[197]\tvalidation_0-rmse:8.29347\tvalidation_1-rmse:7.4002\tvalidation_0-r2:-0.579163\tvalidation_1-r2:-0.634633\n",
      "[198]\tvalidation_0-rmse:8.29183\tvalidation_1-rmse:7.39993\tvalidation_0-r2:-0.579329\tvalidation_1-r2:-0.634659\n",
      "[199]\tvalidation_0-rmse:8.29045\tvalidation_1-rmse:7.39998\tvalidation_0-r2:-0.579469\tvalidation_1-r2:-0.634655\n",
      "[200]\tvalidation_0-rmse:8.28816\tvalidation_1-rmse:7.39932\tvalidation_0-r2:-0.579702\tvalidation_1-r2:-0.634719\n",
      "[201]\tvalidation_0-rmse:8.28694\tvalidation_1-rmse:7.39904\tvalidation_0-r2:-0.579826\tvalidation_1-r2:-0.634747\n",
      "[202]\tvalidation_0-rmse:8.28553\tvalidation_1-rmse:7.39888\tvalidation_0-r2:-0.579968\tvalidation_1-r2:-0.634763\n",
      "[203]\tvalidation_0-rmse:8.28394\tvalidation_1-rmse:7.39816\tvalidation_0-r2:-0.580129\tvalidation_1-r2:-0.634834\n",
      "[204]\tvalidation_0-rmse:8.28281\tvalidation_1-rmse:7.39752\tvalidation_0-r2:-0.580244\tvalidation_1-r2:-0.634897\n",
      "[205]\tvalidation_0-rmse:8.28169\tvalidation_1-rmse:7.39803\tvalidation_0-r2:-0.580357\tvalidation_1-r2:-0.634847\n",
      "[206]\tvalidation_0-rmse:8.28018\tvalidation_1-rmse:7.39747\tvalidation_0-r2:-0.580511\tvalidation_1-r2:-0.634903\n",
      "[207]\tvalidation_0-rmse:8.27836\tvalidation_1-rmse:7.39668\tvalidation_0-r2:-0.580695\tvalidation_1-r2:-0.634981\n",
      "[208]\tvalidation_0-rmse:8.27654\tvalidation_1-rmse:7.39595\tvalidation_0-r2:-0.58088\tvalidation_1-r2:-0.635052\n",
      "[209]\tvalidation_0-rmse:8.27496\tvalidation_1-rmse:7.39591\tvalidation_0-r2:-0.581039\tvalidation_1-r2:-0.635056\n",
      "[210]\tvalidation_0-rmse:8.27284\tvalidation_1-rmse:7.39553\tvalidation_0-r2:-0.581254\tvalidation_1-r2:-0.635094\n",
      "[211]\tvalidation_0-rmse:8.27137\tvalidation_1-rmse:7.39518\tvalidation_0-r2:-0.581403\tvalidation_1-r2:-0.635128\n",
      "[212]\tvalidation_0-rmse:8.27027\tvalidation_1-rmse:7.39481\tvalidation_0-r2:-0.581514\tvalidation_1-r2:-0.635164\n",
      "[213]\tvalidation_0-rmse:8.26745\tvalidation_1-rmse:7.39479\tvalidation_0-r2:-0.5818\tvalidation_1-r2:-0.635167\n",
      "[214]\tvalidation_0-rmse:8.26644\tvalidation_1-rmse:7.39509\tvalidation_0-r2:-0.581901\tvalidation_1-r2:-0.635137\n",
      "[215]\tvalidation_0-rmse:8.26485\tvalidation_1-rmse:7.39513\tvalidation_0-r2:-0.582062\tvalidation_1-r2:-0.635133\n",
      "[216]\tvalidation_0-rmse:8.26298\tvalidation_1-rmse:7.39569\tvalidation_0-r2:-0.582252\tvalidation_1-r2:-0.635078\n",
      "[217]\tvalidation_0-rmse:8.26126\tvalidation_1-rmse:7.39584\tvalidation_0-r2:-0.582425\tvalidation_1-r2:-0.635063\n",
      "[218]\tvalidation_0-rmse:8.26031\tvalidation_1-rmse:7.39617\tvalidation_0-r2:-0.582521\tvalidation_1-r2:-0.635031\n",
      "[219]\tvalidation_0-rmse:8.25902\tvalidation_1-rmse:7.39559\tvalidation_0-r2:-0.582652\tvalidation_1-r2:-0.635088\n",
      "[220]\tvalidation_0-rmse:8.25743\tvalidation_1-rmse:7.39661\tvalidation_0-r2:-0.582812\tvalidation_1-r2:-0.634987\n",
      "[221]\tvalidation_0-rmse:8.25628\tvalidation_1-rmse:7.39661\tvalidation_0-r2:-0.582929\tvalidation_1-r2:-0.634988\n",
      "[222]\tvalidation_0-rmse:8.25427\tvalidation_1-rmse:7.39696\tvalidation_0-r2:-0.583132\tvalidation_1-r2:-0.634953\n",
      "[223]\tvalidation_0-rmse:8.25236\tvalidation_1-rmse:7.39635\tvalidation_0-r2:-0.583324\tvalidation_1-r2:-0.635013\n",
      "[224]\tvalidation_0-rmse:8.2507\tvalidation_1-rmse:7.39632\tvalidation_0-r2:-0.583492\tvalidation_1-r2:-0.635016\n",
      "[225]\tvalidation_0-rmse:8.24938\tvalidation_1-rmse:7.39596\tvalidation_0-r2:-0.583625\tvalidation_1-r2:-0.635051\n",
      "[226]\tvalidation_0-rmse:8.24786\tvalidation_1-rmse:7.396\tvalidation_0-r2:-0.583779\tvalidation_1-r2:-0.635048\n",
      "[227]\tvalidation_0-rmse:8.24699\tvalidation_1-rmse:7.39634\tvalidation_0-r2:-0.583867\tvalidation_1-r2:-0.635014\n",
      "[228]\tvalidation_0-rmse:8.24534\tvalidation_1-rmse:7.39624\tvalidation_0-r2:-0.584033\tvalidation_1-r2:-0.635024\n",
      "[229]\tvalidation_0-rmse:8.24382\tvalidation_1-rmse:7.3965\tvalidation_0-r2:-0.584186\tvalidation_1-r2:-0.634998\n",
      "[230]\tvalidation_0-rmse:8.24262\tvalidation_1-rmse:7.39599\tvalidation_0-r2:-0.584308\tvalidation_1-r2:-0.635049\n",
      "[231]\tvalidation_0-rmse:8.24141\tvalidation_1-rmse:7.39569\tvalidation_0-r2:-0.58443\tvalidation_1-r2:-0.635078\n",
      "[232]\tvalidation_0-rmse:8.23937\tvalidation_1-rmse:7.39508\tvalidation_0-r2:-0.584635\tvalidation_1-r2:-0.635138\n",
      "[233]\tvalidation_0-rmse:8.23745\tvalidation_1-rmse:7.39478\tvalidation_0-r2:-0.584829\tvalidation_1-r2:-0.635168\n",
      "[234]\tvalidation_0-rmse:8.23552\tvalidation_1-rmse:7.39429\tvalidation_0-r2:-0.585023\tvalidation_1-r2:-0.635216\n",
      "[235]\tvalidation_0-rmse:8.23403\tvalidation_1-rmse:7.39368\tvalidation_0-r2:-0.585173\tvalidation_1-r2:-0.635276\n",
      "[236]\tvalidation_0-rmse:8.2332\tvalidation_1-rmse:7.39403\tvalidation_0-r2:-0.585257\tvalidation_1-r2:-0.635242\n",
      "[237]\tvalidation_0-rmse:8.23252\tvalidation_1-rmse:7.3936\tvalidation_0-r2:-0.585326\tvalidation_1-r2:-0.635285\n",
      "[238]\tvalidation_0-rmse:8.23092\tvalidation_1-rmse:7.39503\tvalidation_0-r2:-0.585487\tvalidation_1-r2:-0.635144\n",
      "[239]\tvalidation_0-rmse:8.22964\tvalidation_1-rmse:7.39521\tvalidation_0-r2:-0.585616\tvalidation_1-r2:-0.635125\n",
      "[240]\tvalidation_0-rmse:8.22781\tvalidation_1-rmse:7.39505\tvalidation_0-r2:-0.5858\tvalidation_1-r2:-0.635141\n",
      "[241]\tvalidation_0-rmse:8.22658\tvalidation_1-rmse:7.39509\tvalidation_0-r2:-0.585924\tvalidation_1-r2:-0.635137\n",
      "[242]\tvalidation_0-rmse:8.22532\tvalidation_1-rmse:7.39518\tvalidation_0-r2:-0.586051\tvalidation_1-r2:-0.635128\n",
      "[243]\tvalidation_0-rmse:8.22391\tvalidation_1-rmse:7.39456\tvalidation_0-r2:-0.586193\tvalidation_1-r2:-0.635189\n",
      "[244]\tvalidation_0-rmse:8.22313\tvalidation_1-rmse:7.39443\tvalidation_0-r2:-0.586271\tvalidation_1-r2:-0.635202\n",
      "[245]\tvalidation_0-rmse:8.22149\tvalidation_1-rmse:7.39397\tvalidation_0-r2:-0.586436\tvalidation_1-r2:-0.635247\n",
      "[246]\tvalidation_0-rmse:8.22025\tvalidation_1-rmse:7.39313\tvalidation_0-r2:-0.586561\tvalidation_1-r2:-0.63533\n",
      "[247]\tvalidation_0-rmse:8.21874\tvalidation_1-rmse:7.39331\tvalidation_0-r2:-0.586713\tvalidation_1-r2:-0.635313\n",
      "[248]\tvalidation_0-rmse:8.21753\tvalidation_1-rmse:7.39233\tvalidation_0-r2:-0.586835\tvalidation_1-r2:-0.63541\n",
      "[249]\tvalidation_0-rmse:8.21616\tvalidation_1-rmse:7.39233\tvalidation_0-r2:-0.586972\tvalidation_1-r2:-0.635409\n",
      "[250]\tvalidation_0-rmse:8.21353\tvalidation_1-rmse:7.39183\tvalidation_0-r2:-0.587237\tvalidation_1-r2:-0.635459\n",
      "[251]\tvalidation_0-rmse:8.21223\tvalidation_1-rmse:7.39201\tvalidation_0-r2:-0.587367\tvalidation_1-r2:-0.635441\n",
      "[252]\tvalidation_0-rmse:8.211\tvalidation_1-rmse:7.39156\tvalidation_0-r2:-0.587491\tvalidation_1-r2:-0.635486\n",
      "[253]\tvalidation_0-rmse:8.20954\tvalidation_1-rmse:7.393\tvalidation_0-r2:-0.587638\tvalidation_1-r2:-0.635343\n",
      "[254]\tvalidation_0-rmse:8.20834\tvalidation_1-rmse:7.39328\tvalidation_0-r2:-0.587758\tvalidation_1-r2:-0.635316\n",
      "[255]\tvalidation_0-rmse:8.20712\tvalidation_1-rmse:7.39285\tvalidation_0-r2:-0.587881\tvalidation_1-r2:-0.635358\n",
      "[256]\tvalidation_0-rmse:8.20585\tvalidation_1-rmse:7.3926\tvalidation_0-r2:-0.588008\tvalidation_1-r2:-0.635383\n",
      "[257]\tvalidation_0-rmse:8.20506\tvalidation_1-rmse:7.3918\tvalidation_0-r2:-0.588087\tvalidation_1-r2:-0.635462\n",
      "[258]\tvalidation_0-rmse:8.20374\tvalidation_1-rmse:7.39117\tvalidation_0-r2:-0.58822\tvalidation_1-r2:-0.635524\n",
      "[259]\tvalidation_0-rmse:8.20289\tvalidation_1-rmse:7.39067\tvalidation_0-r2:-0.588305\tvalidation_1-r2:-0.635573\n",
      "[260]\tvalidation_0-rmse:8.20112\tvalidation_1-rmse:7.39001\tvalidation_0-r2:-0.588483\tvalidation_1-r2:-0.635638\n",
      "[261]\tvalidation_0-rmse:8.2004\tvalidation_1-rmse:7.38988\tvalidation_0-r2:-0.588555\tvalidation_1-r2:-0.635651\n",
      "[262]\tvalidation_0-rmse:8.19891\tvalidation_1-rmse:7.39009\tvalidation_0-r2:-0.588704\tvalidation_1-r2:-0.63563\n",
      "[263]\tvalidation_0-rmse:8.19823\tvalidation_1-rmse:7.39007\tvalidation_0-r2:-0.588773\tvalidation_1-r2:-0.635632\n",
      "[264]\tvalidation_0-rmse:8.1974\tvalidation_1-rmse:7.3896\tvalidation_0-r2:-0.588856\tvalidation_1-r2:-0.635679\n",
      "[265]\tvalidation_0-rmse:8.19572\tvalidation_1-rmse:7.38923\tvalidation_0-r2:-0.589024\tvalidation_1-r2:-0.635715\n",
      "[266]\tvalidation_0-rmse:8.19459\tvalidation_1-rmse:7.38946\tvalidation_0-r2:-0.589137\tvalidation_1-r2:-0.635692\n",
      "[267]\tvalidation_0-rmse:8.19334\tvalidation_1-rmse:7.39023\tvalidation_0-r2:-0.589263\tvalidation_1-r2:-0.635616\n",
      "[268]\tvalidation_0-rmse:8.19221\tvalidation_1-rmse:7.39055\tvalidation_0-r2:-0.589376\tvalidation_1-r2:-0.635584\n",
      "[269]\tvalidation_0-rmse:8.19084\tvalidation_1-rmse:7.39149\tvalidation_0-r2:-0.589514\tvalidation_1-r2:-0.635492\n",
      "[270]\tvalidation_0-rmse:8.18974\tvalidation_1-rmse:7.39179\tvalidation_0-r2:-0.589624\tvalidation_1-r2:-0.635462\n",
      "[271]\tvalidation_0-rmse:8.18811\tvalidation_1-rmse:7.39139\tvalidation_0-r2:-0.589787\tvalidation_1-r2:-0.635502\n",
      "[272]\tvalidation_0-rmse:8.18741\tvalidation_1-rmse:7.39144\tvalidation_0-r2:-0.589858\tvalidation_1-r2:-0.635497\n",
      "[273]\tvalidation_0-rmse:8.18592\tvalidation_1-rmse:7.3915\tvalidation_0-r2:-0.590007\tvalidation_1-r2:-0.635491\n",
      "[274]\tvalidation_0-rmse:8.18528\tvalidation_1-rmse:7.39145\tvalidation_0-r2:-0.590071\tvalidation_1-r2:-0.635496\n",
      "[275]\tvalidation_0-rmse:8.18275\tvalidation_1-rmse:7.39103\tvalidation_0-r2:-0.590324\tvalidation_1-r2:-0.635537\n",
      "[276]\tvalidation_0-rmse:8.18211\tvalidation_1-rmse:7.39147\tvalidation_0-r2:-0.590388\tvalidation_1-r2:-0.635494\n",
      "[277]\tvalidation_0-rmse:8.18104\tvalidation_1-rmse:7.39177\tvalidation_0-r2:-0.590496\tvalidation_1-r2:-0.635464\n",
      "[278]\tvalidation_0-rmse:8.17985\tvalidation_1-rmse:7.39177\tvalidation_0-r2:-0.590615\tvalidation_1-r2:-0.635464\n",
      "[279]\tvalidation_0-rmse:8.17895\tvalidation_1-rmse:7.39212\tvalidation_0-r2:-0.590705\tvalidation_1-r2:-0.635431\n",
      "[280]\tvalidation_0-rmse:8.17738\tvalidation_1-rmse:7.39163\tvalidation_0-r2:-0.590861\tvalidation_1-r2:-0.635478\n",
      "[281]\tvalidation_0-rmse:8.17583\tvalidation_1-rmse:7.39118\tvalidation_0-r2:-0.591017\tvalidation_1-r2:-0.635523\n",
      "[282]\tvalidation_0-rmse:8.17475\tvalidation_1-rmse:7.39043\tvalidation_0-r2:-0.591125\tvalidation_1-r2:-0.635597\n",
      "[283]\tvalidation_0-rmse:8.17377\tvalidation_1-rmse:7.39075\tvalidation_0-r2:-0.591223\tvalidation_1-r2:-0.635565\n",
      "[284]\tvalidation_0-rmse:8.1728\tvalidation_1-rmse:7.39064\tvalidation_0-r2:-0.59132\tvalidation_1-r2:-0.635576\n",
      "[285]\tvalidation_0-rmse:8.17151\tvalidation_1-rmse:7.39195\tvalidation_0-r2:-0.591449\tvalidation_1-r2:-0.635447\n",
      "[286]\tvalidation_0-rmse:8.17047\tvalidation_1-rmse:7.39149\tvalidation_0-r2:-0.591553\tvalidation_1-r2:-0.635492\n",
      "[287]\tvalidation_0-rmse:8.16876\tvalidation_1-rmse:7.3918\tvalidation_0-r2:-0.591724\tvalidation_1-r2:-0.635462\n",
      "[288]\tvalidation_0-rmse:8.1679\tvalidation_1-rmse:7.39206\tvalidation_0-r2:-0.59181\tvalidation_1-r2:-0.635437\n",
      "[289]\tvalidation_0-rmse:8.16616\tvalidation_1-rmse:7.39223\tvalidation_0-r2:-0.591984\tvalidation_1-r2:-0.635419\n",
      "[290]\tvalidation_0-rmse:8.16507\tvalidation_1-rmse:7.39174\tvalidation_0-r2:-0.592093\tvalidation_1-r2:-0.635467\n",
      "[291]\tvalidation_0-rmse:8.16433\tvalidation_1-rmse:7.39134\tvalidation_0-r2:-0.592167\tvalidation_1-r2:-0.635507\n",
      "[292]\tvalidation_0-rmse:8.16321\tvalidation_1-rmse:7.39195\tvalidation_0-r2:-0.592278\tvalidation_1-r2:-0.635447\n",
      "[293]\tvalidation_0-rmse:8.162\tvalidation_1-rmse:7.39174\tvalidation_0-r2:-0.5924\tvalidation_1-r2:-0.635467\n",
      "[294]\tvalidation_0-rmse:8.16047\tvalidation_1-rmse:7.39148\tvalidation_0-r2:-0.592552\tvalidation_1-r2:-0.635493\n",
      "[295]\tvalidation_0-rmse:8.15897\tvalidation_1-rmse:7.39173\tvalidation_0-r2:-0.592702\tvalidation_1-r2:-0.635468\n",
      "[296]\tvalidation_0-rmse:8.15808\tvalidation_1-rmse:7.3922\tvalidation_0-r2:-0.59279\tvalidation_1-r2:-0.635423\n",
      "[297]\tvalidation_0-rmse:8.15591\tvalidation_1-rmse:7.39176\tvalidation_0-r2:-0.593007\tvalidation_1-r2:-0.635466\n",
      "[298]\tvalidation_0-rmse:8.1546\tvalidation_1-rmse:7.39148\tvalidation_0-r2:-0.593139\tvalidation_1-r2:-0.635493\n",
      "[299]\tvalidation_0-rmse:8.15356\tvalidation_1-rmse:7.39095\tvalidation_0-r2:-0.593242\tvalidation_1-r2:-0.635545\n",
      "[300]\tvalidation_0-rmse:8.15231\tvalidation_1-rmse:7.39163\tvalidation_0-r2:-0.593366\tvalidation_1-r2:-0.635478\n",
      "[301]\tvalidation_0-rmse:8.15147\tvalidation_1-rmse:7.39174\tvalidation_0-r2:-0.59345\tvalidation_1-r2:-0.635468\n",
      "[302]\tvalidation_0-rmse:8.15047\tvalidation_1-rmse:7.39114\tvalidation_0-r2:-0.59355\tvalidation_1-r2:-0.635527\n",
      "[303]\tvalidation_0-rmse:8.14934\tvalidation_1-rmse:7.39044\tvalidation_0-r2:-0.593663\tvalidation_1-r2:-0.635596\n",
      "[304]\tvalidation_0-rmse:8.14729\tvalidation_1-rmse:7.39001\tvalidation_0-r2:-0.593868\tvalidation_1-r2:-0.635638\n",
      "[305]\tvalidation_0-rmse:8.14573\tvalidation_1-rmse:7.38954\tvalidation_0-r2:-0.594023\tvalidation_1-r2:-0.635685\n",
      "[306]\tvalidation_0-rmse:8.1451\tvalidation_1-rmse:7.38963\tvalidation_0-r2:-0.594086\tvalidation_1-r2:-0.635676\n",
      "[307]\tvalidation_0-rmse:8.14418\tvalidation_1-rmse:7.39002\tvalidation_0-r2:-0.594177\tvalidation_1-r2:-0.635637\n",
      "[308]\tvalidation_0-rmse:8.14271\tvalidation_1-rmse:7.38983\tvalidation_0-r2:-0.594324\tvalidation_1-r2:-0.635656\n",
      "[309]\tvalidation_0-rmse:8.14193\tvalidation_1-rmse:7.39024\tvalidation_0-r2:-0.594402\tvalidation_1-r2:-0.635616\n",
      "[310]\tvalidation_0-rmse:8.14049\tvalidation_1-rmse:7.38983\tvalidation_0-r2:-0.594545\tvalidation_1-r2:-0.635655\n",
      "[311]\tvalidation_0-rmse:8.13842\tvalidation_1-rmse:7.38973\tvalidation_0-r2:-0.594751\tvalidation_1-r2:-0.635666\n",
      "[312]\tvalidation_0-rmse:8.13721\tvalidation_1-rmse:7.39097\tvalidation_0-r2:-0.594872\tvalidation_1-r2:-0.635543\n",
      "[313]\tvalidation_0-rmse:8.13637\tvalidation_1-rmse:7.39135\tvalidation_0-r2:-0.594955\tvalidation_1-r2:-0.635506\n",
      "[314]\tvalidation_0-rmse:8.13525\tvalidation_1-rmse:7.39108\tvalidation_0-r2:-0.595067\tvalidation_1-r2:-0.635532\n",
      "[315]\tvalidation_0-rmse:8.13436\tvalidation_1-rmse:7.39135\tvalidation_0-r2:-0.595155\tvalidation_1-r2:-0.635506\n",
      "[316]\tvalidation_0-rmse:8.13327\tvalidation_1-rmse:7.39101\tvalidation_0-r2:-0.595264\tvalidation_1-r2:-0.63554\n",
      "[317]\tvalidation_0-rmse:8.13128\tvalidation_1-rmse:7.39084\tvalidation_0-r2:-0.595462\tvalidation_1-r2:-0.635557\n",
      "[318]\tvalidation_0-rmse:8.13012\tvalidation_1-rmse:7.39172\tvalidation_0-r2:-0.595577\tvalidation_1-r2:-0.63547\n",
      "[319]\tvalidation_0-rmse:8.12813\tvalidation_1-rmse:7.39157\tvalidation_0-r2:-0.595775\tvalidation_1-r2:-0.635484\n",
      "[320]\tvalidation_0-rmse:8.12708\tvalidation_1-rmse:7.39202\tvalidation_0-r2:-0.595879\tvalidation_1-r2:-0.63544\n",
      "[321]\tvalidation_0-rmse:8.12598\tvalidation_1-rmse:7.39203\tvalidation_0-r2:-0.595989\tvalidation_1-r2:-0.635438\n",
      "[322]\tvalidation_0-rmse:8.12531\tvalidation_1-rmse:7.39247\tvalidation_0-r2:-0.596055\tvalidation_1-r2:-0.635396\n",
      "[323]\tvalidation_0-rmse:8.12406\tvalidation_1-rmse:7.39198\tvalidation_0-r2:-0.596179\tvalidation_1-r2:-0.635444\n",
      "[324]\tvalidation_0-rmse:8.12213\tvalidation_1-rmse:7.39197\tvalidation_0-r2:-0.596371\tvalidation_1-r2:-0.635445\n",
      "[325]\tvalidation_0-rmse:8.12107\tvalidation_1-rmse:7.39164\tvalidation_0-r2:-0.596477\tvalidation_1-r2:-0.635478\n",
      "[326]\tvalidation_0-rmse:8.12005\tvalidation_1-rmse:7.39184\tvalidation_0-r2:-0.596578\tvalidation_1-r2:-0.635458\n",
      "[327]\tvalidation_0-rmse:8.11814\tvalidation_1-rmse:7.39175\tvalidation_0-r2:-0.596769\tvalidation_1-r2:-0.635467\n",
      "[328]\tvalidation_0-rmse:8.11701\tvalidation_1-rmse:7.39271\tvalidation_0-r2:-0.59688\tvalidation_1-r2:-0.635371\n",
      "[329]\tvalidation_0-rmse:8.1161\tvalidation_1-rmse:7.3925\tvalidation_0-r2:-0.59697\tvalidation_1-r2:-0.635392\n",
      "[330]\tvalidation_0-rmse:8.11517\tvalidation_1-rmse:7.39245\tvalidation_0-r2:-0.597064\tvalidation_1-r2:-0.635397\n",
      "[331]\tvalidation_0-rmse:8.11455\tvalidation_1-rmse:7.39239\tvalidation_0-r2:-0.597125\tvalidation_1-r2:-0.635403\n",
      "[332]\tvalidation_0-rmse:8.11378\tvalidation_1-rmse:7.39262\tvalidation_0-r2:-0.597201\tvalidation_1-r2:-0.63538\n",
      "[333]\tvalidation_0-rmse:8.11266\tvalidation_1-rmse:7.39232\tvalidation_0-r2:-0.597312\tvalidation_1-r2:-0.63541\n",
      "[334]\tvalidation_0-rmse:8.11178\tvalidation_1-rmse:7.3921\tvalidation_0-r2:-0.5974\tvalidation_1-r2:-0.635432\n",
      "[335]\tvalidation_0-rmse:8.1099\tvalidation_1-rmse:7.39255\tvalidation_0-r2:-0.597586\tvalidation_1-r2:-0.635387\n",
      "[336]\tvalidation_0-rmse:8.10858\tvalidation_1-rmse:7.39264\tvalidation_0-r2:-0.597718\tvalidation_1-r2:-0.635379\n",
      "[337]\tvalidation_0-rmse:8.10748\tvalidation_1-rmse:7.39332\tvalidation_0-r2:-0.597827\tvalidation_1-r2:-0.635312\n",
      "[338]\tvalidation_0-rmse:8.10648\tvalidation_1-rmse:7.39351\tvalidation_0-r2:-0.597926\tvalidation_1-r2:-0.635293\n",
      "[339]\tvalidation_0-rmse:8.10507\tvalidation_1-rmse:7.39431\tvalidation_0-r2:-0.598066\tvalidation_1-r2:-0.635214\n",
      "[340]\tvalidation_0-rmse:8.10433\tvalidation_1-rmse:7.39471\tvalidation_0-r2:-0.598139\tvalidation_1-r2:-0.635175\n",
      "[341]\tvalidation_0-rmse:8.10357\tvalidation_1-rmse:7.39496\tvalidation_0-r2:-0.598215\tvalidation_1-r2:-0.63515\n",
      "[342]\tvalidation_0-rmse:8.10283\tvalidation_1-rmse:7.39478\tvalidation_0-r2:-0.598287\tvalidation_1-r2:-0.635168\n",
      "[343]\tvalidation_0-rmse:8.10147\tvalidation_1-rmse:7.39557\tvalidation_0-r2:-0.598423\tvalidation_1-r2:-0.63509\n",
      "[344]\tvalidation_0-rmse:8.10023\tvalidation_1-rmse:7.39573\tvalidation_0-r2:-0.598546\tvalidation_1-r2:-0.635074\n",
      "[345]\tvalidation_0-rmse:8.09963\tvalidation_1-rmse:7.39614\tvalidation_0-r2:-0.598605\tvalidation_1-r2:-0.635033\n",
      "[346]\tvalidation_0-rmse:8.09878\tvalidation_1-rmse:7.39604\tvalidation_0-r2:-0.598689\tvalidation_1-r2:-0.635043\n",
      "[347]\tvalidation_0-rmse:8.09792\tvalidation_1-rmse:7.39629\tvalidation_0-r2:-0.598774\tvalidation_1-r2:-0.635019\n",
      "[348]\tvalidation_0-rmse:8.09684\tvalidation_1-rmse:7.3966\tvalidation_0-r2:-0.598882\tvalidation_1-r2:-0.634988\n",
      "[349]\tvalidation_0-rmse:8.09582\tvalidation_1-rmse:7.39662\tvalidation_0-r2:-0.598982\tvalidation_1-r2:-0.634986\n",
      "[350]\tvalidation_0-rmse:8.09499\tvalidation_1-rmse:7.39647\tvalidation_0-r2:-0.599064\tvalidation_1-r2:-0.635001\n",
      "[351]\tvalidation_0-rmse:8.09367\tvalidation_1-rmse:7.39625\tvalidation_0-r2:-0.599196\tvalidation_1-r2:-0.635023\n",
      "[352]\tvalidation_0-rmse:8.09209\tvalidation_1-rmse:7.39639\tvalidation_0-r2:-0.599352\tvalidation_1-r2:-0.635009\n",
      "[353]\tvalidation_0-rmse:8.09118\tvalidation_1-rmse:7.39666\tvalidation_0-r2:-0.599442\tvalidation_1-r2:-0.634982\n",
      "[354]\tvalidation_0-rmse:8.09016\tvalidation_1-rmse:7.39677\tvalidation_0-r2:-0.599543\tvalidation_1-r2:-0.634972\n",
      "[355]\tvalidation_0-rmse:8.08943\tvalidation_1-rmse:7.39675\tvalidation_0-r2:-0.599616\tvalidation_1-r2:-0.634973\n",
      "[356]\tvalidation_0-rmse:8.08852\tvalidation_1-rmse:7.39711\tvalidation_0-r2:-0.599706\tvalidation_1-r2:-0.634938\n",
      "[357]\tvalidation_0-rmse:8.08671\tvalidation_1-rmse:7.39709\tvalidation_0-r2:-0.599884\tvalidation_1-r2:-0.63494\n",
      "[358]\tvalidation_0-rmse:8.08571\tvalidation_1-rmse:7.39743\tvalidation_0-r2:-0.599983\tvalidation_1-r2:-0.634906\n",
      "[359]\tvalidation_0-rmse:8.08477\tvalidation_1-rmse:7.39749\tvalidation_0-r2:-0.600077\tvalidation_1-r2:-0.6349\n",
      "[360]\tvalidation_0-rmse:8.08406\tvalidation_1-rmse:7.39773\tvalidation_0-r2:-0.600147\tvalidation_1-r2:-0.634877\n",
      "[361]\tvalidation_0-rmse:8.08233\tvalidation_1-rmse:7.39778\tvalidation_0-r2:-0.600318\tvalidation_1-r2:-0.634872\n",
      "[362]\tvalidation_0-rmse:8.08128\tvalidation_1-rmse:7.39854\tvalidation_0-r2:-0.600421\tvalidation_1-r2:-0.634797\n",
      "[363]\tvalidation_0-rmse:8.0803\tvalidation_1-rmse:7.39865\tvalidation_0-r2:-0.600519\tvalidation_1-r2:-0.634786\n",
      "[364]\tvalidation_0-rmse:8.07975\tvalidation_1-rmse:7.39881\tvalidation_0-r2:-0.600573\tvalidation_1-r2:-0.63477\n",
      "[365]\tvalidation_0-rmse:8.07807\tvalidation_1-rmse:7.3987\tvalidation_0-r2:-0.600739\tvalidation_1-r2:-0.63478\n",
      "[366]\tvalidation_0-rmse:8.07727\tvalidation_1-rmse:7.39848\tvalidation_0-r2:-0.600818\tvalidation_1-r2:-0.634803\n",
      "[367]\tvalidation_0-rmse:8.07605\tvalidation_1-rmse:7.39832\tvalidation_0-r2:-0.600939\tvalidation_1-r2:-0.634819\n",
      "[368]\tvalidation_0-rmse:8.07416\tvalidation_1-rmse:7.39836\tvalidation_0-r2:-0.601126\tvalidation_1-r2:-0.634814\n",
      "[369]\tvalidation_0-rmse:8.07228\tvalidation_1-rmse:7.39828\tvalidation_0-r2:-0.601311\tvalidation_1-r2:-0.634822\n",
      "[370]\tvalidation_0-rmse:8.07031\tvalidation_1-rmse:7.39833\tvalidation_0-r2:-0.601506\tvalidation_1-r2:-0.634818\n",
      "[371]\tvalidation_0-rmse:8.06894\tvalidation_1-rmse:7.39816\tvalidation_0-r2:-0.601641\tvalidation_1-r2:-0.634834\n",
      "[372]\tvalidation_0-rmse:8.06758\tvalidation_1-rmse:7.39802\tvalidation_0-r2:-0.601775\tvalidation_1-r2:-0.634848\n",
      "[373]\tvalidation_0-rmse:8.0662\tvalidation_1-rmse:7.39786\tvalidation_0-r2:-0.601911\tvalidation_1-r2:-0.634863\n",
      "[374]\tvalidation_0-rmse:8.06489\tvalidation_1-rmse:7.39771\tvalidation_0-r2:-0.602041\tvalidation_1-r2:-0.634878\n",
      "[375]\tvalidation_0-rmse:8.06359\tvalidation_1-rmse:7.39761\tvalidation_0-r2:-0.602169\tvalidation_1-r2:-0.634889\n",
      "[376]\tvalidation_0-rmse:8.0621\tvalidation_1-rmse:7.39712\tvalidation_0-r2:-0.602316\tvalidation_1-r2:-0.634937\n",
      "[377]\tvalidation_0-rmse:8.06078\tvalidation_1-rmse:7.39682\tvalidation_0-r2:-0.602446\tvalidation_1-r2:-0.634967\n",
      "[378]\tvalidation_0-rmse:8.05988\tvalidation_1-rmse:7.39698\tvalidation_0-r2:-0.602535\tvalidation_1-r2:-0.63495\n",
      "[379]\tvalidation_0-rmse:8.05844\tvalidation_1-rmse:7.39686\tvalidation_0-r2:-0.602677\tvalidation_1-r2:-0.634963\n",
      "[380]\tvalidation_0-rmse:8.05673\tvalidation_1-rmse:7.3967\tvalidation_0-r2:-0.602846\tvalidation_1-r2:-0.634978\n",
      "[381]\tvalidation_0-rmse:8.05529\tvalidation_1-rmse:7.39678\tvalidation_0-r2:-0.602987\tvalidation_1-r2:-0.634971\n",
      "[382]\tvalidation_0-rmse:8.05402\tvalidation_1-rmse:7.39651\tvalidation_0-r2:-0.603112\tvalidation_1-r2:-0.634997\n",
      "[383]\tvalidation_0-rmse:8.05278\tvalidation_1-rmse:7.39645\tvalidation_0-r2:-0.603235\tvalidation_1-r2:-0.635003\n",
      "[384]\tvalidation_0-rmse:8.05137\tvalidation_1-rmse:7.39666\tvalidation_0-r2:-0.603374\tvalidation_1-r2:-0.634982\n",
      "[385]\tvalidation_0-rmse:8.05023\tvalidation_1-rmse:7.39659\tvalidation_0-r2:-0.603486\tvalidation_1-r2:-0.634989\n",
      "[386]\tvalidation_0-rmse:8.04863\tvalidation_1-rmse:7.39658\tvalidation_0-r2:-0.603644\tvalidation_1-r2:-0.63499\n",
      "[387]\tvalidation_0-rmse:8.04734\tvalidation_1-rmse:7.39651\tvalidation_0-r2:-0.60377\tvalidation_1-r2:-0.634997\n",
      "[388]\tvalidation_0-rmse:8.04599\tvalidation_1-rmse:7.39664\tvalidation_0-r2:-0.603904\tvalidation_1-r2:-0.634984\n",
      "[389]\tvalidation_0-rmse:8.04486\tvalidation_1-rmse:7.39674\tvalidation_0-r2:-0.604016\tvalidation_1-r2:-0.634974\n",
      "[390]\tvalidation_0-rmse:8.04355\tvalidation_1-rmse:7.39685\tvalidation_0-r2:-0.604144\tvalidation_1-r2:-0.634964\n",
      "[391]\tvalidation_0-rmse:8.04293\tvalidation_1-rmse:7.39725\tvalidation_0-r2:-0.604205\tvalidation_1-r2:-0.634924\n",
      "[392]\tvalidation_0-rmse:8.04161\tvalidation_1-rmse:7.39751\tvalidation_0-r2:-0.604335\tvalidation_1-r2:-0.634899\n",
      "[393]\tvalidation_0-rmse:8.04082\tvalidation_1-rmse:7.398\tvalidation_0-r2:-0.604412\tvalidation_1-r2:-0.63485\n",
      "[394]\tvalidation_0-rmse:8.03964\tvalidation_1-rmse:7.39785\tvalidation_0-r2:-0.604529\tvalidation_1-r2:-0.634864\n",
      "[395]\tvalidation_0-rmse:8.03835\tvalidation_1-rmse:7.39801\tvalidation_0-r2:-0.604656\tvalidation_1-r2:-0.634849\n",
      "[396]\tvalidation_0-rmse:8.03698\tvalidation_1-rmse:7.39814\tvalidation_0-r2:-0.60479\tvalidation_1-r2:-0.634836\n",
      "[397]\tvalidation_0-rmse:8.03584\tvalidation_1-rmse:7.39815\tvalidation_0-r2:-0.604902\tvalidation_1-r2:-0.634835\n",
      "[398]\tvalidation_0-rmse:8.03474\tvalidation_1-rmse:7.39858\tvalidation_0-r2:-0.60501\tvalidation_1-r2:-0.634793\n",
      "[399]\tvalidation_0-rmse:8.0332\tvalidation_1-rmse:7.3986\tvalidation_0-r2:-0.605162\tvalidation_1-r2:-0.63479\n",
      "[400]\tvalidation_0-rmse:8.03207\tvalidation_1-rmse:7.39868\tvalidation_0-r2:-0.605273\tvalidation_1-r2:-0.634783\n",
      "[401]\tvalidation_0-rmse:8.03081\tvalidation_1-rmse:7.39876\tvalidation_0-r2:-0.605397\tvalidation_1-r2:-0.634775\n",
      "[402]\tvalidation_0-rmse:8.02962\tvalidation_1-rmse:7.39892\tvalidation_0-r2:-0.605514\tvalidation_1-r2:-0.634759\n",
      "[403]\tvalidation_0-rmse:8.02854\tvalidation_1-rmse:7.39897\tvalidation_0-r2:-0.60562\tvalidation_1-r2:-0.634755\n",
      "[404]\tvalidation_0-rmse:8.02647\tvalidation_1-rmse:7.39867\tvalidation_0-r2:-0.605823\tvalidation_1-r2:-0.634783\n",
      "[405]\tvalidation_0-rmse:8.02532\tvalidation_1-rmse:7.399\tvalidation_0-r2:-0.605936\tvalidation_1-r2:-0.634751\n",
      "[406]\tvalidation_0-rmse:8.02412\tvalidation_1-rmse:7.399\tvalidation_0-r2:-0.606054\tvalidation_1-r2:-0.634751\n",
      "[407]\tvalidation_0-rmse:8.02247\tvalidation_1-rmse:7.3997\tvalidation_0-r2:-0.606216\tvalidation_1-r2:-0.634682\n",
      "[408]\tvalidation_0-rmse:8.02046\tvalidation_1-rmse:7.39981\tvalidation_0-r2:-0.606414\tvalidation_1-r2:-0.634671\n",
      "[409]\tvalidation_0-rmse:8.01927\tvalidation_1-rmse:7.40008\tvalidation_0-r2:-0.60653\tvalidation_1-r2:-0.634645\n",
      "[410]\tvalidation_0-rmse:8.01793\tvalidation_1-rmse:7.39985\tvalidation_0-r2:-0.606662\tvalidation_1-r2:-0.634667\n",
      "[411]\tvalidation_0-rmse:8.01703\tvalidation_1-rmse:7.40092\tvalidation_0-r2:-0.60675\tvalidation_1-r2:-0.634562\n",
      "[412]\tvalidation_0-rmse:8.01597\tvalidation_1-rmse:7.40083\tvalidation_0-r2:-0.606854\tvalidation_1-r2:-0.63457\n",
      "[413]\tvalidation_0-rmse:8.01484\tvalidation_1-rmse:7.40085\tvalidation_0-r2:-0.606964\tvalidation_1-r2:-0.634568\n",
      "[414]\tvalidation_0-rmse:8.01319\tvalidation_1-rmse:7.40144\tvalidation_0-r2:-0.607127\tvalidation_1-r2:-0.63451\n",
      "[415]\tvalidation_0-rmse:8.01166\tvalidation_1-rmse:7.40103\tvalidation_0-r2:-0.607276\tvalidation_1-r2:-0.63455\n",
      "[416]\tvalidation_0-rmse:8.01074\tvalidation_1-rmse:7.40173\tvalidation_0-r2:-0.607367\tvalidation_1-r2:-0.634481\n",
      "[417]\tvalidation_0-rmse:8.01021\tvalidation_1-rmse:7.40223\tvalidation_0-r2:-0.607419\tvalidation_1-r2:-0.634433\n",
      "[418]\tvalidation_0-rmse:8.00916\tvalidation_1-rmse:7.40261\tvalidation_0-r2:-0.607522\tvalidation_1-r2:-0.634395\n",
      "[419]\tvalidation_0-rmse:8.0073\tvalidation_1-rmse:7.40237\tvalidation_0-r2:-0.607704\tvalidation_1-r2:-0.634419\n",
      "[420]\tvalidation_0-rmse:8.00567\tvalidation_1-rmse:7.40258\tvalidation_0-r2:-0.607864\tvalidation_1-r2:-0.634398\n",
      "[421]\tvalidation_0-rmse:8.0045\tvalidation_1-rmse:7.40232\tvalidation_0-r2:-0.607979\tvalidation_1-r2:-0.634423\n",
      "[422]\tvalidation_0-rmse:8.00348\tvalidation_1-rmse:7.40225\tvalidation_0-r2:-0.608078\tvalidation_1-r2:-0.634431\n",
      "[423]\tvalidation_0-rmse:8.00235\tvalidation_1-rmse:7.40226\tvalidation_0-r2:-0.608189\tvalidation_1-r2:-0.634429\n",
      "[424]\tvalidation_0-rmse:8.00117\tvalidation_1-rmse:7.4023\tvalidation_0-r2:-0.608304\tvalidation_1-r2:-0.634425\n",
      "[425]\tvalidation_0-rmse:8.00005\tvalidation_1-rmse:7.40251\tvalidation_0-r2:-0.608414\tvalidation_1-r2:-0.634404\n",
      "[426]\tvalidation_0-rmse:7.99906\tvalidation_1-rmse:7.40245\tvalidation_0-r2:-0.608511\tvalidation_1-r2:-0.63441\n",
      "[427]\tvalidation_0-rmse:7.9974\tvalidation_1-rmse:7.40235\tvalidation_0-r2:-0.608674\tvalidation_1-r2:-0.634421\n",
      "[428]\tvalidation_0-rmse:7.99631\tvalidation_1-rmse:7.40254\tvalidation_0-r2:-0.608781\tvalidation_1-r2:-0.634402\n",
      "[429]\tvalidation_0-rmse:7.99491\tvalidation_1-rmse:7.40254\tvalidation_0-r2:-0.608917\tvalidation_1-r2:-0.634401\n",
      "[430]\tvalidation_0-rmse:7.99406\tvalidation_1-rmse:7.40283\tvalidation_0-r2:-0.609001\tvalidation_1-r2:-0.634372\n",
      "[431]\tvalidation_0-rmse:7.99226\tvalidation_1-rmse:7.40326\tvalidation_0-r2:-0.609176\tvalidation_1-r2:-0.634331\n",
      "[432]\tvalidation_0-rmse:7.99073\tvalidation_1-rmse:7.4035\tvalidation_0-r2:-0.609326\tvalidation_1-r2:-0.634307\n",
      "[433]\tvalidation_0-rmse:7.98969\tvalidation_1-rmse:7.40382\tvalidation_0-r2:-0.609428\tvalidation_1-r2:-0.634275\n",
      "[434]\tvalidation_0-rmse:7.98842\tvalidation_1-rmse:7.40418\tvalidation_0-r2:-0.609552\tvalidation_1-r2:-0.634239\n",
      "[435]\tvalidation_0-rmse:7.98695\tvalidation_1-rmse:7.40447\tvalidation_0-r2:-0.609695\tvalidation_1-r2:-0.63421\n",
      "[436]\tvalidation_0-rmse:7.98585\tvalidation_1-rmse:7.40421\tvalidation_0-r2:-0.609803\tvalidation_1-r2:-0.634237\n",
      "[437]\tvalidation_0-rmse:7.98409\tvalidation_1-rmse:7.40449\tvalidation_0-r2:-0.609975\tvalidation_1-r2:-0.634209\n",
      "[438]\tvalidation_0-rmse:7.98307\tvalidation_1-rmse:7.40427\tvalidation_0-r2:-0.610075\tvalidation_1-r2:-0.634231\n",
      "[439]\tvalidation_0-rmse:7.98177\tvalidation_1-rmse:7.40437\tvalidation_0-r2:-0.610201\tvalidation_1-r2:-0.634221\n",
      "[440]\tvalidation_0-rmse:7.98052\tvalidation_1-rmse:7.40499\tvalidation_0-r2:-0.610323\tvalidation_1-r2:-0.63416\n",
      "[441]\tvalidation_0-rmse:7.97894\tvalidation_1-rmse:7.40489\tvalidation_0-r2:-0.610478\tvalidation_1-r2:-0.634169\n",
      "[442]\tvalidation_0-rmse:7.97716\tvalidation_1-rmse:7.40542\tvalidation_0-r2:-0.610652\tvalidation_1-r2:-0.634117\n",
      "[443]\tvalidation_0-rmse:7.97614\tvalidation_1-rmse:7.40521\tvalidation_0-r2:-0.610751\tvalidation_1-r2:-0.634138\n",
      "[444]\tvalidation_0-rmse:7.97462\tvalidation_1-rmse:7.40522\tvalidation_0-r2:-0.6109\tvalidation_1-r2:-0.634137\n",
      "[445]\tvalidation_0-rmse:7.97358\tvalidation_1-rmse:7.40525\tvalidation_0-r2:-0.611001\tvalidation_1-r2:-0.634134\n",
      "[446]\tvalidation_0-rmse:7.97216\tvalidation_1-rmse:7.40609\tvalidation_0-r2:-0.61114\tvalidation_1-r2:-0.634051\n",
      "[447]\tvalidation_0-rmse:7.971\tvalidation_1-rmse:7.40633\tvalidation_0-r2:-0.611253\tvalidation_1-r2:-0.634027\n",
      "[448]\tvalidation_0-rmse:7.96998\tvalidation_1-rmse:7.40639\tvalidation_0-r2:-0.611353\tvalidation_1-r2:-0.634021\n",
      "[449]\tvalidation_0-rmse:7.96872\tvalidation_1-rmse:7.40675\tvalidation_0-r2:-0.611475\tvalidation_1-r2:-0.633985\n",
      "[450]\tvalidation_0-rmse:7.96779\tvalidation_1-rmse:7.40747\tvalidation_0-r2:-0.611565\tvalidation_1-r2:-0.633914\n",
      "[451]\tvalidation_0-rmse:7.96675\tvalidation_1-rmse:7.4072\tvalidation_0-r2:-0.611667\tvalidation_1-r2:-0.633941\n",
      "[452]\tvalidation_0-rmse:7.96526\tvalidation_1-rmse:7.40833\tvalidation_0-r2:-0.611812\tvalidation_1-r2:-0.633829\n",
      "[453]\tvalidation_0-rmse:7.96375\tvalidation_1-rmse:7.40875\tvalidation_0-r2:-0.61196\tvalidation_1-r2:-0.633788\n",
      "[454]\tvalidation_0-rmse:7.96274\tvalidation_1-rmse:7.40879\tvalidation_0-r2:-0.612058\tvalidation_1-r2:-0.633784\n",
      "[455]\tvalidation_0-rmse:7.96062\tvalidation_1-rmse:7.40929\tvalidation_0-r2:-0.612265\tvalidation_1-r2:-0.633734\n",
      "[456]\tvalidation_0-rmse:7.95913\tvalidation_1-rmse:7.40928\tvalidation_0-r2:-0.612409\tvalidation_1-r2:-0.633735\n",
      "[457]\tvalidation_0-rmse:7.95812\tvalidation_1-rmse:7.41018\tvalidation_0-r2:-0.612508\tvalidation_1-r2:-0.633646\n",
      "[458]\tvalidation_0-rmse:7.95713\tvalidation_1-rmse:7.4101\tvalidation_0-r2:-0.612605\tvalidation_1-r2:-0.633654\n",
      "[459]\tvalidation_0-rmse:7.95601\tvalidation_1-rmse:7.41021\tvalidation_0-r2:-0.612714\tvalidation_1-r2:-0.633644\n",
      "[460]\tvalidation_0-rmse:7.95479\tvalidation_1-rmse:7.41042\tvalidation_0-r2:-0.612832\tvalidation_1-r2:-0.633622\n",
      "[461]\tvalidation_0-rmse:7.95374\tvalidation_1-rmse:7.41059\tvalidation_0-r2:-0.612934\tvalidation_1-r2:-0.633606\n",
      "[462]\tvalidation_0-rmse:7.95277\tvalidation_1-rmse:7.41046\tvalidation_0-r2:-0.613029\tvalidation_1-r2:-0.633619\n",
      "[463]\tvalidation_0-rmse:7.95159\tvalidation_1-rmse:7.4105\tvalidation_0-r2:-0.613143\tvalidation_1-r2:-0.633615\n",
      "[464]\tvalidation_0-rmse:7.95057\tvalidation_1-rmse:7.41039\tvalidation_0-r2:-0.613243\tvalidation_1-r2:-0.633625\n",
      "[465]\tvalidation_0-rmse:7.94958\tvalidation_1-rmse:7.41046\tvalidation_0-r2:-0.613339\tvalidation_1-r2:-0.633619\n",
      "Stopping. Best iteration:\n",
      "[265]\tvalidation_0-rmse:8.19572\tvalidation_1-rmse:7.38923\tvalidation_0-r2:-0.589024\tvalidation_1-r2:-0.635715\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=100.66931812782134, booster='gbtree',\n",
       "       colsample_bylevel=1, colsample_bytree=0.8, gamma=0,\n",
       "       learning_rate=0.03, max_delta_step=0, max_depth=4,\n",
       "       min_child_weight=5, missing=None, n_estimators=1000, n_jobs=1,\n",
       "       nthread=1, objective='reg:linear', random_state=420, reg_alpha=50,\n",
       "       reg_lambda=100, scale_pos_weight=1, seed=420, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = XGBRegressor(learning_rate=0.03, max_depth=4,\n",
    "                         subsample=1, colsample_bytree=0.8, min_child_weight=5,\n",
    "                         reg_alpha=50, reg_lambda=100, \n",
    "                         n_estimators=1000, base_score=target.mean(), seed=420)\n",
    "model_xgb.fit(X_train, Y_train, eval_metric=r2_eval_xgb, \n",
    "          eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "         early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ica_4', 0.02603369),\n",
       " ('pca_12', 0.021001969),\n",
       " ('pca_46', 0.020126887),\n",
       " ('X47', 0.016845329),\n",
       " ('X315', 0.016626559),\n",
       " ('X314', 0.014657624),\n",
       " ('tsvd_13', 0.014438854),\n",
       " ('ica_24', 0.014438854),\n",
       " ('ica_5', 0.014001313),\n",
       " ('tsvd_11', 0.013782542),\n",
       " ('ica_18', 0.01290746),\n",
       " ('tsvd_8', 0.011813607),\n",
       " ('tsvd_19', 0.011813607),\n",
       " ('grp_13', 0.011594837),\n",
       " ('pca_23', 0.011376066),\n",
       " ('X127', 0.011157296),\n",
       " ('pca_13', 0.011157296),\n",
       " ('pca_48', 0.010719755),\n",
       " ('tsvd_35', 0.010500984),\n",
       " ('X118', 0.010282214),\n",
       " ('pca_45', 0.010282214),\n",
       " ('ica_7', 0.0094071319),\n",
       " ('pca_47', 0.0094071319),\n",
       " ('kc_18', 0.0094071319),\n",
       " ('tsvd_12', 0.0091883615),\n",
       " ('pca_39', 0.0087508205),\n",
       " ('tsvd_6', 0.0085320501),\n",
       " ('pca_9', 0.0083132796),\n",
       " ('ica_28', 0.0083132796),\n",
       " ('tsvd_48', 0.0080945091),\n",
       " ('tsvd_47', 0.0074381973),\n",
       " ('kc_33', 0.0074381973),\n",
       " ('pca_7', 0.0072194268),\n",
       " ('pca_14', 0.0072194268),\n",
       " ('tsvd_18', 0.0072194268),\n",
       " ('pca_19', 0.0072194268),\n",
       " ('tsvd_20', 0.0072194268),\n",
       " ('X5_7', 0.0070006563),\n",
       " ('tsvd_28', 0.0070006563),\n",
       " ('tsvd_32', 0.0067818859),\n",
       " ('pca_25', 0.0065631154),\n",
       " ('ica_45', 0.0065631154),\n",
       " ('ica_2', 0.0063443449),\n",
       " ('pca_18', 0.0063443449),\n",
       " ('tsvd_39', 0.0063443449),\n",
       " ('pca_43', 0.0063443449),\n",
       " ('ica_46', 0.0063443449),\n",
       " ('ica_11', 0.0061255745),\n",
       " ('pca_38', 0.0061255745),\n",
       " ('ica_1', 0.0059068035),\n",
       " ('grp_21', 0.0059068035),\n",
       " ('kc_9', 0.0059068035),\n",
       " ('tsvd_5', 0.0056880331),\n",
       " ('ica_17', 0.0056880331),\n",
       " ('ica_25', 0.0054692626),\n",
       " ('ica_41', 0.0054692626),\n",
       " ('ica_44', 0.0054692626),\n",
       " ('tsvd_1', 0.0052504921),\n",
       " ('ica_8', 0.0052504921),\n",
       " ('ica_16', 0.0052504921),\n",
       " ('pca_35', 0.0052504921),\n",
       " ('tsvd_9', 0.0050317217),\n",
       " ('pca_3', 0.0048129512),\n",
       " ('ica_14', 0.0048129512),\n",
       " ('ica_22', 0.0048129512),\n",
       " ('tsvd_46', 0.0048129512),\n",
       " ('tsvd_14', 0.0045941807),\n",
       " ('ica_23', 0.0045941807),\n",
       " ('ica_32', 0.0045941807),\n",
       " ('ica_37', 0.0045941807),\n",
       " ('tsvd_4', 0.0043754103),\n",
       " ('pca_17', 0.0043754103),\n",
       " ('tsvd_25', 0.0043754103),\n",
       " ('pca_34', 0.0043754103),\n",
       " ('ica_40', 0.0043754103),\n",
       " ('srp_6', 0.0041566398),\n",
       " ('ica_15', 0.0041566398),\n",
       " ('tsvd_17', 0.0041566398),\n",
       " ('ica_27', 0.0041566398),\n",
       " ('ica_31', 0.0041566398),\n",
       " ('pca_36', 0.0041566398),\n",
       " ('tsvd_41', 0.0041566398),\n",
       " ('kc_4', 0.0041566398),\n",
       " ('pca_6', 0.0039378693),\n",
       " ('ica_6', 0.0039378693),\n",
       " ('ica_9', 0.0039378693),\n",
       " ('ica_12', 0.0039378693),\n",
       " ('tsvd_23', 0.0039378693),\n",
       " ('tsvd_40', 0.0039378693),\n",
       " ('ica_42', 0.0039378693),\n",
       " ('tsvd_43', 0.0039378693),\n",
       " ('ica_0', 0.0037190986),\n",
       " ('ica_3', 0.0037190986),\n",
       " ('pca_30', 0.0037190986),\n",
       " ('pca_41', 0.0037190986),\n",
       " ('ica_43', 0.0037190986),\n",
       " ('pca_44', 0.0037190986),\n",
       " ('X189', 0.0035003282),\n",
       " ('X5_17', 0.0035003282),\n",
       " ('pca_1', 0.0035003282),\n",
       " ('grp_2', 0.0035003282),\n",
       " ('pca_20', 0.0035003282),\n",
       " ('pca_27', 0.0035003282),\n",
       " ('ica_36', 0.0035003282),\n",
       " ('ica_38', 0.0035003282),\n",
       " ('kc_2', 0.0035003282),\n",
       " ('pca_8', 0.0032815577),\n",
       " ('pca_10', 0.0032815577),\n",
       " ('tsvd_16', 0.0032815577),\n",
       " ('srp_22', 0.0032815577),\n",
       " ('pca_29', 0.0032815577),\n",
       " ('tsvd_33', 0.0032815577),\n",
       " ('pca_49', 0.0032815577),\n",
       " ('pca_5', 0.0030627872),\n",
       " ('pca_15', 0.0030627872),\n",
       " ('ica_21', 0.0030627872),\n",
       " ('pca_22', 0.0030627872),\n",
       " ('tsvd_22', 0.0030627872),\n",
       " ('ica_29', 0.0030627872),\n",
       " ('tsvd_29', 0.0030627872),\n",
       " ('ica_30', 0.0030627872),\n",
       " ('ica_34', 0.0030627872),\n",
       " ('tsvd_34', 0.0030627872),\n",
       " ('ica_39', 0.0030627872),\n",
       " ('ica_47', 0.0030627872),\n",
       " ('grp_15', 0.0028440165),\n",
       " ('pca_42', 0.0028440165),\n",
       " ('srp_44', 0.0028440165),\n",
       " ('X261', 0.0026252461),\n",
       " ('X5_19', 0.0026252461),\n",
       " ('tsvd_2', 0.0026252461),\n",
       " ('ica_10', 0.0026252461),\n",
       " ('tsvd_10', 0.0026252461),\n",
       " ('ica_20', 0.0026252461),\n",
       " ('ica_26', 0.0026252461),\n",
       " ('tsvd_26', 0.0026252461),\n",
       " ('pca_28', 0.0026252461),\n",
       " ('pca_33', 0.0026252461),\n",
       " ('kc_27', 0.0026252461),\n",
       " ('pca_11', 0.0024064756),\n",
       " ('tsvd_15', 0.0024064756),\n",
       " ('ica_19', 0.0024064756),\n",
       " ('pca_24', 0.0024064756),\n",
       " ('pca_37', 0.0024064756),\n",
       " ('tsvd_37', 0.0024064756),\n",
       " ('pca_4', 0.0021877051),\n",
       " ('grp_5', 0.0021877051),\n",
       " ('srp_7', 0.0021877051),\n",
       " ('tsvd_21', 0.0021877051),\n",
       " ('pca_31', 0.0021877051),\n",
       " ('pca_32', 0.0021877051),\n",
       " ('srp_33', 0.0021877051),\n",
       " ('tsvd_45', 0.0021877051),\n",
       " ('kc_5', 0.0021877051),\n",
       " ('kc_19', 0.0021877051),\n",
       " ('X6', 0.0019689347),\n",
       " ('X115', 0.0019689347),\n",
       " ('X119', 0.0019689347),\n",
       " ('pca_21', 0.0019689347),\n",
       " ('tsvd_24', 0.0019689347),\n",
       " ('tsvd_27', 0.0019689347),\n",
       " ('tsvd_31', 0.0019689347),\n",
       " ('ica_35', 0.0019689347),\n",
       " ('tsvd_49', 0.0019689347),\n",
       " ('kc_8', 0.0019689347),\n",
       " ('kc_20', 0.0019689347),\n",
       " ('kc_30', 0.0019689347),\n",
       " ('X29', 0.0017501641),\n",
       " ('X275', 0.0017501641),\n",
       " ('pca_2', 0.0017501641),\n",
       " ('srp_2', 0.0017501641),\n",
       " ('grp_12', 0.0017501641),\n",
       " ('srp_20', 0.0017501641),\n",
       " ('tsvd_30', 0.0017501641),\n",
       " ('kc_0', 0.0017501641),\n",
       " ('grp_9', 0.0015313936),\n",
       " ('pca_26', 0.0015313936),\n",
       " ('srp_27', 0.0015313936),\n",
       " ('srp_34', 0.0015313936),\n",
       " ('tsvd_36', 0.0015313936),\n",
       " ('tsvd_38', 0.0015313936),\n",
       " ('tsvd_42', 0.0015313936),\n",
       " ('tsvd_44', 0.0015313936),\n",
       " ('ica_49', 0.0015313936),\n",
       " ('kc_10', 0.0015313936),\n",
       " ('X0', 0.001312623),\n",
       " ('X49', 0.001312623),\n",
       " ('tsvd_3', 0.001312623),\n",
       " ('pca_16', 0.001312623),\n",
       " ('srp_21', 0.001312623),\n",
       " ('srp_25', 0.001312623),\n",
       " ('pca_40', 0.001312623),\n",
       " ('ica_48', 0.001312623),\n",
       " ('kc_1', 0.001312623),\n",
       " ('X223', 0.0010938526),\n",
       " ('grp_3', 0.0010938526),\n",
       " ('grp_7', 0.0010938526),\n",
       " ('grp_16', 0.0010938526),\n",
       " ('srp_24', 0.0010938526),\n",
       " ('ica_33', 0.0010938526),\n",
       " ('kc_12', 0.0010938526),\n",
       " ('kc_22', 0.0010938526),\n",
       " ('kc_29', 0.0010938526),\n",
       " ('kc_36', 0.0010938526),\n",
       " ('ID', 0.00087508204),\n",
       " ('X5', 0.00087508204),\n",
       " ('X232', 0.00087508204),\n",
       " ('X5_3', 0.00087508204),\n",
       " ('grp_1', 0.00087508204),\n",
       " ('ica_13', 0.00087508204),\n",
       " ('srp_41', 0.00087508204),\n",
       " ('srp_47', 0.00087508204),\n",
       " ('kc_13', 0.00087508204),\n",
       " ('X51', 0.00065631152),\n",
       " ('X58', 0.00065631152),\n",
       " ('X5_24', 0.00065631152),\n",
       " ('pca_0', 0.00065631152),\n",
       " ('srp_4', 0.00065631152),\n",
       " ('tsvd_7', 0.00065631152),\n",
       " ('srp_16', 0.00065631152),\n",
       " ('srp_19', 0.00065631152),\n",
       " ('grp_23', 0.00065631152),\n",
       " ('srp_31', 0.00065631152),\n",
       " ('grp_32', 0.00065631152),\n",
       " ('grp_44', 0.00065631152),\n",
       " ('srp_49', 0.00065631152),\n",
       " ('kc_3', 0.00065631152),\n",
       " ('kc_25', 0.00065631152),\n",
       " ('kc_38', 0.00065631152),\n",
       " ('X1', 0.00043754102),\n",
       " ('X50', 0.00043754102),\n",
       " ('X64', 0.00043754102),\n",
       " ('X285', 0.00043754102),\n",
       " ('X5_20', 0.00043754102),\n",
       " ('X5_25', 0.00043754102),\n",
       " ('X6_7', 0.00043754102),\n",
       " ('X6_8', 0.00043754102),\n",
       " ('grp_20', 0.00043754102),\n",
       " ('srp_23', 0.00043754102),\n",
       " ('srp_28', 0.00043754102),\n",
       " ('srp_36', 0.00043754102),\n",
       " ('srp_37', 0.00043754102),\n",
       " ('srp_40', 0.00043754102),\n",
       " ('grp_43', 0.00043754102),\n",
       " ('kc_6', 0.00043754102),\n",
       " ('kc_7', 0.00043754102),\n",
       " ('kc_15', 0.00043754102),\n",
       " ('kc_16', 0.00043754102),\n",
       " ('kc_26', 0.00043754102),\n",
       " ('X8', 0.00021877051),\n",
       " ('X54', 0.00021877051),\n",
       " ('X80', 0.00021877051),\n",
       " ('X103', 0.00021877051),\n",
       " ('X114', 0.00021877051),\n",
       " ('X117', 0.00021877051),\n",
       " ('X163', 0.00021877051),\n",
       " ('X171', 0.00021877051),\n",
       " ('X179', 0.00021877051),\n",
       " ('X187', 0.00021877051),\n",
       " ('X220', 0.00021877051),\n",
       " ('X279', 0.00021877051),\n",
       " ('X336', 0.00021877051),\n",
       " ('X355', 0.00021877051),\n",
       " ('X1_20', 0.00021877051),\n",
       " ('X5_22', 0.00021877051),\n",
       " ('X5_23', 0.00021877051),\n",
       " ('X5_28', 0.00021877051),\n",
       " ('X6_0', 0.00021877051),\n",
       " ('grp_0', 0.00021877051),\n",
       " ('grp_8', 0.00021877051),\n",
       " ('srp_8', 0.00021877051),\n",
       " ('srp_9', 0.00021877051),\n",
       " ('srp_15', 0.00021877051),\n",
       " ('grp_18', 0.00021877051),\n",
       " ('grp_22', 0.00021877051),\n",
       " ('srp_26', 0.00021877051),\n",
       " ('srp_29', 0.00021877051),\n",
       " ('srp_30', 0.00021877051),\n",
       " ('grp_31', 0.00021877051),\n",
       " ('grp_33', 0.00021877051),\n",
       " ('grp_34', 0.00021877051),\n",
       " ('grp_37', 0.00021877051),\n",
       " ('srp_39', 0.00021877051),\n",
       " ('srp_42', 0.00021877051),\n",
       " ('srp_45', 0.00021877051),\n",
       " ('srp_46', 0.00021877051),\n",
       " ('srp_48', 0.00021877051),\n",
       " ('kc_11', 0.00021877051),\n",
       " ('kc_14', 0.00021877051),\n",
       " ('kc_17', 0.00021877051),\n",
       " ('kc_24', 0.00021877051),\n",
       " ('kc_32', 0.00021877051),\n",
       " ('kc_34', 0.00021877051),\n",
       " ('kc_35', 0.00021877051),\n",
       " ('kc_37', 0.00021877051),\n",
       " ('kc_39', 0.00021877051),\n",
       " ('X2', 0.0),\n",
       " ('X3', 0.0),\n",
       " ('X4', 0.0),\n",
       " ('X10', 0.0),\n",
       " ('X11', 0.0),\n",
       " ('X12', 0.0),\n",
       " ('X13', 0.0),\n",
       " ('X14', 0.0),\n",
       " ('X15', 0.0),\n",
       " ('X16', 0.0),\n",
       " ('X17', 0.0),\n",
       " ('X18', 0.0),\n",
       " ('X19', 0.0),\n",
       " ('X20', 0.0),\n",
       " ('X21', 0.0),\n",
       " ('X22', 0.0),\n",
       " ('X23', 0.0),\n",
       " ('X24', 0.0),\n",
       " ('X26', 0.0),\n",
       " ('X27', 0.0),\n",
       " ('X28', 0.0),\n",
       " ('X30', 0.0),\n",
       " ('X31', 0.0),\n",
       " ('X32', 0.0),\n",
       " ('X33', 0.0),\n",
       " ('X34', 0.0),\n",
       " ('X35', 0.0),\n",
       " ('X36', 0.0),\n",
       " ('X37', 0.0),\n",
       " ('X38', 0.0),\n",
       " ('X39', 0.0),\n",
       " ('X40', 0.0),\n",
       " ('X41', 0.0),\n",
       " ('X42', 0.0),\n",
       " ('X43', 0.0),\n",
       " ('X44', 0.0),\n",
       " ('X45', 0.0),\n",
       " ('X46', 0.0),\n",
       " ('X48', 0.0),\n",
       " ('X52', 0.0),\n",
       " ('X53', 0.0),\n",
       " ('X55', 0.0),\n",
       " ('X56', 0.0),\n",
       " ('X57', 0.0),\n",
       " ('X59', 0.0),\n",
       " ('X60', 0.0),\n",
       " ('X61', 0.0),\n",
       " ('X62', 0.0),\n",
       " ('X63', 0.0),\n",
       " ('X65', 0.0),\n",
       " ('X66', 0.0),\n",
       " ('X67', 0.0),\n",
       " ('X68', 0.0),\n",
       " ('X69', 0.0),\n",
       " ('X70', 0.0),\n",
       " ('X71', 0.0),\n",
       " ('X73', 0.0),\n",
       " ('X74', 0.0),\n",
       " ('X75', 0.0),\n",
       " ('X76', 0.0),\n",
       " ('X77', 0.0),\n",
       " ('X78', 0.0),\n",
       " ('X79', 0.0),\n",
       " ('X81', 0.0),\n",
       " ('X82', 0.0),\n",
       " ('X83', 0.0),\n",
       " ('X84', 0.0),\n",
       " ('X85', 0.0),\n",
       " ('X86', 0.0),\n",
       " ('X87', 0.0),\n",
       " ('X88', 0.0),\n",
       " ('X89', 0.0),\n",
       " ('X90', 0.0),\n",
       " ('X91', 0.0),\n",
       " ('X92', 0.0),\n",
       " ('X93', 0.0),\n",
       " ('X94', 0.0),\n",
       " ('X95', 0.0),\n",
       " ('X96', 0.0),\n",
       " ('X97', 0.0),\n",
       " ('X98', 0.0),\n",
       " ('X99', 0.0),\n",
       " ('X100', 0.0),\n",
       " ('X101', 0.0),\n",
       " ('X102', 0.0),\n",
       " ('X104', 0.0),\n",
       " ('X105', 0.0),\n",
       " ('X106', 0.0),\n",
       " ('X107', 0.0),\n",
       " ('X108', 0.0),\n",
       " ('X109', 0.0),\n",
       " ('X110', 0.0),\n",
       " ('X111', 0.0),\n",
       " ('X112', 0.0),\n",
       " ('X113', 0.0),\n",
       " ('X116', 0.0),\n",
       " ('X120', 0.0),\n",
       " ('X122', 0.0),\n",
       " ('X123', 0.0),\n",
       " ('X124', 0.0),\n",
       " ('X125', 0.0),\n",
       " ('X126', 0.0),\n",
       " ('X128', 0.0),\n",
       " ('X129', 0.0),\n",
       " ('X130', 0.0),\n",
       " ('X131', 0.0),\n",
       " ('X132', 0.0),\n",
       " ('X133', 0.0),\n",
       " ('X134', 0.0),\n",
       " ('X135', 0.0),\n",
       " ('X136', 0.0),\n",
       " ('X137', 0.0),\n",
       " ('X138', 0.0),\n",
       " ('X139', 0.0),\n",
       " ('X140', 0.0),\n",
       " ('X141', 0.0),\n",
       " ('X142', 0.0),\n",
       " ('X143', 0.0),\n",
       " ('X144', 0.0),\n",
       " ('X145', 0.0),\n",
       " ('X146', 0.0),\n",
       " ('X147', 0.0),\n",
       " ('X148', 0.0),\n",
       " ('X150', 0.0),\n",
       " ('X151', 0.0),\n",
       " ('X152', 0.0),\n",
       " ('X153', 0.0),\n",
       " ('X154', 0.0),\n",
       " ('X155', 0.0),\n",
       " ('X156', 0.0),\n",
       " ('X157', 0.0),\n",
       " ('X158', 0.0),\n",
       " ('X159', 0.0),\n",
       " ('X160', 0.0),\n",
       " ('X161', 0.0),\n",
       " ('X162', 0.0),\n",
       " ('X164', 0.0),\n",
       " ('X165', 0.0),\n",
       " ('X166', 0.0),\n",
       " ('X167', 0.0),\n",
       " ('X168', 0.0),\n",
       " ('X169', 0.0),\n",
       " ('X170', 0.0),\n",
       " ('X172', 0.0),\n",
       " ('X173', 0.0),\n",
       " ('X174', 0.0),\n",
       " ('X175', 0.0),\n",
       " ('X176', 0.0),\n",
       " ('X177', 0.0),\n",
       " ('X178', 0.0),\n",
       " ('X180', 0.0),\n",
       " ('X181', 0.0),\n",
       " ('X182', 0.0),\n",
       " ('X183', 0.0),\n",
       " ('X184', 0.0),\n",
       " ('X185', 0.0),\n",
       " ('X186', 0.0),\n",
       " ('X190', 0.0),\n",
       " ('X191', 0.0),\n",
       " ('X192', 0.0),\n",
       " ('X194', 0.0),\n",
       " ('X195', 0.0),\n",
       " ('X196', 0.0),\n",
       " ('X197', 0.0),\n",
       " ('X198', 0.0),\n",
       " ('X199', 0.0),\n",
       " ('X200', 0.0),\n",
       " ('X201', 0.0),\n",
       " ('X202', 0.0),\n",
       " ('X203', 0.0),\n",
       " ('X204', 0.0),\n",
       " ('X205', 0.0),\n",
       " ('X206', 0.0),\n",
       " ('X207', 0.0),\n",
       " ('X208', 0.0),\n",
       " ('X209', 0.0),\n",
       " ('X210', 0.0),\n",
       " ('X211', 0.0),\n",
       " ('X212', 0.0),\n",
       " ('X213', 0.0),\n",
       " ('X214', 0.0),\n",
       " ('X215', 0.0),\n",
       " ('X216', 0.0),\n",
       " ('X217', 0.0),\n",
       " ('X218', 0.0),\n",
       " ('X219', 0.0),\n",
       " ('X221', 0.0),\n",
       " ('X222', 0.0),\n",
       " ('X224', 0.0),\n",
       " ('X225', 0.0),\n",
       " ('X226', 0.0),\n",
       " ('X227', 0.0),\n",
       " ('X228', 0.0),\n",
       " ('X229', 0.0),\n",
       " ('X230', 0.0),\n",
       " ('X231', 0.0),\n",
       " ('X233', 0.0),\n",
       " ('X234', 0.0),\n",
       " ('X235', 0.0),\n",
       " ('X236', 0.0),\n",
       " ('X237', 0.0),\n",
       " ('X238', 0.0),\n",
       " ('X239', 0.0),\n",
       " ('X240', 0.0),\n",
       " ('X241', 0.0),\n",
       " ('X242', 0.0),\n",
       " ('X243', 0.0),\n",
       " ('X244', 0.0),\n",
       " ('X245', 0.0),\n",
       " ('X246', 0.0),\n",
       " ('X247', 0.0),\n",
       " ('X248', 0.0),\n",
       " ('X249', 0.0),\n",
       " ('X250', 0.0),\n",
       " ('X251', 0.0),\n",
       " ('X252', 0.0),\n",
       " ('X253', 0.0),\n",
       " ('X254', 0.0),\n",
       " ('X255', 0.0),\n",
       " ('X256', 0.0),\n",
       " ('X257', 0.0),\n",
       " ('X258', 0.0),\n",
       " ('X259', 0.0),\n",
       " ('X260', 0.0),\n",
       " ('X262', 0.0),\n",
       " ('X263', 0.0),\n",
       " ('X264', 0.0),\n",
       " ('X265', 0.0),\n",
       " ('X266', 0.0),\n",
       " ('X267', 0.0),\n",
       " ('X268', 0.0),\n",
       " ('X269', 0.0),\n",
       " ('X270', 0.0),\n",
       " ('X271', 0.0),\n",
       " ('X272', 0.0),\n",
       " ('X273', 0.0),\n",
       " ('X274', 0.0),\n",
       " ('X276', 0.0),\n",
       " ('X277', 0.0),\n",
       " ('X278', 0.0),\n",
       " ('X280', 0.0),\n",
       " ('X281', 0.0),\n",
       " ('X282', 0.0),\n",
       " ('X283', 0.0),\n",
       " ('X284', 0.0),\n",
       " ('X286', 0.0),\n",
       " ('X287', 0.0),\n",
       " ('X288', 0.0),\n",
       " ('X289', 0.0),\n",
       " ('X290', 0.0),\n",
       " ('X291', 0.0),\n",
       " ('X292', 0.0),\n",
       " ('X293', 0.0),\n",
       " ('X294', 0.0),\n",
       " ('X295', 0.0),\n",
       " ('X296', 0.0),\n",
       " ('X297', 0.0),\n",
       " ('X298', 0.0),\n",
       " ('X299', 0.0),\n",
       " ('X300', 0.0),\n",
       " ('X301', 0.0),\n",
       " ('X302', 0.0),\n",
       " ('X304', 0.0),\n",
       " ('X305', 0.0),\n",
       " ('X306', 0.0),\n",
       " ('X307', 0.0),\n",
       " ('X308', 0.0),\n",
       " ('X309', 0.0),\n",
       " ('X310', 0.0),\n",
       " ('X311', 0.0),\n",
       " ('X312', 0.0),\n",
       " ('X313', 0.0),\n",
       " ('X316', 0.0),\n",
       " ('X317', 0.0),\n",
       " ('X318', 0.0),\n",
       " ('X319', 0.0),\n",
       " ('X320', 0.0),\n",
       " ('X321', 0.0),\n",
       " ('X322', 0.0),\n",
       " ('X323', 0.0),\n",
       " ('X324', 0.0),\n",
       " ('X325', 0.0),\n",
       " ('X326', 0.0),\n",
       " ('X327', 0.0),\n",
       " ('X328', 0.0),\n",
       " ('X329', 0.0),\n",
       " ('X330', 0.0),\n",
       " ('X331', 0.0),\n",
       " ('X332', 0.0),\n",
       " ('X333', 0.0),\n",
       " ('X334', 0.0),\n",
       " ('X335', 0.0),\n",
       " ('X337', 0.0),\n",
       " ('X338', 0.0),\n",
       " ('X339', 0.0),\n",
       " ('X340', 0.0),\n",
       " ('X341', 0.0),\n",
       " ('X342', 0.0),\n",
       " ('X343', 0.0),\n",
       " ('X344', 0.0),\n",
       " ('X345', 0.0),\n",
       " ('X346', 0.0),\n",
       " ('X347', 0.0),\n",
       " ('X348', 0.0),\n",
       " ('X349', 0.0),\n",
       " ('X350', 0.0),\n",
       " ('X351', 0.0),\n",
       " ('X352', 0.0),\n",
       " ('X353', 0.0),\n",
       " ('X354', 0.0),\n",
       " ('X356', 0.0),\n",
       " ('X357', 0.0),\n",
       " ('X358', 0.0),\n",
       " ('X359', 0.0),\n",
       " ('X360', 0.0),\n",
       " ('X361', 0.0),\n",
       " ('X362', 0.0),\n",
       " ('X363', 0.0),\n",
       " ('X364', 0.0),\n",
       " ('X365', 0.0),\n",
       " ('X366', 0.0),\n",
       " ('X367', 0.0),\n",
       " ('X368', 0.0),\n",
       " ('X369', 0.0),\n",
       " ('X370', 0.0),\n",
       " ('X371', 0.0),\n",
       " ('X372', 0.0),\n",
       " ('X373', 0.0),\n",
       " ('X374', 0.0),\n",
       " ('X375', 0.0),\n",
       " ('X376', 0.0),\n",
       " ('X377', 0.0),\n",
       " ('X378', 0.0),\n",
       " ('X379', 0.0),\n",
       " ('X380', 0.0),\n",
       " ('X382', 0.0),\n",
       " ('X383', 0.0),\n",
       " ('X384', 0.0),\n",
       " ('X385', 0.0),\n",
       " ('X0_0', 0.0),\n",
       " ('X0_1', 0.0),\n",
       " ('X0_2', 0.0),\n",
       " ('X0_3', 0.0),\n",
       " ('X0_4', 0.0),\n",
       " ('X0_5', 0.0),\n",
       " ('X0_6', 0.0),\n",
       " ('X0_7', 0.0),\n",
       " ('X0_8', 0.0),\n",
       " ('X0_9', 0.0),\n",
       " ('X0_10', 0.0),\n",
       " ('X0_11', 0.0),\n",
       " ('X0_12', 0.0),\n",
       " ('X0_13', 0.0),\n",
       " ('X0_14', 0.0),\n",
       " ('X0_15', 0.0),\n",
       " ('X0_16', 0.0),\n",
       " ('X0_17', 0.0),\n",
       " ('X0_18', 0.0),\n",
       " ('X0_19', 0.0),\n",
       " ('X0_20', 0.0),\n",
       " ('X0_21', 0.0),\n",
       " ('X0_22', 0.0),\n",
       " ('X0_23', 0.0),\n",
       " ('X0_24', 0.0),\n",
       " ('X0_25', 0.0),\n",
       " ('X0_26', 0.0),\n",
       " ('X0_27', 0.0),\n",
       " ('X0_28', 0.0),\n",
       " ('X0_29', 0.0),\n",
       " ('X0_30', 0.0),\n",
       " ('X0_31', 0.0),\n",
       " ('X0_32', 0.0),\n",
       " ('X0_33', 0.0),\n",
       " ('X0_34', 0.0),\n",
       " ('X0_35', 0.0),\n",
       " ('X0_36', 0.0),\n",
       " ('X0_37', 0.0),\n",
       " ('X0_38', 0.0),\n",
       " ('X0_39', 0.0),\n",
       " ('X0_40', 0.0),\n",
       " ('X0_41', 0.0),\n",
       " ('X0_42', 0.0),\n",
       " ('X0_43', 0.0),\n",
       " ('X0_44', 0.0),\n",
       " ('X0_45', 0.0),\n",
       " ('X0_46', 0.0),\n",
       " ('X0_47', 0.0),\n",
       " ('X0_48', 0.0),\n",
       " ('X0_49', 0.0),\n",
       " ('X0_50', 0.0),\n",
       " ('X0_51', 0.0),\n",
       " ('X0_52', 0.0),\n",
       " ('X0_ymean', 0.0),\n",
       " ('X1_0', 0.0),\n",
       " ('X1_1', 0.0),\n",
       " ('X1_2', 0.0),\n",
       " ('X1_3', 0.0),\n",
       " ('X1_4', 0.0),\n",
       " ('X1_5', 0.0),\n",
       " ('X1_6', 0.0),\n",
       " ('X1_7', 0.0),\n",
       " ('X1_8', 0.0),\n",
       " ('X1_9', 0.0),\n",
       " ('X1_10', 0.0),\n",
       " ('X1_11', 0.0),\n",
       " ('X1_12', 0.0),\n",
       " ('X1_13', 0.0),\n",
       " ('X1_14', 0.0),\n",
       " ('X1_15', 0.0),\n",
       " ('X1_16', 0.0),\n",
       " ('X1_17', 0.0),\n",
       " ('X1_18', 0.0),\n",
       " ('X1_19', 0.0),\n",
       " ('X1_21', 0.0),\n",
       " ('X1_22', 0.0),\n",
       " ('X1_23', 0.0),\n",
       " ('X1_24', 0.0),\n",
       " ('X1_25', 0.0),\n",
       " ('X1_26', 0.0),\n",
       " ('X1_ymean', 0.0),\n",
       " ('X2_0', 0.0),\n",
       " ('X2_1', 0.0),\n",
       " ('X2_2', 0.0),\n",
       " ('X2_3', 0.0),\n",
       " ('X2_4', 0.0),\n",
       " ('X2_5', 0.0),\n",
       " ('X2_6', 0.0),\n",
       " ('X2_7', 0.0),\n",
       " ('X2_8', 0.0),\n",
       " ('X2_9', 0.0),\n",
       " ('X2_10', 0.0),\n",
       " ('X2_11', 0.0),\n",
       " ('X2_12', 0.0),\n",
       " ('X2_13', 0.0),\n",
       " ('X2_14', 0.0),\n",
       " ('X2_15', 0.0),\n",
       " ('X2_16', 0.0),\n",
       " ('X2_17', 0.0),\n",
       " ('X2_18', 0.0),\n",
       " ('X2_19', 0.0),\n",
       " ('X2_20', 0.0),\n",
       " ('X2_21', 0.0),\n",
       " ('X2_22', 0.0),\n",
       " ('X2_23', 0.0),\n",
       " ('X2_24', 0.0),\n",
       " ('X2_25', 0.0),\n",
       " ('X2_26', 0.0),\n",
       " ('X2_27', 0.0),\n",
       " ('X2_28', 0.0),\n",
       " ('X2_29', 0.0),\n",
       " ('X2_30', 0.0),\n",
       " ('X2_31', 0.0),\n",
       " ('X2_32', 0.0),\n",
       " ('X2_33', 0.0),\n",
       " ('X2_34', 0.0),\n",
       " ('X2_35', 0.0),\n",
       " ('X2_36', 0.0),\n",
       " ('X2_37', 0.0),\n",
       " ('X2_38', 0.0),\n",
       " ('X2_39', 0.0),\n",
       " ('X2_40', 0.0),\n",
       " ('X2_41', 0.0),\n",
       " ('X2_42', 0.0),\n",
       " ('X2_43', 0.0),\n",
       " ('X2_44', 0.0),\n",
       " ('X2_45', 0.0),\n",
       " ('X2_46', 0.0),\n",
       " ('X2_47', 0.0),\n",
       " ('X2_48', 0.0),\n",
       " ('X2_49', 0.0),\n",
       " ('X2_ymean', 0.0),\n",
       " ('X3_0', 0.0),\n",
       " ('X3_1', 0.0),\n",
       " ('X3_2', 0.0),\n",
       " ('X3_3', 0.0),\n",
       " ('X3_4', 0.0),\n",
       " ('X3_5', 0.0),\n",
       " ('X3_6', 0.0),\n",
       " ('X3_ymean', 0.0),\n",
       " ('X4_0', 0.0),\n",
       " ('X4_1', 0.0),\n",
       " ('X4_2', 0.0),\n",
       " ('X4_3', 0.0),\n",
       " ('X4_ymean', 0.0),\n",
       " ('X5_0', 0.0),\n",
       " ('X5_1', 0.0),\n",
       " ('X5_2', 0.0),\n",
       " ('X5_4', 0.0),\n",
       " ('X5_5', 0.0),\n",
       " ('X5_6', 0.0),\n",
       " ('X5_8', 0.0),\n",
       " ('X5_9', 0.0),\n",
       " ('X5_10', 0.0),\n",
       " ('X5_11', 0.0),\n",
       " ('X5_12', 0.0),\n",
       " ('X5_13', 0.0),\n",
       " ('X5_14', 0.0),\n",
       " ('X5_15', 0.0),\n",
       " ('X5_16', 0.0),\n",
       " ('X5_18', 0.0),\n",
       " ('X5_21', 0.0),\n",
       " ('X5_26', 0.0),\n",
       " ('X5_27', 0.0),\n",
       " ('X5_29', 0.0),\n",
       " ('X5_30', 0.0),\n",
       " ('X5_31', 0.0),\n",
       " ('X5_32', 0.0),\n",
       " ('X5_ymean', 0.0),\n",
       " ('X6_1', 0.0),\n",
       " ('X6_2', 0.0),\n",
       " ('X6_3', 0.0),\n",
       " ('X6_4', 0.0),\n",
       " ('X6_5', 0.0),\n",
       " ('X6_6', 0.0),\n",
       " ('X6_9', 0.0),\n",
       " ('X6_10', 0.0),\n",
       " ('X6_11', 0.0),\n",
       " ('X6_ymean', 0.0),\n",
       " ('X8_0', 0.0),\n",
       " ('X8_1', 0.0),\n",
       " ('X8_2', 0.0),\n",
       " ('X8_3', 0.0),\n",
       " ('X8_4', 0.0),\n",
       " ('X8_5', 0.0),\n",
       " ('X8_6', 0.0),\n",
       " ('X8_7', 0.0),\n",
       " ('X8_8', 0.0),\n",
       " ('X8_9', 0.0),\n",
       " ('X8_10', 0.0),\n",
       " ('X8_11', 0.0),\n",
       " ('X8_12', 0.0),\n",
       " ('X8_13', 0.0),\n",
       " ('X8_14', 0.0),\n",
       " ('X8_15', 0.0),\n",
       " ('X8_16', 0.0),\n",
       " ('X8_17', 0.0),\n",
       " ('X8_18', 0.0),\n",
       " ('X8_19', 0.0),\n",
       " ('X8_20', 0.0),\n",
       " ('X8_21', 0.0),\n",
       " ('X8_22', 0.0),\n",
       " ('X8_23', 0.0),\n",
       " ('X8_24', 0.0),\n",
       " ('X8_ymean', 0.0),\n",
       " ('tsvd_0', 0.0),\n",
       " ('srp_0', 0.0),\n",
       " ('srp_1', 0.0),\n",
       " ('srp_3', 0.0),\n",
       " ('grp_4', 0.0),\n",
       " ('srp_5', 0.0),\n",
       " ('grp_6', 0.0),\n",
       " ('grp_10', 0.0),\n",
       " ('srp_10', 0.0),\n",
       " ('grp_11', 0.0),\n",
       " ('srp_11', 0.0),\n",
       " ('srp_12', 0.0),\n",
       " ('srp_13', 0.0),\n",
       " ('grp_14', 0.0),\n",
       " ('srp_14', 0.0),\n",
       " ('grp_17', 0.0),\n",
       " ('srp_17', 0.0),\n",
       " ('srp_18', 0.0),\n",
       " ('grp_19', 0.0),\n",
       " ('grp_24', 0.0),\n",
       " ('grp_25', 0.0),\n",
       " ('grp_26', 0.0),\n",
       " ('grp_27', 0.0),\n",
       " ('grp_28', 0.0),\n",
       " ('grp_29', 0.0),\n",
       " ('grp_30', 0.0),\n",
       " ('srp_32', 0.0),\n",
       " ('grp_35', 0.0),\n",
       " ('srp_35', 0.0),\n",
       " ('grp_36', 0.0),\n",
       " ('grp_38', 0.0),\n",
       " ('srp_38', 0.0),\n",
       " ('grp_39', 0.0),\n",
       " ('grp_40', 0.0),\n",
       " ('grp_41', 0.0),\n",
       " ('grp_42', 0.0),\n",
       " ('srp_43', 0.0),\n",
       " ('grp_45', 0.0),\n",
       " ('grp_46', 0.0),\n",
       " ('grp_47', 0.0),\n",
       " ('grp_48', 0.0),\n",
       " ('grp_49', 0.0),\n",
       " ('kc_21', 0.0),\n",
       " ('kc_23', 0.0),\n",
       " ('kc_28', 0.0),\n",
       " ('kc_31', 0.0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(data.columns.values, feature_importances_(model_xgb.booster())), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feature_split(importance, split):\n",
    "    l = len(list(importance))\n",
    "    bag = [[] for _ in range(split)]\n",
    "    i = 0\n",
    "    for name, _ in list(importance):\n",
    "        bag[i%split].append(name)\n",
    "        i += 1\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subfeatures = feature_split(sorted(zip(range(data.columns.shape[0]), feature_importances_(model_xgb.booster())), \n",
    "                                  key=lambda x: x[1], reverse=True), 5)\n",
    "models_xgb_sub = {}\n",
    "i = 0\n",
    "rounds = [170,445,390,300,490]\n",
    "for bag in subfeatures:\n",
    "    models_xgb_sub[i] = XGBRegressor(learning_rate=0.03, max_depth=4,\n",
    "                         subsample=1, colsample_bytree=0.8, min_child_weight=5,\n",
    "                         reg_alpha=50, reg_lambda=100, \n",
    "                         n_estimators=rounds[i], base_score=target.mean(), seed=420)\n",
    "    '''models_xgb_sub[i].fit(X_train[:,bag], Y_train, eval_metric=r2_eval_xgb, \n",
    "                  eval_set=[(X_train[:,bag], Y_train), (X_val[:,bag], Y_val)],\n",
    "                 early_stopping_rounds=200)'''\n",
    "    models_xgb_sub[i].fit(data_train[:,bag], target)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tsvd_8', 0.065771349), ('ica_4', 0.033861339), ('tsvd_46', 0.029958678), ('pca_17', 0.028236914), ('tsvd_18', 0.027318642), ('pca_19', 0.026859503), ('tsvd_14', 0.024219468), ('kc_28', 0.023530763), ('ica_16', 0.022956841), ('pca_48', 0.022497704), ('pca_14', 0.021579431), ('tsvd_12', 0.018365473), ('ica_7', 0.018365473), ('pca_7', 0.017906336), ('ica_41', 0.017906336), ('ica_42', 0.017217631), ('X315', 0.016873278), ('tsvd_49', 0.016873278), ('kc_6', 0.016873278), ('tsvd_39', 0.015955005), ('tsvd_30', 0.015610652), ('pca_9', 0.014118457), ('ica_39', 0.013774104), ('tsvd_9', 0.013429752), ('tsvd_26', 0.013314968), ('ica_2', 0.0130854), ('pca_27', 0.012396694), ('ica_0', 0.012396694), ('tsvd_31', 0.012396694), ('pca_31', 0.01228191), ('tsvd_19', 0.012167126), ('tsvd_33', 0.011937558), ('tsvd_11', 0.011248852), ('kc_14', 0.011248852), ('pca_16', 0.011134068), ('pca_35', 0.011019284), ('ica_31', 0.010560147), ('srp_22', 0.010445363), ('pca_26', 0.010330578), ('pca_41', 0.0097566573), ('tsvd_16', 0.0096418737), ('grp_2', 0.0096418737), ('ica_8', 0.0094123045), ('X29', 0.0094123045), ('pca_20', 0.0090679526), ('grp_5', 0.0090679526), ('ica_29', 0.0088383835), ('ica_15', 0.0087235998), ('pca_1', 0.0082644625), ('ica_47', 0.0076905419), ('ica_19', 0.0076905419), ('X267', 0.0074609732), ('ica_1', 0.0070018363), ('tsvd_45', 0.0067722681), ('kc_33', 0.0063131312), ('X316', 0.006083563), ('tsvd_38', 0.0053948578), ('kc_20', 0.0053948578), ('grp_17', 0.0049357209), ('grp_20', 0.0048209368), ('ica_20', 0.0047061522), ('kc_38', 0.0047061522), ('X118', 0.0045913681), ('tsvd_2', 0.004476584), ('kc_19', 0.0043617999), ('srp_46', 0.0043617999), ('srp_9', 0.003902663), ('pca_44', 0.0037878789), ('srp_2', 0.0037878789), ('srp_21', 0.0036730946), ('kc_24', 0.0036730946), ('X8', 0.0034435261), ('X2', 0.0034435261), ('pca_38', 0.003328742), ('X5', 0.0030991735), ('srp_20', 0.0029843894), ('kc_1', 0.0029843894), ('kc_32', 0.0028696051), ('srp_7', 0.002754821), ('kc_10', 0.0025252525), ('tsvd_44', 0.0024104684), ('X0_52', 0.0024104684), ('X27', 0.0022956841), ('srp_13', 0.0022956841), ('kc_35', 0.0022956841), ('srp_38', 0.0021809), ('grp_3', 0.0018365473), ('grp_25', 0.0017217631), ('X71', 0.0017217631), ('X377', 0.0016069788), ('grp_44', 0.0014921947), ('tsvd_0', 0.0014921947), ('kc_5', 0.0013774105), ('X142', 0.0013774105), ('X115', 0.0012626263), ('kc_3', 0.001147842), ('srp_37', 0.0010330578), ('srp_17', 0.0010330578), ('srp_24', 0.00091827364), ('X279', 0.00091827364), ('grp_24', 0.00091827364), ('X61', 0.00080348941), ('grp_45', 0.00080348941), ('X179', 0.00068870524), ('X186', 0.00068870524), ('X348', 0.00068870524), ('srp_10', 0.00068870524), ('grp_18', 0.00057392102), ('X362', 0.00057392102), ('X117', 0.00045913682), ('X5_6', 0.00045913682), ('X85', 0.00045913682), ('X273', 0.00045913682), ('srp_1', 0.00045913682), ('grp_27', 0.00045913682), ('grp_29', 0.00045913682), ('grp_31', 0.00045913682), ('X224', 0.00034435262), ('grp_1', 0.00034435262), ('X54', 0.00034435262), ('X176', 0.00034435262), ('X180', 0.00034435262), ('X0_49', 0.00034435262), ('X8_12', 0.00034435262), ('X13', 0.00022956841), ('X336', 0.00022956841), ('X43', 0.00022956841), ('X46', 0.00022956841), ('X130', 0.00022956841), ('X202', 0.00022956841), ('X0_46', 0.00022956841), ('X2_9', 0.00022956841), ('grp_6', 0.00022956841), ('grp_40', 0.00022956841), ('X14', 0.0001147842), ('X126', 0.0001147842), ('X208', 0.0001147842), ('X251', 0.0001147842), ('X301', 0.0001147842), ('X322', 0.0001147842), ('X342', 0.0001147842), ('X359', 0.0001147842), ('X368', 0.0001147842), ('X0_16', 0.0001147842), ('X1_13', 0.0001147842), ('X3_3', 0.0001147842), ('grp_38', 0.0001147842), ('X5_24', 0.0), ('X163', 0.0), ('X6_0', 0.0), ('X17', 0.0), ('X20', 0.0), ('X23', 0.0), ('X31', 0.0), ('X34', 0.0), ('X37', 0.0), ('X40', 0.0), ('X53', 0.0), ('X57', 0.0), ('X65', 0.0), ('X68', 0.0), ('X75', 0.0), ('X78', 0.0), ('X82', 0.0), ('X88', 0.0), ('X91', 0.0), ('X94', 0.0), ('X97', 0.0), ('X100', 0.0), ('X103', 0.0), ('X106', 0.0), ('X109', 0.0), ('X112', 0.0), ('X116', 0.0), ('X123', 0.0), ('X133', 0.0), ('X136', 0.0), ('X139', 0.0), ('X145', 0.0), ('X148', 0.0), ('X153', 0.0), ('X156', 0.0), ('X159', 0.0), ('X162', 0.0), ('X166', 0.0), ('X169', 0.0), ('X173', 0.0), ('X183', 0.0), ('X192', 0.0), ('X196', 0.0), ('X199', 0.0), ('X205', 0.0), ('X211', 0.0), ('X215', 0.0), ('X218', 0.0), ('X221', 0.0), ('X226', 0.0), ('X229', 0.0), ('X233', 0.0), ('X236', 0.0), ('X239', 0.0), ('X242', 0.0), ('X245', 0.0), ('X248', 0.0), ('X254', 0.0), ('X257', 0.0), ('X260', 0.0), ('X264', 0.0), ('X270', 0.0), ('X277', 0.0), ('X281', 0.0), ('X284', 0.0), ('X288', 0.0), ('X291', 0.0), ('X294', 0.0), ('X297', 0.0), ('X305', 0.0), ('X308', 0.0), ('X311', 0.0), ('X319', 0.0), ('X325', 0.0), ('X328', 0.0), ('X331', 0.0), ('X335', 0.0), ('X339', 0.0), ('X345', 0.0), ('X352', 0.0), ('X356', 0.0), ('X365', 0.0), ('X371', 0.0), ('X374', 0.0), ('X380', 0.0), ('X384', 0.0), ('X0_1', 0.0), ('X0_4', 0.0), ('X0_7', 0.0), ('X0_10', 0.0), ('X0_13', 0.0), ('X0_19', 0.0), ('X0_22', 0.0), ('X0_25', 0.0), ('X0_28', 0.0), ('X0_31', 0.0), ('X0_34', 0.0), ('X0_37', 0.0), ('X0_40', 0.0), ('X0_43', 0.0), ('X1_1', 0.0), ('X1_4', 0.0), ('X1_7', 0.0), ('X1_10', 0.0), ('X1_16', 0.0), ('X1_19', 0.0), ('X1_22', 0.0), ('X1_25', 0.0), ('X2_0', 0.0), ('X2_3', 0.0), ('X2_6', 0.0), ('X2_12', 0.0), ('X2_15', 0.0), ('X2_18', 0.0), ('X2_21', 0.0), ('X2_24', 0.0), ('X2_27', 0.0), ('X2_30', 0.0), ('X2_33', 0.0), ('X2_36', 0.0), ('X2_39', 0.0), ('X2_42', 0.0), ('X2_45', 0.0), ('X2_48', 0.0), ('X3_0', 0.0), ('X3_6', 0.0), ('X4_1', 0.0), ('X4_ymean', 0.0), ('X5_2', 0.0), ('X5_8', 0.0), ('X5_11', 0.0), ('X5_14', 0.0), ('X5_18', 0.0), ('X5_25', 0.0), ('X5_29', 0.0), ('X5_32', 0.0), ('X6_2', 0.0), ('X6_5', 0.0), ('X6_10', 0.0), ('X8_0', 0.0), ('X8_3', 0.0), ('X8_6', 0.0), ('X8_9', 0.0), ('X8_15', 0.0), ('X8_18', 0.0), ('X8_21', 0.0), ('X8_24', 0.0), ('srp_35', 0.0), ('grp_42', 0.0)]\n",
      "[('pca_46', 0.049949851), ('tsvd_47', 0.025877632), ('ica_44', 0.023470411), ('ica_18', 0.023069208), ('pca_45', 0.022668004), ('pca_42', 0.021464393), ('X314', 0.020461384), ('pca_25', 0.019658977), ('ica_26', 0.019257773), ('ica_22', 0.018455366), ('pca_6', 0.018254764), ('pca_13', 0.018054163), ('tsvd_28', 0.018054163), ('X47', 0.01785356), ('tsvd_20', 0.01785356), ('pca_34', 0.017452357), ('ica_40', 0.017452357), ('tsvd_1', 0.017051153), ('kc_34', 0.017051153), ('pca_22', 0.016850552), ('ica_14', 0.016449347), ('ica_25', 0.01564694), ('ica_38', 0.01564694), ('pca_37', 0.015245738), ('X119', 0.015245738), ('X232', 0.015245738), ('pca_30', 0.015045135), ('ica_46', 0.014643932), ('ica_34', 0.014643932), ('tsvd_36', 0.014643932), ('tsvd_4', 0.01444333), ('tsvd_22', 0.01444333), ('ica_21', 0.014242728), ('ica_30', 0.013239719), ('tsvd_24', 0.013239719), ('ica_43', 0.012838515), ('tsvd_32', 0.012437312), ('tsvd_34', 0.012437312), ('pca_8', 0.01223671), ('pca_32', 0.01223671), ('pca_5', 0.011434303), ('ica_27', 0.011434303), ('pca_49', 0.011233701), ('pca_11', 0.011033099), ('pca_2', 0.011033099), ('grp_15', 0.010631896), ('pca_24', 0.010230692), ('ica_5', 0.01003009), ('grp_32', 0.0098294886), ('ica_10', 0.0094282851), ('X189', 0.0088264793), ('grp_21', 0.008625878), ('srp_29', 0.008625878), ('tsvd_3', 0.0084252758), ('kc_21', 0.0082246736), ('srp_25', 0.0080240723), ('tsvd_35', 0.00782347), ('ica_48', 0.00782347), ('kc_0', 0.0068204612), ('pca_21', 0.0066198595), ('kc_39', 0.0066198595), ('tsvd_15', 0.0060180542), ('X261', 0.0060180542), ('kc_17', 0.0058174524), ('pca_3', 0.0050150449), ('X337', 0.0050150449), ('kc_12', 0.0046138414), ('srp_33', 0.0042126379), ('srp_34', 0.0040120361), ('kc_22', 0.0038114344), ('X275', 0.0036108324), ('kc_13', 0.0036108324), ('kc_15', 0.0032096289), ('kc_30', 0.0030090271), ('X151', 0.0028084253), ('srp_47', 0.0026078236), ('grp_16', 0.0024072216), ('kc_11', 0.0024072216), ('grp_11', 0.0024072216), ('srp_19', 0.0022066198), ('srp_44', 0.0020060181), ('srp_4', 0.0020060181), ('X120', 0.0018054162), ('srp_32', 0.0018054162), ('X234', 0.0016048144), ('X306', 0.0016048144), ('kc_7', 0.0014042127), ('X300', 0.0012036108), ('srp_12', 0.0012036108), ('srp_18', 0.0012036108), ('srp_40', 0.001003009), ('X58', 0.001003009), ('X79', 0.001003009), ('X0_23', 0.001003009), ('grp_7', 0.00080240722), ('srp_45', 0.00080240722), ('X213', 0.0006018054), ('X363', 0.0006018054), ('X6_11', 0.0006018054), ('ID', 0.00040120361), ('grp_0', 0.00040120361), ('grp_33', 0.00040120361), ('X3', 0.00040120361), ('X154', 0.00040120361), ('X265', 0.00040120361), ('X326', 0.00040120361), ('X8_19', 0.00040120361), ('grp_14', 0.00040120361), ('srp_42', 0.00040120361), ('X49', 0.0002006018), ('X1', 0.0002006018), ('X187', 0.0002006018), ('X6_8', 0.0002006018), ('X76', 0.0002006018), ('X164', 0.0002006018), ('X329', 0.0002006018), ('X1_20', 0.0002006018), ('X8_16', 0.0002006018), ('grp_26', 0.0002006018), ('grp_30', 0.0002006018), ('grp_36', 0.0002006018), ('grp_41', 0.0002006018), ('X50', 0.0), ('X285', 0.0), ('X351', 0.0), ('X5_22', 0.0), ('grp_22', 0.0), ('grp_43', 0.0), ('X10', 0.0), ('X15', 0.0), ('X18', 0.0), ('X21', 0.0), ('X24', 0.0), ('X28', 0.0), ('X32', 0.0), ('X35', 0.0), ('X38', 0.0), ('X41', 0.0), ('X44', 0.0), ('X48', 0.0), ('X55', 0.0), ('X59', 0.0), ('X62', 0.0), ('X66', 0.0), ('X69', 0.0), ('X73', 0.0), ('X83', 0.0), ('X86', 0.0), ('X89', 0.0), ('X92', 0.0), ('X95', 0.0), ('X98', 0.0), ('X101', 0.0), ('X104', 0.0), ('X107', 0.0), ('X110', 0.0), ('X113', 0.0), ('X124', 0.0), ('X128', 0.0), ('X131', 0.0), ('X134', 0.0), ('X137', 0.0), ('X140', 0.0), ('X143', 0.0), ('X146', 0.0), ('X150', 0.0), ('X157', 0.0), ('X160', 0.0), ('X167', 0.0), ('X170', 0.0), ('X174', 0.0), ('X177', 0.0), ('X181', 0.0), ('X184', 0.0), ('X190', 0.0), ('X194', 0.0), ('X197', 0.0), ('X200', 0.0), ('X203', 0.0), ('X206', 0.0), ('X209', 0.0), ('X216', 0.0), ('X219', 0.0), ('X222', 0.0), ('X227', 0.0), ('X230', 0.0), ('X237', 0.0), ('X240', 0.0), ('X243', 0.0), ('X246', 0.0), ('X249', 0.0), ('X252', 0.0), ('X255', 0.0), ('X258', 0.0), ('X262', 0.0), ('X268', 0.0), ('X271', 0.0), ('X274', 0.0), ('X278', 0.0), ('X282', 0.0), ('X286', 0.0), ('X289', 0.0), ('X292', 0.0), ('X295', 0.0), ('X298', 0.0), ('X302', 0.0), ('X309', 0.0), ('X312', 0.0), ('X317', 0.0), ('X320', 0.0), ('X323', 0.0), ('X332', 0.0), ('X340', 0.0), ('X343', 0.0), ('X346', 0.0), ('X349', 0.0), ('X353', 0.0), ('X357', 0.0), ('X360', 0.0), ('X366', 0.0), ('X369', 0.0), ('X372', 0.0), ('X375', 0.0), ('X378', 0.0), ('X382', 0.0), ('X385', 0.0), ('X0_2', 0.0), ('X0_5', 0.0), ('X0_8', 0.0), ('X0_11', 0.0), ('X0_14', 0.0), ('X0_17', 0.0), ('X0_20', 0.0), ('X0_26', 0.0), ('X0_29', 0.0), ('X0_32', 0.0), ('X0_35', 0.0), ('X0_38', 0.0), ('X0_41', 0.0), ('X0_44', 0.0), ('X0_47', 0.0), ('X0_50', 0.0), ('X0_ymean', 0.0), ('X1_2', 0.0), ('X1_5', 0.0), ('X1_8', 0.0), ('X1_11', 0.0), ('X1_14', 0.0), ('X1_17', 0.0), ('X1_23', 0.0), ('X1_26', 0.0), ('X2_1', 0.0), ('X2_4', 0.0), ('X2_7', 0.0), ('X2_10', 0.0), ('X2_13', 0.0), ('X2_16', 0.0), ('X2_19', 0.0), ('X2_22', 0.0), ('X2_25', 0.0), ('X2_28', 0.0), ('X2_31', 0.0), ('X2_34', 0.0), ('X2_37', 0.0), ('X2_40', 0.0), ('X2_43', 0.0), ('X2_46', 0.0), ('X2_49', 0.0), ('X3_1', 0.0), ('X3_4', 0.0), ('X3_ymean', 0.0), ('X4_2', 0.0), ('X5_0', 0.0), ('X5_4', 0.0), ('X5_9', 0.0), ('X5_12', 0.0), ('X5_15', 0.0), ('X5_21', 0.0), ('X5_26', 0.0), ('X5_30', 0.0), ('X5_ymean', 0.0), ('X6_3', 0.0), ('X6_6', 0.0), ('X8_1', 0.0), ('X8_4', 0.0), ('X8_7', 0.0), ('X8_10', 0.0), ('X8_13', 0.0), ('X8_22', 0.0), ('X8_ymean', 0.0), ('grp_4', 0.0), ('grp_8', 0.0), ('grp_28', 0.0), ('grp_39', 0.0), ('grp_48', 0.0)]\n",
      "[('ica_24', 0.026987061), ('pca_12', 0.025693161), ('tsvd_48', 0.023290204), ('pca_47', 0.022550832), ('tsvd_17', 0.022550832), ('pca_18', 0.021996303), ('ica_28', 0.021256931), ('tsvd_13', 0.020887246), ('X127', 0.02051756), ('pca_23', 0.02051756), ('ica_23', 0.020147875), ('pca_33', 0.018484289), ('pca_29', 0.018484289), ('tsvd_10', 0.018114602), ('pca_10', 0.017929759), ('ica_6', 0.017560074), ('X0', 0.017560074), ('tsvd_43', 0.017375231), ('ica_3', 0.016820703), ('ica_32', 0.016635859), ('tsvd_23', 0.016451016), ('pca_39', 0.015896488), ('ica_45', 0.015896488), ('tsvd_25', 0.015711645), ('ica_11', 0.015157117), ('ica_12', 0.015157117), ('kc_8', 0.014972273), ('ica_37', 0.014602588), ('ica_13', 0.014602588), ('ica_36', 0.014417745), ('pca_36', 0.014048059), ('ica_35', 0.013678374), ('grp_13', 0.013308688), ('grp_12', 0.012384473), ('X5_7', 0.011829944), ('tsvd_42', 0.011829944), ('pca_4', 0.011829944), ('ica_17', 0.011645102), ('tsvd_37', 0.011645102), ('kc_4', 0.011460259), ('tsvd_5', 0.011275416), ('pca_43', 0.010905731), ('tsvd_21', 0.010905731), ('ica_9', 0.010166358), ('X52', 0.0096118301), ('tsvd_6', 0.0094269868), ('ica_49', 0.0092421444), ('pca_15', 0.0086876154), ('tsvd_27', 0.0086876154), ('tsvd_7', 0.0086876154), ('tsvd_40', 0.008502773), ('srp_5', 0.0073937154), ('ica_33', 0.0072088726), ('pca_40', 0.0072088726), ('tsvd_29', 0.0070240297), ('kc_27', 0.0070240297), ('kc_26', 0.0070240297), ('pca_28', 0.0068391869), ('grp_9', 0.0068391869), ('srp_36', 0.0064695007), ('kc_23', 0.0059149722), ('kc_36', 0.0055452865), ('srp_6', 0.0053604436), ('kc_31', 0.0053604436), ('srp_16', 0.0051756008), ('srp_28', 0.0051756008), ('kc_2', 0.0044362294), ('kc_18', 0.0042513865), ('srp_15', 0.0042513865), ('tsvd_41', 0.0038817006), ('kc_37', 0.0038817006), ('kc_16', 0.0038817006), ('kc_9', 0.0038817006), ('X5_17', 0.0036968577), ('srp_48', 0.0036968577), ('srp_49', 0.0036968577), ('srp_31', 0.0029574861), ('grp_34', 0.0029574861), ('X313', 0.0029574861), ('srp_27', 0.0027726432), ('X263', 0.0027726432), ('X5_19', 0.0025878004), ('srp_23', 0.0025878004), ('kc_25', 0.0025878004), ('X5_20', 0.0024029575), ('srp_26', 0.0024029575), ('srp_8', 0.0022181147), ('kc_29', 0.0022181147), ('pca_0', 0.0022181147), ('X350', 0.0022181147), ('srp_41', 0.0022181147), ('srp_3', 0.0020332718), ('grp_23', 0.0018484289), ('grp_47', 0.0018484289), ('X355', 0.0018484289), ('X223', 0.001663586), ('srp_11', 0.001663586), ('grp_37', 0.001663586), ('srp_0', 0.001478743), ('X6', 0.0012939002), ('X51', 0.0012939002), ('srp_14', 0.0012939002), ('srp_39', 0.0012939002), ('X132', 0.0011090573), ('X3_2', 0.0011090573), ('grp_35', 0.0011090573), ('grp_49', 0.0011090573), ('X201', 0.00092421443), ('X225', 0.00092421443), ('X171', 0.00073937152), ('X334', 0.00073937152), ('X5_23', 0.00073937152), ('srp_30', 0.00073937152), ('X5_3', 0.00055452867), ('X12', 0.00055452867), ('X178', 0.00055452867), ('X0_30', 0.00055452867), ('grp_19', 0.00055452867), ('X64', 0.00036968576), ('X5_28', 0.00036968576), ('X144', 0.00036968576), ('X241', 0.00036968576), ('X250', 0.00036968576), ('X321', 0.00036968576), ('X327', 0.00036968576), ('X358', 0.00036968576), ('X364', 0.00036968576), ('X0_9', 0.00036968576), ('X1_24', 0.00036968576), ('grp_46', 0.00036968576), ('X6_7', 0.00018484288), ('X63', 0.00018484288), ('X96', 0.00018484288), ('X155', 0.00018484288), ('X158', 0.00018484288), ('X168', 0.00018484288), ('X182', 0.00018484288), ('X191', 0.00018484288), ('X247', 0.00018484288), ('X324', 0.00018484288), ('X361', 0.00018484288), ('X0_36', 0.00018484288), ('X1_3', 0.00018484288), ('X8_8', 0.00018484288), ('X8_11', 0.00018484288), ('X8_17', 0.00018484288), ('grp_10', 0.00018484288), ('X80', 0.0), ('X212', 0.0), ('X354', 0.0), ('X4', 0.0), ('X11', 0.0), ('X16', 0.0), ('X19', 0.0), ('X22', 0.0), ('X26', 0.0), ('X30', 0.0), ('X33', 0.0), ('X36', 0.0), ('X39', 0.0), ('X42', 0.0), ('X45', 0.0), ('X56', 0.0), ('X60', 0.0), ('X67', 0.0), ('X70', 0.0), ('X74', 0.0), ('X77', 0.0), ('X81', 0.0), ('X84', 0.0), ('X87', 0.0), ('X90', 0.0), ('X93', 0.0), ('X99', 0.0), ('X102', 0.0), ('X105', 0.0), ('X108', 0.0), ('X111', 0.0), ('X114', 0.0), ('X122', 0.0), ('X125', 0.0), ('X129', 0.0), ('X135', 0.0), ('X138', 0.0), ('X141', 0.0), ('X147', 0.0), ('X152', 0.0), ('X161', 0.0), ('X165', 0.0), ('X172', 0.0), ('X175', 0.0), ('X185', 0.0), ('X195', 0.0), ('X198', 0.0), ('X204', 0.0), ('X207', 0.0), ('X210', 0.0), ('X214', 0.0), ('X217', 0.0), ('X220', 0.0), ('X228', 0.0), ('X231', 0.0), ('X235', 0.0), ('X238', 0.0), ('X244', 0.0), ('X253', 0.0), ('X256', 0.0), ('X259', 0.0), ('X266', 0.0), ('X269', 0.0), ('X272', 0.0), ('X276', 0.0), ('X280', 0.0), ('X283', 0.0), ('X287', 0.0), ('X290', 0.0), ('X293', 0.0), ('X296', 0.0), ('X299', 0.0), ('X304', 0.0), ('X307', 0.0), ('X310', 0.0), ('X318', 0.0), ('X330', 0.0), ('X333', 0.0), ('X338', 0.0), ('X341', 0.0), ('X344', 0.0), ('X347', 0.0), ('X367', 0.0), ('X370', 0.0), ('X373', 0.0), ('X376', 0.0), ('X379', 0.0), ('X383', 0.0), ('X0_0', 0.0), ('X0_3', 0.0), ('X0_6', 0.0), ('X0_12', 0.0), ('X0_15', 0.0), ('X0_18', 0.0), ('X0_21', 0.0), ('X0_24', 0.0), ('X0_27', 0.0), ('X0_33', 0.0), ('X0_39', 0.0), ('X0_42', 0.0), ('X0_45', 0.0), ('X0_48', 0.0), ('X0_51', 0.0), ('X1_0', 0.0), ('X1_6', 0.0), ('X1_9', 0.0), ('X1_12', 0.0), ('X1_15', 0.0), ('X1_18', 0.0), ('X1_21', 0.0), ('X1_ymean', 0.0), ('X2_2', 0.0), ('X2_5', 0.0), ('X2_8', 0.0), ('X2_11', 0.0), ('X2_14', 0.0), ('X2_17', 0.0), ('X2_20', 0.0), ('X2_23', 0.0), ('X2_26', 0.0), ('X2_29', 0.0), ('X2_32', 0.0), ('X2_35', 0.0), ('X2_38', 0.0), ('X2_41', 0.0), ('X2_44', 0.0), ('X2_47', 0.0), ('X2_ymean', 0.0), ('X3_5', 0.0), ('X4_0', 0.0), ('X4_3', 0.0), ('X5_1', 0.0), ('X5_5', 0.0), ('X5_10', 0.0), ('X5_13', 0.0), ('X5_16', 0.0), ('X5_27', 0.0), ('X5_31', 0.0), ('X6_1', 0.0), ('X6_4', 0.0), ('X6_9', 0.0), ('X6_ymean', 0.0), ('X8_2', 0.0), ('X8_5', 0.0), ('X8_14', 0.0), ('X8_20', 0.0), ('X8_23', 0.0), ('srp_43', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for bag in subfeatures:\n",
    "    print(sorted(zip(data.columns.values[bag], feature_importances_(models_xgb_sub[i].booster())), \n",
    "                 key=lambda x: x[1], reverse=True))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "03c6bdd512e712d854bc1f58490f9f0cebfd69c4",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eval_metrics = model_xgb.evals_result()\n",
    "epochs = len(eval_metrics['validation_0']['r2'])\n",
    "x_axis = range(0, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "ad4261703a3f882969cc0cc052c4c4f69d5d0e00",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIyYf8fnsSRDLDkDdsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcd6bff0908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, eval_metrics['validation_0']['r2'], label='Train')\n",
    "ax.plot(x_axis, eval_metrics['validation_1']['r2'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('R2')\n",
    "pyplot.title('XGBoost R2')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.1.4 Final Model on All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-r2:-0.026004\n",
      "[1]\tvalidation_0-r2:-0.051254\n",
      "[2]\tvalidation_0-r2:-0.075286\n",
      "[3]\tvalidation_0-r2:-0.098151\n",
      "[4]\tvalidation_0-r2:-0.119923\n",
      "[5]\tvalidation_0-r2:-0.140645\n",
      "[6]\tvalidation_0-r2:-0.159779\n",
      "[7]\tvalidation_0-r2:-0.178587\n",
      "[8]\tvalidation_0-r2:-0.196148\n",
      "[9]\tvalidation_0-r2:-0.214141\n",
      "[10]\tvalidation_0-r2:-0.230339\n",
      "[11]\tvalidation_0-r2:-0.245814\n",
      "[12]\tvalidation_0-r2:-0.260292\n",
      "[13]\tvalidation_0-r2:-0.274093\n",
      "[14]\tvalidation_0-r2:-0.287505\n",
      "[15]\tvalidation_0-r2:-0.299993\n",
      "[16]\tvalidation_0-r2:-0.31217\n",
      "[17]\tvalidation_0-r2:-0.323555\n",
      "[18]\tvalidation_0-r2:-0.33464\n",
      "[19]\tvalidation_0-r2:-0.345263\n",
      "[20]\tvalidation_0-r2:-0.355342\n",
      "[21]\tvalidation_0-r2:-0.364963\n",
      "[22]\tvalidation_0-r2:-0.374189\n",
      "[23]\tvalidation_0-r2:-0.382994\n",
      "[24]\tvalidation_0-r2:-0.39119\n",
      "[25]\tvalidation_0-r2:-0.399155\n",
      "[26]\tvalidation_0-r2:-0.406782\n",
      "[27]\tvalidation_0-r2:-0.414059\n",
      "[28]\tvalidation_0-r2:-0.42088\n",
      "[29]\tvalidation_0-r2:-0.427321\n",
      "[30]\tvalidation_0-r2:-0.433686\n",
      "[31]\tvalidation_0-r2:-0.439765\n",
      "[32]\tvalidation_0-r2:-0.445584\n",
      "[33]\tvalidation_0-r2:-0.451143\n",
      "[34]\tvalidation_0-r2:-0.456504\n",
      "[35]\tvalidation_0-r2:-0.461588\n",
      "[36]\tvalidation_0-r2:-0.466357\n",
      "[37]\tvalidation_0-r2:-0.471034\n",
      "[38]\tvalidation_0-r2:-0.475397\n",
      "[39]\tvalidation_0-r2:-0.479712\n",
      "[40]\tvalidation_0-r2:-0.483841\n",
      "[41]\tvalidation_0-r2:-0.487687\n",
      "[42]\tvalidation_0-r2:-0.491372\n",
      "[43]\tvalidation_0-r2:-0.49502\n",
      "[44]\tvalidation_0-r2:-0.498504\n",
      "[45]\tvalidation_0-r2:-0.501857\n",
      "[46]\tvalidation_0-r2:-0.50538\n",
      "[47]\tvalidation_0-r2:-0.507941\n",
      "[48]\tvalidation_0-r2:-0.510943\n",
      "[49]\tvalidation_0-r2:-0.513745\n",
      "[50]\tvalidation_0-r2:-0.516503\n",
      "[51]\tvalidation_0-r2:-0.519125\n",
      "[52]\tvalidation_0-r2:-0.521723\n",
      "[53]\tvalidation_0-r2:-0.524168\n",
      "[54]\tvalidation_0-r2:-0.526525\n",
      "[55]\tvalidation_0-r2:-0.528835\n",
      "[56]\tvalidation_0-r2:-0.531079\n",
      "[57]\tvalidation_0-r2:-0.533231\n",
      "[58]\tvalidation_0-r2:-0.535271\n",
      "[59]\tvalidation_0-r2:-0.537248\n",
      "[60]\tvalidation_0-r2:-0.539176\n",
      "[61]\tvalidation_0-r2:-0.540998\n",
      "[62]\tvalidation_0-r2:-0.542765\n",
      "[63]\tvalidation_0-r2:-0.544488\n",
      "[64]\tvalidation_0-r2:-0.546144\n",
      "[65]\tvalidation_0-r2:-0.547716\n",
      "[66]\tvalidation_0-r2:-0.549265\n",
      "[67]\tvalidation_0-r2:-0.550772\n",
      "[68]\tvalidation_0-r2:-0.552211\n",
      "[69]\tvalidation_0-r2:-0.553606\n",
      "[70]\tvalidation_0-r2:-0.555015\n",
      "[71]\tvalidation_0-r2:-0.556321\n",
      "[72]\tvalidation_0-r2:-0.557598\n",
      "[73]\tvalidation_0-r2:-0.558722\n",
      "[74]\tvalidation_0-r2:-0.559935\n",
      "[75]\tvalidation_0-r2:-0.561162\n",
      "[76]\tvalidation_0-r2:-0.562254\n",
      "[77]\tvalidation_0-r2:-0.563375\n",
      "[78]\tvalidation_0-r2:-0.564486\n",
      "[79]\tvalidation_0-r2:-0.565614\n",
      "[80]\tvalidation_0-r2:-0.566727\n",
      "[81]\tvalidation_0-r2:-0.567797\n",
      "[82]\tvalidation_0-r2:-0.568799\n",
      "[83]\tvalidation_0-r2:-0.569774\n",
      "[84]\tvalidation_0-r2:-0.570717\n",
      "[85]\tvalidation_0-r2:-0.571631\n",
      "[86]\tvalidation_0-r2:-0.572494\n",
      "[87]\tvalidation_0-r2:-0.573362\n",
      "[88]\tvalidation_0-r2:-0.574211\n",
      "[89]\tvalidation_0-r2:-0.575046\n",
      "[90]\tvalidation_0-r2:-0.57581\n",
      "[91]\tvalidation_0-r2:-0.57657\n",
      "[92]\tvalidation_0-r2:-0.577365\n",
      "[93]\tvalidation_0-r2:-0.578051\n",
      "[94]\tvalidation_0-r2:-0.578806\n",
      "[95]\tvalidation_0-r2:-0.579503\n",
      "[96]\tvalidation_0-r2:-0.580229\n",
      "[97]\tvalidation_0-r2:-0.580796\n",
      "[98]\tvalidation_0-r2:-0.581459\n",
      "[99]\tvalidation_0-r2:-0.582093\n",
      "[100]\tvalidation_0-r2:-0.582688\n",
      "[101]\tvalidation_0-r2:-0.583247\n",
      "[102]\tvalidation_0-r2:-0.583782\n",
      "[103]\tvalidation_0-r2:-0.58427\n",
      "[104]\tvalidation_0-r2:-0.584882\n",
      "[105]\tvalidation_0-r2:-0.585454\n",
      "[106]\tvalidation_0-r2:-0.585997\n",
      "[107]\tvalidation_0-r2:-0.586536\n",
      "[108]\tvalidation_0-r2:-0.587026\n",
      "[109]\tvalidation_0-r2:-0.587418\n",
      "[110]\tvalidation_0-r2:-0.587926\n",
      "[111]\tvalidation_0-r2:-0.588419\n",
      "[112]\tvalidation_0-r2:-0.588911\n",
      "[113]\tvalidation_0-r2:-0.589376\n",
      "[114]\tvalidation_0-r2:-0.589814\n",
      "[115]\tvalidation_0-r2:-0.590227\n",
      "[116]\tvalidation_0-r2:-0.590564\n",
      "[117]\tvalidation_0-r2:-0.591024\n",
      "[118]\tvalidation_0-r2:-0.591443\n",
      "[119]\tvalidation_0-r2:-0.591887\n",
      "[120]\tvalidation_0-r2:-0.592221\n",
      "[121]\tvalidation_0-r2:-0.592632\n",
      "[122]\tvalidation_0-r2:-0.592946\n",
      "[123]\tvalidation_0-r2:-0.59333\n",
      "[124]\tvalidation_0-r2:-0.593698\n",
      "[125]\tvalidation_0-r2:-0.594058\n",
      "[126]\tvalidation_0-r2:-0.594435\n",
      "[127]\tvalidation_0-r2:-0.594937\n",
      "[128]\tvalidation_0-r2:-0.595226\n",
      "[129]\tvalidation_0-r2:-0.595541\n",
      "[130]\tvalidation_0-r2:-0.595817\n",
      "[131]\tvalidation_0-r2:-0.596088\n",
      "[132]\tvalidation_0-r2:-0.596346\n",
      "[133]\tvalidation_0-r2:-0.596618\n",
      "[134]\tvalidation_0-r2:-0.596875\n",
      "[135]\tvalidation_0-r2:-0.597125\n",
      "[136]\tvalidation_0-r2:-0.5974\n",
      "[137]\tvalidation_0-r2:-0.597809\n",
      "[138]\tvalidation_0-r2:-0.598214\n",
      "[139]\tvalidation_0-r2:-0.598589\n",
      "[140]\tvalidation_0-r2:-0.598963\n",
      "[141]\tvalidation_0-r2:-0.599238\n",
      "[142]\tvalidation_0-r2:-0.599592\n",
      "[143]\tvalidation_0-r2:-0.599828\n",
      "[144]\tvalidation_0-r2:-0.600217\n",
      "[145]\tvalidation_0-r2:-0.600537\n",
      "[146]\tvalidation_0-r2:-0.600747\n",
      "[147]\tvalidation_0-r2:-0.600996\n",
      "[148]\tvalidation_0-r2:-0.601236\n",
      "[149]\tvalidation_0-r2:-0.601477\n",
      "[150]\tvalidation_0-r2:-0.60185\n",
      "[151]\tvalidation_0-r2:-0.602172\n",
      "[152]\tvalidation_0-r2:-0.602487\n",
      "[153]\tvalidation_0-r2:-0.602798\n",
      "[154]\tvalidation_0-r2:-0.603345\n",
      "[155]\tvalidation_0-r2:-0.603669\n",
      "[156]\tvalidation_0-r2:-0.603879\n",
      "[157]\tvalidation_0-r2:-0.604438\n",
      "[158]\tvalidation_0-r2:-0.604774\n",
      "[159]\tvalidation_0-r2:-0.605054\n",
      "[160]\tvalidation_0-r2:-0.605397\n",
      "[161]\tvalidation_0-r2:-0.605725\n",
      "[162]\tvalidation_0-r2:-0.606029\n",
      "[163]\tvalidation_0-r2:-0.606337\n",
      "[164]\tvalidation_0-r2:-0.606674\n",
      "[165]\tvalidation_0-r2:-0.607059\n",
      "[166]\tvalidation_0-r2:-0.607373\n",
      "[167]\tvalidation_0-r2:-0.607566\n",
      "[168]\tvalidation_0-r2:-0.607803\n",
      "[169]\tvalidation_0-r2:-0.608376\n",
      "[170]\tvalidation_0-r2:-0.608712\n",
      "[171]\tvalidation_0-r2:-0.608993\n",
      "[172]\tvalidation_0-r2:-0.609522\n",
      "[173]\tvalidation_0-r2:-0.609965\n",
      "[174]\tvalidation_0-r2:-0.610176\n",
      "[175]\tvalidation_0-r2:-0.610671\n",
      "[176]\tvalidation_0-r2:-0.610988\n",
      "[177]\tvalidation_0-r2:-0.611271\n",
      "[178]\tvalidation_0-r2:-0.611567\n",
      "[179]\tvalidation_0-r2:-0.611862\n",
      "[180]\tvalidation_0-r2:-0.612166\n",
      "[181]\tvalidation_0-r2:-0.612392\n",
      "[182]\tvalidation_0-r2:-0.612607\n",
      "[183]\tvalidation_0-r2:-0.612872\n",
      "[184]\tvalidation_0-r2:-0.613066\n",
      "[185]\tvalidation_0-r2:-0.613327\n",
      "[186]\tvalidation_0-r2:-0.61354\n",
      "[187]\tvalidation_0-r2:-0.613812\n",
      "[188]\tvalidation_0-r2:-0.614088\n",
      "[189]\tvalidation_0-r2:-0.614435\n",
      "[190]\tvalidation_0-r2:-0.614657\n",
      "[191]\tvalidation_0-r2:-0.614933\n",
      "[192]\tvalidation_0-r2:-0.615186\n",
      "[193]\tvalidation_0-r2:-0.615711\n",
      "[194]\tvalidation_0-r2:-0.615893\n",
      "[195]\tvalidation_0-r2:-0.616171\n",
      "[196]\tvalidation_0-r2:-0.616518\n",
      "[197]\tvalidation_0-r2:-0.616838\n",
      "[198]\tvalidation_0-r2:-0.617323\n",
      "[199]\tvalidation_0-r2:-0.617765\n",
      "[200]\tvalidation_0-r2:-0.617966\n",
      "[201]\tvalidation_0-r2:-0.618446\n",
      "[202]\tvalidation_0-r2:-0.618625\n",
      "[203]\tvalidation_0-r2:-0.618872\n",
      "[204]\tvalidation_0-r2:-0.619106\n",
      "[205]\tvalidation_0-r2:-0.619426\n",
      "[206]\tvalidation_0-r2:-0.619665\n",
      "[207]\tvalidation_0-r2:-0.620136\n",
      "[208]\tvalidation_0-r2:-0.620312\n",
      "[209]\tvalidation_0-r2:-0.620463\n",
      "[210]\tvalidation_0-r2:-0.620981\n",
      "[211]\tvalidation_0-r2:-0.621276\n",
      "[212]\tvalidation_0-r2:-0.621611\n",
      "[213]\tvalidation_0-r2:-0.621833\n",
      "[214]\tvalidation_0-r2:-0.622076\n",
      "[215]\tvalidation_0-r2:-0.622541\n",
      "[216]\tvalidation_0-r2:-0.622829\n",
      "[217]\tvalidation_0-r2:-0.623124\n",
      "[218]\tvalidation_0-r2:-0.623423\n",
      "[219]\tvalidation_0-r2:-0.623615\n",
      "[220]\tvalidation_0-r2:-0.623784\n",
      "[221]\tvalidation_0-r2:-0.624236\n",
      "[222]\tvalidation_0-r2:-0.624532\n",
      "[223]\tvalidation_0-r2:-0.624916\n",
      "[224]\tvalidation_0-r2:-0.625153\n",
      "[225]\tvalidation_0-r2:-0.625436\n",
      "[226]\tvalidation_0-r2:-0.625648\n",
      "[227]\tvalidation_0-r2:-0.625833\n",
      "[228]\tvalidation_0-r2:-0.626135\n",
      "[229]\tvalidation_0-r2:-0.626589\n",
      "[230]\tvalidation_0-r2:-0.626769\n",
      "[231]\tvalidation_0-r2:-0.627058\n",
      "[232]\tvalidation_0-r2:-0.627347\n",
      "[233]\tvalidation_0-r2:-0.627593\n",
      "[234]\tvalidation_0-r2:-0.627868\n",
      "[235]\tvalidation_0-r2:-0.628106\n",
      "[236]\tvalidation_0-r2:-0.628441\n",
      "[237]\tvalidation_0-r2:-0.628666\n",
      "[238]\tvalidation_0-r2:-0.628987\n",
      "[239]\tvalidation_0-r2:-0.629435\n",
      "[240]\tvalidation_0-r2:-0.629664\n",
      "[241]\tvalidation_0-r2:-0.629893\n",
      "[242]\tvalidation_0-r2:-0.630111\n",
      "[243]\tvalidation_0-r2:-0.630461\n",
      "[244]\tvalidation_0-r2:-0.630649\n",
      "[245]\tvalidation_0-r2:-0.630826\n",
      "[246]\tvalidation_0-r2:-0.631068\n",
      "[247]\tvalidation_0-r2:-0.631286\n",
      "[248]\tvalidation_0-r2:-0.631492\n",
      "[249]\tvalidation_0-r2:-0.631904\n",
      "[250]\tvalidation_0-r2:-0.632121\n",
      "[251]\tvalidation_0-r2:-0.632301\n",
      "[252]\tvalidation_0-r2:-0.632507\n",
      "[253]\tvalidation_0-r2:-0.632716\n",
      "[254]\tvalidation_0-r2:-0.633025\n",
      "[255]\tvalidation_0-r2:-0.633333\n",
      "[256]\tvalidation_0-r2:-0.633757\n",
      "[257]\tvalidation_0-r2:-0.634079\n",
      "[258]\tvalidation_0-r2:-0.634321\n",
      "[259]\tvalidation_0-r2:-0.634756\n",
      "[260]\tvalidation_0-r2:-0.634983\n",
      "[261]\tvalidation_0-r2:-0.635205\n",
      "[262]\tvalidation_0-r2:-0.635595\n",
      "[263]\tvalidation_0-r2:-0.635928\n",
      "[264]\tvalidation_0-r2:-0.636344\n",
      "[265]\tvalidation_0-r2:-0.636534\n",
      "[266]\tvalidation_0-r2:-0.636799\n",
      "[267]\tvalidation_0-r2:-0.637061\n",
      "[268]\tvalidation_0-r2:-0.637463\n",
      "[269]\tvalidation_0-r2:-0.637847\n",
      "[270]\tvalidation_0-r2:-0.638165\n",
      "[271]\tvalidation_0-r2:-0.638458\n",
      "[272]\tvalidation_0-r2:-0.638736\n",
      "[273]\tvalidation_0-r2:-0.63909\n",
      "[274]\tvalidation_0-r2:-0.639278\n",
      "[275]\tvalidation_0-r2:-0.639452\n",
      "[276]\tvalidation_0-r2:-0.639722\n",
      "[277]\tvalidation_0-r2:-0.639934\n",
      "[278]\tvalidation_0-r2:-0.640248\n",
      "[279]\tvalidation_0-r2:-0.640678\n",
      "[280]\tvalidation_0-r2:-0.640866\n",
      "[281]\tvalidation_0-r2:-0.641066\n",
      "[282]\tvalidation_0-r2:-0.641254\n",
      "[283]\tvalidation_0-r2:-0.641511\n",
      "[284]\tvalidation_0-r2:-0.641762\n",
      "[285]\tvalidation_0-r2:-0.641941\n",
      "[286]\tvalidation_0-r2:-0.642299\n",
      "[287]\tvalidation_0-r2:-0.642477\n",
      "[288]\tvalidation_0-r2:-0.64273\n",
      "[289]\tvalidation_0-r2:-0.643118\n",
      "[290]\tvalidation_0-r2:-0.64344\n",
      "[291]\tvalidation_0-r2:-0.643728\n",
      "[292]\tvalidation_0-r2:-0.643932\n",
      "[293]\tvalidation_0-r2:-0.644334\n",
      "[294]\tvalidation_0-r2:-0.644563\n",
      "[295]\tvalidation_0-r2:-0.644885\n",
      "[296]\tvalidation_0-r2:-0.645277\n",
      "[297]\tvalidation_0-r2:-0.645654\n",
      "[298]\tvalidation_0-r2:-0.645878\n",
      "[299]\tvalidation_0-r2:-0.646067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=100.66931812782134, colsample_bylevel=1,\n",
       "       colsample_bytree=0.7, gamma=0, learning_rate=0.03, max_delta_step=0,\n",
       "       max_depth=4, min_child_weight=5, missing=None, n_estimators=300,\n",
       "       nthread=-1, objective='reg:linear', reg_alpha=0, reg_lambda=150,\n",
       "       scale_pos_weight=1, seed=420, silent=True, subsample=1)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb2 = XGBRegressor(learning_rate=0.03, max_depth=4, n_estimators=300,\n",
    "                         subsample=1, colsample_bytree=0.7, min_child_weight=5,\n",
    "                         reg_alpha=0, reg_lambda=150, \n",
    "                         base_score=target.mean(), seed=420)\n",
    "model_xgb2.fit(data_train, target, eval_metric=r2_eval_xgb, \n",
    "               eval_set=[(data_train, target)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5.2 More Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "seed = 420\n",
    "test_size = 0.2\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data_train, target, test_size=test_size, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.119e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:377: RuntimeWarning: overflow encountered in true_divide\n",
      "  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny))\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.304e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.088e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:381: RuntimeWarning: overflow encountered in true_divide\n",
      "  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny))\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.050e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.349e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.318e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.143e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.491e-02, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.431e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.345e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.249e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.249e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.248e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 35 iterations, alpha=1.223e-02, previous alpha=1.199e-02, with an active set of 32 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.254e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.674e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.294e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.091e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.528e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.105e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.071e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.815e-02, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.815e-02, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.709e-02, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.639e-02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.629e-02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.142e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.142e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=1.161e-02, previous alpha=1.142e-02, with an active set of 28 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.605e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.605e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.004e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.275e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.275e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.888e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.787e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.787e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.625e-02, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.625e-02, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.492e-02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.303e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.124e-02, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.042e-02, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=9.280e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.823e-03, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.823e-03, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 52 iterations, alpha=8.705e-03, previous alpha=8.410e-03, with an active set of 47 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.169e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.931e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.664e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.572e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.027e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 8 iterations, alpha=2.070e-02, previous alpha=2.015e-02, with an active set of 7 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LassoLarsCV(copy_X=True, cv=None, eps=2.2204460492503131e-16,\n",
       "      fit_intercept=True, max_iter=1000, max_n_alphas=1000, n_jobs=1,\n",
       "      normalize=True, positive=False, precompute='auto', verbose=False)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lasso lars\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "\n",
    "model_llc = LassoLarsCV(normalize=True, max_iter=1000)\n",
    "model_llc.fit(X_train, Y_train)\n",
    "#model_llc.fit(data_train, target)\n",
    "#model_llc.score(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-9225daf8d369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_ard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mARDRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_ard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel_ard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    391\u001b[0m                            np.dot(X[:, keep_lambda] *\n\u001b[1;32m    392\u001b[0m                            \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeep_lambda\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m                            X[:, keep_lambda].T))\n\u001b[0m\u001b[1;32m    394\u001b[0m             sigma_ = np.dot(sigma_, X[:, keep_lambda] *\n\u001b[1;32m    395\u001b[0m                             np.reshape(1. / lambda_[keep_lambda], [1, -1]))\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mpinvh\u001b[0;34m(a, cond, rcond, lower)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \"\"\"\n\u001b[1;32m    539\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m     \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrcond\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/scipy/linalg/decomp.py\u001b[0m in \u001b[0;36meigh\u001b[0;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meigvals\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             w, v, info = evr(a1, uplo=uplo, jobz=_job, range=\"A\", il=1,\n\u001b[0;32m--> 345\u001b[0;31m                              iu=a1.shape[0], overwrite_a=overwrite_a)\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigvals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ARDRegression\n",
    "\n",
    "model_ard = ARDRegression(verbose=True, compute_score=True)\n",
    "model_ard.fit(X_train, Y_train)\n",
    "model_ard.score(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence after  13  iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=True, copy_X=True,\n",
       "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
       "       normalize=False, tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "model_br = BayesianRidge(verbose=True, compute_score=True)\n",
    "model_br.fit(X_train, Y_train)\n",
    "#model_br.score(X_val, Y_val)\n",
    "#model_br.fit(data_train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "........................................................................................../home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/coordinate_descent.py:479: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "..................................................................................................................................................................................................................[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    4.3s finished\n",
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/sklearn/linear_model/coordinate_descent.py:479: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.31576646634140026"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "model_en = ElasticNetCV(max_iter=3000, verbose=True)\n",
    "model_en.fit(X_train, Y_train)\n",
    "model_en.score(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=1000,\n",
       "        tol=1e-05, warm_start=False)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "model_hr = HuberRegressor(max_iter=1000)\n",
    "model_hr.fit(X_train, Y_train)\n",
    "#model_hr.score(X_val, Y_val)\n",
    "#model_hr.fit(data_train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 11302901546692.47, NNZs: 843, Bias: -634079412.870475, T: 3367, Avg. loss: 4931619670286549226021572589912064.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12334944735766.04, NNZs: 843, Bias: 563636074.051377, T: 6734, Avg. loss: 3574630187811347207742632310603776.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6589340963356.97, NNZs: 843, Bias: -481994047.502473, T: 10101, Avg. loss: 2934663722480389940055343978512384.000000\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7859202551115.11, NNZs: 843, Bias: 483591783.479959, T: 13468, Avg. loss: 2547259399930537313355023075246080.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8186460194084.69, NNZs: 843, Bias: -425914350.025124, T: 16835, Avg. loss: 2283961124695247218025134771142656.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 7801919114857.74, NNZs: 843, Bias: 431422844.743469, T: 20202, Avg. loss: 2090805100457072124905034503684096.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 13900865342085.54, NNZs: 843, Bias: -2759419859.777860, T: 23569, Avg. loss: 1938597847547888500963265107460096.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 13431101217941.05, NNZs: 843, Bias: -328812965.904329, T: 26936, Avg. loss: 1815909143358928180734695212843008.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 11102252692794.19, NNZs: 843, Bias: 438351779.586563, T: 30303, Avg. loss: 1714206045725200596165225841950720.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 10987185368509.13, NNZs: 843, Bias: -311346254.828735, T: 33670, Avg. loss: 1626985799196318438497859936452608.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 10112226981565.37, NNZs: 843, Bias: 416457875.932966, T: 37037, Avg. loss: 1553119910405970584471992228904960.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 9651429856306.42, NNZs: 843, Bias: -296749289.015228, T: 40404, Avg. loss: 1488089478027380208677418975625216.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 7846583235027.37, NNZs: 843, Bias: 224335695.168286, T: 43771, Avg. loss: 1430397191139216799881216495976448.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 7785801686924.42, NNZs: 843, Bias: -459514862.090648, T: 47138, Avg. loss: 1380072872414548103939174917210112.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 7724707711253.03, NNZs: 843, Bias: -1118202912.497294, T: 50505, Avg. loss: 1333878193396107594387238615515136.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 8531177513807.24, NNZs: 843, Bias: 867696204.134618, T: 53872, Avg. loss: 1292108574435150638485056910262272.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 6799286221370.49, NNZs: 843, Bias: 207684841.556788, T: 57239, Avg. loss: 1253617233904372304781980416868352.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 7344377269270.88, NNZs: 843, Bias: -432608063.447017, T: 60606, Avg. loss: 1218423513882748383076458581458944.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 4210192668497.92, NNZs: 843, Bias: 201613648.384456, T: 63973, Avg. loss: 1186518448206601119605627318960128.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 6591571466307.73, NNZs: 843, Bias: 819880616.177906, T: 67340, Avg. loss: 1156906046998048040898421199994880.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 4531432211481.05, NNZs: 843, Bias: 196364176.470357, T: 70707, Avg. loss: 1129301293918209779441721558433792.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 5405766140772.54, NNZs: 843, Bias: 798982406.002685, T: 74074, Avg. loss: 1103757079761587652453652758528000.000000\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 4165084256982.71, NNZs: 843, Bias: 190089833.618678, T: 77441, Avg. loss: 1080120307942460437611482777124864.000000\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 5642359066106.62, NNZs: 843, Bias: -405428669.935748, T: 80808, Avg. loss: 1057687896330034940693471486279680.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 6025744754398.11, NNZs: 843, Bias: 185361468.527005, T: 84175, Avg. loss: 1036531805032250675722768906977280.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 7322221282628.38, NNZs: 843, Bias: -696302113.510991, T: 87542, Avg. loss: 1016534023580445640799045604605952.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 8043427216283.58, NNZs: 843, Bias: -113885099.981439, T: 90909, Avg. loss: 997751863659122760827946754113536.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 6723629125910.66, NNZs: 843, Bias: -683386578.546863, T: 94276, Avg. loss: 980158562110479114189593290735616.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 6375858938063.35, NNZs: 843, Bias: 32230656.910543, T: 97643, Avg. loss: 963310950450754279136626867175424.000000\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 6198960435296.66, NNZs: 843, Bias: -528942714.215662, T: 101010, Avg. loss: 947519983674529312430990710276096.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 6791638078764.62, NNZs: 843, Bias: 31745189.512685, T: 104377, Avg. loss: 932560071408839645916796356132864.000000\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 6831741620160.60, NNZs: 843, Bias: 583660659.820816, T: 107744, Avg. loss: 918213801839936618241593182257152.000000\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 8150537525989.05, NNZs: 843, Bias: 1126954716.633335, T: 111111, Avg. loss: 904531507593818926248890550190080.000000\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 7934727984554.58, NNZs: 843, Bias: 574766111.702414, T: 114478, Avg. loss: 891205833450978444448497622056960.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 5634464198445.90, NNZs: 843, Bias: 30763269.615468, T: 117845, Avg. loss: 878567567582824961231857806999552.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 7718680907442.89, NNZs: 843, Bias: -505488867.935212, T: 121212, Avg. loss: 866527996961628743198581838577664.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 7142068851655.45, NNZs: 843, Bias: 1094518472.712904, T: 124579, Avg. loss: 854711466141818381859076656070656.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 5527956245567.94, NNZs: 843, Bias: -499212769.318492, T: 127946, Avg. loss: 843415242237077876546099379961856.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 6270256644842.53, NNZs: 843, Bias: 29132480.919343, T: 131313, Avg. loss: 832401716037534943933283459661824.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 7848155136970.15, NNZs: 843, Bias: -493400424.579769, T: 134680, Avg. loss: 822158852061471728554771839188992.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 6317951385628.34, NNZs: 843, Bias: 27857192.720649, T: 138047, Avg. loss: 812205160510654579742293943648256.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 6950132566113.64, NNZs: 843, Bias: -487931521.516009, T: 141414, Avg. loss: 802714383266257693111016952430592.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 7248031801687.02, NNZs: 843, Bias: 27680465.784056, T: 144781, Avg. loss: 793510920891463063287293959733248.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 5403255104478.13, NNZs: 843, Bias: -482229015.911913, T: 148148, Avg. loss: 784580752996059907665917133193216.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 7580651202269.42, NNZs: 843, Bias: 27234114.192674, T: 151515, Avg. loss: 775812767752877348087791741829120.000000\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 5641678689808.91, NNZs: 843, Bias: -476954426.127863, T: 154882, Avg. loss: 767439400079182072215529171451904.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 6935674047975.75, NNZs: 843, Bias: 26734013.206567, T: 158249, Avg. loss: 759458076695648545815208413626368.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 4581718101005.60, NNZs: 843, Bias: -472362974.404516, T: 161616, Avg. loss: 751528603271457445787407545794560.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 4333995186495.12, NNZs: 843, Bias: 26125200.946479, T: 164983, Avg. loss: 743875519178187097230233340739584.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 5078907613581.26, NNZs: 843, Bias: 519478065.155878, T: 168350, Avg. loss: 736593710369458329957695857098752.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 6510153051663.82, NNZs: 843, Bias: 25618569.079383, T: 171717, Avg. loss: 729300313181900233480775076413440.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 5051199653024.95, NNZs: 843, Bias: -463598326.921037, T: 175084, Avg. loss: 722272868888004462825951826280448.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 6299515405421.85, NNZs: 843, Bias: -947965544.381525, T: 178451, Avg. loss: 715361141242760845308478034542592.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 4914494875786.95, NNZs: 843, Bias: 509172059.076897, T: 181818, Avg. loss: 708797490461476768114791967358976.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 4967283262201.67, NNZs: 843, Bias: 24955837.760040, T: 185185, Avg. loss: 702374656807275264604067697000448.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 5637073156881.37, NNZs: 843, Bias: -454994188.340202, T: 188552, Avg. loss: 696133465577592499096288774586368.000000\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 5153985025407.04, NNZs: 843, Bias: 24625907.003203, T: 191919, Avg. loss: 690171459780106522007006187880448.000000\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 5510347669935.79, NNZs: 843, Bias: 518762662.835901, T: 195286, Avg. loss: 684315748848098842789550831435776.000000\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 5376040993851.52, NNZs: 843, Bias: 42641784.872155, T: 198653, Avg. loss: 678592051190826232512511026921472.000000\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 5239215726517.19, NNZs: 843, Bias: -429308065.317585, T: 202020, Avg. loss: 672928303032557312667496277344256.000000\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 6686092215428.00, NNZs: 843, Bias: 42153705.033233, T: 205387, Avg. loss: 667479504691934133445886126063616.000000\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 5832873948451.47, NNZs: 843, Bias: -425825850.017792, T: 208754, Avg. loss: 662127427185951773506752915439616.000000\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 6469161316422.60, NNZs: 843, Bias: 973744367.164214, T: 212121, Avg. loss: 656984398855483424111736513363968.000000\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 5803596836447.07, NNZs: 843, Bias: 134192134.743090, T: 215488, Avg. loss: 651790593833615656738814276141056.000000\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 6423731287556.67, NNZs: 843, Bias: -328555860.435125, T: 218855, Avg. loss: 646826639132124732468452258742272.000000\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 7966847200680.56, NNZs: 843, Bias: 133255156.817608, T: 222222, Avg. loss: 641928366892394732892657765318656.000000\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 5568534132101.42, NNZs: 843, Bias: -326238672.391123, T: 225589, Avg. loss: 637190753358732031700621192069120.000000\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 5910167264628.18, NNZs: 843, Bias: 132037539.482095, T: 228956, Avg. loss: 632454325513338446029308731326464.000000\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 6595033002973.29, NNZs: 843, Bias: -324052514.377906, T: 232323, Avg. loss: 627903581347673698906355892486144.000000\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 6951434890325.84, NNZs: 843, Bias: 130983801.562382, T: 235690, Avg. loss: 623374861835201291941542528811008.000000\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 6258845868086.76, NNZs: 843, Bias: -321722884.738115, T: 239057, Avg. loss: 619049978885542313208766724571136.000000\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 8313581002331.68, NNZs: 843, Bias: 129962558.031332, T: 242424, Avg. loss: 614734715361412800304484796334080.000000\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 9057928069720.36, NNZs: 843, Bias: 355061054.398055, T: 245791, Avg. loss: 610534207017997626511430789365760.000000\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 7688365482638.92, NNZs: 843, Bias: -93811382.245033, T: 249158, Avg. loss: 606429030456634346746598645563392.000000\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 8388026363622.88, NNZs: 843, Bias: -539454625.611295, T: 252525, Avg. loss: 602386822853306434246894061355008.000000\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 7807691940691.97, NNZs: 843, Bias: -92995566.218559, T: 255892, Avg. loss: 598434899349699550685752671076352.000000\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 7674719746215.72, NNZs: 843, Bias: 350513060.526673, T: 259259, Avg. loss: 594555117249138637760192762085376.000000\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 8467777849092.63, NNZs: 843, Bias: -92355763.930479, T: 262626, Avg. loss: 590786491723068319924692990296064.000000\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 6920011373969.38, NNZs: 843, Bias: -532295759.695663, T: 265993, Avg. loss: 587022807972095786961808432037888.000000\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 7854500846075.83, NNZs: 843, Bias: -969619207.056592, T: 269360, Avg. loss: 583383771239535579864357582929920.000000\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 6217554079277.61, NNZs: 843, Bias: 425996713.902114, T: 272727, Avg. loss: 579751470491357073857720707186688.000000\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 7280709823107.12, NNZs: 843, Bias: -11583038.264209, T: 276094, Avg. loss: 576271223041177243553438158028800.000000\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 6503841201397.13, NNZs: 843, Bias: -446417236.702131, T: 279461, Avg. loss: 572856735448721504003416941133824.000000\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 6300501027465.09, NNZs: 843, Bias: -878778908.762544, T: 282828, Avg. loss: 569481630352881633387728307486720.000000\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 7464162050162.06, NNZs: 843, Bias: 420817080.665481, T: 286195, Avg. loss: 566184246382705512409754104758272.000000\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 6506588275573.31, NNZs: 843, Bias: -11441717.940701, T: 289562, Avg. loss: 562918162786662681218781173252096.000000\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 6916191785542.73, NNZs: 843, Bias: 50701252.926222, T: 292929, Avg. loss: 559646421769236177456791497474048.000000\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 5738353199766.50, NNZs: 843, Bias: 479016648.665381, T: 296296, Avg. loss: 556457386979613697024761647857664.000000\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 7858742034640.08, NNZs: 843, Bias: -662476044.826380, T: 299663, Avg. loss: 553333711264514553398513715118080.000000\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 7549359836994.14, NNZs: 843, Bias: 617972215.023374, T: 303030, Avg. loss: 550240519202334701909916046589952.000000\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 6273215148282.03, NNZs: 843, Bias: 191236352.888045, T: 306397, Avg. loss: 547189013039890631605348959518720.000000\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 6028656485869.75, NNZs: 843, Bias: -233172100.200297, T: 309764, Avg. loss: 544236244217806270599956065681408.000000\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 5735794842980.04, NNZs: 843, Bias: 190193941.137567, T: 313131, Avg. loss: 541307827480332376234491922874368.000000\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 6481459924337.07, NNZs: 843, Bias: -278051228.100353, T: 316498, Avg. loss: 538435019033005089487672404082688.000000\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 7117757000224.25, NNZs: 843, Bias: 548603031.358871, T: 319865, Avg. loss: 535575457055360635478467101589504.000000\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 5399908010574.86, NNZs: 843, Bias: -711060049.224564, T: 323232, Avg. loss: 532747739296383594349743898099712.000000\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 4958163559601.93, NNZs: 843, Bias: -291041331.101829, T: 326599, Avg. loss: 530002301526192952346233035292672.000000\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 6102552462098.57, NNZs: 843, Bias: 126922330.469689, T: 329966, Avg. loss: 527296387079764772447498893524992.000000\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 7347374274205.97, NNZs: 843, Bias: -289745323.202458, T: 333333, Avg. loss: 524695008332137024581767838826496.000000\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 7726991413218.90, NNZs: 843, Bias: 126112857.648891, T: 336700, Avg. loss: 522102484182692987676468667482112.000000\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 7246764274573.66, NNZs: 843, Bias: 291799305.656184, T: 340067, Avg. loss: 519532818268793838934600854798336.000000\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 6797185641621.08, NNZs: 843, Bias: -121982999.614561, T: 343434, Avg. loss: 516979928282139009726284296093696.000000\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 8695322292502.18, NNZs: 843, Bias: 290385889.483893, T: 346801, Avg. loss: 514461403907332089837660984573952.000000\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 7945651954031.15, NNZs: 843, Bias: -943573526.881253, T: 350168, Avg. loss: 511959592462295820969732265213952.000000\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 7902384624855.19, NNZs: 843, Bias: -2586584.818244, T: 353535, Avg. loss: 509490135672015273755767652483072.000000\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 8192539538739.42, NNZs: 843, Bias: -411751260.752155, T: 356902, Avg. loss: 507094656544899161492331949981696.000000\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 7768782359356.70, NNZs: 843, Bias: -2664944.465826, T: 360269, Avg. loss: 504706958589819358529404268445696.000000\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 7663823428524.93, NNZs: 843, Bias: -150855525.911601, T: 363636, Avg. loss: 502389653848629230063026597527552.000000\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 8525350762516.14, NNZs: 843, Bias: -556802215.954795, T: 367003, Avg. loss: 500100995918632103173799081410560.000000\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 8422895007262.68, NNZs: 843, Bias: -150179104.432143, T: 370370, Avg. loss: 497850605434552614591358545428480.000000\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 7618045864329.77, NNZs: 843, Bias: -77755682.246983, T: 373737, Avg. loss: 495614314644452941621254424625152.000000\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 6316618634818.41, NNZs: 843, Bias: 325897445.379777, T: 377104, Avg. loss: 493442349070001135336649320824832.000000\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 7042545129846.21, NNZs: 843, Bias: -77439807.994792, T: 380471, Avg. loss: 491229299239290276809139900383232.000000\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 7440968515609.28, NNZs: 843, Bias: 324519420.882923, T: 383838, Avg. loss: 489098762302608191841481126838272.000000\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 7341560826642.05, NNZs: 843, Bias: -76956649.905840, T: 387205, Avg. loss: 486975259073219645070055995080704.000000\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 10022619685969.05, NNZs: 843, Bias: -476810961.553462, T: 390572, Avg. loss: 484878556950391004331478083436544.000000\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 7936263341392.67, NNZs: 843, Bias: 618924131.584971, T: 393939, Avg. loss: 482820158254167696146638527528960.000000\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 8016967103448.53, NNZs: 843, Bias: -688891651.923408, T: 397306, Avg. loss: 480744059256083475968566719676416.000000\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 7388795648849.78, NNZs: 843, Bias: -290086823.027550, T: 400673, Avg. loss: 478756956324922007274246578896896.000000\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 6839240373950.66, NNZs: 843, Bias: -686178360.800812, T: 404040, Avg. loss: 476788845145729868286461392977920.000000\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 7366126436160.37, NNZs: 843, Bias: 502667013.544689, T: 407407, Avg. loss: 474834494869552435001156956061696.000000\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 7198440185671.57, NNZs: 843, Bias: 41649976.064509, T: 410774, Avg. loss: 472905934244252533355908534632448.000000\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 9130237879465.99, NNZs: 843, Bias: 1224048424.980450, T: 414141, Avg. loss: 471005442548024213030530321481728.000000\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 6585654598985.51, NNZs: 843, Bias: 41273499.885107, T: 417508, Avg. loss: 469118512269610122786329038159872.000000\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 7514017913465.66, NNZs: 843, Bias: -351578588.721609, T: 420875, Avg. loss: 467236177578608969349022613504000.000000\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 6267036627002.24, NNZs: 843, Bias: 40894884.410049, T: 424242, Avg. loss: 465377222972538016194601175482368.000000\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 6278135082959.57, NNZs: 843, Bias: 29223228.629474, T: 427609, Avg. loss: 463578186510971749316684886835200.000000\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 6016968230764.88, NNZs: 843, Bias: 419430128.416494, T: 430976, Avg. loss: 461784814933265338835337320857600.000000\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 6464858139373.87, NNZs: 843, Bias: 29019456.699648, T: 434343, Avg. loss: 460033305091241460787351161667584.000000\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 7730336322116.55, NNZs: 843, Bias: -359910673.221279, T: 437710, Avg. loss: 458264893732146834343446183936000.000000\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 6758169509103.72, NNZs: 843, Bias: 28700681.358311, T: 441077, Avg. loss: 456524370484362071677699149004800.000000\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 7054476871643.01, NNZs: 843, Bias: -358661923.421974, T: 444444, Avg. loss: 454791728813802780765858599796736.000000\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 7964960673711.60, NNZs: 843, Bias: -744590927.227148, T: 447811, Avg. loss: 453096137419680594059993192333312.000000\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 8198113358396.04, NNZs: 843, Bias: -357415173.066945, T: 451178, Avg. loss: 451437505725482207891217476222976.000000\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 6903915477975.41, NNZs: 843, Bias: 52524908.602178, T: 454545, Avg. loss: 449755677189278590219961898106880.000000\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 7111658146996.77, NNZs: 843, Bias: 436774372.668657, T: 457912, Avg. loss: 448098828551211460531773818011648.000000\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 7578337690738.16, NNZs: 843, Bias: 560075315.804815, T: 461279, Avg. loss: 446460145610361941055543525244928.000000\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 7693502243533.38, NNZs: 843, Bias: 176035310.737031, T: 464646, Avg. loss: 444863733633732358374866387206144.000000\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 7860308772387.01, NNZs: 843, Bias: -206633326.852862, T: 468013, Avg. loss: 443268210098673200723441267769344.000000\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 7147030329287.05, NNZs: 843, Bias: 175298952.335586, T: 471380, Avg. loss: 441694298777606543540164927422464.000000\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 7388688439136.85, NNZs: 843, Bias: -206028155.560876, T: 474747, Avg. loss: 440123167067464855165723543076864.000000\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 7300965790565.69, NNZs: 843, Bias: 174590121.167497, T: 478114, Avg. loss: 438552011857699558454857851469824.000000\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 7411680140073.44, NNZs: 843, Bias: -205352311.158278, T: 481481, Avg. loss: 437045776653354987763818330849280.000000\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 7697501424666.43, NNZs: 843, Bias: -279737391.610944, T: 484848, Avg. loss: 435521341437470769752556955500544.000000\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 7392233177224.74, NNZs: 843, Bias: 99030770.730334, T: 488215, Avg. loss: 434078315454112778368924373745664.000000\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 6513246664114.68, NNZs: 843, Bias: -278857088.969091, T: 491582, Avg. loss: 432611479740305965748015599714304.000000\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 6293353510687.41, NNZs: 843, Bias: 98568493.886743, T: 494949, Avg. loss: 431152909258080112428504854822912.000000\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 7723269942845.92, NNZs: 843, Bias: 474714262.054846, T: 498316, Avg. loss: 429719306343899930769950202920960.000000\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 6538301318419.06, NNZs: 843, Bias: 98110894.804138, T: 501683, Avg. loss: 428297232974127028697686750527488.000000\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 6634404802291.28, NNZs: 843, Bias: -277187911.919336, T: 505050, Avg. loss: 426885296692953059097896976121856.000000\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 6757065181681.55, NNZs: 843, Bias: 97707510.194286, T: 508417, Avg. loss: 425472150522298970384960153714688.000000\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 7352437782306.82, NNZs: 843, Bias: 471386327.435232, T: 511784, Avg. loss: 424071449913366595322676665909248.000000\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 6846331866129.65, NNZs: 843, Bias: 97294650.497747, T: 515151, Avg. loss: 422677309075965295187842767519744.000000\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 7277230176729.03, NNZs: 843, Bias: -275576243.367062, T: 518518, Avg. loss: 421311133113538122391506395332608.000000\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 6266680835074.01, NNZs: 843, Bias: 274283460.487069, T: 521885, Avg. loss: 419968376219566874011473742397440.000000\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 7191665872643.70, NNZs: 843, Bias: 645261301.912702, T: 525252, Avg. loss: 418612097124717087097655593533440.000000\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 6323945036888.98, NNZs: 843, Bias: 273354309.089001, T: 528619, Avg. loss: 417260770316411507220275951304704.000000\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 6338300087659.65, NNZs: 843, Bias: -97372107.622428, T: 531986, Avg. loss: 415928393160509094853554424774656.000000\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 5854495099165.54, NNZs: 843, Bias: -466868575.175556, T: 535353, Avg. loss: 414655488200507044725906345033728.000000\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 6217365624278.21, NNZs: 843, Bias: -96992310.933266, T: 538720, Avg. loss: 413355895328911516531893565652992.000000\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 6069643341082.99, NNZs: 843, Bias: -465395801.971614, T: 542087, Avg. loss: 412092824104551695686500456857600.000000\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 6238212485338.13, NNZs: 843, Bias: -96751071.223670, T: 545454, Avg. loss: 410826883478933586600963373793280.000000\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 7101387537927.48, NNZs: 843, Bias: 270777755.537362, T: 548821, Avg. loss: 409574212102184617021114427637760.000000\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 7236836536437.36, NNZs: 843, Bias: 637166731.626025, T: 552188, Avg. loss: 408310231162162773222300781117440.000000\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 6143913662975.59, NNZs: 843, Bias: 269950506.947784, T: 555555, Avg. loss: 407107505416935904162129375133696.000000\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 6221341970035.62, NNZs: 843, Bias: 635292868.684333, T: 558922, Avg. loss: 405919085439532434750423032659968.000000\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 5672516384316.58, NNZs: 843, Bias: 269102897.197843, T: 562289, Avg. loss: 404692869234331503050644852310016.000000\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 5138233528559.37, NNZs: 843, Bias: -95910494.703615, T: 565656, Avg. loss: 403505738219833052798773159264256.000000\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 5215886868333.65, NNZs: 843, Bias: -459826392.899670, T: 569023, Avg. loss: 402330035595217296483285119008768.000000\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 5727211958567.50, NNZs: 843, Bias: 631527655.279966, T: 572390, Avg. loss: 401157163325384734238176551370752.000000\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 5433328477335.91, NNZs: 843, Bias: -447612726.010854, T: 575757, Avg. loss: 399962390484616733786158036680704.000000\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 5522396701828.52, NNZs: 843, Bias: -84467487.420204, T: 579124, Avg. loss: 398808555828533261391712391004160.000000\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 6843727964757.46, NNZs: 843, Bias: 536228902.760096, T: 582491, Avg. loss: 397654725984622876295480190435328.000000\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 5718364767780.06, NNZs: 843, Bias: 173970724.832893, T: 585858, Avg. loss: 396526921031906506257844223868928.000000\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 7298865853093.28, NNZs: 843, Bias: 534652884.804897, T: 589225, Avg. loss: 395360652208120467195373851508736.000000\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 7353723373140.06, NNZs: 843, Bias: -547409263.052144, T: 592592, Avg. loss: 394219625072662142000825024643072.000000\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 6079797149830.90, NNZs: 843, Bias: 116228622.927373, T: 595959, Avg. loss: 393090243988830790986785794031616.000000\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 5579152459775.75, NNZs: 843, Bias: -243377310.260026, T: 599326, Avg. loss: 391988839844517810142377502834688.000000\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 5947255405664.31, NNZs: 843, Bias: 115788062.980350, T: 602693, Avg. loss: 390900250294575832787909527732224.000000\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 5676617564251.01, NNZs: 843, Bias: 473975395.012449, T: 606060, Avg. loss: 389820446352187196553825697660928.000000\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 5749245248114.36, NNZs: 843, Bias: -600440478.990607, T: 609427, Avg. loss: 388749549525867260817285629083648.000000\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 5761381940059.25, NNZs: 843, Bias: 472551843.960693, T: 612794, Avg. loss: 387700422582316240180638820335616.000000\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 6053474097082.11, NNZs: 843, Bias: 114927506.967696, T: 616161, Avg. loss: 386633202942548491442137933545472.000000\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 6046493223012.56, NNZs: 843, Bias: 471197078.023615, T: 619528, Avg. loss: 385589810226484447521425489657856.000000\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 6197474209671.67, NNZs: 843, Bias: -597380309.377563, T: 622895, Avg. loss: 384547197312417369303778271952896.000000\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 6790985262626.15, NNZs: 843, Bias: -241175751.190238, T: 626262, Avg. loss: 383505904528600926846626329264128.000000\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 6910322753171.65, NNZs: 843, Bias: 114099139.531233, T: 629629, Avg. loss: 382492453565780582290166982377472.000000\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 6857133071829.45, NNZs: 843, Bias: 468423324.568264, T: 632996, Avg. loss: 381471162130775562091875746709504.000000\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 6959865888744.65, NNZs: 843, Bias: 113726858.243619, T: 636363, Avg. loss: 380458198659114016672078582775808.000000\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 6576000263629.29, NNZs: 843, Bias: -240110771.840185, T: 639730, Avg. loss: 379470602035476676656481121075200.000000\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 6802883582213.96, NNZs: 843, Bias: 113276605.430033, T: 643097, Avg. loss: 378463927188008092150684515827712.000000\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 7513156751794.43, NNZs: 843, Bias: -239526082.972141, T: 646464, Avg. loss: 377491444894270499537379309649920.000000\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 6330728774768.64, NNZs: 843, Bias: 112983333.567746, T: 649831, Avg. loss: 376530133664985751793190968492032.000000\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 5957243395511.65, NNZs: 843, Bias: 167792259.518734, T: 653198, Avg. loss: 375547243906686215718100849917952.000000\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 6784873489438.95, NNZs: 843, Bias: 518843136.351601, T: 656565, Avg. loss: 374587080520684749062005017542656.000000\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 6036748588994.91, NNZs: 843, Bias: 167293135.953248, T: 659932, Avg. loss: 373629079010516050659851972378624.000000\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 6634013808168.61, NNZs: 843, Bias: -461866099.370877, T: 663299, Avg. loss: 372702126345172056245809732124672.000000\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 7814069878577.28, NNZs: 843, Bias: 588664216.762423, T: 666666, Avg. loss: 371753587493865821100338646614016.000000\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 6672991422084.31, NNZs: 843, Bias: -496357091.005356, T: 670033, Avg. loss: 370808211232282588210268444033024.000000\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 6261088568210.07, NNZs: 843, Bias: -146684215.082595, T: 673400, Avg. loss: 369907338083255178687915139006464.000000\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 5930117599677.46, NNZs: 843, Bias: 202152444.065073, T: 676767, Avg. loss: 368983828134659173246499509239808.000000\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 5544831940050.40, NNZs: 843, Bias: -146330848.664778, T: 680134, Avg. loss: 368074355971403732122799995617280.000000\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 6231002312572.89, NNZs: 843, Bias: 201617298.585848, T: 683501, Avg. loss: 367162538627739445600188148744192.000000\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 5233347945447.77, NNZs: 843, Bias: 172569227.609366, T: 686868, Avg. loss: 366268691527222598780613279350784.000000\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 7163379594682.73, NNZs: 843, Bias: 519261704.516633, T: 690235, Avg. loss: 365364092250530383608435968049152.000000\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 5481455865413.66, NNZs: 843, Bias: 172083731.306260, T: 693602, Avg. loss: 364490183174897675395358374494208.000000\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 5689757418519.54, NNZs: 843, Bias: 279886630.220498, T: 696969, Avg. loss: 363589150168386441916523644715008.000000\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 5505167622388.62, NNZs: 843, Bias: 399100921.523153, T: 700336, Avg. loss: 362740966647618967250539173117952.000000\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 6463692302557.40, NNZs: 843, Bias: 743830958.582052, T: 703703, Avg. loss: 361883920119469239599529990291456.000000\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 5136287706729.19, NNZs: 843, Bias: -291650225.855726, T: 707070, Avg. loss: 361037223730965192312744820342784.000000\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 6142043156615.47, NNZs: 843, Bias: 53101225.406517, T: 710437, Avg. loss: 360184960877884816722158348664832.000000\n",
      "Total training time: 1.54 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 5442587587479.49, NNZs: 843, Bias: -291067439.739771, T: 713804, Avg. loss: 359329183299358319644543912247296.000000\n",
      "Total training time: 1.54 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 6022387419832.50, NNZs: 843, Bias: -634390450.806311, T: 717171, Avg. loss: 358495171595029791750269976969216.000000\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 5879909985606.22, NNZs: 843, Bias: 396016827.736442, T: 720538, Avg. loss: 357666044928800335098534195888128.000000\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 5884731617600.63, NNZs: 843, Bias: -632949892.944110, T: 723905, Avg. loss: 356836653702135465779376139796480.000000\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 5973436614190.65, NNZs: 843, Bias: -289784269.237394, T: 727272, Avg. loss: 356003332305722916080030559764480.000000\n",
      "Total training time: 1.57 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 6452957647157.38, NNZs: 843, Bias: 52581790.218893, T: 730639, Avg. loss: 355181841667876767651700272005120.000000\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 5357218436468.86, NNZs: 843, Bias: -289160234.219308, T: 734006, Avg. loss: 354376426816194418072075223695360.000000\n",
      "Total training time: 1.59 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 5622328621509.26, NNZs: 843, Bias: -630131136.598595, T: 737373, Avg. loss: 353578281877388902477489867063296.000000\n",
      "Total training time: 1.59 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 5209303116539.45, NNZs: 843, Bias: 393182201.327220, T: 740740, Avg. loss: 352781767908423497484445432152064.000000\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 6667898132775.44, NNZs: 843, Bias: 52205523.730692, T: 744107, Avg. loss: 351973160663680486379034440105984.000000\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 6061632877898.83, NNZs: 843, Bias: -968190402.551932, T: 747474, Avg. loss: 351181647451266635071238321471488.000000\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 6592385781308.11, NNZs: 843, Bias: 985532122.372180, T: 750841, Avg. loss: 350404183812100633122936251744256.000000\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 6139567186587.50, NNZs: 843, Bias: -33595749.559395, T: 754208, Avg. loss: 349630439249981113859234003419136.000000\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 6838664863411.92, NNZs: 843, Bias: 305389711.466365, T: 757575, Avg. loss: 348842842063078432162201808666624.000000\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 6265300504799.05, NNZs: 843, Bias: -33523908.762917, T: 760942, Avg. loss: 348083886672654522097635608756224.000000\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 6285726452934.99, NNZs: 843, Bias: -371717144.700305, T: 764309, Avg. loss: 347320148402724800278624219430912.000000\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 6264151115284.10, NNZs: 843, Bias: -8467320.753019, T: 767676, Avg. loss: 346570039860704053338207592382464.000000\n",
      "Total training time: 1.66 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 6092391673952.92, NNZs: 843, Bias: -99131499.404826, T: 771043, Avg. loss: 345820218652695558310216584921088.000000\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 6501000761407.34, NNZs: 843, Bias: -393697117.667407, T: 774410, Avg. loss: 345071305585158335178713251971072.000000\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 6006119852327.82, NNZs: 843, Bias: -56506408.891606, T: 777777, Avg. loss: 344317830738791826865560771100672.000000\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 7217253402538.50, NNZs: 843, Bias: -194157689.293193, T: 781144, Avg. loss: 343575863896263442608582949863424.000000\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 7392684647606.61, NNZs: 843, Bias: -452428744.696408, T: 784511, Avg. loss: 342840660759874025637938150244352.000000\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 6949733222074.62, NNZs: 843, Bias: -116305316.050965, T: 787878, Avg. loss: 342114610389543293907129851707392.000000\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 6875496218226.46, NNZs: 843, Bias: -336284890.265604, T: 791245, Avg. loss: 341387835212677234115557162221568.000000\n",
      "Total training time: 1.71 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 8295870748496.97, NNZs: 843, Bias: -1039020.678038, T: 794612, Avg. loss: 340658922858515287214337004929024.000000\n",
      "Total training time: 1.72 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 7366714975175.58, NNZs: 843, Bias: 1002679343.184765, T: 797979, Avg. loss: 339944360555114734874063282372608.000000\n",
      "Total training time: 1.72 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 7763969048161.80, NNZs: 843, Bias: -669561152.159429, T: 801346, Avg. loss: 339230618650002848471818321264640.000000\n",
      "Total training time: 1.73 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 7151343248644.59, NNZs: 843, Bias: -335017787.231044, T: 804713, Avg. loss: 338530767331633285122718504583168.000000\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 7012136365253.62, NNZs: 843, Bias: -668244438.129095, T: 808080, Avg. loss: 337828176633583439211872636633088.000000\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 6490451587784.54, NNZs: 843, Bias: 331932597.001284, T: 811447, Avg. loss: 337148163000588916635878717128704.000000\n",
      "Total training time: 1.75 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 6746364687580.90, NNZs: 843, Bias: -1300935.694836, T: 814814, Avg. loss: 336454939914543759051589404327936.000000\n",
      "Total training time: 1.76 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 7309302478566.99, NNZs: 843, Bias: -333831632.893655, T: 818181, Avg. loss: 335759114673223344587372085903360.000000\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 6459713032733.02, NNZs: 843, Bias: -1342785.879600, T: 821548, Avg. loss: 335070637345015607926942725046272.000000\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 7170663314222.33, NNZs: 843, Bias: -333225490.931293, T: 824915, Avg. loss: 334393757824218600621773617627136.000000\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 8490932694156.33, NNZs: 843, Bias: 661510968.910505, T: 828282, Avg. loss: 333722802744216307277687731257344.000000\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 7342373229898.86, NNZs: 843, Bias: -332613325.032880, T: 831649, Avg. loss: 333048903115888668452065680293888.000000\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 7096627209571.07, NNZs: 843, Bias: -1516464.509065, T: 835016, Avg. loss: 332381392352373984504756708573184.000000\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 7321073033117.06, NNZs: 843, Bias: 328895797.616087, T: 838383, Avg. loss: 331712929168373626511379541262336.000000\n",
      "Total training time: 1.81 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 8349177477028.34, NNZs: 843, Bias: 658697294.814461, T: 841750, Avg. loss: 331062090154493039097341473193984.000000\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 7598499354997.70, NNZs: 843, Bias: 328188180.447884, T: 845117, Avg. loss: 330396630716296787386545591025664.000000\n",
      "Total training time: 1.82 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 7031457114146.84, NNZs: 843, Bias: -1655700.345169, T: 848484, Avg. loss: 329754338758886627441994178232320.000000\n",
      "Total training time: 1.83 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 6403371818601.97, NNZs: 843, Bias: -124927502.956112, T: 851851, Avg. loss: 329095280927206363453875277529088.000000\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 5985858802180.95, NNZs: 843, Bias: 204019173.909085, T: 855218, Avg. loss: 328451181969144998908000131350528.000000\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 6873986784614.31, NNZs: 843, Bias: -124705889.334219, T: 858585, Avg. loss: 327807131099420280078510189445120.000000\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 5827704976142.24, NNZs: 843, Bias: -91254747.609292, T: 861952, Avg. loss: 327163230771255926050064881942528.000000\n",
      "Total training time: 1.86 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 6889803333951.02, NNZs: 843, Bias: -276341866.343676, T: 865319, Avg. loss: 326538431161979184516909183795200.000000\n",
      "Total training time: 1.87 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 6238644155679.95, NNZs: 843, Bias: 51451408.755860, T: 868686, Avg. loss: 325915835027767667791369540206592.000000\n",
      "Total training time: 1.87 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 6740752630768.61, NNZs: 843, Bias: 268867361.028156, T: 872053, Avg. loss: 325275044468720960802008602771456.000000\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 6618964638503.35, NNZs: 843, Bias: -58307623.812186, T: 875420, Avg. loss: 324643125815913998368502055960576.000000\n",
      "Total training time: 1.89 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 6698815351857.57, NNZs: 843, Bias: -384894100.563113, T: 878787, Avg. loss: 324020697024731475954659311484928.000000\n",
      "Total training time: 1.90 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 7415784528982.90, NNZs: 843, Bias: -58237136.326395, T: 882154, Avg. loss: 323414202159100689173194075013120.000000\n",
      "Total training time: 1.90 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 7028424575423.12, NNZs: 843, Bias: 267762500.690810, T: 885521, Avg. loss: 322800230626673911037065743564800.000000\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 7407021188701.96, NNZs: 843, Bias: -273839391.170183, T: 888888, Avg. loss: 322188575342269739197021057712128.000000\n",
      "Total training time: 1.92 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 6240363529472.04, NNZs: 843, Bias: 51779453.365400, T: 892255, Avg. loss: 321580693026604469410151645839360.000000\n",
      "Total training time: 1.92 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 6908914522014.13, NNZs: 843, Bias: -273339734.252967, T: 895622, Avg. loss: 320975695032986079500642537177088.000000\n",
      "Total training time: 1.93 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 8035946106693.96, NNZs: 843, Bias: -597840270.284315, T: 898989, Avg. loss: 320366527561530673103840657539072.000000\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 6809034190761.90, NNZs: 843, Bias: -272838937.631204, T: 902356, Avg. loss: 319772716701291536812115796951040.000000\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 6667779429000.07, NNZs: 843, Bias: 51523382.182531, T: 905723, Avg. loss: 319191657026132839379357359144960.000000\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 7945478561123.37, NNZs: 843, Bias: 375296424.852636, T: 909090, Avg. loss: 318606191573795694871282957942784.000000\n",
      "Total training time: 1.96 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 7319763069278.58, NNZs: 843, Bias: 51358767.615665, T: 912457, Avg. loss: 318005064164795031533719780327424.000000\n",
      "Total training time: 1.97 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 6193974742860.57, NNZs: 843, Bias: -271965880.470259, T: 915824, Avg. loss: 317421074804904513507382549020672.000000\n",
      "Total training time: 1.97 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 6742451779940.20, NNZs: 843, Bias: 51200275.789607, T: 919191, Avg. loss: 316845490637096037403458552201216.000000\n",
      "Total training time: 1.98 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 6409729164215.69, NNZs: 843, Bias: -271518623.044944, T: 922558, Avg. loss: 316267049771918281992500850720768.000000\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 7123086493275.13, NNZs: 843, Bias: 51055110.867073, T: 925925, Avg. loss: 315698435240813541629180881403904.000000\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 6379597043973.68, NNZs: 843, Bias: 43252715.841756, T: 929292, Avg. loss: 315124457160611582271236647944192.000000\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 6919333049051.97, NNZs: 843, Bias: -278589906.506863, T: 932659, Avg. loss: 314553067671592129073686880714752.000000\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 6365851032535.85, NNZs: 843, Bias: 43133486.578919, T: 936026, Avg. loss: 313994214654530802680902008700928.000000\n",
      "Total training time: 2.02 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 6183346755361.48, NNZs: 843, Bias: -150999516.731409, T: 939393, Avg. loss: 313433269620892875223001355780096.000000\n",
      "Total training time: 2.02 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 6982531685278.62, NNZs: 843, Bias: 170039911.013509, T: 942760, Avg. loss: 312880561888978651161250037235712.000000\n",
      "Total training time: 2.03 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 6622563895136.14, NNZs: 843, Bias: -150740087.496381, T: 946127, Avg. loss: 312322540145233018790722997846016.000000\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 6643844460705.46, NNZs: 843, Bias: -470950633.137233, T: 949494, Avg. loss: 311778913477258505380733558718464.000000\n",
      "Total training time: 2.05 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 7325504372430.02, NNZs: 843, Bias: -150474753.631284, T: 952861, Avg. loss: 311233228474605253387303243481088.000000\n",
      "Total training time: 2.05 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 6802701468992.23, NNZs: 843, Bias: 169438648.819285, T: 956228, Avg. loss: 310682806090451725107970015494144.000000\n",
      "Total training time: 2.06 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 6847094447560.43, NNZs: 843, Bias: -150218870.584220, T: 959595, Avg. loss: 310147168731751566497393986764800.000000\n",
      "Total training time: 2.07 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 8278683750817.59, NNZs: 843, Bias: 137958107.131160, T: 962962, Avg. loss: 309599898136915343565435944566784.000000\n",
      "Total training time: 2.07 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 7119643173927.35, NNZs: 843, Bias: 456763304.729224, T: 966329, Avg. loss: 309062599023121911782986966106112.000000\n",
      "Total training time: 2.08 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 6647302922582.89, NNZs: 843, Bias: -499644924.894789, T: 969696, Avg. loss: 308531389026446225298358829842432.000000\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 7202763916967.77, NNZs: 843, Bias: 455969728.335568, T: 973063, Avg. loss: 307998825875242089702233672253440.000000\n",
      "Total training time: 2.10 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 6452069014304.86, NNZs: 843, Bias: 137444922.215744, T: 976430, Avg. loss: 307457389808215652587489441349632.000000\n",
      "Total training time: 2.10 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 6702448297733.80, NNZs: 843, Bias: 455187404.845504, T: 979797, Avg. loss: 306928259287833280662868645117952.000000\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 7217034403055.07, NNZs: 843, Bias: -497910101.568688, T: 983164, Avg. loss: 306414754226541470385663668912128.000000\n",
      "Total training time: 2.12 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 8215948129918.94, NNZs: 843, Bias: 103890496.037418, T: 986531, Avg. loss: 305889030745944565985984628391936.000000\n",
      "Total training time: 2.12 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 7090439609854.47, NNZs: 843, Bias: -341740008.659101, T: 989898, Avg. loss: 305362880393564309759229161373696.000000\n",
      "Total training time: 2.13 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 7619233650093.65, NNZs: 843, Bias: -24696209.976307, T: 993265, Avg. loss: 304850420320013941528607768707072.000000\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 6807474587547.40, NNZs: 843, Bias: -341169540.823144, T: 996632, Avg. loss: 304333401043643735376359864664064.000000\n",
      "Total training time: 2.15 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 6416363023812.37, NNZs: 843, Bias: -24678113.684321, T: 999999, Avg. loss: 303825366409334116856324908122112.000000\n",
      "Total training time: 2.15 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 7070839113232.74, NNZs: 843, Bias: -340643235.996600, T: 1003366, Avg. loss: 303334870598301825261836439650304.000000\n",
      "Total training time: 2.16 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 6121422978099.24, NNZs: 843, Bias: -97757134.098111, T: 1006733, Avg. loss: 302840203035849440615562452402176.000000\n",
      "Total training time: 2.17 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 6425438848935.94, NNZs: 843, Bias: 217726846.847347, T: 1010100, Avg. loss: 302338622886639605917363281592320.000000\n",
      "Total training time: 2.17 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 6393043592929.40, NNZs: 843, Bias: -97622987.022108, T: 1013467, Avg. loss: 301837544197528446410158485012480.000000\n",
      "Total training time: 2.18 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 5938831666408.45, NNZs: 843, Bias: -492208916.406872, T: 1016834, Avg. loss: 301339119825121730096478975688704.000000\n",
      "Total training time: 2.19 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 5508426983398.05, NNZs: 843, Bias: -177168513.045657, T: 1020201, Avg. loss: 300836463311945111816221531045888.000000\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 6231987148092.56, NNZs: 843, Bias: -491439170.840703, T: 1023568, Avg. loss: 300350039372826309238550205300736.000000\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 6122159165890.55, NNZs: 843, Bias: -176916165.647711, T: 1026935, Avg. loss: 299858895683528582775306065018880.000000\n",
      "Total training time: 2.21 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 6442120561243.67, NNZs: 843, Bias: -490656455.599534, T: 1030302, Avg. loss: 299368471490015398970316583600128.000000\n",
      "Total training time: 2.22 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 6289111531671.97, NNZs: 843, Bias: -176668041.066444, T: 1033669, Avg. loss: 298885320795182434325264374693888.000000\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 7056454662893.60, NNZs: 843, Bias: 136820753.724347, T: 1037036, Avg. loss: 298399914984987432115512901894144.000000\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 5957183418477.98, NNZs: 843, Bias: -254347431.854331, T: 1040403, Avg. loss: 297917572513803569827483725332480.000000\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 6491972668549.84, NNZs: 843, Bias: -567014267.265029, T: 1043770, Avg. loss: 297438135368495586435411071729664.000000\n",
      "Total training time: 2.25 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 6581833914692.00, NNZs: 843, Bias: -253981078.204950, T: 1047137, Avg. loss: 296957528470728183032607928221696.000000\n",
      "Total training time: 2.25 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 6912821370570.95, NNZs: 843, Bias: 58535363.198679, T: 1050504, Avg. loss: 296479057110803404081223440531456.000000\n",
      "Total training time: 2.26 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 7201081289522.30, NNZs: 843, Bias: -253662104.837722, T: 1053871, Avg. loss: 296015282784402019302727250608128.000000\n",
      "Total training time: 2.27 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 7593751516014.47, NNZs: 843, Bias: -523072566.882962, T: 1057238, Avg. loss: 295534677213186387330938389397504.000000\n",
      "Total training time: 2.28 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 6744161870226.95, NNZs: 843, Bias: -211060945.757527, T: 1060605, Avg. loss: 295068033290533820814082575433728.000000\n",
      "Total training time: 2.28 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 6985358552003.71, NNZs: 843, Bias: 100466334.847502, T: 1063972, Avg. loss: 294609458059679557161957864767488.000000\n",
      "Total training time: 2.29 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 6879064786914.88, NNZs: 843, Bias: 411480647.312167, T: 1067339, Avg. loss: 294143591655930574407360724336640.000000\n",
      "Total training time: 2.30 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 7343541435008.93, NNZs: 843, Bias: -372709532.122476, T: 1070706, Avg. loss: 293685159790393215202926937505792.000000\n",
      "Total training time: 2.30 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 7303326259548.57, NNZs: 843, Bias: -61783614.556756, T: 1074073, Avg. loss: 293232446229606294289156025614336.000000\n",
      "Total training time: 2.31 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 7285691406605.34, NNZs: 843, Bias: 248649360.078213, T: 1077440, Avg. loss: 292773636902336089570135836196864.000000\n",
      "Total training time: 2.32 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 7931225262203.44, NNZs: 843, Bias: 558586120.324731, T: 1080807, Avg. loss: 292315595189882959767566469300224.000000\n",
      "Total training time: 2.33 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 7225563215749.04, NNZs: 843, Bias: 248226357.036144, T: 1084174, Avg. loss: 291865709729389076907926952607744.000000\n",
      "Total training time: 2.33 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 6756902282497.00, NNZs: 843, Bias: -61629621.636623, T: 1087541, Avg. loss: 291415274597951154637324677021696.000000\n",
      "Total training time: 2.34 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 6699150899418.04, NNZs: 843, Bias: 247843872.230949, T: 1090908, Avg. loss: 290968940340895275727731468271616.000000\n",
      "Total training time: 2.35 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 6474395812453.20, NNZs: 843, Bias: 177755543.406479, T: 1094275, Avg. loss: 290521848851169966418139562377216.000000\n",
      "Total training time: 2.35 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 6916661943090.38, NNZs: 843, Bias: -131328749.707273, T: 1097642, Avg. loss: 290071820060011964314024817983488.000000\n",
      "Total training time: 2.36 seconds.\n",
      "-- Epoch 327\n",
      "Norm: 5774995186450.55, NNZs: 843, Bias: -439952600.205971, T: 1101009, Avg. loss: 289627722621645554087407271280640.000000\n",
      "Total training time: 2.37 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 5562536104922.31, NNZs: 843, Bias: -131126449.942954, T: 1104376, Avg. loss: 289194118581280480917917983047680.000000\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 6237970145787.12, NNZs: 843, Bias: -439291164.972348, T: 1107743, Avg. loss: 288757131229209340210994288263168.000000\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 6395120017569.56, NNZs: 843, Bias: -159731992.007385, T: 1111110, Avg. loss: 288322018653125107852973610893312.000000\n",
      "Total training time: 2.39 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 5854134266469.11, NNZs: 843, Bias: 148140637.302747, T: 1114477, Avg. loss: 287890585004442492860864393117696.000000\n",
      "Total training time: 2.40 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 5501716276338.37, NNZs: 843, Bias: -159515045.399213, T: 1117844, Avg. loss: 287460831561141449570529264533504.000000\n",
      "Total training time: 2.40 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 6819351114152.30, NNZs: 843, Bias: 147901918.530796, T: 1121211, Avg. loss: 287025001053411736240807670185984.000000\n",
      "Total training time: 2.41 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 5382321166648.49, NNZs: 843, Bias: 42816392.246265, T: 1124578, Avg. loss: 286602931278330598741723616116736.000000\n",
      "Total training time: 2.42 seconds.\n",
      "-- Epoch 335\n",
      "Norm: 5530502686574.38, NNZs: 843, Bias: 349635095.671161, T: 1127945, Avg. loss: 286173126700648353829859918610432.000000\n",
      "Total training time: 2.43 seconds.\n",
      "-- Epoch 336\n",
      "Norm: 6067953903503.42, NNZs: 843, Bias: 42737709.710299, T: 1131312, Avg. loss: 285752750181427762643877878562816.000000\n",
      "Total training time: 2.43 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 5272337686673.30, NNZs: 843, Bias: 349107108.708934, T: 1134679, Avg. loss: 285328823634574830915652717379584.000000\n",
      "Total training time: 2.44 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 6466082969630.21, NNZs: 843, Bias: 42669633.534219, T: 1138046, Avg. loss: 284912478718488818225794556887040.000000\n",
      "Total training time: 2.45 seconds.\n",
      "-- Epoch 339\n",
      "Norm: 6042900283324.33, NNZs: 843, Bias: -263311389.041158, T: 1141413, Avg. loss: 284497214524696747423588656611328.000000\n",
      "Total training time: 2.45 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 6149193375450.42, NNZs: 843, Bias: 42593573.838196, T: 1144780, Avg. loss: 284072839200706620021390975172608.000000\n",
      "Total training time: 2.46 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 5931474656882.82, NNZs: 843, Bias: -424267828.567090, T: 1148147, Avg. loss: 283662935317131074657599545671680.000000\n",
      "Total training time: 2.47 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 6161874661183.68, NNZs: 843, Bias: 315578574.761717, T: 1151514, Avg. loss: 283247156667084745535933781114880.000000\n",
      "Total training time: 2.48 seconds.\n",
      "-- Epoch 343\n",
      "Norm: 6475448940047.29, NNZs: 843, Bias: 183565205.735414, T: 1154881, Avg. loss: 282830277743057688595634082283520.000000\n",
      "Total training time: 2.48 seconds.\n",
      "-- Epoch 344\n",
      "Norm: 7225725804531.94, NNZs: 843, Bias: 488237711.541042, T: 1158248, Avg. loss: 282415615087146194070019294363648.000000\n",
      "Total training time: 2.49 seconds.\n",
      "-- Epoch 345\n",
      "Norm: 6530247409612.34, NNZs: 843, Bias: 183277460.542609, T: 1161615, Avg. loss: 282004019436291444468953345163264.000000\n",
      "Total training time: 2.50 seconds.\n",
      "-- Epoch 346\n",
      "Norm: 6952051319966.84, NNZs: 843, Bias: 487513641.007745, T: 1164982, Avg. loss: 281590807945263459686496109330432.000000\n",
      "Total training time: 2.50 seconds.\n",
      "-- Epoch 347\n",
      "Norm: 6476855385082.87, NNZs: 843, Bias: -387916024.195529, T: 1168349, Avg. loss: 281189737201258579455141100912640.000000\n",
      "Total training time: 2.51 seconds.\n",
      "-- Epoch 348\n",
      "Norm: 6623586144510.66, NNZs: 843, Bias: -83711518.413658, T: 1171716, Avg. loss: 280791421642161589927136121061376.000000\n",
      "Total training time: 2.52 seconds.\n",
      "-- Epoch 349\n",
      "Norm: 6147801409496.83, NNZs: 843, Bias: 220064903.557408, T: 1175083, Avg. loss: 280390208029529342187013631115264.000000\n",
      "Total training time: 2.53 seconds.\n",
      "-- Epoch 350\n",
      "Norm: 5701214557374.13, NNZs: 843, Bias: 265905602.793743, T: 1178450, Avg. loss: 279997399178975768207142588252160.000000\n",
      "Total training time: 2.53 seconds.\n",
      "-- Epoch 351\n",
      "Norm: 5703485510291.83, NNZs: 843, Bias: -37575936.178474, T: 1181817, Avg. loss: 279595447833977074063917910589440.000000\n",
      "Total training time: 2.54 seconds.\n",
      "-- Epoch 352\n",
      "Norm: 6860578998798.30, NNZs: 843, Bias: 265516108.854275, T: 1185184, Avg. loss: 279198713279398949229070827651072.000000\n",
      "Total training time: 2.55 seconds.\n",
      "-- Epoch 353\n",
      "Norm: 6572889183348.69, NNZs: 843, Bias: 568196705.574612, T: 1188551, Avg. loss: 278797505691303548270934537273344.000000\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 354\n",
      "Norm: 6453055029531.22, NNZs: 843, Bias: -340153942.978055, T: 1191918, Avg. loss: 278399963175615931170839681040384.000000\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 355\n",
      "Norm: 6740410373227.02, NNZs: 843, Bias: -37493490.368417, T: 1195285, Avg. loss: 278017295077219178289703434584064.000000\n",
      "Total training time: 2.57 seconds.\n",
      "-- Epoch 356\n",
      "Norm: 6222959600853.50, NNZs: 843, Bias: 264741902.497376, T: 1198652, Avg. loss: 277626806507813610353224176369664.000000\n",
      "Total training time: 2.58 seconds.\n",
      "-- Epoch 357\n",
      "Norm: 6810302944835.29, NNZs: 843, Bias: 566574115.111055, T: 1202019, Avg. loss: 277233898937784211230607741550592.000000\n",
      "Total training time: 2.58 seconds.\n",
      "-- Epoch 358\n",
      "Norm: 6026700214526.77, NNZs: 843, Bias: -67934990.066007, T: 1205386, Avg. loss: 276848976242623564787769396953088.000000\n",
      "Total training time: 2.59 seconds.\n",
      "-- Epoch 359\n",
      "Norm: 5563792069913.13, NNZs: 843, Bias: 36715456.402259, T: 1208753, Avg. loss: 276468290938499037739334313705472.000000\n",
      "Total training time: 2.60 seconds.\n",
      "-- Epoch 360\n",
      "Norm: 5733522657712.75, NNZs: 843, Bias: -264711481.328812, T: 1212120, Avg. loss: 276082720205791518304345580896256.000000\n",
      "Total training time: 2.61 seconds.\n",
      "-- Epoch 361\n",
      "Norm: 5598950156588.64, NNZs: 843, Bias: 36631840.882635, T: 1215487, Avg. loss: 275706145581166275520281362563072.000000\n",
      "Total training time: 2.61 seconds.\n",
      "-- Epoch 362\n",
      "Norm: 5707777395632.72, NNZs: 843, Bias: 191333441.679856, T: 1218854, Avg. loss: 275330119359653682616019437223936.000000\n",
      "Total training time: 2.62 seconds.\n",
      "-- Epoch 363\n",
      "Norm: 6599688224954.97, NNZs: 843, Bias: 491962840.146366, T: 1222221, Avg. loss: 274957335641853940948390272565248.000000\n",
      "Total training time: 2.63 seconds.\n",
      "-- Epoch 364\n",
      "Norm: 5863969916802.17, NNZs: 843, Bias: 191073156.881231, T: 1225588, Avg. loss: 274576777035346955647275084283904.000000\n",
      "Total training time: 2.63 seconds.\n",
      "-- Epoch 365\n",
      "Norm: 6371566080225.86, NNZs: 843, Bias: -710091342.092320, T: 1228955, Avg. loss: 274204165512140624205503120015360.000000\n",
      "Total training time: 2.64 seconds.\n",
      "-- Epoch 366\n",
      "Norm: 6800818776041.13, NNZs: 843, Bias: 190805822.812074, T: 1232322, Avg. loss: 273830921318429411861625183404032.000000\n",
      "Total training time: 2.65 seconds.\n",
      "-- Epoch 367\n",
      "Norm: 6354011355643.17, NNZs: 843, Bias: -109241292.365863, T: 1235689, Avg. loss: 273452514365651790111880878489600.000000\n",
      "Total training time: 2.66 seconds.\n",
      "-- Epoch 368\n",
      "Norm: 6030418175561.80, NNZs: 843, Bias: 190556764.113341, T: 1239056, Avg. loss: 273087473725774957169835379785728.000000\n",
      "Total training time: 2.66 seconds.\n",
      "-- Epoch 369\n",
      "Norm: 5948684144638.32, NNZs: 843, Bias: -109096454.145458, T: 1242423, Avg. loss: 272722061911373853185633477984256.000000\n",
      "Total training time: 2.67 seconds.\n",
      "-- Epoch 370\n",
      "Norm: 7082412493525.41, NNZs: 843, Bias: -557659955.595387, T: 1245790, Avg. loss: 272354031957463184091780369874944.000000\n",
      "Total training time: 2.68 seconds.\n",
      "-- Epoch 371\n",
      "Norm: 7010064408800.27, NNZs: 843, Bias: 340065193.275982, T: 1249157, Avg. loss: 271990683763808311284633880756224.000000\n",
      "Total training time: 2.68 seconds.\n",
      "-- Epoch 372\n",
      "Norm: 5853032861820.16, NNZs: 843, Bias: -277418273.434066, T: 1252524, Avg. loss: 271628075511007572143098988331008.000000\n",
      "Total training time: 2.69 seconds.\n",
      "-- Epoch 373\n",
      "Norm: 6216122210839.00, NNZs: 843, Bias: 21476641.549169, T: 1255891, Avg. loss: 271266063856420983379987898826752.000000\n",
      "Total training time: 2.70 seconds.\n",
      "-- Epoch 374\n",
      "Norm: 5848219537911.01, NNZs: 843, Bias: -277095149.312542, T: 1259258, Avg. loss: 270894500821582214860095547768832.000000\n",
      "Total training time: 2.71 seconds.\n",
      "-- Epoch 375\n",
      "Norm: 6578757134371.66, NNZs: 843, Bias: 618036397.261054, T: 1262625, Avg. loss: 270528145496419141523980624592896.000000\n",
      "Total training time: 2.71 seconds.\n",
      "-- Epoch 376\n",
      "Norm: 5847120969453.69, NNZs: 843, Bias: 319494057.540216, T: 1265992, Avg. loss: 270172508869983810601704816115712.000000\n",
      "Total training time: 2.72 seconds.\n",
      "-- Epoch 377\n",
      "Norm: 5436975033487.66, NNZs: 843, Bias: -7393644.174341, T: 1269359, Avg. loss: 269816413324024258838321766072320.000000\n",
      "Total training time: 2.73 seconds.\n",
      "-- Epoch 378\n",
      "Norm: 6866595553128.57, NNZs: 843, Bias: 290332654.977414, T: 1272726, Avg. loss: 269465870113863256431055211593728.000000\n",
      "Total training time: 2.74 seconds.\n",
      "-- Epoch 379\n",
      "Norm: 5214549908190.76, NNZs: 843, Bias: -7380738.282908, T: 1276093, Avg. loss: 269108261373485882887196488237056.000000\n",
      "Total training time: 2.74 seconds.\n",
      "-- Epoch 380\n",
      "Norm: 6276592606085.32, NNZs: 843, Bias: 289956764.645197, T: 1279460, Avg. loss: 268760826411101281177753442320384.000000\n",
      "Total training time: 2.75 seconds.\n",
      "-- Epoch 381\n",
      "Norm: 5290511355648.10, NNZs: 843, Bias: -7370360.083788, T: 1282827, Avg. loss: 268408933087329812007507829194752.000000\n",
      "Total training time: 2.76 seconds.\n",
      "-- Epoch 382\n",
      "Norm: 5815172796599.01, NNZs: 843, Bias: 289575780.851754, T: 1286194, Avg. loss: 268055294088319140916597708816384.000000\n",
      "Total training time: 2.76 seconds.\n",
      "-- Epoch 383\n",
      "Norm: 5538322623676.88, NNZs: 843, Bias: 458233842.207103, T: 1289561, Avg. loss: 267704464297711270707426081374208.000000\n",
      "Total training time: 2.77 seconds.\n",
      "-- Epoch 384\n",
      "Norm: 5115525892533.95, NNZs: 843, Bias: 161370827.643022, T: 1292928, Avg. loss: 267358003715307308689690820870144.000000\n",
      "Total training time: 2.78 seconds.\n",
      "-- Epoch 385\n",
      "Norm: 5226762613956.76, NNZs: 843, Bias: -135105841.426066, T: 1296295, Avg. loss: 267009725861516742892689188454400.000000\n",
      "Total training time: 2.79 seconds.\n",
      "-- Epoch 386\n",
      "Norm: 5512597850064.54, NNZs: 843, Bias: 176339985.669101, T: 1299662, Avg. loss: 266661496134151602329393012146176.000000\n",
      "Total training time: 2.79 seconds.\n",
      "-- Epoch 387\n",
      "Norm: 6185850057398.58, NNZs: 843, Bias: -119760941.270695, T: 1303029, Avg. loss: 266312382756633870821155613442048.000000\n",
      "Total training time: 2.80 seconds.\n",
      "-- Epoch 388\n",
      "Norm: 5982435417990.05, NNZs: 843, Bias: -415471038.107433, T: 1306396, Avg. loss: 265969382884692671206210958000128.000000\n",
      "Total training time: 2.81 seconds.\n",
      "-- Epoch 389\n",
      "Norm: 5676616449270.80, NNZs: 843, Bias: -119606819.475473, T: 1309763, Avg. loss: 265631039427182243666538702831616.000000\n",
      "Total training time: 2.81 seconds.\n",
      "-- Epoch 390\n",
      "Norm: 5773484027842.38, NNZs: 843, Bias: 21056828.053021, T: 1313130, Avg. loss: 265290872737059788983203214131200.000000\n",
      "Total training time: 2.82 seconds.\n",
      "-- Epoch 391\n",
      "Norm: 5910014492448.13, NNZs: 843, Bias: -274195175.273943, T: 1316497, Avg. loss: 264958044323267633525004924092416.000000\n",
      "Total training time: 2.83 seconds.\n",
      "-- Epoch 392\n",
      "Norm: 6401093120009.05, NNZs: 843, Bias: -208360910.476852, T: 1319864, Avg. loss: 264621781676688739294478640087040.000000\n",
      "Total training time: 2.84 seconds.\n",
      "-- Epoch 393\n",
      "Norm: 6220907139649.10, NNZs: 843, Bias: 86597583.307298, T: 1323231, Avg. loss: 264280851080121301320209665097728.000000\n",
      "Total training time: 2.84 seconds.\n",
      "-- Epoch 394\n",
      "Norm: 7091032942324.37, NNZs: 843, Bias: -208116334.422723, T: 1326598, Avg. loss: 263938301249702892763504281911296.000000\n",
      "Total training time: 2.85 seconds.\n",
      "-- Epoch 395\n",
      "Norm: 5967507786796.05, NNZs: 843, Bias: 86472718.982121, T: 1329965, Avg. loss: 263604693841761834654992317808640.000000\n",
      "Total training time: 2.86 seconds.\n",
      "-- Epoch 396\n",
      "Norm: 6855643255894.56, NNZs: 843, Bias: -207866425.129419, T: 1333332, Avg. loss: 263267408852849350992435299221504.000000\n",
      "Total training time: 2.86 seconds.\n",
      "-- Epoch 397\n",
      "Norm: 5947414246105.29, NNZs: 843, Bias: 86337863.371060, T: 1336699, Avg. loss: 262925744650075867176621792296960.000000\n",
      "Total training time: 2.87 seconds.\n",
      "-- Epoch 398\n",
      "Norm: 6891099299344.82, NNZs: 843, Bias: 380184343.617451, T: 1340066, Avg. loss: 262599171715242081209431007166464.000000\n",
      "Total training time: 2.88 seconds.\n",
      "-- Epoch 399\n",
      "Norm: 5672980301756.53, NNZs: 843, Bias: 86211913.973045, T: 1343433, Avg. loss: 262269837773200797500519747682304.000000\n",
      "Total training time: 2.89 seconds.\n",
      "-- Epoch 400\n",
      "Norm: 6006987044346.59, NNZs: 843, Bias: -207384342.685679, T: 1346800, Avg. loss: 261942386943546942319639668981760.000000\n",
      "Total training time: 2.89 seconds.\n",
      "-- Epoch 401\n",
      "Norm: 5740224400673.52, NNZs: 843, Bias: 86091031.433312, T: 1350167, Avg. loss: 261613313053613961335608346411008.000000\n",
      "Total training time: 2.90 seconds.\n",
      "-- Epoch 402\n",
      "Norm: 6628678196512.87, NNZs: 843, Bias: 379220310.866954, T: 1353534, Avg. loss: 261295593760596078735876656463872.000000\n",
      "Total training time: 2.91 seconds.\n",
      "-- Epoch 403\n",
      "Norm: 6878305978542.13, NNZs: 843, Bias: 356027336.317388, T: 1356901, Avg. loss: 260969291667730051024591429566464.000000\n",
      "Total training time: 2.92 seconds.\n",
      "-- Epoch 404\n",
      "Norm: 6361604428955.40, NNZs: 843, Bias: -522653189.233736, T: 1360268, Avg. loss: 260654191333356645915784397193216.000000\n",
      "Total training time: 2.92 seconds.\n",
      "-- Epoch 405\n",
      "Norm: 6195624314535.99, NNZs: 843, Bias: 115507937.137150, T: 1363635, Avg. loss: 260335639148304559463964921036800.000000\n",
      "Total training time: 2.93 seconds.\n",
      "-- Epoch 406\n",
      "Norm: 6140748504059.53, NNZs: 843, Bias: 407877873.998639, T: 1367002, Avg. loss: 260020169104489055022759054147584.000000\n",
      "Total training time: 2.94 seconds.\n",
      "-- Epoch 407\n",
      "Norm: 5999327784711.22, NNZs: 843, Bias: 115337574.123311, T: 1370369, Avg. loss: 259697586278213696463517339615232.000000\n",
      "Total training time: 2.94 seconds.\n",
      "-- Epoch 408\n",
      "Norm: 5637377324601.68, NNZs: 843, Bias: -176843336.164227, T: 1373736, Avg. loss: 259383669017482817789808424779776.000000\n",
      "Total training time: 2.95 seconds.\n",
      "-- Epoch 409\n",
      "Norm: 6005759807267.77, NNZs: 843, Bias: 115170493.033921, T: 1377103, Avg. loss: 259074046510530271051130580500480.000000\n",
      "Total training time: 2.96 seconds.\n",
      "-- Epoch 410\n",
      "Norm: 5665924043375.03, NNZs: 843, Bias: 248664633.715981, T: 1380470, Avg. loss: 258759395354033759975744752058368.000000\n",
      "Total training time: 2.97 seconds.\n",
      "-- Epoch 411\n",
      "Norm: 5657814346472.56, NNZs: 843, Bias: -321236745.905894, T: 1383837, Avg. loss: 258448084771733007624051858866176.000000\n",
      "Total training time: 2.97 seconds.\n",
      "-- Epoch 412\n",
      "Norm: 5914256547943.13, NNZs: 843, Bias: 547445540.488009, T: 1387204, Avg. loss: 258137241933108004863894966239232.000000\n",
      "Total training time: 2.98 seconds.\n",
      "-- Epoch 413\n",
      "Norm: 5589627638944.62, NNZs: 843, Bias: 255901093.566859, T: 1390571, Avg. loss: 257827564853031305474556091695104.000000\n",
      "Total training time: 2.99 seconds.\n",
      "-- Epoch 414\n",
      "Norm: 5747276090979.59, NNZs: 843, Bias: -35283687.308331, T: 1393938, Avg. loss: 257517801156301963281591582064640.000000\n",
      "Total training time: 2.99 seconds.\n",
      "-- Epoch 415\n",
      "Norm: 5517763736435.09, NNZs: 843, Bias: 255604012.836138, T: 1397305, Avg. loss: 257210914320873996434038350413824.000000\n",
      "Total training time: 3.00 seconds.\n",
      "-- Epoch 416\n",
      "Norm: 5286177068463.17, NNZs: 843, Bias: -35224007.759431, T: 1400672, Avg. loss: 256902695951418243487542053699584.000000\n",
      "Total training time: 3.01 seconds.\n",
      "-- Epoch 417\n",
      "Norm: 5487931765386.17, NNZs: 843, Bias: 255297551.976997, T: 1404039, Avg. loss: 256594380602251520889510770507776.000000\n",
      "Total training time: 3.02 seconds.\n",
      "-- Epoch 418\n",
      "Norm: 5903609852701.87, NNZs: 843, Bias: 545453236.670713, T: 1407406, Avg. loss: 256289180448197570980114121883648.000000\n",
      "Total training time: 3.02 seconds.\n",
      "-- Epoch 419\n",
      "Norm: 5774216935005.93, NNZs: 843, Bias: -15352104.914583, T: 1410773, Avg. loss: 255986707766554619189941705375744.000000\n",
      "Total training time: 3.03 seconds.\n",
      "-- Epoch 420\n",
      "Norm: 6585331706411.24, NNZs: 843, Bias: -305343823.512368, T: 1414140, Avg. loss: 255675475421668470383138505228288.000000\n",
      "Total training time: 3.04 seconds.\n",
      "-- Epoch 421\n",
      "Norm: 6214854941809.09, NNZs: 843, Bias: 564274203.549543, T: 1417507, Avg. loss: 255368840395184291554155363827712.000000\n",
      "Total training time: 3.04 seconds.\n",
      "-- Epoch 422\n",
      "Norm: 6345721418974.87, NNZs: 843, Bias: 274277022.432932, T: 1420874, Avg. loss: 255066421598615557350550870687744.000000\n",
      "Total training time: 3.05 seconds.\n",
      "-- Epoch 423\n",
      "Norm: 5254556929422.89, NNZs: 843, Bias: -15369976.871624, T: 1424241, Avg. loss: 254765959837496140712611470639104.000000\n",
      "Total training time: 3.06 seconds.\n",
      "-- Epoch 424\n",
      "Norm: 5800459182007.56, NNZs: 843, Bias: -304674751.097865, T: 1427608, Avg. loss: 254465590106892299534870119448576.000000\n",
      "Total training time: 3.07 seconds.\n",
      "-- Epoch 425\n",
      "Norm: 5884536436231.59, NNZs: 843, Bias: -15365486.327052, T: 1430975, Avg. loss: 254166208747574858659007159074816.000000\n",
      "Total training time: 3.07 seconds.\n",
      "-- Epoch 426\n",
      "Norm: 6899302020945.15, NNZs: 843, Bias: 273604560.167786, T: 1434342, Avg. loss: 253864720050582062039423493603328.000000\n",
      "Total training time: 3.08 seconds.\n",
      "-- Epoch 427\n",
      "Norm: 6081008234162.67, NNZs: 843, Bias: -15358566.090244, T: 1437709, Avg. loss: 253566219829725220365228987908096.000000\n",
      "Total training time: 3.09 seconds.\n",
      "-- Epoch 428\n",
      "Norm: 6269742663784.11, NNZs: 843, Bias: 273260210.967416, T: 1441076, Avg. loss: 253274455791660809080471103209472.000000\n",
      "Total training time: 3.10 seconds.\n",
      "-- Epoch 429\n",
      "Norm: 6295956193393.09, NNZs: 843, Bias: -15377917.524840, T: 1444443, Avg. loss: 252979827051935149313966628405248.000000\n",
      "Total training time: 3.10 seconds.\n",
      "-- Epoch 430\n",
      "Norm: 6356767457723.55, NNZs: 843, Bias: -195524674.571704, T: 1447810, Avg. loss: 252693362102385305927556377083904.000000\n",
      "Total training time: 3.11 seconds.\n",
      "-- Epoch 431\n",
      "Norm: 6229899594265.98, NNZs: 843, Bias: 14406747.616719, T: 1451177, Avg. loss: 252405189648586503151387548844032.000000\n",
      "Total training time: 3.12 seconds.\n",
      "-- Epoch 432\n",
      "Norm: 6038188004330.50, NNZs: 843, Bias: 177454466.063721, T: 1454544, Avg. loss: 252113029386659603947210607165440.000000\n",
      "Total training time: 3.12 seconds.\n",
      "-- Epoch 433\n",
      "Norm: 6266678652124.08, NNZs: 843, Bias: 203752920.462215, T: 1457911, Avg. loss: 251824809598043134474616695685120.000000\n",
      "Total training time: 3.13 seconds.\n",
      "-- Epoch 434\n",
      "Norm: 6441387348873.08, NNZs: 843, Bias: 489217602.339552, T: 1461278, Avg. loss: 251535578981804729398477864304640.000000\n",
      "Total training time: 3.14 seconds.\n",
      "-- Epoch 435\n",
      "Norm: 6172813594142.44, NNZs: 843, Bias: 201472744.031276, T: 1464645, Avg. loss: 251246584781125917512028334850048.000000\n",
      "Total training time: 3.15 seconds.\n",
      "-- Epoch 436\n",
      "Norm: 6419347579492.82, NNZs: 843, Bias: 488653183.310794, T: 1468012, Avg. loss: 250953835935680620617727567986688.000000\n",
      "Total training time: 3.15 seconds.\n",
      "-- Epoch 437\n",
      "Norm: 6422351563409.33, NNZs: 843, Bias: -373020903.366679, T: 1471379, Avg. loss: 250670507287519568570843758854144.000000\n",
      "Total training time: 3.16 seconds.\n",
      "-- Epoch 438\n",
      "Norm: 6304178226109.74, NNZs: 843, Bias: -85858455.890289, T: 1474746, Avg. loss: 250387192291303921923332247650304.000000\n",
      "Total training time: 3.17 seconds.\n",
      "-- Epoch 439\n",
      "Norm: 7074989008103.49, NNZs: 843, Bias: -946208325.392396, T: 1478113, Avg. loss: 250097363137720586212369787518976.000000\n",
      "Total training time: 3.17 seconds.\n",
      "-- Epoch 440\n",
      "Norm: 5852872331977.02, NNZs: 843, Bias: -85788458.717190, T: 1481480, Avg. loss: 249811872500088913694781243129856.000000\n",
      "Total training time: 3.18 seconds.\n",
      "-- Epoch 441\n",
      "Norm: 6663885805653.65, NNZs: 843, Bias: 200740218.608317, T: 1484847, Avg. loss: 249523276894151108382782543364096.000000\n",
      "Total training time: 3.19 seconds.\n",
      "-- Epoch 442\n",
      "Norm: 6665416503858.22, NNZs: 843, Bias: 467328732.052394, T: 1488214, Avg. loss: 249243372842205869539280268820480.000000\n",
      "Total training time: 3.20 seconds.\n",
      "-- Epoch 443\n",
      "Norm: 6153356835623.75, NNZs: 843, Bias: -150835990.813380, T: 1491581, Avg. loss: 248962731416507572025671060291584.000000\n",
      "Total training time: 3.20 seconds.\n",
      "-- Epoch 444\n",
      "Norm: 7216529695813.64, NNZs: 843, Bias: 135224972.648027, T: 1494948, Avg. loss: 248681151938214741424550178717696.000000\n",
      "Total training time: 3.21 seconds.\n",
      "-- Epoch 445\n",
      "Norm: 6566571430453.97, NNZs: 843, Bias: -150673350.506112, T: 1498315, Avg. loss: 248396442965640657675699714260992.000000\n",
      "Total training time: 3.22 seconds.\n",
      "-- Epoch 446\n",
      "Norm: 6600059578707.94, NNZs: 843, Bias: 135055742.433935, T: 1501682, Avg. loss: 248116332101242328587293358555136.000000\n",
      "Total training time: 3.22 seconds.\n",
      "-- Epoch 447\n",
      "Norm: 7038795986587.19, NNZs: 843, Bias: -150532657.939041, T: 1505049, Avg. loss: 247839431702248284752256696320000.000000\n",
      "Total training time: 3.23 seconds.\n",
      "-- Epoch 448\n",
      "Norm: 6966122807927.59, NNZs: 843, Bias: 134874071.621411, T: 1508416, Avg. loss: 247560558111548457623808871759872.000000\n",
      "Total training time: 3.24 seconds.\n",
      "-- Epoch 449\n",
      "Norm: 7149061322849.27, NNZs: 843, Bias: -541000650.884633, T: 1511783, Avg. loss: 247284937476036818336246969401344.000000\n",
      "Total training time: 3.25 seconds.\n",
      "-- Epoch 450\n",
      "Norm: 7222439042679.01, NNZs: 843, Bias: -216908670.706626, T: 1515150, Avg. loss: 247011314917258048974522758987776.000000\n",
      "Total training time: 3.25 seconds.\n",
      "-- Epoch 451\n",
      "Norm: 6949551105428.03, NNZs: 843, Bias: 68064678.045280, T: 1518517, Avg. loss: 246736034857895717762013664903168.000000\n",
      "Total training time: 3.26 seconds.\n",
      "-- Epoch 452\n",
      "Norm: 6836505550189.44, NNZs: 843, Bias: -305667843.786914, T: 1521884, Avg. loss: 246463163997161295633220873748480.000000\n",
      "Total training time: 3.27 seconds.\n",
      "-- Epoch 453\n",
      "Norm: 6577967189309.00, NNZs: 843, Bias: -136253270.038121, T: 1525251, Avg. loss: 246196982607363496805344238108672.000000\n",
      "Total training time: 3.28 seconds.\n",
      "-- Epoch 454\n",
      "Norm: 6245802166871.58, NNZs: 843, Bias: 148212668.737542, T: 1528618, Avg. loss: 245921993269805416567693893435392.000000\n",
      "Total training time: 3.28 seconds.\n",
      "-- Epoch 455\n",
      "Norm: 6478435005725.18, NNZs: 843, Bias: -136120583.668076, T: 1531985, Avg. loss: 245648405500185062507695654830080.000000\n",
      "Total training time: 3.29 seconds.\n",
      "-- Epoch 456\n",
      "Norm: 6045032546112.11, NNZs: 843, Bias: -37658466.137875, T: 1535352, Avg. loss: 245381367294315086495508942815232.000000\n",
      "Total training time: 3.30 seconds.\n",
      "-- Epoch 457\n",
      "Norm: 6446576818324.88, NNZs: 843, Bias: -321569947.712707, T: 1538719, Avg. loss: 245112204388548315936244740653056.000000\n",
      "Total training time: 3.30 seconds.\n",
      "-- Epoch 458\n",
      "Norm: 6419495787226.97, NNZs: 843, Bias: 22625214.149490, T: 1542086, Avg. loss: 244847546780580795020009237643264.000000\n",
      "Total training time: 3.31 seconds.\n",
      "-- Epoch 459\n",
      "Norm: 6258940545741.63, NNZs: 843, Bias: 306203667.514812, T: 1545453, Avg. loss: 244581750963972693396969752625152.000000\n",
      "Total training time: 3.32 seconds.\n",
      "-- Epoch 460\n",
      "Norm: 6326031998163.76, NNZs: 843, Bias: 51566518.292870, T: 1548820, Avg. loss: 244317525341717054043466862952448.000000\n",
      "Total training time: 3.33 seconds.\n",
      "-- Epoch 461\n",
      "Norm: 6501375257134.16, NNZs: 843, Bias: 334828215.323819, T: 1552187, Avg. loss: 244058736909468240656406646095872.000000\n",
      "Total training time: 3.33 seconds.\n",
      "-- Epoch 462\n",
      "Norm: 7066110325953.19, NNZs: 843, Bias: 51475655.925285, T: 1555554, Avg. loss: 243789615639608792057923741155328.000000\n",
      "Total training time: 3.34 seconds.\n",
      "-- Epoch 463\n",
      "Norm: 6415562357923.61, NNZs: 843, Bias: -231569481.221369, T: 1558921, Avg. loss: 243529839303961947172527912517632.000000\n",
      "Total training time: 3.35 seconds.\n",
      "-- Epoch 464\n",
      "Norm: 6676474274374.08, NNZs: 843, Bias: -514323027.003232, T: 1562288, Avg. loss: 243266890245454567779290004324352.000000\n",
      "Total training time: 3.35 seconds.\n",
      "-- Epoch 465\n",
      "Norm: 6374063437472.46, NNZs: 843, Bias: -231362694.830518, T: 1565655, Avg. loss: 243002920077280918623470013644800.000000\n",
      "Total training time: 3.36 seconds.\n",
      "-- Epoch 466\n",
      "Norm: 6649343630839.24, NNZs: 843, Bias: 213187839.026770, T: 1569022, Avg. loss: 242735721708580830133667072311296.000000\n",
      "Total training time: 3.37 seconds.\n",
      "-- Epoch 467\n",
      "Norm: 6804963946982.24, NNZs: 843, Bias: 399126735.126416, T: 1572389, Avg. loss: 242476572220880067946252343443456.000000\n",
      "Total training time: 3.38 seconds.\n",
      "-- Epoch 468\n",
      "Norm: 6821391463298.09, NNZs: 843, Bias: 116660735.278528, T: 1575756, Avg. loss: 242224286194541199891416571969536.000000\n",
      "Total training time: 3.38 seconds.\n",
      "-- Epoch 469\n",
      "Norm: 5833373360286.78, NNZs: 843, Bias: -165506630.632266, T: 1579123, Avg. loss: 241968729654359308331710226104320.000000\n",
      "Total training time: 3.39 seconds.\n",
      "-- Epoch 470\n",
      "Norm: 7504058221633.52, NNZs: 843, Bias: 680404836.945911, T: 1582490, Avg. loss: 241712235354636398864335638626304.000000\n",
      "Total training time: 3.40 seconds.\n",
      "-- Epoch 471\n",
      "Norm: 6116935126985.43, NNZs: 843, Bias: 297244309.859401, T: 1585857, Avg. loss: 241459659497000806764917962047488.000000\n",
      "Total training time: 3.40 seconds.\n",
      "-- Epoch 472\n",
      "Norm: 6485968806904.51, NNZs: 843, Bias: 15420194.583116, T: 1589224, Avg. loss: 241202923986503395105714663849984.000000\n",
      "Total training time: 3.41 seconds.\n",
      "-- Epoch 473\n",
      "Norm: 6830422376242.15, NNZs: 843, Bias: 296881506.247077, T: 1592591, Avg. loss: 240948028211999073315102729961472.000000\n",
      "Total training time: 3.42 seconds.\n",
      "-- Epoch 474\n",
      "Norm: 6809923954036.77, NNZs: 843, Bias: 15364284.597445, T: 1595958, Avg. loss: 240697466470210227393690127564800.000000\n",
      "Total training time: 3.43 seconds.\n",
      "-- Epoch 475\n",
      "Norm: 7452510967830.60, NNZs: 843, Bias: -265857299.361818, T: 1599325, Avg. loss: 240445223431575362427476741455872.000000\n",
      "Total training time: 3.43 seconds.\n",
      "-- Epoch 476\n",
      "Norm: 6905143386284.49, NNZs: 843, Bias: -254176002.649267, T: 1602692, Avg. loss: 240192683952613986381457973575680.000000\n",
      "Total training time: 3.44 seconds.\n",
      "-- Epoch 477\n",
      "Norm: 6254397808885.70, NNZs: 843, Bias: 89430066.322437, T: 1606059, Avg. loss: 239941498553454393916677257428992.000000\n",
      "Total training time: 3.45 seconds.\n",
      "-- Epoch 478\n",
      "Norm: 7318742347622.30, NNZs: 843, Bias: -191395901.981481, T: 1609426, Avg. loss: 239687611428954045954528294993920.000000\n",
      "Total training time: 3.46 seconds.\n",
      "-- Epoch 479\n",
      "Norm: 6353584332923.43, NNZs: 843, Bias: 89289431.276364, T: 1612793, Avg. loss: 239436554160102837274895676604416.000000\n",
      "Total training time: 3.46 seconds.\n",
      "-- Epoch 480\n",
      "Norm: 6898803375805.28, NNZs: 843, Bias: -191235533.427475, T: 1616160, Avg. loss: 239186634042159975369624493490176.000000\n",
      "Total training time: 3.47 seconds.\n",
      "-- Epoch 481\n",
      "Norm: 7441923829876.19, NNZs: 843, Bias: 89162635.998055, T: 1619527, Avg. loss: 238935347223909098956483001843712.000000\n",
      "Total training time: 3.48 seconds.\n",
      "-- Epoch 482\n",
      "Norm: 7282970370451.17, NNZs: 843, Bias: -505806047.089796, T: 1622894, Avg. loss: 238689689721321847977662151131136.000000\n",
      "Total training time: 3.48 seconds.\n",
      "-- Epoch 483\n",
      "Norm: 6853795563719.63, NNZs: 843, Bias: -225532874.840530, T: 1626261, Avg. loss: 238442602072040047873331417841664.000000\n",
      "Total training time: 3.49 seconds.\n",
      "-- Epoch 484\n",
      "Norm: 6782344609762.77, NNZs: 843, Bias: 148652721.683660, T: 1629628, Avg. loss: 238199668788202143365306962149376.000000\n",
      "Total training time: 3.50 seconds.\n",
      "-- Epoch 485\n",
      "Norm: 6998041321530.56, NNZs: 843, Bias: -131180130.509861, T: 1632995, Avg. loss: 237953897508467564129067269619712.000000\n",
      "Total training time: 3.51 seconds.\n",
      "-- Epoch 486\n",
      "Norm: 7151603873450.68, NNZs: 843, Bias: -410718260.143000, T: 1636362, Avg. loss: 237707631676288780828739528294400.000000\n",
      "Total training time: 3.51 seconds.\n",
      "-- Epoch 487\n",
      "Norm: 7094332075217.73, NNZs: 843, Bias: 427838001.073600, T: 1639729, Avg. loss: 237458082587164006242208446414848.000000\n",
      "Total training time: 3.52 seconds.\n",
      "-- Epoch 488\n",
      "Norm: 6774216750411.25, NNZs: 843, Bias: 148309474.775439, T: 1643096, Avg. loss: 237212719385347134545261095813120.000000\n",
      "Total training time: 3.53 seconds.\n",
      "-- Epoch 489\n",
      "Norm: 7802275147003.44, NNZs: 843, Bias: -130938397.931374, T: 1646463, Avg. loss: 236977350837407619094493792305152.000000\n",
      "Total training time: 3.53 seconds.\n",
      "-- Epoch 490\n",
      "Norm: 6755050120328.09, NNZs: 843, Bias: 148153437.788831, T: 1649830, Avg. loss: 236730522336265659757728909754368.000000\n",
      "Total training time: 3.54 seconds.\n",
      "-- Epoch 491\n",
      "Norm: 6668366372320.92, NNZs: 843, Bias: -130804580.753798, T: 1653197, Avg. loss: 236487680233846150151718236585984.000000\n",
      "Total training time: 3.55 seconds.\n",
      "-- Epoch 492\n",
      "Norm: 6623463104326.44, NNZs: 843, Bias: 147993811.488840, T: 1656564, Avg. loss: 236245455444889934459931891597312.000000\n",
      "Total training time: 3.56 seconds.\n",
      "-- Epoch 493\n",
      "Norm: 6917681882978.50, NNZs: 843, Bias: 426506249.020938, T: 1659931, Avg. loss: 236009553296304656850960303259648.000000\n",
      "Total training time: 3.56 seconds.\n",
      "-- Epoch 494\n",
      "Norm: 7290983537513.53, NNZs: 843, Bias: 147828194.015678, T: 1663298, Avg. loss: 235767676796254060418708153565184.000000\n",
      "Total training time: 3.57 seconds.\n",
      "-- Epoch 495\n",
      "Norm: 7048047232296.99, NNZs: 843, Bias: -130561923.697529, T: 1666665, Avg. loss: 235532020530803490962348393365504.000000\n",
      "Total training time: 3.58 seconds.\n",
      "-- Epoch 496\n",
      "Norm: 6813691040953.09, NNZs: 843, Bias: 147677872.013341, T: 1670032, Avg. loss: 235291212019014586660301163200512.000000\n",
      "Total training time: 3.58 seconds.\n",
      "-- Epoch 497\n",
      "Norm: 7004430724910.20, NNZs: 843, Bias: -130434199.332677, T: 1673399, Avg. loss: 235059189460506964159380001390592.000000\n",
      "Total training time: 3.59 seconds.\n",
      "-- Epoch 498\n",
      "Norm: 8194618685860.04, NNZs: 843, Bias: 147523971.191648, T: 1676766, Avg. loss: 234825735208262547953398224781312.000000\n",
      "Total training time: 3.60 seconds.\n",
      "-- Epoch 499\n",
      "Norm: 7236586150577.24, NNZs: 843, Bias: -130315818.787499, T: 1680133, Avg. loss: 234596229036551482163352748163072.000000\n",
      "Total training time: 3.61 seconds.\n",
      "-- Epoch 500\n",
      "Norm: 7613081871754.41, NNZs: 843, Bias: -407867979.750464, T: 1683500, Avg. loss: 234365466894457753131132697706496.000000\n",
      "Total training time: 3.61 seconds.\n",
      "-- Epoch 501\n",
      "Norm: 7942060671325.90, NNZs: 843, Bias: 146963720.448163, T: 1686867, Avg. loss: 234128913399951232218133524643840.000000\n",
      "Total training time: 3.62 seconds.\n",
      "-- Epoch 502\n",
      "Norm: 7926762611548.41, NNZs: 843, Bias: 97968022.485771, T: 1690234, Avg. loss: 233894367465918781387506683215872.000000\n",
      "Total training time: 3.63 seconds.\n",
      "-- Epoch 503\n",
      "Norm: 7763866657790.04, NNZs: 843, Bias: 375117931.133475, T: 1693601, Avg. loss: 233659295271262761463316161232896.000000\n",
      "Total training time: 3.63 seconds.\n",
      "-- Epoch 504\n",
      "Norm: 7434670107762.33, NNZs: 843, Bias: -104510930.864218, T: 1696968, Avg. loss: 233422957069922584944260084137984.000000\n",
      "Total training time: 3.64 seconds.\n",
      "-- Epoch 505\n",
      "Norm: 7544176289986.32, NNZs: 843, Bias: -381393313.039783, T: 1700335, Avg. loss: 233189556288752181136176801906688.000000\n",
      "Total training time: 3.65 seconds.\n",
      "-- Epoch 506\n",
      "Norm: 6749767834075.03, NNZs: 843, Bias: -104423821.885729, T: 1703702, Avg. loss: 232959186661149409568552411725824.000000\n",
      "Total training time: 3.66 seconds.\n",
      "-- Epoch 507\n",
      "Norm: 7139543643980.75, NNZs: 843, Bias: 573815228.257931, T: 1707069, Avg. loss: 232729232832985382175505114988544.000000\n",
      "Total training time: 3.66 seconds.\n",
      "-- Epoch 508\n",
      "Norm: 8152638310167.85, NNZs: 843, Bias: 850048958.144818, T: 1710436, Avg. loss: 232500279036918018293368881152000.000000\n",
      "Total training time: 3.67 seconds.\n",
      "-- Epoch 509\n",
      "Norm: 7565899945747.92, NNZs: 843, Bias: -401829126.204424, T: 1713803, Avg. loss: 232271571847574427266616811061248.000000\n",
      "Total training time: 3.68 seconds.\n",
      "-- Epoch 510\n",
      "Norm: 6849396582881.00, NNZs: 843, Bias: 71156638.056212, T: 1717170, Avg. loss: 232044879189690760820281242025984.000000\n",
      "Total training time: 3.69 seconds.\n",
      "-- Epoch 511\n",
      "Norm: 7261873513761.99, NNZs: 843, Bias: 347210412.860716, T: 1720537, Avg. loss: 231817368125213025786367009882112.000000\n",
      "Total training time: 3.69 seconds.\n",
      "-- Epoch 512\n",
      "Norm: 7048600580834.05, NNZs: 843, Bias: 71039602.907257, T: 1723904, Avg. loss: 231591773127666399684513736163328.000000\n",
      "Total training time: 3.70 seconds.\n",
      "-- Epoch 513\n",
      "Norm: 7508993048908.66, NNZs: 843, Bias: -204849275.071657, T: 1727271, Avg. loss: 231367344935134106058090994466816.000000\n",
      "Total training time: 3.71 seconds.\n",
      "-- Epoch 514\n",
      "Norm: 7072329443309.79, NNZs: 843, Bias: 70936341.680483, T: 1730638, Avg. loss: 231144677607854395396990131240960.000000\n",
      "Total training time: 3.71 seconds.\n",
      "-- Epoch 515\n",
      "Norm: 7467613695472.67, NNZs: 843, Bias: -204684984.638912, T: 1734005, Avg. loss: 230920921279765673226845189832704.000000\n",
      "Total training time: 3.72 seconds.\n",
      "-- Epoch 516\n",
      "Norm: 7690168536965.37, NNZs: 843, Bias: 621720835.128365, T: 1737372, Avg. loss: 230694732930103131123195901378560.000000\n",
      "Total training time: 3.73 seconds.\n",
      "-- Epoch 517\n",
      "Norm: 7482549363375.48, NNZs: 843, Bias: -11864576.482443, T: 1740739, Avg. loss: 230471406267716109795886647738368.000000\n",
      "Total training time: 3.74 seconds.\n",
      "-- Epoch 518\n",
      "Norm: 7454568303086.57, NNZs: 843, Bias: 263307422.418769, T: 1744106, Avg. loss: 230248553943982385451624392818688.000000\n",
      "Total training time: 3.74 seconds.\n",
      "-- Epoch 519\n",
      "Norm: 7063740534705.67, NNZs: 843, Bias: -11861929.713850, T: 1747473, Avg. loss: 230027730810632818946949131534336.000000\n",
      "Total training time: 3.75 seconds.\n",
      "-- Epoch 520\n",
      "Norm: 7322467568278.16, NNZs: 843, Bias: -286763131.576067, T: 1750840, Avg. loss: 229801748273394386086972822126592.000000\n",
      "Total training time: 3.76 seconds.\n",
      "-- Epoch 521\n",
      "Norm: 7954667974575.24, NNZs: 843, Bias: -345292119.674986, T: 1754207, Avg. loss: 229582342694881327644413328883712.000000\n",
      "Total training time: 3.76 seconds.\n",
      "-- Epoch 522\n",
      "Norm: 7724633835789.63, NNZs: 843, Bias: 232326807.151157, T: 1757574, Avg. loss: 229364392036986174210154149445632.000000\n",
      "Total training time: 3.77 seconds.\n",
      "-- Epoch 523\n",
      "Norm: 7524638974365.17, NNZs: 843, Bias: -485207140.861267, T: 1760941, Avg. loss: 229142569831520215149444054646784.000000\n",
      "Total training time: 3.78 seconds.\n",
      "-- Epoch 524\n",
      "Norm: 7476620220270.97, NNZs: 843, Bias: -210596345.140934, T: 1764308, Avg. loss: 228922756721425479448346383876096.000000\n",
      "Total training time: 3.79 seconds.\n",
      "-- Epoch 525\n",
      "Norm: 7692286631037.79, NNZs: 843, Bias: 63753303.513463, T: 1767675, Avg. loss: 228705770641968249841075663929344.000000\n",
      "Total training time: 3.79 seconds.\n",
      "-- Epoch 526\n",
      "Norm: 7596114264731.97, NNZs: 843, Bias: -210400885.359110, T: 1771042, Avg. loss: 228488550519019292317540434313216.000000\n",
      "Total training time: 3.80 seconds.\n",
      "-- Epoch 527\n",
      "Norm: 7409567018571.41, NNZs: 843, Bias: 63679371.325280, T: 1774409, Avg. loss: 228272289257172030485114306691072.000000\n",
      "Total training time: 3.81 seconds.\n",
      "-- Epoch 528\n",
      "Norm: 7443327932975.37, NNZs: 843, Bias: -210228133.260409, T: 1777776, Avg. loss: 228056007331706296368146759548928.000000\n",
      "Total training time: 3.82 seconds.\n",
      "-- Epoch 529\n",
      "Norm: 7263173723776.92, NNZs: 843, Bias: 132839302.586174, T: 1781143, Avg. loss: 227842969081831793905360011526144.000000\n",
      "Total training time: 3.82 seconds.\n",
      "-- Epoch 530\n",
      "Norm: 7715332341565.00, NNZs: 843, Bias: -140829438.138635, T: 1784510, Avg. loss: 227629917324635692646573551910912.000000\n",
      "Total training time: 3.83 seconds.\n",
      "-- Epoch 531\n",
      "Norm: 7609659702484.29, NNZs: 843, Bias: -164625259.796478, T: 1787877, Avg. loss: 227417662704751329992476622585856.000000\n",
      "Total training time: 3.84 seconds.\n",
      "-- Epoch 532\n",
      "Norm: 7835582674531.76, NNZs: 843, Bias: 371001064.882143, T: 1791244, Avg. loss: 227202763876711591382154320281600.000000\n",
      "Total training time: 3.84 seconds.\n",
      "-- Epoch 533\n",
      "Norm: 7571340773135.70, NNZs: 843, Bias: 640805827.015645, T: 1794611, Avg. loss: 226988543016540060271691067031552.000000\n",
      "Total training time: 3.85 seconds.\n",
      "-- Epoch 534\n",
      "Norm: 7027497177014.37, NNZs: 843, Bias: -84382012.334507, T: 1797978, Avg. loss: 226775982065996382723594247995392.000000\n",
      "Total training time: 3.86 seconds.\n",
      "-- Epoch 535\n",
      "Norm: 7108036769292.37, NNZs: 843, Bias: 188625254.479245, T: 1801345, Avg. loss: 226564039958829216817734525911040.000000\n",
      "Total training time: 3.87 seconds.\n",
      "-- Epoch 536\n",
      "Norm: 7946029281228.00, NNZs: 843, Bias: -84306949.371946, T: 1804712, Avg. loss: 226347861013427149279293696114688.000000\n",
      "Total training time: 3.87 seconds.\n",
      "-- Epoch 537\n",
      "Norm: 7251626966442.95, NNZs: 843, Bias: 733855327.429195, T: 1808079, Avg. loss: 226137445007168485112482491269120.000000\n",
      "Total training time: 3.88 seconds.\n",
      "-- Epoch 538\n",
      "Norm: 7044320102088.53, NNZs: 843, Bias: -84233066.213396, T: 1811446, Avg. loss: 225928024877021661895508434616320.000000\n",
      "Total training time: 3.89 seconds.\n",
      "-- Epoch 539\n",
      "Norm: 7644139583748.04, NNZs: 843, Bias: 188258343.778376, T: 1814813, Avg. loss: 225718839471344445120836576215040.000000\n",
      "Total training time: 3.90 seconds.\n",
      "-- Epoch 540\n",
      "Norm: 6770681892133.38, NNZs: 843, Bias: 321097260.589021, T: 1818180, Avg. loss: 225510834634350968740082129829888.000000\n",
      "Total training time: 3.90 seconds.\n",
      "-- Epoch 541\n",
      "Norm: 7498625855108.14, NNZs: 843, Bias: 48726521.061177, T: 1821547, Avg. loss: 225301890887150696797372780904448.000000\n",
      "Total training time: 3.91 seconds.\n",
      "-- Epoch 542\n",
      "Norm: 6507981695861.83, NNZs: 843, Bias: -223396983.769448, T: 1824914, Avg. loss: 225091576630168506283277257015296.000000\n",
      "Total training time: 3.92 seconds.\n",
      "-- Epoch 543\n",
      "Norm: 6881329706928.07, NNZs: 843, Bias: -495249827.273869, T: 1828281, Avg. loss: 224888860103421074645206777724928.000000\n",
      "Total training time: 3.92 seconds.\n",
      "-- Epoch 544\n",
      "Norm: 6472051482478.89, NNZs: 843, Bias: -223214799.212824, T: 1831648, Avg. loss: 224685095486789871348159959007232.000000\n",
      "Total training time: 3.93 seconds.\n",
      "-- Epoch 545\n",
      "Norm: 6371824906755.25, NNZs: 843, Bias: 48572834.910594, T: 1835015, Avg. loss: 224480718743615724617174670639104.000000\n",
      "Total training time: 3.94 seconds.\n",
      "-- Epoch 546\n",
      "Norm: 6423429981508.74, NNZs: 843, Bias: -196090622.054077, T: 1838382, Avg. loss: 224275523066601048887978461495296.000000\n",
      "Total training time: 3.95 seconds.\n",
      "-- Epoch 547\n",
      "Norm: 6302037659184.59, NNZs: 843, Bias: 75431609.987878, T: 1841749, Avg. loss: 224070384838771289436708684169216.000000\n",
      "Total training time: 3.95 seconds.\n",
      "-- Epoch 548\n",
      "Norm: 5981866511273.79, NNZs: 843, Bias: -195944602.769576, T: 1845116, Avg. loss: 223866837172096173856606931386368.000000\n",
      "Total training time: 3.96 seconds.\n",
      "-- Epoch 549\n",
      "Norm: 6016635432087.38, NNZs: 843, Bias: 75342670.812202, T: 1848483, Avg. loss: 223660503684326126249836946653184.000000\n",
      "Total training time: 3.97 seconds.\n",
      "-- Epoch 550\n",
      "Norm: 6393428723064.70, NNZs: 843, Bias: -195790793.880243, T: 1851850, Avg. loss: 223457318036180301845513096921088.000000\n",
      "Total training time: 3.97 seconds.\n",
      "-- Epoch 551\n",
      "Norm: 5812038803482.42, NNZs: 843, Bias: 163989991.221900, T: 1855217, Avg. loss: 223252433990580041083073222148096.000000\n",
      "Total training time: 3.98 seconds.\n",
      "-- Epoch 552\n",
      "Norm: 5742874445872.84, NNZs: 843, Bias: -106923172.787554, T: 1858584, Avg. loss: 223052299326309049973033423339520.000000\n",
      "Total training time: 3.99 seconds.\n",
      "-- Epoch 553\n",
      "Norm: 6499466276864.79, NNZs: 843, Bias: -377597130.202078, T: 1861951, Avg. loss: 222850138640716442477996231622656.000000\n",
      "Total training time: 4.00 seconds.\n",
      "-- Epoch 554\n",
      "Norm: 5868634522678.86, NNZs: 843, Bias: -237543625.077994, T: 1865318, Avg. loss: 222649866709202330145778305073152.000000\n",
      "Total training time: 4.00 seconds.\n",
      "-- Epoch 555\n",
      "Norm: 6080036418576.20, NNZs: 843, Bias: -184206420.193585, T: 1868685, Avg. loss: 222451042161450216932914512265216.000000\n",
      "Total training time: 4.01 seconds.\n",
      "-- Epoch 556\n",
      "Norm: 6622198527651.29, NNZs: 843, Bias: -59372489.426456, T: 1872052, Avg. loss: 222250707507196929279406030979072.000000\n",
      "Total training time: 4.02 seconds.\n",
      "-- Epoch 557\n",
      "Norm: 6323380282275.04, NNZs: 843, Bias: -671100650.361312, T: 1875419, Avg. loss: 222052104057262385538289450876928.000000\n",
      "Total training time: 4.03 seconds.\n",
      "-- Epoch 558\n",
      "Norm: 5927781100724.51, NNZs: 843, Bias: 139499168.460610, T: 1878786, Avg. loss: 221853354232407239814808441192448.000000\n",
      "Total training time: 4.03 seconds.\n",
      "-- Epoch 559\n",
      "Norm: 6740392220486.51, NNZs: 843, Bias: -670515001.752839, T: 1882153, Avg. loss: 221657556990988297602410183917568.000000\n",
      "Total training time: 4.04 seconds.\n",
      "-- Epoch 560\n",
      "Norm: 6289244153844.22, NNZs: 843, Bias: -400370039.714982, T: 1885520, Avg. loss: 221458622097474690781190549405696.000000\n",
      "Total training time: 4.05 seconds.\n",
      "-- Epoch 561\n",
      "Norm: 6085473090025.50, NNZs: 843, Bias: -130454551.887210, T: 1888887, Avg. loss: 221262023266778584666973726048256.000000\n",
      "Total training time: 4.05 seconds.\n",
      "-- Epoch 562\n",
      "Norm: 5900396058704.77, NNZs: 843, Bias: -400034815.694886, T: 1892254, Avg. loss: 221064370663366573115819637604352.000000\n",
      "Total training time: 4.06 seconds.\n",
      "-- Epoch 563\n",
      "Norm: 6039068187800.82, NNZs: 843, Bias: -130359347.028185, T: 1895621, Avg. loss: 220868699222904035276016442146816.000000\n",
      "Total training time: 4.07 seconds.\n",
      "-- Epoch 564\n",
      "Norm: 6631721233953.98, NNZs: 843, Bias: 139081380.117359, T: 1898988, Avg. loss: 220667800748446341985797266210816.000000\n",
      "Total training time: 4.08 seconds.\n",
      "-- Epoch 565\n",
      "Norm: 5935780981303.34, NNZs: 843, Bias: -304411428.276794, T: 1902355, Avg. loss: 220473411217650774451761627267072.000000\n",
      "Total training time: 4.08 seconds.\n",
      "-- Epoch 566\n",
      "Norm: 6721627323303.36, NNZs: 843, Bias: -35140103.951905, T: 1905722, Avg. loss: 220285839559615137864944709533696.000000\n",
      "Total training time: 4.09 seconds.\n",
      "-- Epoch 567\n",
      "Norm: 6200508276154.35, NNZs: 843, Bias: -304162114.562468, T: 1909089, Avg. loss: 220096696389211642512874085023744.000000\n",
      "Total training time: 4.10 seconds.\n",
      "-- Epoch 568\n",
      "Norm: 6146874826381.99, NNZs: 843, Bias: -35134853.033770, T: 1912456, Avg. loss: 219904904181037350635949768310784.000000\n",
      "Total training time: 4.10 seconds.\n",
      "-- Epoch 569\n",
      "Norm: 6513890545521.21, NNZs: 843, Bias: 233642593.924565, T: 1915823, Avg. loss: 219714626522255121420429740212224.000000\n",
      "Total training time: 4.11 seconds.\n",
      "-- Epoch 570\n",
      "Norm: 6606422071185.53, NNZs: 843, Bias: -35143950.082841, T: 1919190, Avg. loss: 219522161014496674024891984379904.000000\n",
      "Total training time: 4.12 seconds.\n",
      "-- Epoch 571\n",
      "Norm: 6578304284877.95, NNZs: 843, Bias: -303688291.231275, T: 1922557, Avg. loss: 219332454145121027290108971188224.000000\n",
      "Total training time: 4.13 seconds.\n",
      "-- Epoch 572\n",
      "Norm: 6504402333766.40, NNZs: 843, Bias: -35130434.860944, T: 1925924, Avg. loss: 219142187968482509068158430609408.000000\n",
      "Total training time: 4.13 seconds.\n",
      "-- Epoch 573\n",
      "Norm: 6398368819439.55, NNZs: 843, Bias: -113839125.582202, T: 1929291, Avg. loss: 218950719610238461065457706205184.000000\n",
      "Total training time: 4.14 seconds.\n",
      "-- Epoch 574\n",
      "Norm: 6369193164186.10, NNZs: 843, Bias: 154402725.192644, T: 1932658, Avg. loss: 218763904028360336533664674349056.000000\n",
      "Total training time: 4.15 seconds.\n",
      "-- Epoch 575\n",
      "Norm: 6362261218187.04, NNZs: 843, Bias: -113755206.013981, T: 1936025, Avg. loss: 218574147657781543201194911989760.000000\n",
      "Total training time: 4.15 seconds.\n",
      "-- Epoch 576\n",
      "Norm: 6027987232235.89, NNZs: 843, Bias: 154253902.415961, T: 1939392, Avg. loss: 218387134726893624916488518369280.000000\n",
      "Total training time: 4.16 seconds.\n",
      "-- Epoch 577\n",
      "Norm: 6057207649408.93, NNZs: 843, Bias: -113672065.484169, T: 1942759, Avg. loss: 218198146281367410863468850446336.000000\n",
      "Total training time: 4.17 seconds.\n",
      "-- Epoch 578\n",
      "Norm: 6191795074436.72, NNZs: 843, Bias: 154111375.890336, T: 1946126, Avg. loss: 218007524956816253084369659363328.000000\n",
      "Total training time: 4.18 seconds.\n",
      "-- Epoch 579\n",
      "Norm: 6380707182360.71, NNZs: 843, Bias: 421669161.418168, T: 1949493, Avg. loss: 217820697501568303196400986882048.000000\n",
      "Total training time: 4.18 seconds.\n",
      "-- Epoch 580\n",
      "Norm: 6246433882329.23, NNZs: 843, Bias: -26801760.529964, T: 1952860, Avg. loss: 217636338342424449352243798343680.000000\n",
      "Total training time: 4.19 seconds.\n",
      "-- Epoch 581\n",
      "Norm: 6693059716847.14, NNZs: 843, Bias: -98360101.401545, T: 1956227, Avg. loss: 217446644514586150384622232403968.000000\n",
      "Total training time: 4.20 seconds.\n",
      "-- Epoch 582\n",
      "Norm: 6256191607907.34, NNZs: 843, Bias: 168951407.593237, T: 1959594, Avg. loss: 217261005893788617003569598431232.000000\n",
      "Total training time: 4.21 seconds.\n",
      "-- Epoch 583\n",
      "Norm: 6403974237652.29, NNZs: 843, Bias: -98296767.339846, T: 1962961, Avg. loss: 217074806175525706948568879726592.000000\n",
      "Total training time: 4.21 seconds.\n",
      "-- Epoch 584\n",
      "Norm: 6015664344469.85, NNZs: 843, Bias: 168783076.332014, T: 1966328, Avg. loss: 216887668494272494481769198780416.000000\n",
      "Total training time: 4.22 seconds.\n",
      "-- Epoch 585\n",
      "Norm: 6638473114550.33, NNZs: 843, Bias: 435639112.998179, T: 1969695, Avg. loss: 216699871579709427799601813913600.000000\n",
      "Total training time: 4.23 seconds.\n",
      "-- Epoch 586\n",
      "Norm: 6111188019733.28, NNZs: 843, Bias: -50575006.428963, T: 1973062, Avg. loss: 216515090995015731869787055194112.000000\n",
      "Total training time: 4.23 seconds.\n",
      "-- Epoch 587\n",
      "Norm: 6236396300103.36, NNZs: 843, Bias: -317264485.213612, T: 1976429, Avg. loss: 216329408776720248613089426014208.000000\n",
      "Total training time: 4.24 seconds.\n",
      "-- Epoch 588\n",
      "Norm: 6257116913740.15, NNZs: 843, Bias: -50547710.250235, T: 1979796, Avg. loss: 216143638634859288088818948767744.000000\n",
      "Total training time: 4.25 seconds.\n",
      "-- Epoch 589\n",
      "Norm: 6450012664736.00, NNZs: 843, Bias: 215935653.798418, T: 1983163, Avg. loss: 215961123731372059009015456202752.000000\n",
      "Total training time: 4.26 seconds.\n",
      "-- Epoch 590\n",
      "Norm: 6068514083104.34, NNZs: 843, Bias: -50535579.270127, T: 1986530, Avg. loss: 215779670030422255042360233689088.000000\n",
      "Total training time: 4.26 seconds.\n",
      "-- Epoch 591\n",
      "Norm: 6129812984953.41, NNZs: 843, Bias: 215723971.216510, T: 1989897, Avg. loss: 215596638056845190796022235791360.000000\n",
      "Total training time: 4.27 seconds.\n",
      "-- Epoch 592\n",
      "Norm: 6550829386817.21, NNZs: 843, Bias: -582785594.610428, T: 1993264, Avg. loss: 215415043350679743013222189367296.000000\n",
      "Total training time: 4.28 seconds.\n",
      "-- Epoch 593\n",
      "Norm: 6768386494460.42, NNZs: 843, Bias: -316527187.356988, T: 1996631, Avg. loss: 215235490977644690859909474942976.000000\n",
      "Total training time: 4.28 seconds.\n",
      "-- Epoch 594\n",
      "Norm: 6516570426688.09, NNZs: 843, Bias: -50485514.949478, T: 1999998, Avg. loss: 215054337131450603564320413450240.000000\n",
      "Total training time: 4.29 seconds.\n",
      "-- Epoch 595\n",
      "Norm: 6491894783221.15, NNZs: 843, Bias: -174991107.354184, T: 2003365, Avg. loss: 214875188793100016228118579642368.000000\n",
      "Total training time: 4.30 seconds.\n",
      "-- Epoch 596\n",
      "Norm: 6676276522846.95, NNZs: 843, Bias: 167020847.965894, T: 2006732, Avg. loss: 214697056773708028188228830887936.000000\n",
      "Total training time: 4.31 seconds.\n",
      "-- Epoch 597\n",
      "Norm: 6594282386643.96, NNZs: 843, Bias: -98639631.622287, T: 2010099, Avg. loss: 214518273114314136690254422212608.000000\n",
      "Total training time: 4.31 seconds.\n",
      "-- Epoch 598\n",
      "Norm: 6595693052206.91, NNZs: 843, Bias: -135645657.553089, T: 2013466, Avg. loss: 214339683261707699133983932547072.000000\n",
      "Total training time: 4.32 seconds.\n",
      "-- Epoch 599\n",
      "Norm: 6731426954984.25, NNZs: 843, Bias: -58727243.790817, T: 2016833, Avg. loss: 214160426004305103461354137714688.000000\n",
      "Total training time: 4.33 seconds.\n",
      "-- Epoch 600\n",
      "Norm: 7085599567993.03, NNZs: 843, Bias: -471395146.914141, T: 2020200, Avg. loss: 213983107127759062794315447140352.000000\n",
      "Total training time: 4.33 seconds.\n",
      "-- Epoch 601\n",
      "Norm: 7030941677214.80, NNZs: 843, Bias: 201255390.037591, T: 2023567, Avg. loss: 213804718204937517167498231808000.000000\n",
      "Total training time: 4.34 seconds.\n",
      "-- Epoch 602\n",
      "Norm: 7030804676468.47, NNZs: 843, Bias: -363331276.801334, T: 2026934, Avg. loss: 213626605421275077441009217437696.000000\n",
      "Total training time: 4.35 seconds.\n",
      "-- Epoch 603\n",
      "Norm: 6709767596836.56, NNZs: 843, Bias: -98277703.483254, T: 2030301, Avg. loss: 213446694093819408266430118887424.000000\n",
      "Total training time: 4.36 seconds.\n",
      "-- Epoch 604\n",
      "Norm: 6953042402774.96, NNZs: 843, Bias: 166560934.807783, T: 2033668, Avg. loss: 213269452293151080450274386509824.000000\n",
      "Total training time: 4.36 seconds.\n",
      "-- Epoch 605\n",
      "Norm: 7099106172940.24, NNZs: 843, Bias: -98206648.142182, T: 2037035, Avg. loss: 213094556210507902830458473283584.000000\n",
      "Total training time: 4.37 seconds.\n",
      "-- Epoch 606\n",
      "Norm: 6919088299560.67, NNZs: 843, Bias: 166411416.111364, T: 2040402, Avg. loss: 212919443310883973732868839964672.000000\n",
      "Total training time: 4.38 seconds.\n",
      "-- Epoch 607\n",
      "Norm: 6665670860451.50, NNZs: 843, Bias: -98141005.064167, T: 2043769, Avg. loss: 212743872286889896822028961316864.000000\n",
      "Total training time: 4.38 seconds.\n",
      "-- Epoch 608\n",
      "Norm: 6739362605204.21, NNZs: 843, Bias: -362489184.806146, T: 2047136, Avg. loss: 212570797627298581010355706658816.000000\n",
      "Total training time: 4.39 seconds.\n",
      "-- Epoch 609\n",
      "Norm: 6530850607563.18, NNZs: 843, Bias: -98086745.763545, T: 2050503, Avg. loss: 212397287396890668892844007096320.000000\n",
      "Total training time: 4.40 seconds.\n",
      "-- Epoch 610\n",
      "Norm: 6560904745427.89, NNZs: 843, Bias: 166092322.782265, T: 2053870, Avg. loss: 212222920087222775412078791360512.000000\n",
      "Total training time: 4.41 seconds.\n",
      "-- Epoch 611\n",
      "Norm: 6073732135242.14, NNZs: 843, Bias: -98029738.874321, T: 2057237, Avg. loss: 212049058877641456615987714457600.000000\n",
      "Total training time: 4.41 seconds.\n",
      "-- Epoch 612\n",
      "Norm: 6301873707064.74, NNZs: 843, Bias: 165937572.124275, T: 2060604, Avg. loss: 211874675718567876434529131429888.000000\n",
      "Total training time: 4.42 seconds.\n",
      "-- Epoch 613\n",
      "Norm: 6524435886400.18, NNZs: 843, Bias: -97974818.923621, T: 2063971, Avg. loss: 211701948442746204757676458508288.000000\n",
      "Total training time: 4.43 seconds.\n",
      "-- Epoch 614\n",
      "Norm: 6329477222161.31, NNZs: 843, Bias: -361668775.270650, T: 2067338, Avg. loss: 211526945039716981074417762172928.000000\n",
      "Total training time: 4.44 seconds.\n",
      "-- Epoch 615\n",
      "Norm: 6762541207179.22, NNZs: 843, Bias: 429318809.034461, T: 2070705, Avg. loss: 211350468335703108602022231801856.000000\n",
      "Total training time: 4.44 seconds.\n",
      "-- Epoch 616\n",
      "Norm: 6531149615512.48, NNZs: 843, Bias: -361384464.541744, T: 2074072, Avg. loss: 211174989792984273362558388273152.000000\n",
      "Total training time: 4.45 seconds.\n",
      "-- Epoch 617\n",
      "Norm: 6316065146505.80, NNZs: 843, Bias: -97851507.576671, T: 2077439, Avg. loss: 211005425576349800268664823349248.000000\n",
      "Total training time: 4.46 seconds.\n",
      "-- Epoch 618\n",
      "Norm: 6846129782124.26, NNZs: 843, Bias: 165472098.099596, T: 2080806, Avg. loss: 210834953853064886221312884736000.000000\n",
      "Total training time: 4.46 seconds.\n",
      "-- Epoch 619\n",
      "Norm: 6155173139503.71, NNZs: 843, Bias: -97798191.091764, T: 2084173, Avg. loss: 210663424768209186569807927967744.000000\n",
      "Total training time: 4.47 seconds.\n",
      "-- Epoch 620\n",
      "Norm: 7195413612097.09, NNZs: 843, Bias: 165310183.357296, T: 2087540, Avg. loss: 210496200481176477146152683175936.000000\n",
      "Total training time: 4.48 seconds.\n",
      "-- Epoch 621\n",
      "Norm: 7150532173918.31, NNZs: 843, Bias: 491858867.302781, T: 2090907, Avg. loss: 210324139396876215969262057029632.000000\n",
      "Total training time: 4.49 seconds.\n",
      "-- Epoch 622\n",
      "Norm: 6549523628244.46, NNZs: 843, Bias: -296972447.541290, T: 2094274, Avg. loss: 210157312336533332130719902728192.000000\n",
      "Total training time: 4.49 seconds.\n",
      "-- Epoch 623\n",
      "Norm: 6191918153755.58, NNZs: 843, Bias: -397981303.450026, T: 2097641, Avg. loss: 209988813516491331397904007954432.000000\n",
      "Total training time: 4.50 seconds.\n",
      "-- Epoch 624\n",
      "Norm: 5930556826948.31, NNZs: 843, Bias: -135168057.301801, T: 2101008, Avg. loss: 209821342218524893059346325307392.000000\n",
      "Total training time: 4.51 seconds.\n",
      "-- Epoch 625\n",
      "Norm: 6013586704172.45, NNZs: 843, Bias: 127439656.627131, T: 2104375, Avg. loss: 209651886537446400426084344528896.000000\n",
      "Total training time: 4.51 seconds.\n",
      "-- Epoch 626\n",
      "Norm: 6255250875686.57, NNZs: 843, Bias: -135080103.875313, T: 2107742, Avg. loss: 209485651608061656242785668300800.000000\n",
      "Total training time: 4.52 seconds.\n",
      "-- Epoch 627\n",
      "Norm: 6058257643667.77, NNZs: 843, Bias: -278764392.121245, T: 2111109, Avg. loss: 209319739999277618565832886976512.000000\n",
      "Total training time: 4.53 seconds.\n",
      "-- Epoch 628\n",
      "Norm: 5988226721869.55, NNZs: 843, Bias: -135220201.467884, T: 2114476, Avg. loss: 209152077983011394029355830083584.000000\n",
      "Total training time: 4.54 seconds.\n",
      "-- Epoch 629\n",
      "Norm: 6081068966037.29, NNZs: 843, Bias: -397315528.844979, T: 2117843, Avg. loss: 208985493861334505758659178397696.000000\n",
      "Total training time: 4.54 seconds.\n",
      "-- Epoch 630\n",
      "Norm: 6320186603395.47, NNZs: 843, Bias: 388926577.798811, T: 2121210, Avg. loss: 208818939178790580015979315593216.000000\n",
      "Total training time: 4.55 seconds.\n",
      "-- Epoch 631\n",
      "Norm: 6190183746527.76, NNZs: 843, Bias: 126833450.916222, T: 2124577, Avg. loss: 208654367278045566462847214419968.000000\n",
      "Total training time: 4.56 seconds.\n",
      "-- Epoch 632\n",
      "Norm: 6240707475206.89, NNZs: 843, Bias: -135043592.642015, T: 2127944, Avg. loss: 208488676896971810623870434541568.000000\n",
      "Total training time: 4.56 seconds.\n",
      "-- Epoch 633\n",
      "Norm: 6167736013286.68, NNZs: 843, Bias: 126728843.479877, T: 2131311, Avg. loss: 208324082269540896713961308684288.000000\n",
      "Total training time: 4.57 seconds.\n",
      "-- Epoch 634\n",
      "Norm: 5862085313061.69, NNZs: 843, Bias: -134947517.814507, T: 2134678, Avg. loss: 208161676463057071759639462281216.000000\n",
      "Total training time: 4.58 seconds.\n",
      "-- Epoch 635\n",
      "Norm: 5831228159236.52, NNZs: 843, Bias: 126617463.891685, T: 2138045, Avg. loss: 207996277999412851687441792237568.000000\n",
      "Total training time: 4.59 seconds.\n",
      "-- Epoch 636\n",
      "Norm: 5915863423773.44, NNZs: 843, Bias: -134861242.682326, T: 2141412, Avg. loss: 207833322406131580261511068647424.000000\n",
      "Total training time: 4.59 seconds.\n",
      "-- Epoch 637\n",
      "Norm: 6419652527647.13, NNZs: 843, Bias: -243122190.674468, T: 2144779, Avg. loss: 207670973962555448112525132955648.000000\n",
      "Total training time: 4.60 seconds.\n",
      "-- Epoch 638\n",
      "Norm: 5915867408832.67, NNZs: 843, Bias: 18169946.671825, T: 2148146, Avg. loss: 207512822646221144933619879378944.000000\n",
      "Total training time: 4.61 seconds.\n",
      "-- Epoch 639\n",
      "Norm: 6254417572715.29, NNZs: 843, Bias: -242963097.817039, T: 2151513, Avg. loss: 207350829126140616778931375177728.000000\n",
      "Total training time: 4.62 seconds.\n",
      "-- Epoch 640\n",
      "Norm: 5949020229212.02, NNZs: 843, Bias: 18110152.539260, T: 2154880, Avg. loss: 207188804591970670306307541565440.000000\n",
      "Total training time: 4.62 seconds.\n",
      "-- Epoch 641\n",
      "Norm: 6321056312819.88, NNZs: 843, Bias: 278983643.023270, T: 2158247, Avg. loss: 207030093423663661141728553861120.000000\n",
      "Total training time: 4.63 seconds.\n",
      "-- Epoch 642\n",
      "Norm: 5891471680699.08, NNZs: 843, Bias: 18062846.329703, T: 2161614, Avg. loss: 206870480749999151466486826008576.000000\n",
      "Total training time: 4.64 seconds.\n",
      "-- Epoch 643\n",
      "Norm: 5863720534507.50, NNZs: 843, Bias: -242662239.099486, T: 2164981, Avg. loss: 206709154342255056851983740698624.000000\n",
      "Total training time: 4.64 seconds.\n",
      "-- Epoch 644\n",
      "Norm: 6006261246457.97, NNZs: 843, Bias: 18012234.158785, T: 2168348, Avg. loss: 206548584704887007780605526016000.000000\n",
      "Total training time: 4.65 seconds.\n",
      "-- Epoch 645\n",
      "Norm: 5936429703935.48, NNZs: 843, Bias: -242502253.704217, T: 2171715, Avg. loss: 206391573616381565750516813660160.000000\n",
      "Total training time: 4.66 seconds.\n",
      "-- Epoch 646\n",
      "Norm: 6689820290011.01, NNZs: 843, Bias: 538757463.987872, T: 2175082, Avg. loss: 206230809985799195874462808408064.000000\n",
      "Total training time: 4.67 seconds.\n",
      "-- Epoch 647\n",
      "Norm: 6116670452323.25, NNZs: 843, Bias: 68732814.249518, T: 2178449, Avg. loss: 206072081698096192903059628621824.000000\n",
      "Total training time: 4.67 seconds.\n",
      "-- Epoch 648\n",
      "Norm: 6301290462451.11, NNZs: 843, Bias: 328879920.896701, T: 2181816, Avg. loss: 205913664543030379316225473773568.000000\n",
      "Total training time: 4.68 seconds.\n",
      "-- Epoch 649\n",
      "Norm: 6899973457196.93, NNZs: 843, Bias: 588832053.695732, T: 2185183, Avg. loss: 205755935949066257979581759750144.000000\n",
      "Total training time: 4.69 seconds.\n",
      "-- Epoch 650\n",
      "Norm: 6399129679463.10, NNZs: 843, Bias: 328611595.828404, T: 2188550, Avg. loss: 205597955970591160364239647932416.000000\n",
      "Total training time: 4.69 seconds.\n",
      "-- Epoch 651\n",
      "Norm: 6129011135445.48, NNZs: 843, Bias: 68593185.829460, T: 2191917, Avg. loss: 205443526781936631449803888263168.000000\n",
      "Total training time: 4.70 seconds.\n",
      "-- Epoch 652\n",
      "Norm: 6331611608754.87, NNZs: 843, Bias: 328344923.942722, T: 2195284, Avg. loss: 205283305315837261038483977273344.000000\n",
      "Total training time: 4.71 seconds.\n",
      "-- Epoch 653\n",
      "Norm: 6209017929542.03, NNZs: 843, Bias: 68515347.099642, T: 2198651, Avg. loss: 205126928732407731877413428133888.000000\n",
      "Total training time: 4.72 seconds.\n",
      "-- Epoch 654\n",
      "Norm: 6244951914112.89, NNZs: 843, Bias: 328075767.579605, T: 2202018, Avg. loss: 204969226067757812720492873777152.000000\n",
      "Total training time: 4.72 seconds.\n",
      "-- Epoch 655\n",
      "Norm: 6332884427061.84, NNZs: 843, Bias: 68450122.567506, T: 2205385, Avg. loss: 204813496003461285996899692707840.000000\n",
      "Total training time: 4.73 seconds.\n",
      "-- Epoch 656\n",
      "Norm: 6132964391044.15, NNZs: 843, Bias: 327809582.023638, T: 2208752, Avg. loss: 204657101763948136503538280824832.000000\n",
      "Total training time: 4.74 seconds.\n",
      "-- Epoch 657\n",
      "Norm: 5898606548696.85, NNZs: 843, Bias: 68370769.155030, T: 2212119, Avg. loss: 204500282662574065636572867330048.000000\n",
      "Total training time: 4.74 seconds.\n",
      "-- Epoch 658\n",
      "Norm: 6009802382938.75, NNZs: 843, Bias: 327537660.370972, T: 2215486, Avg. loss: 204348333535734981087426563276800.000000\n",
      "Total training time: 4.75 seconds.\n",
      "-- Epoch 659\n",
      "Norm: 6188712145224.17, NNZs: 843, Bias: -449896308.812143, T: 2218853, Avg. loss: 204191596043113125200426109501440.000000\n",
      "Total training time: 4.76 seconds.\n",
      "-- Epoch 660\n",
      "Norm: 6506877716162.38, NNZs: 843, Bias: 39846749.945125, T: 2222220, Avg. loss: 204035362211398886315871686361088.000000\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 661\n",
      "Norm: 5706902832056.45, NNZs: 843, Bias: -219085282.633001, T: 2225587, Avg. loss: 203879128780019057853099621220352.000000\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 662\n",
      "Norm: 5892651450039.46, NNZs: 843, Bias: 39790880.322707, T: 2228954, Avg. loss: 203728180104009728895132991750144.000000\n",
      "Total training time: 4.78 seconds.\n",
      "-- Epoch 663\n",
      "Norm: 5785930821283.62, NNZs: 843, Bias: -154671392.409047, T: 2232321, Avg. loss: 203577449277396562171705971179520.000000\n",
      "Total training time: 4.79 seconds.\n",
      "-- Epoch 664\n",
      "Norm: 5800464027655.69, NNZs: 843, Bias: 240536966.465538, T: 2235688, Avg. loss: 203427682014836195727468868403200.000000\n",
      "Total training time: 4.80 seconds.\n",
      "-- Epoch 665\n",
      "Norm: 5363222843849.27, NNZs: 843, Bias: 93297158.962877, T: 2239055, Avg. loss: 203274798324643851309693439836160.000000\n",
      "Total training time: 4.80 seconds.\n",
      "-- Epoch 666\n",
      "Norm: 5647562134728.41, NNZs: 843, Bias: 351666762.903230, T: 2242422, Avg. loss: 203124368347963095933643653120000.000000\n",
      "Total training time: 4.81 seconds.\n",
      "-- Epoch 667\n",
      "Norm: 5470562461478.04, NNZs: 843, Bias: 93211958.698306, T: 2245789, Avg. loss: 202972495467350366292241463377920.000000\n",
      "Total training time: 4.82 seconds.\n",
      "-- Epoch 668\n",
      "Norm: 5666035106351.57, NNZs: 843, Bias: 351389359.587191, T: 2249156, Avg. loss: 202820827594677331331451176615936.000000\n",
      "Total training time: 4.82 seconds.\n",
      "-- Epoch 669\n",
      "Norm: 5754370341083.04, NNZs: 843, Bias: -423135672.721153, T: 2252523, Avg. loss: 202667966589677422463637444886528.000000\n",
      "Total training time: 4.83 seconds.\n",
      "-- Epoch 670\n",
      "Norm: 5425712029281.03, NNZs: 843, Bias: -164961886.447435, T: 2255890, Avg. loss: 202515945447932263529737200074752.000000\n",
      "Total training time: 4.84 seconds.\n",
      "-- Epoch 671\n",
      "Norm: 5488534258825.31, NNZs: 843, Bias: 93028476.198290, T: 2259257, Avg. loss: 202362658675956751182644060880896.000000\n",
      "Total training time: 4.85 seconds.\n",
      "-- Epoch 672\n",
      "Norm: 5815131702377.80, NNZs: 843, Bias: -164850545.273356, T: 2262624, Avg. loss: 202210247720129844016888334516224.000000\n",
      "Total training time: 4.85 seconds.\n",
      "-- Epoch 673\n",
      "Norm: 5338616734483.02, NNZs: 843, Bias: 92949755.114658, T: 2265991, Avg. loss: 202060098495195581412375093313536.000000\n",
      "Total training time: 4.86 seconds.\n",
      "-- Epoch 674\n",
      "Norm: 5518243737796.01, NNZs: 843, Bias: 412764009.056056, T: 2269358, Avg. loss: 201908629967874868543176732311552.000000\n",
      "Total training time: 4.87 seconds.\n",
      "-- Epoch 675\n",
      "Norm: 5402720359056.58, NNZs: 843, Bias: 155047014.070796, T: 2272725, Avg. loss: 201757259031053430881462354182144.000000\n",
      "Total training time: 4.87 seconds.\n",
      "-- Epoch 676\n",
      "Norm: 5597189601364.33, NNZs: 843, Bias: 412441857.745378, T: 2276092, Avg. loss: 201608262575983612502688426622976.000000\n",
      "Total training time: 4.88 seconds.\n",
      "-- Epoch 677\n",
      "Norm: 5423977720864.89, NNZs: 843, Bias: 154929410.257012, T: 2279459, Avg. loss: 201461070736873759869076053688320.000000\n",
      "Total training time: 4.89 seconds.\n",
      "-- Epoch 678\n",
      "Norm: 6287714827041.95, NNZs: 843, Bias: -102405634.894415, T: 2282826, Avg. loss: 201310972404849972686268852600832.000000\n",
      "Total training time: 4.90 seconds.\n",
      "-- Epoch 679\n",
      "Norm: 5588735699558.26, NNZs: 843, Bias: -359541856.496257, T: 2286193, Avg. loss: 201162992908978907369299159023616.000000\n",
      "Total training time: 4.90 seconds.\n",
      "-- Epoch 680\n",
      "Norm: 6526505451929.38, NNZs: 843, Bias: -569092033.815129, T: 2289560, Avg. loss: 201016732668845133621983646318592.000000\n",
      "Total training time: 4.91 seconds.\n",
      "-- Epoch 681\n",
      "Norm: 5461060401866.69, NNZs: 843, Bias: 202054889.163843, T: 2292927, Avg. loss: 200867963783346974327622542032896.000000\n",
      "Total training time: 4.92 seconds.\n",
      "-- Epoch 682\n",
      "Norm: 5536571696203.41, NNZs: 843, Bias: -54911668.272981, T: 2296294, Avg. loss: 200721466907556316503949231259648.000000\n",
      "Total training time: 4.92 seconds.\n",
      "-- Epoch 683\n",
      "Norm: 6335091772987.28, NNZs: 843, Bias: 373200119.667948, T: 2299661, Avg. loss: 200577153738931971996781784334336.000000\n",
      "Total training time: 4.93 seconds.\n",
      "-- Epoch 684\n",
      "Norm: 5915438301257.14, NNZs: 843, Bias: 116362596.841983, T: 2303028, Avg. loss: 200430120338120603930772333658112.000000\n",
      "Total training time: 4.94 seconds.\n",
      "-- Epoch 685\n",
      "Norm: 6077430425878.46, NNZs: 843, Bias: -140292849.499590, T: 2306395, Avg. loss: 200284752449072464073970031788032.000000\n",
      "Total training time: 4.95 seconds.\n",
      "-- Epoch 686\n",
      "Norm: 6592882014980.34, NNZs: 843, Bias: 629276870.656621, T: 2309762, Avg. loss: 200140126635491690931288484610048.000000\n",
      "Total training time: 4.95 seconds.\n",
      "-- Epoch 687\n",
      "Norm: 5633619146664.01, NNZs: 843, Bias: -140214697.626382, T: 2313129, Avg. loss: 199996954991688152489742873657344.000000\n",
      "Total training time: 4.96 seconds.\n",
      "-- Epoch 688\n",
      "Norm: 5620476439517.47, NNZs: 843, Bias: 116155248.367147, T: 2316496, Avg. loss: 199851254947983821363622912720896.000000\n",
      "Total training time: 4.97 seconds.\n",
      "-- Epoch 689\n",
      "Norm: 6157912050389.23, NNZs: 843, Bias: 372335879.539173, T: 2319863, Avg. loss: 199703055946793415115011761635328.000000\n",
      "Total training time: 4.97 seconds.\n",
      "-- Epoch 690\n",
      "Norm: 5689049327730.06, NNZs: 843, Bias: 116058343.325765, T: 2323230, Avg. loss: 199557006015157466687761920557056.000000\n",
      "Total training time: 4.98 seconds.\n",
      "-- Epoch 691\n",
      "Norm: 5834115635011.38, NNZs: 843, Bias: -140046081.969312, T: 2326597, Avg. loss: 199410956787664544175456878854144.000000\n",
      "Total training time: 4.99 seconds.\n",
      "-- Epoch 692\n",
      "Norm: 5922730409747.13, NNZs: 843, Bias: 115950458.028721, T: 2329964, Avg. loss: 199267858527724277266638834761728.000000\n",
      "Total training time: 5.00 seconds.\n",
      "-- Epoch 693\n",
      "Norm: 6133210299535.81, NNZs: 843, Bias: 413270720.392661, T: 2333331, Avg. loss: 199126012075169674361511549599744.000000\n",
      "Total training time: 5.00 seconds.\n",
      "-- Epoch 694\n",
      "Norm: 6332817976509.67, NNZs: 843, Bias: -10211912.487444, T: 2336698, Avg. loss: 198984638605057078812856520015872.000000\n",
      "Total training time: 5.01 seconds.\n",
      "-- Epoch 695\n",
      "Norm: 5835108029958.13, NNZs: 843, Bias: -265909096.651391, T: 2340065, Avg. loss: 198836200741535376979754473226240.000000\n",
      "Total training time: 5.02 seconds.\n",
      "-- Epoch 696\n",
      "Norm: 6150079367971.06, NNZs: 843, Bias: 500932231.131989, T: 2343432, Avg. loss: 198693849203611868734236628353024.000000\n",
      "Total training time: 5.03 seconds.\n",
      "-- Epoch 697\n",
      "Norm: 5868740678887.09, NNZs: 843, Bias: -265742796.323064, T: 2346799, Avg. loss: 198552590389854380735422596644864.000000\n",
      "Total training time: 5.03 seconds.\n",
      "-- Epoch 698\n",
      "Norm: 5830700137454.25, NNZs: 843, Bias: -126168385.666926, T: 2350166, Avg. loss: 198411554769033792655805089054720.000000\n",
      "Total training time: 5.04 seconds.\n",
      "-- Epoch 699\n",
      "Norm: 5995380439252.14, NNZs: 843, Bias: -271389686.814706, T: 2353533, Avg. loss: 198269378695196687925081200918528.000000\n",
      "Total training time: 5.05 seconds.\n",
      "-- Epoch 700\n",
      "Norm: 6239925399078.02, NNZs: 843, Bias: -16096982.711063, T: 2356900, Avg. loss: 198126975318010686063374397079552.000000\n",
      "Total training time: 5.05 seconds.\n",
      "-- Epoch 701\n",
      "Norm: 5879931321452.62, NNZs: 843, Bias: 239014344.989269, T: 2360267, Avg. loss: 197987676800497004397279706087424.000000\n",
      "Total training time: 5.06 seconds.\n",
      "-- Epoch 702\n",
      "Norm: 5806150481442.53, NNZs: 843, Bias: -16120585.925761, T: 2363634, Avg. loss: 197846860286642966692361335209984.000000\n",
      "Total training time: 5.07 seconds.\n",
      "-- Epoch 703\n",
      "Norm: 5949447171069.74, NNZs: 843, Bias: -271080835.064566, T: 2367001, Avg. loss: 197706895738604823819757283180544.000000\n",
      "Total training time: 5.08 seconds.\n",
      "-- Epoch 704\n",
      "Norm: 5864078497468.85, NNZs: 843, Bias: -16146462.757443, T: 2370368, Avg. loss: 197567619477124652485992979103744.000000\n",
      "Total training time: 5.08 seconds.\n",
      "-- Epoch 705\n",
      "Norm: 6376708248936.81, NNZs: 843, Bias: 238613719.000283, T: 2373735, Avg. loss: 197427783390077820933708917506048.000000\n",
      "Total training time: 5.09 seconds.\n",
      "-- Epoch 706\n",
      "Norm: 6325517837464.48, NNZs: 843, Bias: 493182851.321287, T: 2377102, Avg. loss: 197289496862682844775345119297536.000000\n",
      "Total training time: 5.10 seconds.\n",
      "-- Epoch 707\n",
      "Norm: 6006181975015.55, NNZs: 843, Bias: 238407395.920545, T: 2380469, Avg. loss: 197152299903371117851944511602688.000000\n",
      "Total training time: 5.10 seconds.\n",
      "-- Epoch 708\n",
      "Norm: 6119300632602.09, NNZs: 843, Bias: -525175744.628281, T: 2383836, Avg. loss: 197012739559065059812588543541248.000000\n",
      "Total training time: 5.11 seconds.\n",
      "-- Epoch 709\n",
      "Norm: 6211727771021.87, NNZs: 843, Bias: 238213176.037764, T: 2387203, Avg. loss: 196877514511959440448876961595392.000000\n",
      "Total training time: 5.12 seconds.\n",
      "-- Epoch 710\n",
      "Norm: 5511987073833.60, NNZs: 843, Bias: -16195559.943888, T: 2390570, Avg. loss: 196739509882203423514481909563392.000000\n",
      "Total training time: 5.13 seconds.\n",
      "-- Epoch 711\n",
      "Norm: 5704120231584.67, NNZs: 843, Bias: 238024910.705394, T: 2393937, Avg. loss: 196604378136520727687622891143168.000000\n",
      "Total training time: 5.13 seconds.\n",
      "-- Epoch 712\n",
      "Norm: 5644744755210.49, NNZs: 843, Bias: -16202604.443664, T: 2397304, Avg. loss: 196464376361871387525040341778432.000000\n",
      "Total training time: 5.14 seconds.\n",
      "-- Epoch 713\n",
      "Norm: 5822537706598.46, NNZs: 843, Bias: 424799198.490454, T: 2400671, Avg. loss: 196329887322046824044236164300800.000000\n",
      "Total training time: 5.15 seconds.\n",
      "-- Epoch 714\n",
      "Norm: 5715065148963.50, NNZs: 843, Bias: 2372964.080758, T: 2404038, Avg. loss: 196191713279844057049737075884032.000000\n",
      "Total training time: 5.16 seconds.\n",
      "-- Epoch 715\n",
      "Norm: 5762855306678.58, NNZs: 843, Bias: -251511295.748131, T: 2407405, Avg. loss: 196055004614140673399225661259776.000000\n",
      "Total training time: 5.16 seconds.\n",
      "-- Epoch 716\n",
      "Norm: 6331703876960.75, NNZs: 843, Bias: 176198507.650492, T: 2410772, Avg. loss: 195917855988525194684665719422976.000000\n",
      "Total training time: 5.17 seconds.\n",
      "-- Epoch 717\n",
      "Norm: 5913853046085.25, NNZs: 843, Bias: 314867604.574702, T: 2414139, Avg. loss: 195781486036584342634179942416384.000000\n",
      "Total training time: 5.18 seconds.\n",
      "-- Epoch 718\n",
      "Norm: 5920120109909.34, NNZs: 843, Bias: 61149009.219427, T: 2417506, Avg. loss: 195644483352257601563245762576384.000000\n",
      "Total training time: 5.18 seconds.\n",
      "-- Epoch 719\n",
      "Norm: 6125770526852.88, NNZs: 843, Bias: -30526958.609017, T: 2420873, Avg. loss: 195508573467599079973535419990016.000000\n",
      "Total training time: 5.19 seconds.\n",
      "-- Epoch 720\n",
      "Norm: 6505075303130.20, NNZs: 843, Bias: -283943354.927216, T: 2424240, Avg. loss: 195373140487094423112403302481920.000000\n",
      "Total training time: 5.20 seconds.\n",
      "-- Epoch 721\n",
      "Norm: 6656146552336.64, NNZs: 843, Bias: -30512595.514261, T: 2427607, Avg. loss: 195236531829085112900181488041984.000000\n",
      "Total training time: 5.21 seconds.\n",
      "-- Epoch 722\n",
      "Norm: 6171203946004.60, NNZs: 843, Bias: -283760592.595124, T: 2430974, Avg. loss: 195104426648249830941394640830464.000000\n",
      "Total training time: 5.21 seconds.\n",
      "-- Epoch 723\n",
      "Norm: 5940619351910.08, NNZs: 843, Bias: -28788456.313808, T: 2434341, Avg. loss: 194970889464241117430849394966528.000000\n",
      "Total training time: 5.22 seconds.\n",
      "-- Epoch 724\n",
      "Norm: 6229366725119.19, NNZs: 843, Bias: 224284221.340456, T: 2437708, Avg. loss: 194835280653550930136500904394752.000000\n",
      "Total training time: 5.23 seconds.\n",
      "-- Epoch 725\n",
      "Norm: 5990195634687.53, NNZs: 843, Bias: -28789478.686799, T: 2441075, Avg. loss: 194700666072460523193008310452224.000000\n",
      "Total training time: 5.23 seconds.\n",
      "-- Epoch 726\n",
      "Norm: 6791068985182.48, NNZs: 843, Bias: 337401185.374336, T: 2444442, Avg. loss: 194566484208964855559989151924224.000000\n",
      "Total training time: 5.24 seconds.\n",
      "-- Epoch 727\n",
      "Norm: 5882735370326.96, NNZs: 843, Bias: -58975614.108979, T: 2447809, Avg. loss: 194432342272105131156699304951808.000000\n",
      "Total training time: 5.25 seconds.\n",
      "-- Epoch 728\n",
      "Norm: 5758335548640.67, NNZs: 843, Bias: 193765362.584252, T: 2451176, Avg. loss: 194296680027092466151616705200128.000000\n",
      "Total training time: 5.26 seconds.\n",
      "-- Epoch 729\n",
      "Norm: 5973735362748.81, NNZs: 843, Bias: -58954294.226888, T: 2454543, Avg. loss: 194163478042479500758708589166592.000000\n",
      "Total training time: 5.26 seconds.\n",
      "-- Epoch 730\n",
      "Norm: 5519605999033.63, NNZs: 843, Bias: -35634079.027999, T: 2457910, Avg. loss: 194032922003401660450580836909056.000000\n",
      "Total training time: 5.27 seconds.\n",
      "-- Epoch 731\n",
      "Norm: 5652061869959.44, NNZs: 843, Bias: 216833540.618949, T: 2461277, Avg. loss: 193902461986558592289523875446784.000000\n",
      "Total training time: 5.28 seconds.\n",
      "-- Epoch 732\n",
      "Norm: 6023479888699.73, NNZs: 843, Bias: -441030351.083620, T: 2464644, Avg. loss: 193770390893597512703378953601024.000000\n",
      "Total training time: 5.29 seconds.\n",
      "-- Epoch 733\n",
      "Norm: 5862736086073.56, NNZs: 843, Bias: -188599194.463555, T: 2468011, Avg. loss: 193636925293693971964614074695680.000000\n",
      "Total training time: 5.29 seconds.\n",
      "-- Epoch 734\n",
      "Norm: 5922666651861.77, NNZs: 843, Bias: 63653116.918960, T: 2471378, Avg. loss: 193505410123216060464365089849344.000000\n",
      "Total training time: 5.30 seconds.\n",
      "-- Epoch 735\n",
      "Norm: 5926213140460.17, NNZs: 843, Bias: -188512325.825480, T: 2474745, Avg. loss: 193374063754748162469582195392512.000000\n",
      "Total training time: 5.31 seconds.\n",
      "-- Epoch 736\n",
      "Norm: 5985904087138.35, NNZs: 843, Bias: 63574947.465954, T: 2478112, Avg. loss: 193242436783967242564681936666624.000000\n",
      "Total training time: 5.31 seconds.\n",
      "-- Epoch 737\n",
      "Norm: 5821600231810.40, NNZs: 843, Bias: -188423715.164720, T: 2481479, Avg. loss: 193110872627161869190541030195200.000000\n",
      "Total training time: 5.32 seconds.\n",
      "-- Epoch 738\n",
      "Norm: 5889212178021.65, NNZs: 843, Bias: 63500022.885096, T: 2484846, Avg. loss: 192979535137800692413573881659392.000000\n",
      "Total training time: 5.33 seconds.\n",
      "-- Epoch 739\n",
      "Norm: 6959995977270.57, NNZs: 843, Bias: 315252692.643919, T: 2488213, Avg. loss: 192847405945008093139443883442176.000000\n",
      "Total training time: 5.34 seconds.\n",
      "-- Epoch 740\n",
      "Norm: 5963323900591.99, NNZs: 843, Bias: 63428414.881370, T: 2491580, Avg. loss: 192716420803626127183956051755008.000000\n",
      "Total training time: 5.34 seconds.\n",
      "-- Epoch 741\n",
      "Norm: 6512139155479.41, NNZs: 843, Bias: -188220155.175114, T: 2494947, Avg. loss: 192586099736727438067350865182720.000000\n",
      "Total training time: 5.35 seconds.\n",
      "-- Epoch 742\n",
      "Norm: 6228307978901.80, NNZs: 843, Bias: -439701369.170278, T: 2498314, Avg. loss: 192459854489305910122280044724224.000000\n",
      "Total training time: 5.36 seconds.\n",
      "-- Epoch 743\n",
      "Norm: 6179127503247.00, NNZs: 843, Bias: -188121513.148632, T: 2501681, Avg. loss: 192330160762410113806692937367552.000000\n",
      "Total training time: 5.36 seconds.\n",
      "-- Epoch 744\n",
      "Norm: 6338773458040.35, NNZs: 843, Bias: 63288421.983530, T: 2505048, Avg. loss: 192201012020836311046575024504832.000000\n",
      "Total training time: 5.37 seconds.\n",
      "-- Epoch 745\n",
      "Norm: 7346625855540.25, NNZs: 843, Bias: -486122672.844964, T: 2508415, Avg. loss: 192070336818668509901520029876224.000000\n",
      "Total training time: 5.38 seconds.\n",
      "-- Epoch 746\n",
      "Norm: 7091879092145.12, NNZs: 843, Bias: -234787615.392821, T: 2511782, Avg. loss: 191939897632744461644895109513216.000000\n",
      "Total training time: 5.39 seconds.\n",
      "-- Epoch 747\n",
      "Norm: 7079447320873.19, NNZs: 843, Bias: 16380356.477251, T: 2515149, Avg. loss: 191811605585957079051515805564928.000000\n",
      "Total training time: 5.39 seconds.\n",
      "-- Epoch 748\n",
      "Norm: 6655667628233.81, NNZs: 843, Bias: -29162452.232597, T: 2518516, Avg. loss: 191682445354499390747853319045120.000000\n",
      "Total training time: 5.40 seconds.\n",
      "-- Epoch 749\n",
      "Norm: 6579677209172.31, NNZs: 843, Bias: 221774183.033544, T: 2521883, Avg. loss: 191558958465784856789617244897280.000000\n",
      "Total training time: 5.41 seconds.\n",
      "-- Epoch 750\n",
      "Norm: 6866088430309.29, NNZs: 843, Bias: -86834947.513969, T: 2525250, Avg. loss: 191432992553070969867203283779584.000000\n",
      "Total training time: 5.42 seconds.\n",
      "-- Epoch 751\n",
      "Norm: 6749826745574.87, NNZs: 843, Bias: -337578246.927087, T: 2528617, Avg. loss: 191307316091479381997144195989504.000000\n",
      "Total training time: 5.42 seconds.\n",
      "-- Epoch 752\n",
      "Norm: 6188781365904.51, NNZs: 843, Bias: -86790526.046171, T: 2531984, Avg. loss: 191180700161539916768330383360000.000000\n",
      "Total training time: 5.43 seconds.\n",
      "-- Epoch 753\n",
      "Norm: 6183031663998.67, NNZs: 843, Bias: 163829473.147128, T: 2535351, Avg. loss: 191055280364189800749309714497536.000000\n",
      "Total training time: 5.44 seconds.\n",
      "-- Epoch 754\n",
      "Norm: 5988126207407.41, NNZs: 843, Bias: -86756942.520421, T: 2538718, Avg. loss: 190928284951717749559629738672128.000000\n",
      "Total training time: 5.44 seconds.\n",
      "-- Epoch 755\n",
      "Norm: 6404141991666.14, NNZs: 843, Bias: -337172653.848673, T: 2542085, Avg. loss: 190802484821490297867182974959616.000000\n",
      "Total training time: 5.45 seconds.\n",
      "-- Epoch 756\n",
      "Norm: 7182489426178.19, NNZs: 843, Bias: -84423746.443431, T: 2545452, Avg. loss: 190678141459338383123706653179904.000000\n",
      "Total training time: 5.46 seconds.\n",
      "-- Epoch 757\n",
      "Norm: 6757004815852.54, NNZs: 843, Bias: -334678476.295940, T: 2548819, Avg. loss: 190554079650909267039149795311616.000000\n",
      "Total training time: 5.47 seconds.\n",
      "-- Epoch 758\n",
      "Norm: 6510648173875.40, NNZs: 843, Bias: -84394063.950297, T: 2552186, Avg. loss: 190431613428132788716132863311872.000000\n",
      "Total training time: 5.47 seconds.\n",
      "-- Epoch 759\n",
      "Norm: 6983192808333.49, NNZs: 843, Bias: 165738157.211249, T: 2555553, Avg. loss: 190305229144332824900596196704256.000000\n",
      "Total training time: 5.48 seconds.\n",
      "-- Epoch 760\n",
      "Norm: 6725005243591.25, NNZs: 843, Bias: -84353613.801912, T: 2558920, Avg. loss: 190179261046073487937423607332864.000000\n",
      "Total training time: 5.49 seconds.\n",
      "-- Epoch 761\n",
      "Norm: 6351726589412.58, NNZs: 843, Bias: -334278258.341822, T: 2562287, Avg. loss: 190055228196061236657911809179648.000000\n",
      "Total training time: 5.49 seconds.\n",
      "-- Epoch 762\n",
      "Norm: 6429891594137.99, NNZs: 843, Bias: -356426846.971756, T: 2565654, Avg. loss: 189930473136514310383238100549632.000000\n",
      "Total training time: 5.50 seconds.\n",
      "-- Epoch 763\n",
      "Norm: 6708610305804.07, NNZs: 843, Bias: -106540951.827211, T: 2569021, Avg. loss: 189807903574118948965826460385280.000000\n",
      "Total training time: 5.51 seconds.\n",
      "-- Epoch 764\n",
      "Norm: 6475080320682.26, NNZs: 843, Bias: 143187026.315413, T: 2572388, Avg. loss: 189684773404221351553934699790336.000000\n",
      "Total training time: 5.52 seconds.\n",
      "-- Epoch 765\n",
      "Norm: 6710841437132.36, NNZs: 843, Bias: -106484484.005687, T: 2575755, Avg. loss: 189561012526466586059337067659264.000000\n",
      "Total training time: 5.52 seconds.\n",
      "-- Epoch 766\n",
      "Norm: 6673248777280.40, NNZs: 843, Bias: 143070471.141121, T: 2579122, Avg. loss: 189438551511739041825219147726848.000000\n",
      "Total training time: 5.53 seconds.\n",
      "-- Epoch 767\n",
      "Norm: 6997225263113.12, NNZs: 843, Bias: -106434498.660778, T: 2582489, Avg. loss: 189316698927781331486677909110784.000000\n",
      "Total training time: 5.54 seconds.\n",
      "-- Epoch 768\n",
      "Norm: 6718792230122.46, NNZs: 843, Bias: -355779621.054407, T: 2585856, Avg. loss: 189192442461020216778328681480192.000000\n",
      "Total training time: 5.54 seconds.\n",
      "-- Epoch 769\n",
      "Norm: 7491939493017.60, NNZs: 843, Bias: 505851726.104872, T: 2589223, Avg. loss: 189067763545390291812512960610304.000000\n",
      "Total training time: 5.55 seconds.\n",
      "-- Epoch 770\n",
      "Norm: 6760051640995.24, NNZs: 843, Bias: -200336071.278289, T: 2592590, Avg. loss: 188944980215187691728379909767168.000000\n",
      "Total training time: 5.56 seconds.\n",
      "-- Epoch 771\n",
      "Norm: 6696584459679.56, NNZs: 843, Bias: 48848275.776440, T: 2595957, Avg. loss: 188821245493521780366886601490432.000000\n",
      "Total training time: 5.57 seconds.\n",
      "-- Epoch 772\n",
      "Norm: 6528908583752.54, NNZs: 843, Bias: 297866763.863472, T: 2599324, Avg. loss: 188697466098588444430513744117760.000000\n",
      "Total training time: 5.57 seconds.\n",
      "-- Epoch 773\n",
      "Norm: 6556454755940.86, NNZs: 843, Bias: -412218642.849796, T: 2602691, Avg. loss: 188574021242221758559422756421632.000000\n",
      "Total training time: 5.58 seconds.\n",
      "-- Epoch 774\n",
      "Norm: 6242454864188.52, NNZs: 843, Bias: -163200234.500215, T: 2606058, Avg. loss: 188450705770129956105426466504704.000000\n",
      "Total training time: 5.59 seconds.\n",
      "-- Epoch 775\n",
      "Norm: 6250439851611.47, NNZs: 843, Bias: 69164891.048460, T: 2609425, Avg. loss: 188330137158056004247270650281984.000000\n",
      "Total training time: 5.59 seconds.\n",
      "-- Epoch 776\n",
      "Norm: 6783424613085.45, NNZs: 843, Bias: -179587874.060933, T: 2612792, Avg. loss: 188208950181994589789249966440448.000000\n",
      "Total training time: 5.60 seconds.\n",
      "-- Epoch 777\n",
      "Norm: 6796813624520.14, NNZs: 843, Bias: 566395424.002003, T: 2616159, Avg. loss: 188085253451759575153069476806656.000000\n",
      "Total training time: 5.61 seconds.\n",
      "-- Epoch 778\n",
      "Norm: 6548805411389.22, NNZs: 843, Bias: -179502695.462939, T: 2619526, Avg. loss: 187965928387053582210733797539840.000000\n",
      "Total training time: 5.62 seconds.\n",
      "-- Epoch 779\n",
      "Norm: 6426534494053.87, NNZs: 843, Bias: 69029473.355572, T: 2622893, Avg. loss: 187843628484673983013974370680832.000000\n",
      "Total training time: 5.62 seconds.\n",
      "-- Epoch 780\n",
      "Norm: 6354537906247.30, NNZs: 843, Bias: -121355153.826260, T: 2626260, Avg. loss: 187723315566624081724606360059904.000000\n",
      "Total training time: 5.63 seconds.\n",
      "-- Epoch 781\n",
      "Norm: 7072136623590.23, NNZs: 843, Bias: 126997742.706435, T: 2629627, Avg. loss: 187604873143689611087435630903296.000000\n",
      "Total training time: 5.64 seconds.\n",
      "-- Epoch 782\n",
      "Norm: 6689406074544.18, NNZs: 843, Bias: -121297719.894611, T: 2632994, Avg. loss: 187484042669347383756107590664192.000000\n",
      "Total training time: 5.64 seconds.\n",
      "-- Epoch 783\n",
      "Norm: 6802911781334.44, NNZs: 843, Bias: 507938446.380371, T: 2636361, Avg. loss: 187363218720809611296497886822400.000000\n",
      "Total training time: 5.65 seconds.\n",
      "-- Epoch 784\n",
      "Norm: 6677037920567.03, NNZs: 843, Bias: -236506263.142871, T: 2639728, Avg. loss: 187243473741045094502692575772672.000000\n",
      "Total training time: 5.66 seconds.\n",
      "-- Epoch 785\n",
      "Norm: 7208131778362.92, NNZs: 843, Bias: 11569347.277250, T: 2643095, Avg. loss: 187124356376998919757906782978048.000000\n",
      "Total training time: 5.67 seconds.\n",
      "-- Epoch 786\n",
      "Norm: 7077161788578.14, NNZs: 843, Bias: 259481221.082704, T: 2646462, Avg. loss: 187006617176431381745201190010880.000000\n",
      "Total training time: 5.67 seconds.\n",
      "-- Epoch 787\n",
      "Norm: 6771896413733.35, NNZs: 843, Bias: 11522014.829077, T: 2649829, Avg. loss: 186888204544871208328268536086528.000000\n",
      "Total training time: 5.68 seconds.\n",
      "-- Epoch 788\n",
      "Norm: 6473809286779.47, NNZs: 843, Bias: 259278075.594745, T: 2653196, Avg. loss: 186771832090316267483419126005760.000000\n",
      "Total training time: 5.69 seconds.\n",
      "-- Epoch 789\n",
      "Norm: 6983837760760.65, NNZs: 843, Bias: 11484728.532445, T: 2656563, Avg. loss: 186653453453310596070835027443712.000000\n",
      "Total training time: 5.70 seconds.\n",
      "-- Epoch 790\n",
      "Norm: 6476690837651.66, NNZs: 843, Bias: -236154948.945080, T: 2659930, Avg. loss: 186534865991380258484391665205248.000000\n",
      "Total training time: 5.70 seconds.\n",
      "-- Epoch 791\n",
      "Norm: 6068198216842.32, NNZs: 843, Bias: 11439142.591690, T: 2663297, Avg. loss: 186417955618152299994915073425408.000000\n",
      "Total training time: 5.71 seconds.\n",
      "-- Epoch 792\n",
      "Norm: 6557488264292.39, NNZs: 843, Bias: -320434811.080017, T: 2666664, Avg. loss: 186299722691555845341813950906368.000000\n",
      "Total training time: 5.72 seconds.\n",
      "-- Epoch 793\n",
      "Norm: 6736140741043.08, NNZs: 843, Bias: -141777549.865752, T: 2670031, Avg. loss: 186183580643750977408182813458432.000000\n",
      "Total training time: 5.72 seconds.\n",
      "-- Epoch 794\n",
      "Norm: 6396592758426.79, NNZs: 843, Bias: 174982966.089756, T: 2673398, Avg. loss: 186066149235185785387012560257024.000000\n",
      "Total training time: 5.73 seconds.\n",
      "-- Epoch 795\n",
      "Norm: 6263986902498.19, NNZs: 843, Bias: -72307141.446914, T: 2676765, Avg. loss: 185949455724563645480274709446656.000000\n",
      "Total training time: 5.74 seconds.\n",
      "-- Epoch 796\n",
      "Norm: 6346017683250.46, NNZs: 843, Bias: 174858856.378180, T: 2680132, Avg. loss: 185832762602531068599092247527424.000000\n",
      "Total training time: 5.75 seconds.\n",
      "-- Epoch 797\n",
      "Norm: 6134932576174.23, NNZs: 843, Bias: 140502856.842772, T: 2683499, Avg. loss: 185717989604075715983966149279744.000000\n",
      "Total training time: 5.75 seconds.\n",
      "-- Epoch 798\n",
      "Norm: 5920485026736.63, NNZs: 843, Bias: -106547604.592987, T: 2686866, Avg. loss: 185600790273272635762658127118336.000000\n",
      "Total training time: 5.76 seconds.\n",
      "-- Epoch 799\n",
      "Norm: 6031404756768.88, NNZs: 843, Bias: 140391373.211917, T: 2690233, Avg. loss: 185485269346183826499345074618368.000000\n",
      "Total training time: 5.77 seconds.\n",
      "-- Epoch 800\n",
      "Norm: 6391454202444.33, NNZs: 843, Bias: -106510402.272023, T: 2693600, Avg. loss: 185369271367770001868449401274368.000000\n",
      "Total training time: 5.77 seconds.\n",
      "-- Epoch 801\n",
      "Norm: 6230367706558.59, NNZs: 843, Bias: 130639161.329754, T: 2696967, Avg. loss: 185254152120493436246048464961536.000000\n",
      "Total training time: 5.78 seconds.\n",
      "-- Epoch 802\n",
      "Norm: 6125601502805.32, NNZs: 843, Bias: -116096231.532038, T: 2700334, Avg. loss: 185138765598265821769056074596352.000000\n",
      "Total training time: 5.79 seconds.\n",
      "-- Epoch 803\n",
      "Norm: 6010195874852.09, NNZs: 843, Bias: 130534590.729957, T: 2703701, Avg. loss: 185023806011855720204929865875456.000000\n",
      "Total training time: 5.80 seconds.\n",
      "-- Epoch 804\n",
      "Norm: 6146397267313.33, NNZs: 843, Bias: -116043038.714544, T: 2707068, Avg. loss: 184911108129626116775419047837696.000000\n",
      "Total training time: 5.80 seconds.\n",
      "-- Epoch 805\n",
      "Norm: 6050865555817.06, NNZs: 843, Bias: 130432943.716312, T: 2710435, Avg. loss: 184797511965152099692872911028224.000000\n",
      "Total training time: 5.81 seconds.\n",
      "-- Epoch 806\n",
      "Norm: 6368317059375.92, NNZs: 843, Bias: 19546860.498245, T: 2713802, Avg. loss: 184684142290174321348580583931904.000000\n",
      "Total training time: 5.82 seconds.\n",
      "-- Epoch 807\n",
      "Norm: 5976979058151.90, NNZs: 843, Bias: -216947394.272876, T: 2717169, Avg. loss: 184568692645107668597108736262144.000000\n",
      "Total training time: 5.82 seconds.\n",
      "-- Epoch 808\n",
      "Norm: 6182870492359.33, NNZs: 843, Bias: 411047446.065462, T: 2720536, Avg. loss: 184454917044761595753187038461952.000000\n",
      "Total training time: 5.83 seconds.\n",
      "-- Epoch 809\n",
      "Norm: 6165930683502.52, NNZs: 843, Bias: 164765288.130388, T: 2723903, Avg. loss: 184341855949348356406842016923648.000000\n",
      "Total training time: 5.84 seconds.\n",
      "-- Epoch 810\n",
      "Norm: 6185726172598.67, NNZs: 843, Bias: -81367070.179463, T: 2727270, Avg. loss: 184227014683135229996921398493184.000000\n",
      "Total training time: 5.85 seconds.\n",
      "-- Epoch 811\n",
      "Norm: 6779291922002.48, NNZs: 843, Bias: -327341721.803139, T: 2730637, Avg. loss: 184115555355959935825914233880576.000000\n",
      "Total training time: 5.85 seconds.\n",
      "-- Epoch 812\n",
      "Norm: 7055541931025.55, NNZs: 843, Bias: -81326318.372772, T: 2734004, Avg. loss: 184000843754553579752975912402944.000000\n",
      "Total training time: 5.86 seconds.\n",
      "-- Epoch 813\n",
      "Norm: 6445164127905.33, NNZs: 843, Bias: -40611391.572983, T: 2737371, Avg. loss: 183887201408702899805068109807616.000000\n",
      "Total training time: 5.87 seconds.\n",
      "-- Epoch 814\n",
      "Norm: 6383940689497.29, NNZs: 843, Bias: -286382787.360078, T: 2740738, Avg. loss: 183776152999946050459668460863488.000000\n",
      "Total training time: 5.87 seconds.\n",
      "-- Epoch 815\n",
      "Norm: 6696118072735.54, NNZs: 843, Bias: -227380590.915646, T: 2744105, Avg. loss: 183664959058648895033046177153024.000000\n",
      "Total training time: 5.88 seconds.\n",
      "-- Epoch 816\n",
      "Norm: 6213166969844.22, NNZs: 843, Bias: 18293577.002237, T: 2747472, Avg. loss: 183552960473971855086997500592128.000000\n",
      "Total training time: 5.89 seconds.\n",
      "-- Epoch 817\n",
      "Norm: 6763417650662.19, NNZs: 843, Bias: 198723356.945565, T: 2750839, Avg. loss: 183439563864443327269426878742528.000000\n",
      "Total training time: 5.90 seconds.\n",
      "-- Epoch 818\n",
      "Norm: 6260577655989.70, NNZs: 843, Bias: 245123300.320100, T: 2754206, Avg. loss: 183329099062953764387814182486016.000000\n",
      "Total training time: 5.90 seconds.\n",
      "-- Epoch 819\n",
      "Norm: 6715748448182.82, NNZs: 843, Bias: -491155296.960937, T: 2757573, Avg. loss: 183217251669162048291822129643520.000000\n",
      "Total training time: 5.91 seconds.\n",
      "-- Epoch 820\n",
      "Norm: 6080634188206.98, NNZs: 843, Bias: 244945166.066097, T: 2760940, Avg. loss: 183107705196748235059382655647744.000000\n",
      "Total training time: 5.92 seconds.\n",
      "-- Epoch 821\n",
      "Norm: 6372054405856.41, NNZs: 843, Bias: 554615140.470267, T: 2764307, Avg. loss: 182994337162109163088857199017984.000000\n",
      "Total training time: 5.93 seconds.\n",
      "-- Epoch 822\n",
      "Norm: 6265835859266.59, NNZs: 843, Bias: 309261290.925901, T: 2767674, Avg. loss: 182883031911942367057004468371456.000000\n",
      "Total training time: 5.93 seconds.\n",
      "-- Epoch 823\n",
      "Norm: 6161847923430.62, NNZs: 843, Bias: -426144955.942425, T: 2771041, Avg. loss: 182773379250678363292625725816832.000000\n",
      "Total training time: 5.94 seconds.\n",
      "-- Epoch 824\n",
      "Norm: 6201417724139.40, NNZs: 843, Bias: -181000180.007506, T: 2774408, Avg. loss: 182662275691265184627391608127488.000000\n",
      "Total training time: 5.95 seconds.\n",
      "-- Epoch 825\n",
      "Norm: 6145006547938.47, NNZs: 843, Bias: 63997809.947783, T: 2777775, Avg. loss: 182551350737932407644802361851904.000000\n",
      "Total training time: 5.95 seconds.\n",
      "-- Epoch 826\n",
      "Norm: 6050497673784.25, NNZs: 843, Bias: 308834588.718763, T: 2781142, Avg. loss: 182441331547728457430496691879936.000000\n",
      "Total training time: 5.96 seconds.\n",
      "-- Epoch 827\n",
      "Norm: 6115792006238.17, NNZs: 843, Bias: 63930118.726785, T: 2784509, Avg. loss: 182331664195338690437713993138176.000000\n",
      "Total training time: 5.97 seconds.\n",
      "-- Epoch 828\n",
      "Norm: 6243656227152.02, NNZs: 843, Bias: 584579399.928095, T: 2787876, Avg. loss: 182220478738779638132713227026432.000000\n",
      "Total training time: 5.98 seconds.\n",
      "-- Epoch 829\n",
      "Norm: 5735011202655.72, NNZs: 843, Bias: -149568590.597700, T: 2791243, Avg. loss: 182110768734082226927688977743872.000000\n",
      "Total training time: 5.98 seconds.\n",
      "-- Epoch 830\n",
      "Norm: 6331476984934.35, NNZs: 843, Bias: -394112659.868891, T: 2794610, Avg. loss: 182001563827439604608620577161216.000000\n",
      "Total training time: 5.99 seconds.\n",
      "-- Epoch 831\n",
      "Norm: 5607267262068.94, NNZs: 843, Bias: -149497198.941840, T: 2797977, Avg. loss: 181893385130533845446401240072192.000000\n",
      "Total training time: 6.00 seconds.\n",
      "-- Epoch 832\n",
      "Norm: 5846475652846.49, NNZs: 843, Bias: 94977095.294143, T: 2801344, Avg. loss: 181785753092807456944070716293120.000000\n",
      "Total training time: 6.00 seconds.\n",
      "-- Epoch 833\n",
      "Norm: 6742856698508.66, NNZs: 843, Bias: 508280892.977381, T: 2804711, Avg. loss: 181677719248391470543507438960640.000000\n",
      "Total training time: 6.01 seconds.\n",
      "-- Epoch 834\n",
      "Norm: 6101527580001.29, NNZs: 843, Bias: -224746249.852954, T: 2808078, Avg. loss: 181567967751049592202480133865472.000000\n",
      "Total training time: 6.02 seconds.\n",
      "-- Epoch 835\n",
      "Norm: 6122233453378.60, NNZs: 843, Bias: 131890225.247740, T: 2811445, Avg. loss: 181459187505144950147602498715648.000000\n",
      "Total training time: 6.03 seconds.\n",
      "-- Epoch 836\n",
      "Norm: 5589433635220.46, NNZs: 843, Bias: 61829200.878651, T: 2814812, Avg. loss: 181349339422701644816900247519232.000000\n",
      "Total training time: 6.03 seconds.\n",
      "-- Epoch 837\n",
      "Norm: 5911911264674.25, NNZs: 843, Bias: -182264918.503433, T: 2818179, Avg. loss: 181239280504879710016445532340224.000000\n",
      "Total training time: 6.04 seconds.\n",
      "-- Epoch 838\n",
      "Norm: 6071815788821.84, NNZs: 843, Bias: -426215454.552336, T: 2821546, Avg. loss: 181130921577996512259824622239744.000000\n",
      "Total training time: 6.05 seconds.\n",
      "-- Epoch 839\n",
      "Norm: 6364197655783.64, NNZs: 843, Bias: -670022714.615541, T: 2824913, Avg. loss: 181022534798310450465646047133696.000000\n",
      "Total training time: 6.05 seconds.\n",
      "-- Epoch 840\n",
      "Norm: 5999024154917.68, NNZs: 843, Bias: 61709914.001443, T: 2828280, Avg. loss: 180915097548127396344114035294208.000000\n",
      "Total training time: 6.06 seconds.\n",
      "-- Epoch 841\n",
      "Norm: 6702394750167.37, NNZs: 843, Bias: 159524287.337140, T: 2831647, Avg. loss: 180806710658090209341228500123648.000000\n",
      "Total training time: 6.07 seconds.\n",
      "-- Epoch 842\n",
      "Norm: 6038519452241.89, NNZs: 843, Bias: -156538958.542147, T: 2835014, Avg. loss: 180699398634667793642262250913792.000000\n",
      "Total training time: 6.08 seconds.\n",
      "-- Epoch 843\n",
      "Norm: 6290621165738.83, NNZs: 843, Bias: 574395183.646214, T: 2838381, Avg. loss: 180591895568113344529317274583040.000000\n",
      "Total training time: 6.08 seconds.\n",
      "-- Epoch 844\n",
      "Norm: 6085058715558.08, NNZs: 843, Bias: 308864958.013711, T: 2841748, Avg. loss: 180485237273552339397825102086144.000000\n",
      "Total training time: 6.09 seconds.\n",
      "-- Epoch 845\n",
      "Norm: 5427230100176.96, NNZs: 843, Bias: 65275960.659534, T: 2845115, Avg. loss: 180378021429227628911832365793280.000000\n",
      "Total training time: 6.10 seconds.\n",
      "-- Epoch 846\n",
      "Norm: 6016868841495.38, NNZs: 843, Bias: 308657491.470102, T: 2848482, Avg. loss: 180269768748814071636496058679296.000000\n",
      "Total training time: 6.11 seconds.\n",
      "-- Epoch 847\n",
      "Norm: 5524275710883.44, NNZs: 843, Bias: 65216971.722359, T: 2851849, Avg. loss: 180165650320060111942641042587648.000000\n",
      "Total training time: 6.11 seconds.\n",
      "-- Epoch 848\n",
      "Norm: 5859941821033.73, NNZs: 843, Bias: -178078515.113114, T: 2855216, Avg. loss: 180058406297181591907844726194176.000000\n",
      "Total training time: 6.12 seconds.\n",
      "-- Epoch 849\n",
      "Norm: 5762893257834.79, NNZs: 843, Bias: 65165872.304572, T: 2858583, Avg. loss: 179953167001105338884877017153536.000000\n",
      "Total training time: 6.13 seconds.\n",
      "-- Epoch 850\n",
      "Norm: 5670720117312.22, NNZs: 843, Bias: -188283299.649900, T: 2861950, Avg. loss: 179848330219868940565927090454528.000000\n",
      "Total training time: 6.13 seconds.\n",
      "-- Epoch 851\n",
      "Norm: 5920725111349.71, NNZs: 843, Bias: -178880525.303514, T: 2865317, Avg. loss: 179741546657215061071713691238400.000000\n",
      "Total training time: 6.14 seconds.\n",
      "-- Epoch 852\n",
      "Norm: 5592186310490.15, NNZs: 843, Bias: 64140696.315314, T: 2868684, Avg. loss: 179636019789330154742882836152320.000000\n",
      "Total training time: 6.15 seconds.\n",
      "-- Epoch 853\n",
      "Norm: 5483130046271.20, NNZs: 843, Bias: -178805456.457989, T: 2872051, Avg. loss: 179528947280337706644721432002560.000000\n",
      "Total training time: 6.16 seconds.\n",
      "-- Epoch 854\n",
      "Norm: 5974214661663.87, NNZs: 843, Bias: 64080750.499846, T: 2875418, Avg. loss: 179423985097289103065437326802944.000000\n",
      "Total training time: 6.16 seconds.\n",
      "-- Epoch 855\n",
      "Norm: 5608501906504.09, NNZs: 843, Bias: -178723426.771104, T: 2878785, Avg. loss: 179319849275944707301767626358784.000000\n",
      "Total training time: 6.17 seconds.\n",
      "-- Epoch 856\n",
      "Norm: 5951510250552.68, NNZs: 843, Bias: -86892422.664866, T: 2882152, Avg. loss: 179216168139380147045126696337408.000000\n",
      "Total training time: 6.18 seconds.\n",
      "-- Epoch 857\n",
      "Norm: 5350632634328.48, NNZs: 843, Bias: 155751760.865376, T: 2885519, Avg. loss: 179111636336973488200657257103360.000000\n",
      "Total training time: 6.18 seconds.\n",
      "-- Epoch 858\n",
      "Norm: 5495038217039.27, NNZs: 843, Bias: -86862761.554751, T: 2888886, Avg. loss: 179007987084983769968864749682688.000000\n",
      "Total training time: 6.19 seconds.\n",
      "-- Epoch 859\n",
      "Norm: 5401206387409.29, NNZs: 843, Bias: 155641745.972398, T: 2892253, Avg. loss: 178903390024212806833668641259520.000000\n",
      "Total training time: 6.20 seconds.\n",
      "-- Epoch 860\n",
      "Norm: 5392739699785.44, NNZs: 843, Bias: -86831424.427523, T: 2895620, Avg. loss: 178798655763350707569714245140480.000000\n",
      "Total training time: 6.21 seconds.\n",
      "-- Epoch 861\n",
      "Norm: 5719036329168.09, NNZs: 843, Bias: 94045425.911571, T: 2898987, Avg. loss: 178694588609144669650341370789888.000000\n",
      "Total training time: 6.21 seconds.\n",
      "-- Epoch 862\n",
      "Norm: 5799672204965.59, NNZs: 843, Bias: -148265258.178044, T: 2902354, Avg. loss: 178591266301101445910320002367488.000000\n",
      "Total training time: 6.22 seconds.\n",
      "-- Epoch 863\n",
      "Norm: 5826486181206.64, NNZs: 843, Bias: 93976494.584829, T: 2905721, Avg. loss: 178488674674579688446130334466048.000000\n",
      "Total training time: 6.23 seconds.\n",
      "-- Epoch 864\n",
      "Norm: 6547539438926.45, NNZs: 843, Bias: 336074324.459527, T: 2909088, Avg. loss: 178385027226478626438321170022400.000000\n",
      "Total training time: 6.23 seconds.\n",
      "-- Epoch 865\n",
      "Norm: 6017716262850.50, NNZs: 843, Bias: 93903013.662359, T: 2912455, Avg. loss: 178280303669788046236844048777216.000000\n",
      "Total training time: 6.24 seconds.\n",
      "-- Epoch 866\n",
      "Norm: 5807456954606.50, NNZs: 843, Bias: -148126059.101888, T: 2915822, Avg. loss: 178177752092103949126568280850432.000000\n",
      "Total training time: 6.25 seconds.\n",
      "-- Epoch 867\n",
      "Norm: 6371609400638.03, NNZs: 843, Bias: 93835645.200665, T: 2919189, Avg. loss: 178073317245059170245886542348288.000000\n",
      "Total training time: 6.26 seconds.\n",
      "-- Epoch 868\n",
      "Norm: 5744879298057.86, NNZs: 843, Bias: 82329102.454370, T: 2922556, Avg. loss: 177968208948854667409729389592576.000000\n",
      "Total training time: 6.26 seconds.\n",
      "-- Epoch 869\n",
      "Norm: 6158203945004.89, NNZs: 843, Bias: -159499221.208627, T: 2925923, Avg. loss: 177864479929828492786526781964288.000000\n",
      "Total training time: 6.27 seconds.\n",
      "-- Epoch 870\n",
      "Norm: 5876579960132.54, NNZs: 843, Bias: 82247064.175957, T: 2929290, Avg. loss: 177764053040683976654228568408064.000000\n",
      "Total training time: 6.28 seconds.\n",
      "-- Epoch 871\n",
      "Norm: 5725008942703.79, NNZs: 843, Bias: -159439215.093456, T: 2932657, Avg. loss: 177664131391330203815000751996928.000000\n",
      "Total training time: 6.28 seconds.\n",
      "-- Epoch 872\n",
      "Norm: 5920617765044.88, NNZs: 843, Bias: 82177040.576773, T: 2936024, Avg. loss: 177560980129905783639881436626944.000000\n",
      "Total training time: 6.29 seconds.\n",
      "-- Epoch 873\n",
      "Norm: 5816829341234.47, NNZs: 843, Bias: -159369129.756113, T: 2939391, Avg. loss: 177459939018925839227722301177856.000000\n",
      "Total training time: 6.30 seconds.\n",
      "-- Epoch 874\n",
      "Norm: 6011708631885.19, NNZs: 843, Bias: 31203719.275085, T: 2942758, Avg. loss: 177358741480126125327405472022528.000000\n",
      "Total training time: 6.31 seconds.\n",
      "-- Epoch 875\n",
      "Norm: 5814855520530.17, NNZs: 843, Bias: -210189918.699189, T: 2946125, Avg. loss: 177256188858386866065041164075008.000000\n",
      "Total training time: 6.31 seconds.\n",
      "-- Epoch 876\n",
      "Norm: 6123031863091.19, NNZs: 843, Bias: 31154996.648743, T: 2949492, Avg. loss: 177155608785338730729759492276224.000000\n",
      "Total training time: 6.32 seconds.\n",
      "-- Epoch 877\n",
      "Norm: 6120704322752.63, NNZs: 843, Bias: -210104487.302620, T: 2952859, Avg. loss: 177054687563173815117366232612864.000000\n",
      "Total training time: 6.33 seconds.\n",
      "-- Epoch 878\n",
      "Norm: 6197553758123.85, NNZs: 843, Bias: 164378546.411299, T: 2956226, Avg. loss: 176954082863025400330298314457088.000000\n",
      "Total training time: 6.34 seconds.\n",
      "-- Epoch 879\n",
      "Norm: 6025960230681.74, NNZs: 843, Bias: 268703807.661716, T: 2959593, Avg. loss: 176853759664808568631374251032576.000000\n",
      "Total training time: 6.34 seconds.\n",
      "-- Epoch 880\n",
      "Norm: 6168115909919.61, NNZs: 843, Bias: 27580528.480511, T: 2962960, Avg. loss: 176751383289856396193258981556224.000000\n",
      "Total training time: 6.35 seconds.\n",
      "-- Epoch 881\n",
      "Norm: 6158802727168.93, NNZs: 843, Bias: -213406964.962544, T: 2966327, Avg. loss: 176649976897108370162110122950656.000000\n",
      "Total training time: 6.36 seconds.\n",
      "-- Epoch 882\n",
      "Norm: 6458196923247.32, NNZs: 843, Bias: -454252913.161102, T: 2969694, Avg. loss: 176549488467919120163944490074112.000000\n",
      "Total training time: 6.36 seconds.\n",
      "-- Epoch 883\n",
      "Norm: 6619134701765.95, NNZs: 843, Bias: 268331310.864660, T: 2973061, Avg. loss: 176449201564618594677379869704192.000000\n",
      "Total training time: 6.37 seconds.\n",
      "-- Epoch 884\n",
      "Norm: 5656512435575.52, NNZs: 843, Bias: 27483428.076431, T: 2976428, Avg. loss: 176351177419432429303351891460096.000000\n",
      "Total training time: 6.38 seconds.\n",
      "-- Epoch 885\n",
      "Norm: 5772460853421.81, NNZs: 843, Bias: 268153375.961810, T: 2979795, Avg. loss: 176250927911393107319001685426176.000000\n",
      "Total training time: 6.39 seconds.\n",
      "-- Epoch 886\n",
      "Norm: 5981951854813.34, NNZs: 843, Bias: 27441374.347074, T: 2983162, Avg. loss: 176154545462780452820613481889792.000000\n",
      "Total training time: 6.39 seconds.\n",
      "-- Epoch 887\n",
      "Norm: 5877453649332.78, NNZs: 843, Bias: 95608368.647561, T: 2986529, Avg. loss: 176053399339285244312526845378560.000000\n",
      "Total training time: 6.40 seconds.\n",
      "-- Epoch 888\n",
      "Norm: 6735403894567.93, NNZs: 843, Bias: -144912000.051297, T: 2989896, Avg. loss: 175955641672085175369466157989888.000000\n",
      "Total training time: 6.41 seconds.\n",
      "-- Epoch 889\n",
      "Norm: 6250369960280.32, NNZs: 843, Bias: -704030721.972105, T: 2993263, Avg. loss: 175857333629991191481316578164736.000000\n",
      "Total training time: 6.41 seconds.\n",
      "-- Epoch 890\n",
      "Norm: 6324354479891.86, NNZs: 843, Bias: -300122761.450946, T: 2996630, Avg. loss: 175760408596729832382060604424192.000000\n",
      "Total training time: 6.42 seconds.\n",
      "-- Epoch 891\n",
      "Norm: 6559551084202.83, NNZs: 843, Bias: -59765800.030216, T: 2999997, Avg. loss: 175661885262314235274874557825024.000000\n",
      "Total training time: 6.43 seconds.\n",
      "-- Epoch 892\n",
      "Norm: 6460526469810.38, NNZs: 843, Bias: 180454732.882351, T: 3003364, Avg. loss: 175563437706123998252408565137408.000000\n",
      "Total training time: 6.44 seconds.\n",
      "-- Epoch 893\n",
      "Norm: 6525378793826.88, NNZs: 843, Bias: -59745310.437270, T: 3006731, Avg. loss: 175465811086385288816176503193600.000000\n",
      "Total training time: 6.44 seconds.\n",
      "-- Epoch 894\n",
      "Norm: 6336988601810.50, NNZs: 843, Bias: 180342857.015971, T: 3010098, Avg. loss: 175368506618770348166505666969600.000000\n",
      "Total training time: 6.45 seconds.\n",
      "-- Epoch 895\n",
      "Norm: 6089093552259.34, NNZs: 843, Bias: -59722365.638097, T: 3013465, Avg. loss: 175271015851014541826007240802304.000000\n",
      "Total training time: 6.46 seconds.\n",
      "-- Epoch 896\n",
      "Norm: 5955549427297.53, NNZs: 843, Bias: -299658195.849863, T: 3016832, Avg. loss: 175173552051437178829219242704896.000000\n",
      "Total training time: 6.46 seconds.\n",
      "-- Epoch 897\n",
      "Norm: 6225477175926.84, NNZs: 843, Bias: 108265342.655433, T: 3020199, Avg. loss: 175075787088477987312775077036032.000000\n",
      "Total training time: 6.47 seconds.\n",
      "-- Epoch 898\n",
      "Norm: 5994180478773.70, NNZs: 843, Bias: -131588158.324154, T: 3023566, Avg. loss: 174978233749916226507175035404288.000000\n",
      "Total training time: 6.48 seconds.\n",
      "-- Epoch 899\n",
      "Norm: 5994951753490.81, NNZs: 843, Bias: 78296659.766235, T: 3026933, Avg. loss: 174880857588887228917181461823488.000000\n",
      "Total training time: 6.49 seconds.\n",
      "-- Epoch 900\n",
      "Norm: 6169223032671.85, NNZs: 843, Bias: -161416732.477159, T: 3030300, Avg. loss: 174783657567456872802648661688320.000000\n",
      "Total training time: 6.49 seconds.\n",
      "-- Epoch 901\n",
      "Norm: 6036377038179.93, NNZs: 843, Bias: 78228104.464265, T: 3033667, Avg. loss: 174687203589582620786702327742464.000000\n",
      "Total training time: 6.50 seconds.\n",
      "-- Epoch 902\n",
      "Norm: 5883204959842.98, NNZs: 843, Bias: -161352499.471193, T: 3037034, Avg. loss: 174591783308908503378516135378944.000000\n",
      "Total training time: 6.51 seconds.\n",
      "-- Epoch 903\n",
      "Norm: 6232834040428.32, NNZs: 843, Bias: -400800145.161932, T: 3040401, Avg. loss: 174494584690940411547394905210880.000000\n",
      "Total training time: 6.51 seconds.\n",
      "-- Epoch 904\n",
      "Norm: 5614874068487.02, NNZs: 843, Bias: -161291674.364443, T: 3043768, Avg. loss: 174398317682697950481148624764928.000000\n",
      "Total training time: 6.52 seconds.\n",
      "-- Epoch 905\n",
      "Norm: 6168053460705.34, NNZs: 843, Bias: -400612645.035477, T: 3047135, Avg. loss: 174300366609938172040251587952640.000000\n",
      "Total training time: 6.53 seconds.\n",
      "-- Epoch 906\n",
      "Norm: 5434411727724.84, NNZs: 843, Bias: -161229373.699606, T: 3050502, Avg. loss: 174203780719653033530121386983424.000000\n",
      "Total training time: 6.54 seconds.\n",
      "-- Epoch 907\n",
      "Norm: 5517792077331.67, NNZs: 843, Bias: -165606287.746034, T: 3053869, Avg. loss: 174107104527588940427483766849536.000000\n",
      "Total training time: 6.54 seconds.\n",
      "-- Epoch 908\n",
      "Norm: 5262386886797.19, NNZs: 843, Bias: 149014422.042872, T: 3057236, Avg. loss: 174011413631585491835244477677568.000000\n",
      "Total training time: 6.55 seconds.\n",
      "-- Epoch 909\n",
      "Norm: 5481500304565.25, NNZs: 843, Bias: -288147089.804028, T: 3060603, Avg. loss: 173915849331864702264410875363328.000000\n",
      "Total training time: 6.56 seconds.\n",
      "-- Epoch 910\n",
      "Norm: 5407100199724.11, NNZs: 843, Bias: 137856067.354641, T: 3063970, Avg. loss: 173819264687449528004937615671296.000000\n",
      "Total training time: 6.56 seconds.\n",
      "-- Epoch 911\n",
      "Norm: 5429459885753.02, NNZs: 843, Bias: -101142385.716128, T: 3067337, Avg. loss: 173723078969949627850093053870080.000000\n",
      "Total training time: 6.57 seconds.\n",
      "-- Epoch 912\n",
      "Norm: 5409632222924.27, NNZs: 843, Bias: 137769570.802567, T: 3070704, Avg. loss: 173629324962963848505147747467264.000000\n",
      "Total training time: 6.58 seconds.\n",
      "-- Epoch 913\n",
      "Norm: 5919381288094.43, NNZs: 843, Bias: -75983547.527716, T: 3074071, Avg. loss: 173534245355145806748060536012800.000000\n",
      "Total training time: 6.59 seconds.\n",
      "-- Epoch 914\n",
      "Norm: 6027428379240.08, NNZs: 843, Bias: 214878707.070087, T: 3077438, Avg. loss: 173439717401483205321489001742336.000000\n",
      "Total training time: 6.59 seconds.\n",
      "-- Epoch 915\n",
      "Norm: 5652482037628.60, NNZs: 843, Bias: -501262532.414879, T: 3080805, Avg. loss: 173346136470293349841777991876608.000000\n",
      "Total training time: 6.60 seconds.\n",
      "-- Epoch 916\n",
      "Norm: 5612473290170.39, NNZs: 843, Bias: 214743057.513941, T: 3084172, Avg. loss: 173251154432527772609551464923136.000000\n",
      "Total training time: 6.61 seconds.\n",
      "-- Epoch 917\n",
      "Norm: 5487403872045.87, NNZs: 843, Bias: -6860985.521558, T: 3087539, Avg. loss: 173158203518049123563019607998464.000000\n",
      "Total training time: 6.61 seconds.\n",
      "-- Epoch 918\n",
      "Norm: 5814409917048.38, NNZs: 843, Bias: -245361859.890486, T: 3090906, Avg. loss: 173061901148511887282622036967424.000000\n",
      "Total training time: 6.62 seconds.\n",
      "-- Epoch 919\n",
      "Norm: 5600722362570.93, NNZs: 843, Bias: -6869419.618428, T: 3094273, Avg. loss: 172969929526905971695683448602624.000000\n",
      "Total training time: 6.63 seconds.\n",
      "-- Epoch 920\n",
      "Norm: 6216653250399.81, NNZs: 843, Bias: -721968423.842756, T: 3097640, Avg. loss: 172878406644799624861668595466240.000000\n",
      "Total training time: 6.64 seconds.\n",
      "-- Epoch 921\n",
      "Norm: 6018973508125.89, NNZs: 843, Bias: -483483776.694273, T: 3101007, Avg. loss: 172784989552406052150369104953344.000000\n",
      "Total training time: 6.64 seconds.\n",
      "-- Epoch 922\n",
      "Norm: 5833362098493.42, NNZs: 843, Bias: -245126683.586628, T: 3104374, Avg. loss: 172689673698017130136603788836864.000000\n",
      "Total training time: 6.65 seconds.\n",
      "-- Epoch 923\n",
      "Norm: 6358655266469.70, NNZs: 843, Bias: -6899458.855879, T: 3107741, Avg. loss: 172596239877951199639121132781568.000000\n",
      "Total training time: 6.66 seconds.\n",
      "-- Epoch 924\n",
      "Norm: 6614623391221.15, NNZs: 843, Bias: -245011441.443475, T: 3111108, Avg. loss: 172502239330059270005817153159168.000000\n",
      "Total training time: 6.67 seconds.\n",
      "-- Epoch 925\n",
      "Norm: 6065190750135.46, NNZs: 843, Bias: -1660455.273290, T: 3114475, Avg. loss: 172407515514112625769292050202624.000000\n",
      "Total training time: 6.67 seconds.\n",
      "-- Epoch 926\n",
      "Norm: 6138640556308.44, NNZs: 843, Bias: -239642910.549656, T: 3117842, Avg. loss: 172315334835831094610853730713600.000000\n",
      "Total training time: 6.68 seconds.\n",
      "-- Epoch 927\n",
      "Norm: 6523332130005.92, NNZs: 843, Bias: -382268877.020097, T: 3121209, Avg. loss: 172222332675837285032458295705600.000000\n",
      "Total training time: 6.69 seconds.\n",
      "-- Epoch 928\n",
      "Norm: 6144656178648.04, NNZs: 843, Bias: -144323923.499732, T: 3124576, Avg. loss: 172129816151640356929658448510976.000000\n",
      "Total training time: 6.69 seconds.\n",
      "-- Epoch 929\n",
      "Norm: 6360870241021.03, NNZs: 843, Bias: -260126602.850145, T: 3127943, Avg. loss: 172035588943603556888831753977856.000000\n",
      "Total training time: 6.70 seconds.\n",
      "-- Epoch 930\n",
      "Norm: 6383897605315.73, NNZs: 843, Bias: -22342776.349576, T: 3131310, Avg. loss: 171944673770171120682478170275840.000000\n",
      "Total training time: 6.71 seconds.\n",
      "-- Epoch 931\n",
      "Norm: 6439191236855.56, NNZs: 843, Bias: -24240880.770207, T: 3134677, Avg. loss: 171853363541776832929433215041536.000000\n",
      "Total training time: 6.72 seconds.\n",
      "-- Epoch 932\n",
      "Norm: 6615358419033.42, NNZs: 843, Bias: -32579041.452720, T: 3138044, Avg. loss: 171761317222188583567665806704640.000000\n",
      "Total training time: 6.72 seconds.\n",
      "-- Epoch 933\n",
      "Norm: 6179413449487.47, NNZs: 843, Bias: 204952565.551053, T: 3141411, Avg. loss: 171670369993042862171722080059392.000000\n",
      "Total training time: 6.73 seconds.\n",
      "-- Epoch 934\n",
      "Norm: 6665844463146.11, NNZs: 843, Bias: 138230497.693072, T: 3144778, Avg. loss: 171577978356841557122631048626176.000000\n",
      "Total training time: 6.74 seconds.\n",
      "-- Epoch 935\n",
      "Norm: 7174646086296.34, NNZs: 843, Bias: -99219332.932113, T: 3148145, Avg. loss: 171485511773582125537734697156608.000000\n",
      "Total training time: 6.74 seconds.\n",
      "-- Epoch 936\n",
      "Norm: 6200359394729.38, NNZs: 843, Bias: 138141035.468686, T: 3151512, Avg. loss: 171394958589393080936685212008448.000000\n",
      "Total training time: 6.75 seconds.\n",
      "-- Epoch 937\n",
      "Norm: 6104778650130.26, NNZs: 843, Bias: -99178684.878972, T: 3154879, Avg. loss: 171302918630604926118358130098176.000000\n",
      "Total training time: 6.76 seconds.\n",
      "-- Epoch 938\n",
      "Norm: 6274819694480.87, NNZs: 843, Bias: 342945423.135165, T: 3158246, Avg. loss: 171212948366200830585239249092608.000000\n",
      "Total training time: 6.77 seconds.\n",
      "-- Epoch 939\n",
      "Norm: 5518870813600.76, NNZs: 843, Bias: -22149706.046647, T: 3161613, Avg. loss: 171121727340457256581380135976960.000000\n",
      "Total training time: 6.77 seconds.\n",
      "-- Epoch 940\n",
      "Norm: 5645423513771.07, NNZs: 843, Bias: 214937122.772958, T: 3164980, Avg. loss: 171031816095438659852008945614848.000000\n",
      "Total training time: 6.78 seconds.\n",
      "-- Epoch 941\n",
      "Norm: 5595101915435.70, NNZs: 843, Bias: -22149761.597569, T: 3168347, Avg. loss: 170942170755798038573013413134336.000000\n",
      "Total training time: 6.79 seconds.\n",
      "-- Epoch 942\n",
      "Norm: 5420843070443.97, NNZs: 843, Bias: -259119730.354410, T: 3171714, Avg. loss: 170851277294836363039973784420352.000000\n",
      "Total training time: 6.79 seconds.\n",
      "-- Epoch 943\n",
      "Norm: 5419607650828.96, NNZs: 843, Bias: -247097718.201274, T: 3175081, Avg. loss: 170760103926261147496323926196224.000000\n",
      "Total training time: 6.80 seconds.\n",
      "-- Epoch 944\n",
      "Norm: 5869710135830.02, NNZs: 843, Bias: -253442825.220348, T: 3178448, Avg. loss: 170670402272629973635399153614848.000000\n",
      "Total training time: 6.81 seconds.\n",
      "-- Epoch 945\n",
      "Norm: 5732510488266.83, NNZs: 843, Bias: -16612258.673471, T: 3181815, Avg. loss: 170579569643334856992499949371392.000000\n",
      "Total training time: 6.82 seconds.\n",
      "-- Epoch 946\n",
      "Norm: 5740150656886.78, NNZs: 843, Bias: 220095807.227855, T: 3185182, Avg. loss: 170489987084655334527891421528064.000000\n",
      "Total training time: 6.82 seconds.\n",
      "-- Epoch 947\n",
      "Norm: 5650564534544.83, NNZs: 843, Bias: -111589324.339572, T: 3188549, Avg. loss: 170401363001387902600893016571904.000000\n",
      "Total training time: 6.83 seconds.\n",
      "-- Epoch 948\n",
      "Norm: 5762966870020.24, NNZs: 843, Bias: -497251039.210755, T: 3191916, Avg. loss: 170312483487907558501275341422592.000000\n",
      "Total training time: 6.84 seconds.\n",
      "-- Epoch 949\n",
      "Norm: 5868157777133.93, NNZs: 843, Bias: 212432256.945124, T: 3195283, Avg. loss: 170224912719193024473391423291392.000000\n",
      "Total training time: 6.85 seconds.\n",
      "-- Epoch 950\n",
      "Norm: 5326543245780.48, NNZs: 843, Bias: -24091058.127723, T: 3198650, Avg. loss: 170136471542057906421947359559680.000000\n",
      "Total training time: 6.85 seconds.\n",
      "-- Epoch 951\n",
      "Norm: 5436888203094.51, NNZs: 843, Bias: 212308485.424403, T: 3202017, Avg. loss: 170047390172983947999433575104512.000000\n",
      "Total training time: 6.86 seconds.\n",
      "-- Epoch 952\n",
      "Norm: 5412966853066.16, NNZs: 843, Bias: 137193869.652262, T: 3205384, Avg. loss: 169959250820266237555716621598720.000000\n",
      "Total training time: 6.87 seconds.\n",
      "-- Epoch 953\n",
      "Norm: 5541244652667.00, NNZs: 843, Bias: -99121223.545489, T: 3208751, Avg. loss: 169868836116971814923669828796416.000000\n",
      "Total training time: 6.87 seconds.\n",
      "-- Epoch 954\n",
      "Norm: 5819523793968.29, NNZs: 843, Bias: -606424398.085762, T: 3212118, Avg. loss: 169781555154148767096806620266496.000000\n",
      "Total training time: 6.88 seconds.\n",
      "-- Epoch 955\n",
      "Norm: 5388194682449.98, NNZs: 843, Bias: 102180867.301901, T: 3215485, Avg. loss: 169693853790648082824379320238080.000000\n",
      "Total training time: 6.89 seconds.\n",
      "-- Epoch 956\n",
      "Norm: 5576545466509.23, NNZs: 843, Bias: -133942542.257015, T: 3218852, Avg. loss: 169604092754619122483478216572928.000000\n",
      "Total training time: 6.90 seconds.\n",
      "-- Epoch 957\n",
      "Norm: 5643325991942.63, NNZs: 843, Bias: -369945055.171721, T: 3222219, Avg. loss: 169515405788690413043203323199488.000000\n",
      "Total training time: 6.90 seconds.\n",
      "-- Epoch 958\n",
      "Norm: 5499267514717.43, NNZs: 843, Bias: 42208680.689200, T: 3225586, Avg. loss: 169427946444375964544717977288704.000000\n",
      "Total training time: 6.91 seconds.\n",
      "-- Epoch 959\n",
      "Norm: 5870080443142.09, NNZs: 843, Bias: 32515819.154403, T: 3228953, Avg. loss: 169340375735794897864568661344256.000000\n",
      "Total training time: 6.92 seconds.\n",
      "-- Epoch 960\n",
      "Norm: 5601539113162.39, NNZs: 843, Bias: -203344376.302355, T: 3232320, Avg. loss: 169251344899108428831166535041024.000000\n",
      "Total training time: 6.92 seconds.\n",
      "-- Epoch 961\n",
      "Norm: 5952517114701.83, NNZs: 843, Bias: -439080717.171572, T: 3235687, Avg. loss: 169164060153861753627437531398144.000000\n",
      "Total training time: 6.93 seconds.\n",
      "-- Epoch 962\n",
      "Norm: 5912618955182.83, NNZs: 843, Bias: 417957718.119443, T: 3239054, Avg. loss: 169076023349548205887514791116800.000000\n",
      "Total training time: 6.94 seconds.\n",
      "-- Epoch 963\n",
      "Norm: 5471111346588.75, NNZs: 843, Bias: -289127561.248130, T: 3242421, Avg. loss: 168987802597994193600912346316800.000000\n",
      "Total training time: 6.95 seconds.\n",
      "-- Epoch 964\n",
      "Norm: 5406654020620.33, NNZs: 843, Bias: -53462193.669001, T: 3245788, Avg. loss: 168902749561904727185553820221440.000000\n",
      "Total training time: 6.95 seconds.\n",
      "-- Epoch 965\n",
      "Norm: 5546548877655.60, NNZs: 843, Bias: -288987816.772888, T: 3249155, Avg. loss: 168814022713940806892468471595008.000000\n",
      "Total training time: 6.96 seconds.\n",
      "-- Epoch 966\n",
      "Norm: 5385365578445.44, NNZs: 843, Bias: 62079459.855467, T: 3252522, Avg. loss: 168727202639103695729859839918080.000000\n",
      "Total training time: 6.97 seconds.\n",
      "-- Epoch 967\n",
      "Norm: 5599318317635.62, NNZs: 843, Bias: -173353977.181065, T: 3255889, Avg. loss: 168640874685971648708397496270848.000000\n",
      "Total training time: 6.98 seconds.\n",
      "-- Epoch 968\n",
      "Norm: 5882517917949.58, NNZs: 843, Bias: -408675114.641812, T: 3259256, Avg. loss: 168552951766679898898719955746816.000000\n",
      "Total training time: 6.98 seconds.\n",
      "-- Epoch 969\n",
      "Norm: 6276971048408.21, NNZs: 843, Bias: -643874065.832855, T: 3262623, Avg. loss: 168464551890688553722517437546496.000000\n",
      "Total training time: 6.99 seconds.\n",
      "-- Epoch 970\n",
      "Norm: 5748692078989.26, NNZs: 843, Bias: 316735928.547694, T: 3265990, Avg. loss: 168378445786060038825567252905984.000000\n",
      "Total training time: 7.00 seconds.\n",
      "-- Epoch 971\n",
      "Norm: 5814319823739.02, NNZs: 843, Bias: 81476924.860762, T: 3269357, Avg. loss: 168293081964865970120264511389696.000000\n",
      "Total training time: 7.00 seconds.\n",
      "-- Epoch 972\n",
      "Norm: 5630722052542.69, NNZs: 843, Bias: 316559352.897291, T: 3272724, Avg. loss: 168206878002949405661232512368640.000000\n",
      "Total training time: 7.01 seconds.\n",
      "-- Epoch 973\n",
      "Norm: 5676324288177.06, NNZs: 843, Bias: 16651594.822575, T: 3276091, Avg. loss: 168120747137288278166907014610944.000000\n",
      "Total training time: 7.02 seconds.\n",
      "-- Epoch 974\n",
      "Norm: 5578654713189.40, NNZs: 843, Bias: -218355577.586360, T: 3279458, Avg. loss: 168034936200925140077372394962944.000000\n",
      "Total training time: 7.03 seconds.\n",
      "-- Epoch 975\n",
      "Norm: 5633619524386.01, NNZs: 843, Bias: 16614534.340607, T: 3282825, Avg. loss: 167948398449582937360853278654464.000000\n",
      "Total training time: 7.03 seconds.\n",
      "-- Epoch 976\n",
      "Norm: 5788733447437.37, NNZs: 843, Bias: -218270901.375513, T: 3286192, Avg. loss: 167863543174761585637601692352512.000000\n",
      "Total training time: 7.04 seconds.\n",
      "-- Epoch 977\n",
      "Norm: 5629178209150.87, NNZs: 843, Bias: -803453.341405, T: 3289559, Avg. loss: 167778452320339560161909669888000.000000\n",
      "Total training time: 7.05 seconds.\n",
      "-- Epoch 978\n",
      "Norm: 5837871565445.98, NNZs: 843, Bias: -235562544.096280, T: 3292926, Avg. loss: 167691042110212270516150061760512.000000\n",
      "Total training time: 7.05 seconds.\n",
      "-- Epoch 979\n",
      "Norm: 6391642112949.01, NNZs: 843, Bias: 468559901.164250, T: 3296293, Avg. loss: 167606017242984542992320637698048.000000\n",
      "Total training time: 7.06 seconds.\n",
      "-- Epoch 980\n",
      "Norm: 6339165221139.75, NNZs: 843, Bias: -235453596.252201, T: 3299660, Avg. loss: 167522071369168832350075362476032.000000\n",
      "Total training time: 7.07 seconds.\n",
      "-- Epoch 981\n",
      "Norm: 6111049170941.38, NNZs: 843, Bias: -469974808.295613, T: 3303027, Avg. loss: 167436897054494925244987703558144.000000\n",
      "Total training time: 7.08 seconds.\n",
      "-- Epoch 982\n",
      "Norm: 5704982012167.64, NNZs: 843, Bias: -235351493.528989, T: 3306394, Avg. loss: 167352482030931130556553032105984.000000\n",
      "Total training time: 7.08 seconds.\n",
      "-- Epoch 983\n",
      "Norm: 5480814223105.13, NNZs: 843, Bias: -850152.213290, T: 3309761, Avg. loss: 167267622770185886204523096047616.000000\n",
      "Total training time: 7.09 seconds.\n",
      "-- Epoch 984\n",
      "Norm: 5967633990074.69, NNZs: 843, Bias: 233532249.842758, T: 3313128, Avg. loss: 167181746606365999548559179907072.000000\n",
      "Total training time: 7.10 seconds.\n",
      "-- Epoch 985\n",
      "Norm: 5360903701504.92, NNZs: 843, Bias: -867998.502723, T: 3316495, Avg. loss: 167098402406012704742413311148032.000000\n",
      "Total training time: 7.11 seconds.\n",
      "-- Epoch 986\n",
      "Norm: 5581541565985.12, NNZs: 843, Bias: 233391272.616769, T: 3319862, Avg. loss: 167014338311210751886029595082752.000000\n",
      "Total training time: 7.11 seconds.\n",
      "-- Epoch 987\n",
      "Norm: 5982873825960.31, NNZs: 843, Bias: -469318341.893206, T: 3323229, Avg. loss: 166929359266162524896468309901312.000000\n",
      "Total training time: 7.12 seconds.\n",
      "-- Epoch 988\n",
      "Norm: 5487201533406.50, NNZs: 843, Bias: -235055098.679364, T: 3326596, Avg. loss: 166845956947420682180952764448768.000000\n",
      "Total training time: 7.13 seconds.\n",
      "-- Epoch 989\n",
      "Norm: 5753314613576.16, NNZs: 843, Bias: 146289595.214492, T: 3329963, Avg. loss: 166760634845388234752739707453440.000000\n",
      "Total training time: 7.13 seconds.\n",
      "-- Epoch 990\n",
      "Norm: 5867386076650.00, NNZs: 843, Bias: -87787998.391330, T: 3333330, Avg. loss: 166677529887482762246502291079168.000000\n",
      "Total training time: 7.14 seconds.\n",
      "-- Epoch 991\n",
      "Norm: 5739996442252.95, NNZs: 843, Bias: 146205145.671937, T: 3336697, Avg. loss: 166592132402637867511358079107072.000000\n",
      "Total training time: 7.15 seconds.\n",
      "-- Epoch 992\n",
      "Norm: 5837882374441.47, NNZs: 843, Bias: 380077612.961580, T: 3340064, Avg. loss: 166507744708013796662711645372416.000000\n",
      "Total training time: 7.16 seconds.\n",
      "-- Epoch 993\n",
      "Norm: 5763519850948.98, NNZs: 843, Bias: 146113350.380562, T: 3343431, Avg. loss: 166423233202013208550892158058496.000000\n",
      "Total training time: 7.16 seconds.\n",
      "-- Epoch 994\n",
      "Norm: 5727252682609.41, NNZs: 843, Bias: -87730181.416060, T: 3346798, Avg. loss: 166340023537744724659176269152256.000000\n",
      "Total training time: 7.17 seconds.\n",
      "-- Epoch 995\n",
      "Norm: 5955263149067.63, NNZs: 843, Bias: 263730365.422240, T: 3350165, Avg. loss: 166254777412460740432208062316544.000000\n",
      "Total training time: 7.18 seconds.\n",
      "-- Epoch 996\n",
      "Norm: 5754214222552.22, NNZs: 843, Bias: 29979162.412601, T: 3353532, Avg. loss: 166171946992827632533861748965376.000000\n",
      "Total training time: 7.18 seconds.\n",
      "-- Epoch 997\n",
      "Norm: 5608267094360.29, NNZs: 843, Bias: 100798099.271185, T: 3356899, Avg. loss: 166087386772688280942356453130240.000000\n",
      "Total training time: 7.19 seconds.\n",
      "-- Epoch 998\n",
      "Norm: 5717224574347.00, NNZs: 843, Bias: -132800259.233760, T: 3360266, Avg. loss: 166004343650832176245653226127360.000000\n",
      "Total training time: 7.20 seconds.\n",
      "-- Epoch 999\n",
      "Norm: 5916995087228.14, NNZs: 843, Bias: 100730173.513942, T: 3363633, Avg. loss: 165921161329278558359565201571840.000000\n",
      "Total training time: 7.21 seconds.\n",
      "-- Epoch 1000\n",
      "Norm: 6275317154241.57, NNZs: 843, Bias: -132741970.028199, T: 3367000, Avg. loss: 165838520755466663872339063603200.000000\n",
      "Total training time: 7.21 seconds.\n",
      "-- Epoch 1001\n",
      "Norm: 6352020904964.77, NNZs: 843, Bias: 100664884.621451, T: 3370367, Avg. loss: 165756937972757666486419682492416.000000\n",
      "Total training time: 7.22 seconds.\n",
      "-- Epoch 1002\n",
      "Norm: 6243932056465.53, NNZs: 843, Bias: -611610312.011800, T: 3373734, Avg. loss: 165673186982029912924375390093312.000000\n",
      "Total training time: 7.23 seconds.\n",
      "-- Epoch 1003\n",
      "Norm: 5929235918088.33, NNZs: 843, Bias: -378194725.579643, T: 3377101, Avg. loss: 165590739555902009588518209191936.000000\n",
      "Total training time: 7.23 seconds.\n",
      "-- Epoch 1004\n",
      "Norm: 6155545490193.98, NNZs: 843, Bias: -144894582.526410, T: 3380468, Avg. loss: 165507919121431516870912629538816.000000\n",
      "Total training time: 7.24 seconds.\n",
      "-- Epoch 1005\n",
      "Norm: 6086834090772.84, NNZs: 843, Bias: 255636953.136771, T: 3383835, Avg. loss: 165424972362507964506219462262784.000000\n",
      "Total training time: 7.25 seconds.\n",
      "-- Epoch 1006\n",
      "Norm: 6147773205810.65, NNZs: 843, Bias: 22461249.906208, T: 3387202, Avg. loss: 165343998563771703835663345385472.000000\n",
      "Total training time: 7.26 seconds.\n",
      "-- Epoch 1007\n",
      "Norm: 6789234402411.30, NNZs: 843, Bias: -210590960.882764, T: 3390569, Avg. loss: 165262856263317022074203373830144.000000\n",
      "Total training time: 7.26 seconds.\n",
      "-- Epoch 1008\n",
      "Norm: 6629876943867.76, NNZs: 843, Bias: 22432264.350111, T: 3393936, Avg. loss: 165181383936338017681005397147648.000000\n",
      "Total training time: 7.27 seconds.\n",
      "-- Epoch 1009\n",
      "Norm: 5696560643680.56, NNZs: 843, Bias: -50578541.345530, T: 3397303, Avg. loss: 165101380566290140372744280211456.000000\n",
      "Total training time: 7.28 seconds.\n",
      "-- Epoch 1010\n",
      "Norm: 5687944364857.13, NNZs: 843, Bias: 182290755.202080, T: 3400670, Avg. loss: 165019204684649851844439092232192.000000\n",
      "Total training time: 7.29 seconds.\n",
      "-- Epoch 1011\n",
      "Norm: 6141879511948.12, NNZs: 843, Bias: -516188284.057200, T: 3404037, Avg. loss: 164938532403445776224607254085632.000000\n",
      "Total training time: 7.29 seconds.\n",
      "-- Epoch 1012\n",
      "Norm: 6178342578339.93, NNZs: 843, Bias: -283316362.959985, T: 3407404, Avg. loss: 164858398312967768222832311402496.000000\n",
      "Total training time: 7.30 seconds.\n",
      "-- Epoch 1013\n",
      "Norm: 6037416365068.67, NNZs: 843, Bias: -50558269.629773, T: 3410771, Avg. loss: 164779250202023241422259624083456.000000\n",
      "Total training time: 7.31 seconds.\n",
      "-- Epoch 1014\n",
      "Norm: 5677656647451.63, NNZs: 843, Bias: 182089140.967890, T: 3414138, Avg. loss: 164697723179428263380619330846720.000000\n",
      "Total training time: 7.31 seconds.\n",
      "-- Epoch 1015\n",
      "Norm: 5872250390931.49, NNZs: 843, Bias: -50543155.774729, T: 3417505, Avg. loss: 164618348107696497390185059713024.000000\n",
      "Total training time: 7.32 seconds.\n",
      "-- Epoch 1016\n",
      "Norm: 6126372099178.94, NNZs: 843, Bias: -748106092.550590, T: 3420872, Avg. loss: 164538379983474019182854252003328.000000\n",
      "Total training time: 7.33 seconds.\n",
      "-- Epoch 1017\n",
      "Norm: 5528917815637.69, NNZs: 843, Bias: -50531992.329146, T: 3424239, Avg. loss: 164459510787516347883191934124032.000000\n",
      "Total training time: 7.34 seconds.\n",
      "-- Epoch 1018\n",
      "Norm: 5622232359945.44, NNZs: 843, Bias: 181884335.960689, T: 3427606, Avg. loss: 164379992603284952690071590404096.000000\n",
      "Total training time: 7.34 seconds.\n",
      "-- Epoch 1019\n",
      "Norm: 5908809944741.64, NNZs: 843, Bias: 99587998.893309, T: 3430973, Avg. loss: 164299556236228566838135966662656.000000\n",
      "Total training time: 7.35 seconds.\n",
      "-- Epoch 1020\n",
      "Norm: 5701439709454.84, NNZs: 843, Bias: -132741981.468636, T: 3434340, Avg. loss: 164219310622883482823757775503360.000000\n",
      "Total training time: 7.36 seconds.\n",
      "-- Epoch 1021\n",
      "Norm: 5990146257811.24, NNZs: 843, Bias: 77092430.272252, T: 3437707, Avg. loss: 164139570354741187614218228072448.000000\n",
      "Total training time: 7.36 seconds.\n",
      "-- Epoch 1022\n",
      "Norm: 6010880773332.71, NNZs: 843, Bias: -155119951.811327, T: 3441074, Avg. loss: 164058691574688398777739806507008.000000\n",
      "Total training time: 7.37 seconds.\n",
      "-- Epoch 1023\n",
      "Norm: 6322851360508.28, NNZs: 843, Bias: -387212754.399567, T: 3444441, Avg. loss: 163978556688806313925593276612608.000000\n",
      "Total training time: 7.38 seconds.\n",
      "-- Epoch 1024\n",
      "Norm: 6042056958608.97, NNZs: 843, Bias: 309074035.898255, T: 3447808, Avg. loss: 163899390503451587629226085318656.000000\n",
      "Total training time: 7.39 seconds.\n",
      "-- Epoch 1025\n",
      "Norm: 6537237753238.94, NNZs: 843, Bias: -387041979.222886, T: 3451175, Avg. loss: 163818434434800619187235043934208.000000\n",
      "Total training time: 7.39 seconds.\n",
      "-- Epoch 1026\n",
      "Norm: 5799637760057.96, NNZs: 843, Bias: -155006158.400082, T: 3454542, Avg. loss: 163737990098434929029307864973312.000000\n",
      "Total training time: 7.40 seconds.\n",
      "-- Epoch 1027\n",
      "Norm: 5684885625251.78, NNZs: 843, Bias: 76916508.893658, T: 3457909, Avg. loss: 163657994262117459468597803876352.000000\n",
      "Total training time: 7.41 seconds.\n",
      "-- Epoch 1028\n",
      "Norm: 5580822230879.72, NNZs: 843, Bias: -154956348.103900, T: 3461276, Avg. loss: 163579565395722584846347821645824.000000\n",
      "Total training time: 7.41 seconds.\n",
      "-- Epoch 1029\n",
      "Norm: 5418955007856.49, NNZs: 843, Bias: 76859086.656992, T: 3464643, Avg. loss: 163500271117118839493726945935360.000000\n",
      "Total training time: 7.42 seconds.\n",
      "-- Epoch 1030\n",
      "Norm: 5325343061610.94, NNZs: 843, Bias: -154899663.191980, T: 3468010, Avg. loss: 163420724283905704132784529866752.000000\n",
      "Total training time: 7.43 seconds.\n",
      "-- Epoch 1031\n",
      "Norm: 5269207319502.18, NNZs: 843, Bias: 76804577.481361, T: 3471377, Avg. loss: 163341765618954545207636293320704.000000\n",
      "Total training time: 7.44 seconds.\n",
      "-- Epoch 1032\n",
      "Norm: 5371682669537.40, NNZs: 843, Bias: -154842481.352164, T: 3474744, Avg. loss: 163260821654490845388700003598336.000000\n",
      "Total training time: 7.44 seconds.\n",
      "-- Epoch 1033\n",
      "Norm: 5700896055044.05, NNZs: 843, Bias: -386374072.846872, T: 3478111, Avg. loss: 163181888727420640290532916985856.000000\n",
      "Total training time: 7.45 seconds.\n",
      "-- Epoch 1034\n",
      "Norm: 5793114399740.28, NNZs: 843, Bias: 308220288.080015, T: 3481478, Avg. loss: 163101981233376087574186813292544.000000\n",
      "Total training time: 7.46 seconds.\n",
      "-- Epoch 1035\n",
      "Norm: 5619363332060.17, NNZs: 843, Bias: 76691983.357179, T: 3484845, Avg. loss: 163022892181828227617222928891904.000000\n",
      "Total training time: 7.46 seconds.\n",
      "-- Epoch 1036\n",
      "Norm: 5591161262152.09, NNZs: 843, Bias: 73236090.627878, T: 3488212, Avg. loss: 162944378269197321384094278352896.000000\n",
      "Total training time: 7.47 seconds.\n",
      "-- Epoch 1037\n",
      "Norm: 6097798143994.89, NNZs: 843, Bias: -620807167.325858, T: 3491579, Avg. loss: 162866110541223459424307500810240.000000\n",
      "Total training time: 7.48 seconds.\n",
      "-- Epoch 1038\n",
      "Norm: 6180320567604.82, NNZs: 843, Bias: 535732333.437886, T: 3494946, Avg. loss: 162786913197119515438410448764928.000000\n",
      "Total training time: 7.49 seconds.\n",
      "-- Epoch 1039\n",
      "Norm: 5902932044966.68, NNZs: 843, Bias: -158087633.545556, T: 3498313, Avg. loss: 162707649012846313519315440107520.000000\n",
      "Total training time: 7.49 seconds.\n",
      "-- Epoch 1040\n",
      "Norm: 6298523714114.61, NNZs: 843, Bias: -851564048.633509, T: 3501680, Avg. loss: 162629184144028472128498594480128.000000\n",
      "Total training time: 7.50 seconds.\n",
      "-- Epoch 1041\n",
      "Norm: 6047604462045.10, NNZs: 843, Bias: 115917249.910240, T: 3505047, Avg. loss: 162550534471572938867226355695616.000000\n",
      "Total training time: 7.51 seconds.\n",
      "-- Epoch 1042\n",
      "Norm: 5563528775465.80, NNZs: 843, Bias: 346935619.073118, T: 3508414, Avg. loss: 162472556922971269960128041844736.000000\n",
      "Total training time: 7.52 seconds.\n",
      "-- Epoch 1043\n",
      "Norm: 5712278776331.54, NNZs: 843, Bias: 115840472.417996, T: 3511781, Avg. loss: 162395357796298304780254954127360.000000\n",
      "Total training time: 7.52 seconds.\n",
      "-- Epoch 1044\n",
      "Norm: 5631797811750.79, NNZs: 843, Bias: -98049130.586953, T: 3515148, Avg. loss: 162317421197865270098611369148416.000000\n",
      "Total training time: 7.53 seconds.\n",
      "-- Epoch 1045\n",
      "Norm: 5474844890086.19, NNZs: 843, Bias: 132856624.172822, T: 3518515, Avg. loss: 162239539688268607415008456867840.000000\n",
      "Total training time: 7.54 seconds.\n",
      "-- Epoch 1046\n",
      "Norm: 5412536175071.02, NNZs: 843, Bias: -98021061.909178, T: 3521882, Avg. loss: 162162290247257620953329823645696.000000\n",
      "Total training time: 7.54 seconds.\n",
      "-- Epoch 1047\n",
      "Norm: 5603579106997.97, NNZs: 843, Bias: 132769830.855321, T: 3525249, Avg. loss: 162084118401851980762276031365120.000000\n",
      "Total training time: 7.55 seconds.\n",
      "-- Epoch 1048\n",
      "Norm: 5470118241062.36, NNZs: 843, Bias: -98001278.330980, T: 3528616, Avg. loss: 162008106056785301272003969286144.000000\n",
      "Total training time: 7.56 seconds.\n",
      "-- Epoch 1049\n",
      "Norm: 5734494714318.62, NNZs: 843, Bias: -328659262.961140, T: 3531983, Avg. loss: 161929590067017649673140246151168.000000\n",
      "Total training time: 7.57 seconds.\n",
      "-- Epoch 1050\n",
      "Norm: 5743525693303.68, NNZs: 843, Bias: -17406982.993440, T: 3535350, Avg. loss: 161852079823149437511026090180608.000000\n",
      "Total training time: 7.57 seconds.\n",
      "-- Epoch 1051\n",
      "Norm: 5624566026731.01, NNZs: 843, Bias: -247980772.940321, T: 3538717, Avg. loss: 161775194865728796893859161309184.000000\n",
      "Total training time: 7.58 seconds.\n",
      "-- Epoch 1052\n",
      "Norm: 5538807058174.46, NNZs: 843, Bias: -17429345.067667, T: 3542084, Avg. loss: 161698334231707780525583362949120.000000\n",
      "Total training time: 7.59 seconds.\n",
      "-- Epoch 1053\n",
      "Norm: 5607104595736.23, NNZs: 843, Bias: 213015187.438146, T: 3545451, Avg. loss: 161622947322296927431512165449728.000000\n",
      "Total training time: 7.59 seconds.\n",
      "-- Epoch 1054\n",
      "Norm: 5848074869396.75, NNZs: 843, Bias: 443348435.606942, T: 3548818, Avg. loss: 161547669466994244070472029306880.000000\n",
      "Total training time: 7.60 seconds.\n",
      "-- Epoch 1055\n",
      "Norm: 5856175488451.98, NNZs: 843, Bias: 212887629.578334, T: 3552185, Avg. loss: 161471646051208666321188835819520.000000\n",
      "Total training time: 7.61 seconds.\n",
      "-- Epoch 1056\n",
      "Norm: 5488971197939.90, NNZs: 843, Bias: 199152687.071768, T: 3555552, Avg. loss: 161394684605033697562332275671040.000000\n",
      "Total training time: 7.62 seconds.\n",
      "-- Epoch 1057\n",
      "Norm: 5930927723736.08, NNZs: 843, Bias: -529554051.527078, T: 3558919, Avg. loss: 161317884870285548308025877463040.000000\n",
      "Total training time: 7.62 seconds.\n",
      "-- Epoch 1058\n",
      "Norm: 5483695176434.96, NNZs: 843, Bias: -299261356.714912, T: 3562286, Avg. loss: 161240649960553595126128939368448.000000\n",
      "Total training time: 7.63 seconds.\n",
      "-- Epoch 1059\n",
      "Norm: 5883426352090.18, NNZs: 843, Bias: 391177439.697414, T: 3565653, Avg. loss: 161165109393841986990128932847616.000000\n",
      "Total training time: 7.64 seconds.\n",
      "-- Epoch 1060\n",
      "Norm: 5304050089830.03, NNZs: 843, Bias: 75850945.480851, T: 3569020, Avg. loss: 161088934329895827411513609551872.000000\n",
      "Total training time: 7.64 seconds.\n",
      "-- Epoch 1061\n",
      "Norm: 5476314719869.26, NNZs: 843, Bias: 294964859.086742, T: 3572387, Avg. loss: 161014371248720142478025072574464.000000\n",
      "Total training time: 7.65 seconds.\n",
      "-- Epoch 1062\n",
      "Norm: 5324326823087.17, NNZs: 843, Bias: 64917800.213615, T: 3575754, Avg. loss: 160937205253466670619411380961280.000000\n",
      "Total training time: 7.66 seconds.\n",
      "-- Epoch 1063\n",
      "Norm: 5731926034476.41, NNZs: 843, Bias: -165021774.327321, T: 3579121, Avg. loss: 160861199991888520521870429126656.000000\n",
      "Total training time: 7.67 seconds.\n",
      "-- Epoch 1064\n",
      "Norm: 5401325625057.05, NNZs: 843, Bias: 64863629.527221, T: 3582488, Avg. loss: 160784389915371262419776883916800.000000\n",
      "Total training time: 7.67 seconds.\n",
      "-- Epoch 1065\n",
      "Norm: 5353923229524.42, NNZs: 843, Bias: -164961928.809245, T: 3585855, Avg. loss: 160708364124906109915562709614592.000000\n",
      "Total training time: 7.68 seconds.\n",
      "-- Epoch 1066\n",
      "Norm: 5411841922988.52, NNZs: 843, Bias: 64808598.817157, T: 3589222, Avg. loss: 160631398159980017531050570809344.000000\n",
      "Total training time: 7.69 seconds.\n",
      "-- Epoch 1067\n",
      "Norm: 5786030196127.52, NNZs: 843, Bias: -624296732.145493, T: 3592589, Avg. loss: 160556331776932649059549666344960.000000\n",
      "Total training time: 7.70 seconds.\n",
      "-- Epoch 1068\n",
      "Norm: 5394918009151.17, NNZs: 843, Bias: -176807468.087374, T: 3595956, Avg. loss: 160481976866380716080220780625920.000000\n",
      "Total training time: 7.70 seconds.\n",
      "-- Epoch 1069\n",
      "Norm: 5435705986455.21, NNZs: 843, Bias: -406363550.452543, T: 3599323, Avg. loss: 160407201337208143643212729286656.000000\n",
      "Total training time: 7.71 seconds.\n",
      "-- Epoch 1070\n",
      "Norm: 5116572258093.38, NNZs: 843, Bias: -176751128.560617, T: 3602690, Avg. loss: 160331708166139368410940987932672.000000\n",
      "Total training time: 7.72 seconds.\n",
      "-- Epoch 1071\n",
      "Norm: 5052342448795.96, NNZs: 843, Bias: 52757013.126771, T: 3606057, Avg. loss: 160258109141851768076868102651904.000000\n",
      "Total training time: 7.72 seconds.\n",
      "-- Epoch 1072\n",
      "Norm: 5562245662198.78, NNZs: 843, Bias: -635544500.491307, T: 3609424, Avg. loss: 160182903132283618037097977348096.000000\n",
      "Total training time: 7.73 seconds.\n",
      "-- Epoch 1073\n",
      "Norm: 5254031882175.97, NNZs: 843, Bias: 170798494.331843, T: 3612791, Avg. loss: 160108874770622978682954115448832.000000\n",
      "Total training time: 7.74 seconds.\n",
      "-- Epoch 1074\n",
      "Norm: 5207386082428.38, NNZs: 843, Bias: 191363011.347337, T: 3616158, Avg. loss: 160034430489461952544178012422144.000000\n",
      "Total training time: 7.75 seconds.\n",
      "-- Epoch 1075\n",
      "Norm: 5441728888547.74, NNZs: 843, Bias: -176384150.407704, T: 3619525, Avg. loss: 159960494826321662060869418221568.000000\n",
      "Total training time: 7.75 seconds.\n",
      "-- Epoch 1076\n",
      "Norm: 5791876413391.98, NNZs: 843, Bias: 52854073.729769, T: 3622892, Avg. loss: 159886395610359521835518619811840.000000\n",
      "Total training time: 7.76 seconds.\n",
      "-- Epoch 1077\n",
      "Norm: 5397265000015.34, NNZs: 843, Bias: -176323811.416620, T: 3626259, Avg. loss: 159810689387230364536165011292160.000000\n",
      "Total training time: 7.77 seconds.\n",
      "-- Epoch 1078\n",
      "Norm: 5471503035321.97, NNZs: 843, Bias: 52805943.348322, T: 3629626, Avg. loss: 159738002981930288883683519627264.000000\n",
      "Total training time: 7.77 seconds.\n",
      "-- Epoch 1079\n",
      "Norm: 5553409097525.50, NNZs: 843, Bias: 281829070.913418, T: 3632993, Avg. loss: 159663294460807063214212591386624.000000\n",
      "Total training time: 7.78 seconds.\n",
      "-- Epoch 1080\n",
      "Norm: 5619443804153.79, NNZs: 843, Bias: 52750726.274696, T: 3636360, Avg. loss: 159588768459352737778822226640896.000000\n",
      "Total training time: 7.79 seconds.\n",
      "-- Epoch 1081\n",
      "Norm: 5542059848504.54, NNZs: 843, Bias: 281674009.001278, T: 3639727, Avg. loss: 159514079996192629491641480118272.000000\n",
      "Total training time: 7.80 seconds.\n",
      "-- Epoch 1082\n",
      "Norm: 5229419791972.44, NNZs: 843, Bias: 52700862.737070, T: 3643094, Avg. loss: 159440945256711925532396747751424.000000\n",
      "Total training time: 7.80 seconds.\n",
      "-- Epoch 1083\n",
      "Norm: 5282686212900.77, NNZs: 843, Bias: 68148279.497688, T: 3646461, Avg. loss: 159366101561419896555002098876416.000000\n",
      "Total training time: 7.81 seconds.\n",
      "-- Epoch 1084\n",
      "Norm: 5706159922662.38, NNZs: 843, Bias: 296904730.459309, T: 3649828, Avg. loss: 159294108688623832847438730756096.000000\n",
      "Total training time: 7.82 seconds.\n",
      "-- Epoch 1085\n",
      "Norm: 5376111162383.13, NNZs: 843, Bias: 19147093.241764, T: 3653195, Avg. loss: 159220962783278356640787779813376.000000\n",
      "Total training time: 7.82 seconds.\n",
      "-- Epoch 1086\n",
      "Norm: 5500644520920.48, NNZs: 843, Bias: -209552955.235639, T: 3656562, Avg. loss: 159150349415113172929870437548032.000000\n",
      "Total training time: 7.83 seconds.\n",
      "-- Epoch 1087\n",
      "Norm: 5264003266659.53, NNZs: 843, Bias: 19107085.028591, T: 3659929, Avg. loss: 159076139419395225059414352330752.000000\n",
      "Total training time: 7.84 seconds.\n",
      "-- Epoch 1088\n",
      "Norm: 5412567799732.08, NNZs: 843, Bias: 247661876.236072, T: 3663296, Avg. loss: 159002223507675643405028550508544.000000\n",
      "Total training time: 7.85 seconds.\n",
      "-- Epoch 1089\n",
      "Norm: 5176715676908.56, NNZs: 843, Bias: 89502115.360865, T: 3666663, Avg. loss: 158929518342478233582212392943616.000000\n",
      "Total training time: 7.85 seconds.\n",
      "-- Epoch 1090\n",
      "Norm: 5070112343952.88, NNZs: 843, Bias: 7932930.594451, T: 3670030, Avg. loss: 158856244667997679409200025829376.000000\n",
      "Total training time: 7.86 seconds.\n",
      "-- Epoch 1091\n",
      "Norm: 5349658688198.10, NNZs: 843, Bias: 236335291.839217, T: 3673397, Avg. loss: 158782942451644242804538959986688.000000\n",
      "Total training time: 7.87 seconds.\n",
      "-- Epoch 1092\n",
      "Norm: 5528130608993.58, NNZs: 843, Bias: 7898748.077176, T: 3676764, Avg. loss: 158710113363658628826394055933952.000000\n",
      "Total training time: 7.88 seconds.\n",
      "-- Epoch 1093\n",
      "Norm: 5092287919856.31, NNZs: 843, Bias: -220431863.536847, T: 3680131, Avg. loss: 158636643513444668714344714862592.000000\n",
      "Total training time: 7.88 seconds.\n",
      "-- Epoch 1094\n",
      "Norm: 5214641506518.41, NNZs: 843, Bias: 7864766.625025, T: 3683498, Avg. loss: 158563640012529557640293438717952.000000\n",
      "Total training time: 7.89 seconds.\n",
      "-- Epoch 1095\n",
      "Norm: 5432595077602.33, NNZs: 843, Bias: 319558311.202252, T: 3686865, Avg. loss: 158490278918403988657142205775872.000000\n",
      "Total training time: 7.90 seconds.\n",
      "-- Epoch 1096\n",
      "Norm: 5471357499466.50, NNZs: 843, Bias: 40138952.682667, T: 3690232, Avg. loss: 158417480250521841189950651891712.000000\n",
      "Total training time: 7.90 seconds.\n",
      "-- Epoch 1097\n",
      "Norm: 5685557191368.11, NNZs: 843, Bias: -187990898.491219, T: 3693599, Avg. loss: 158344220198320112576534404923392.000000\n",
      "Total training time: 7.91 seconds.\n",
      "-- Epoch 1098\n",
      "Norm: 5187808809034.27, NNZs: 843, Bias: -81610325.936784, T: 3696966, Avg. loss: 158272751883820843900083564969984.000000\n",
      "Total training time: 7.92 seconds.\n",
      "-- Epoch 1099\n",
      "Norm: 4989816155506.77, NNZs: 843, Bias: 146397498.266289, T: 3700333, Avg. loss: 158200703599205933460569309839360.000000\n",
      "Total training time: 7.93 seconds.\n",
      "-- Epoch 1100\n",
      "Norm: 4991830270541.30, NNZs: 843, Bias: -81599958.858344, T: 3703700, Avg. loss: 158129517646778831177511350566912.000000\n",
      "Total training time: 7.93 seconds.\n",
      "-- Epoch 1101\n",
      "Norm: 5359818664561.87, NNZs: 843, Bias: 33225186.788287, T: 3707067, Avg. loss: 158058102380742451193960168685568.000000\n",
      "Total training time: 7.94 seconds.\n",
      "-- Epoch 1102\n",
      "Norm: 5711951369285.25, NNZs: 843, Bias: -181967983.063317, T: 3710434, Avg. loss: 157986682817263940958365190979584.000000\n",
      "Total training time: 7.95 seconds.\n",
      "-- Epoch 1103\n",
      "Norm: 5417850062957.50, NNZs: 843, Bias: 45855961.403452, T: 3713801, Avg. loss: 157916138656457761481687971135488.000000\n",
      "Total training time: 7.95 seconds.\n",
      "-- Epoch 1104\n",
      "Norm: 5890719678702.55, NNZs: 843, Bias: -12034642.017393, T: 3717168, Avg. loss: 157845840738347973842555474280448.000000\n",
      "Total training time: 7.96 seconds.\n",
      "-- Epoch 1105\n",
      "Norm: 6289990625007.56, NNZs: 843, Bias: -239740467.388548, T: 3720535, Avg. loss: 157775215964246704203821473595392.000000\n",
      "Total training time: 7.97 seconds.\n",
      "-- Epoch 1106\n",
      "Norm: 6232404572893.39, NNZs: 843, Bias: -12056169.931952, T: 3723902, Avg. loss: 157703619824724343510004234452992.000000\n",
      "Total training time: 7.98 seconds.\n",
      "-- Epoch 1107\n",
      "Norm: 5549249240343.90, NNZs: 843, Bias: -273449260.206634, T: 3727269, Avg. loss: 157632864100650342864596318027776.000000\n",
      "Total training time: 7.98 seconds.\n",
      "-- Epoch 1108\n",
      "Norm: 5741631828277.38, NNZs: 843, Bias: 409216557.743212, T: 3730636, Avg. loss: 157562502475001104870113382760448.000000\n",
      "Total training time: 7.99 seconds.\n",
      "-- Epoch 1109\n",
      "Norm: 6250803953248.28, NNZs: 843, Bias: -273351990.489506, T: 3734003, Avg. loss: 157492112272912236008047338586112.000000\n",
      "Total training time: 8.00 seconds.\n",
      "-- Epoch 1110\n",
      "Norm: 5718712073136.45, NNZs: 843, Bias: -295212692.474499, T: 3737370, Avg. loss: 157420785835189188200895828983808.000000\n",
      "Total training time: 8.01 seconds.\n",
      "-- Epoch 1111\n",
      "Norm: 5436944754098.60, NNZs: 843, Bias: -67778079.807200, T: 3740737, Avg. loss: 157350163867558718473726645501952.000000\n",
      "Total training time: 8.01 seconds.\n",
      "-- Epoch 1112\n",
      "Norm: 5397105542927.69, NNZs: 843, Bias: 220546623.857032, T: 3744104, Avg. loss: 157279514118895504975513419513856.000000\n",
      "Total training time: 8.02 seconds.\n",
      "-- Epoch 1113\n",
      "Norm: 5387608501364.42, NNZs: 843, Bias: -6801799.111320, T: 3747471, Avg. loss: 157209524057998114276412454076416.000000\n",
      "Total training time: 8.03 seconds.\n",
      "-- Epoch 1114\n",
      "Norm: 5238060771873.86, NNZs: 843, Bias: 220422243.832002, T: 3750838, Avg. loss: 157138591081246556847306168074240.000000\n",
      "Total training time: 8.03 seconds.\n",
      "-- Epoch 1115\n",
      "Norm: 5025934694202.30, NNZs: 843, Bias: -6823527.808924, T: 3754205, Avg. loss: 157068178662967253340278872866816.000000\n",
      "Total training time: 8.04 seconds.\n",
      "-- Epoch 1116\n",
      "Norm: 5062399973290.77, NNZs: 843, Bias: -233960800.796049, T: 3757572, Avg. loss: 156996675370765452227912262483968.000000\n",
      "Total training time: 8.05 seconds.\n",
      "-- Epoch 1117\n",
      "Norm: 5070869718170.73, NNZs: 843, Bias: -260317941.322265, T: 3760939, Avg. loss: 156926944273324721122732075384832.000000\n",
      "Total training time: 8.06 seconds.\n",
      "-- Epoch 1118\n",
      "Norm: 5134967355549.02, NNZs: 843, Bias: -33250650.263307, T: 3764306, Avg. loss: 156857909317775283385598266572800.000000\n",
      "Total training time: 8.06 seconds.\n",
      "-- Epoch 1119\n",
      "Norm: 5229934821843.27, NNZs: 843, Bias: 193721463.296170, T: 3767673, Avg. loss: 156787044622465831334239828705280.000000\n",
      "Total training time: 8.07 seconds.\n",
      "-- Epoch 1120\n",
      "Norm: 5249081815680.10, NNZs: 843, Bias: -33260243.475331, T: 3771040, Avg. loss: 156718222303071187545170548621312.000000\n",
      "Total training time: 8.08 seconds.\n",
      "-- Epoch 1121\n",
      "Norm: 5466437940632.32, NNZs: 843, Bias: 193610153.613899, T: 3774407, Avg. loss: 156648755079865832626258234572800.000000\n",
      "Total training time: 8.08 seconds.\n",
      "-- Epoch 1122\n",
      "Norm: 5594713286262.73, NNZs: 843, Bias: -486921320.446836, T: 3777774, Avg. loss: 156578239570463050894634240704512.000000\n",
      "Total training time: 8.09 seconds.\n",
      "-- Epoch 1123\n",
      "Norm: 5950417961477.31, NNZs: 843, Bias: -260051359.068834, T: 3781141, Avg. loss: 156507948739378595536396470976512.000000\n",
      "Total training time: 8.10 seconds.\n",
      "-- Epoch 1124\n",
      "Norm: 5321415016005.77, NNZs: 843, Bias: -33283960.408674, T: 3784508, Avg. loss: 156437847487870235767298701918208.000000\n",
      "Total training time: 8.11 seconds.\n",
      "-- Epoch 1125\n",
      "Norm: 5111454435317.37, NNZs: 843, Bias: 193381379.303517, T: 3787875, Avg. loss: 156369525516668719707901308960768.000000\n",
      "Total training time: 8.11 seconds.\n",
      "-- Epoch 1126\n",
      "Norm: 5597818118733.67, NNZs: 843, Bias: -33296753.253746, T: 3791242, Avg. loss: 156299147459969381523799568023552.000000\n",
      "Total training time: 8.12 seconds.\n",
      "-- Epoch 1127\n",
      "Norm: 4949462071715.01, NNZs: 843, Bias: 193272425.297761, T: 3794609, Avg. loss: 156231228021348948338904156602368.000000\n",
      "Total training time: 8.13 seconds.\n",
      "-- Epoch 1128\n",
      "Norm: 4904762427742.12, NNZs: 843, Bias: -192781424.409025, T: 3797976, Avg. loss: 156161138238659355230686712692736.000000\n",
      "Total training time: 8.13 seconds.\n",
      "-- Epoch 1129\n",
      "Norm: 5104038637982.91, NNZs: 843, Bias: -419223883.204709, T: 3801343, Avg. loss: 156093401190883345730856848195584.000000\n",
      "Total training time: 8.14 seconds.\n",
      "-- Epoch 1130\n",
      "Norm: 4994067648322.34, NNZs: 843, Bias: 260125103.185241, T: 3804710, Avg. loss: 156024468689764213476149063319552.000000\n",
      "Total training time: 8.15 seconds.\n",
      "-- Epoch 1131\n",
      "Norm: 5166975567162.13, NNZs: 843, Bias: 33681087.231163, T: 3808077, Avg. loss: 155955300909467901309449505079296.000000\n",
      "Total training time: 8.16 seconds.\n",
      "-- Epoch 1132\n",
      "Norm: 5806963478945.01, NNZs: 843, Bias: -192659872.596541, T: 3811444, Avg. loss: 155884421633282372160387178037248.000000\n",
      "Total training time: 8.16 seconds.\n",
      "-- Epoch 1133\n",
      "Norm: 5614066544956.72, NNZs: 843, Bias: 33640578.808204, T: 3814811, Avg. loss: 155816248151595344111471572287488.000000\n",
      "Total training time: 8.17 seconds.\n",
      "-- Epoch 1134\n",
      "Norm: 5023835390065.11, NNZs: 843, Bias: -192602797.460851, T: 3818178, Avg. loss: 155747279345209138505630657544192.000000\n",
      "Total training time: 8.18 seconds.\n",
      "-- Epoch 1135\n",
      "Norm: 5008363578476.96, NNZs: 843, Bias: -71072944.199514, T: 3821545, Avg. loss: 155678795587243597583111424049152.000000\n",
      "Total training time: 8.19 seconds.\n",
      "-- Epoch 1136\n",
      "Norm: 5260653340194.28, NNZs: 843, Bias: 273133186.792180, T: 3824912, Avg. loss: 155611691960635751510429040377856.000000\n",
      "Total training time: 8.19 seconds.\n",
      "-- Epoch 1137\n",
      "Norm: 5254958968837.44, NNZs: 843, Bias: 46983059.368994, T: 3828279, Avg. loss: 155542866743856412348044382568448.000000\n",
      "Total training time: 8.20 seconds.\n",
      "-- Epoch 1138\n",
      "Norm: 5241691026245.75, NNZs: 843, Bias: -179059432.472475, T: 3831646, Avg. loss: 155474310993753897878447357689856.000000\n",
      "Total training time: 8.21 seconds.\n",
      "-- Epoch 1139\n",
      "Norm: 5645848776225.33, NNZs: 843, Bias: 46942785.961215, T: 3835013, Avg. loss: 155406584604985544966769504944128.000000\n",
      "Total training time: 8.21 seconds.\n",
      "-- Epoch 1140\n",
      "Norm: 5065899797915.54, NNZs: 843, Bias: 272842308.548701, T: 3838380, Avg. loss: 155337340589474088909254147702784.000000\n",
      "Total training time: 8.22 seconds.\n",
      "-- Epoch 1141\n",
      "Norm: 5002565037483.34, NNZs: 843, Bias: 46896178.513074, T: 3841747, Avg. loss: 155269429346061103409854100275200.000000\n",
      "Total training time: 8.23 seconds.\n",
      "-- Epoch 1142\n",
      "Norm: 4890736805697.75, NNZs: 843, Bias: -123724989.139799, T: 3845114, Avg. loss: 155202088774085105908839081836544.000000\n",
      "Total training time: 8.24 seconds.\n",
      "-- Epoch 1143\n",
      "Norm: 4910063902426.91, NNZs: 843, Bias: 102067578.795453, T: 3848481, Avg. loss: 155134027703511621588284447129600.000000\n",
      "Total training time: 8.24 seconds.\n",
      "-- Epoch 1144\n",
      "Norm: 5102038445344.57, NNZs: 843, Bias: -123688632.983662, T: 3851848, Avg. loss: 155068317113531152407437577814016.000000\n",
      "Total training time: 8.25 seconds.\n",
      "-- Epoch 1145\n",
      "Norm: 4978896732837.68, NNZs: 843, Bias: 102003777.849133, T: 3855215, Avg. loss: 155000344470645044816022800957440.000000\n",
      "Total training time: 8.26 seconds.\n",
      "-- Epoch 1146\n",
      "Norm: 5107276942287.75, NNZs: 843, Bias: 105037688.473526, T: 3858582, Avg. loss: 154933198523644054434770004738048.000000\n",
      "Total training time: 8.26 seconds.\n",
      "-- Epoch 1147\n",
      "Norm: 6192476287118.79, NNZs: 843, Bias: -283887234.304062, T: 3861949, Avg. loss: 154865827149543815684142267367424.000000\n",
      "Total training time: 8.27 seconds.\n",
      "-- Epoch 1148\n",
      "Norm: 5330033388449.42, NNZs: 843, Bias: -91192979.963196, T: 3865316, Avg. loss: 154799177304198585829438582161408.000000\n",
      "Total training time: 8.28 seconds.\n",
      "-- Epoch 1149\n",
      "Norm: 5221714680076.03, NNZs: 843, Bias: 134296717.684498, T: 3868683, Avg. loss: 154730959480487866980814003634176.000000\n",
      "Total training time: 8.29 seconds.\n",
      "-- Epoch 1150\n",
      "Norm: 5432669447949.28, NNZs: 843, Bias: -91173558.070385, T: 3872050, Avg. loss: 154662876837016767709011555385344.000000\n",
      "Total training time: 8.29 seconds.\n",
      "-- Epoch 1151\n",
      "Norm: 5616822471492.27, NNZs: 843, Bias: -316545787.732489, T: 3875417, Avg. loss: 154595368798775239664136366325760.000000\n",
      "Total training time: 8.30 seconds.\n",
      "-- Epoch 1152\n",
      "Norm: 5543099854068.08, NNZs: 843, Bias: -184794484.549474, T: 3878784, Avg. loss: 154527429962715028763404595625984.000000\n",
      "Total training time: 8.31 seconds.\n",
      "-- Epoch 1153\n",
      "Norm: 5202282664728.85, NNZs: 843, Bias: 40515268.580556, T: 3882151, Avg. loss: 154461367936905139873466286080000.000000\n",
      "Total training time: 8.32 seconds.\n",
      "-- Epoch 1154\n",
      "Norm: 6175000152419.65, NNZs: 843, Bias: -280856093.515084, T: 3885518, Avg. loss: 154393653799604157306032927801344.000000\n",
      "Total training time: 8.32 seconds.\n",
      "-- Epoch 1155\n",
      "Norm: 5625147478776.61, NNZs: 843, Bias: 114130980.310612, T: 3888885, Avg. loss: 154327641542930175731325342842880.000000\n",
      "Total training time: 8.33 seconds.\n",
      "-- Epoch 1156\n",
      "Norm: 6839415716172.07, NNZs: 843, Bias: 360801334.019225, T: 3892252, Avg. loss: 154260814302164267679542111371264.000000\n",
      "Total training time: 8.34 seconds.\n",
      "-- Epoch 1157\n",
      "Norm: 6181496995030.38, NNZs: 843, Bias: 135625231.371830, T: 3895619, Avg. loss: 154194727851171835709469136781312.000000\n",
      "Total training time: 8.34 seconds.\n",
      "-- Epoch 1158\n",
      "Norm: 6022563423684.71, NNZs: 843, Bias: -271316597.145335, T: 3898986, Avg. loss: 154127085148736786297126198968320.000000\n",
      "Total training time: 8.35 seconds.\n",
      "-- Epoch 1159\n",
      "Norm: 6170325757399.07, NNZs: 843, Bias: -46279266.337721, T: 3902353, Avg. loss: 154061785000566649640618878304256.000000\n",
      "Total training time: 8.36 seconds.\n",
      "-- Epoch 1160\n",
      "Norm: 6450913636093.34, NNZs: 843, Bias: 178662459.830320, T: 3905720, Avg. loss: 153996309200328419870992339304448.000000\n",
      "Total training time: 8.37 seconds.\n",
      "-- Epoch 1161\n",
      "Norm: 5834547291317.86, NNZs: 843, Bias: 95622581.267229, T: 3909087, Avg. loss: 153931154678882314259711307808768.000000\n",
      "Total training time: 8.37 seconds.\n",
      "-- Epoch 1162\n",
      "Norm: 6145302670445.37, NNZs: 843, Bias: -578951235.180698, T: 3912454, Avg. loss: 153864299015239686772092703866880.000000\n",
      "Total training time: 8.38 seconds.\n",
      "-- Epoch 1163\n",
      "Norm: 5669606896367.21, NNZs: 843, Bias: 95557971.051630, T: 3915821, Avg. loss: 153799416428471914762238831886336.000000\n",
      "Total training time: 8.39 seconds.\n",
      "-- Epoch 1164\n",
      "Norm: 5656095044994.14, NNZs: 843, Bias: 322333631.101906, T: 3919188, Avg. loss: 153733657682415149063410596970496.000000\n",
      "Total training time: 8.39 seconds.\n",
      "-- Epoch 1165\n",
      "Norm: 6009243464990.01, NNZs: 843, Bias: 97558011.546112, T: 3922555, Avg. loss: 153668838741204109996580197629952.000000\n",
      "Total training time: 8.40 seconds.\n",
      "-- Epoch 1166\n",
      "Norm: 6252843624763.69, NNZs: 843, Bias: 322180843.870899, T: 3925922, Avg. loss: 153602560254354709573357921107968.000000\n",
      "Total training time: 8.41 seconds.\n",
      "-- Epoch 1167\n",
      "Norm: 6118147041487.01, NNZs: 843, Bias: -351717734.877005, T: 3929289, Avg. loss: 153535908724875536782921486565376.000000\n",
      "Total training time: 8.42 seconds.\n",
      "-- Epoch 1168\n",
      "Norm: 5872491318877.02, NNZs: 843, Bias: -84476355.702618, T: 3932656, Avg. loss: 153471779253729143910349494288384.000000\n",
      "Total training time: 8.42 seconds.\n",
      "-- Epoch 1169\n",
      "Norm: 5788729940856.38, NNZs: 843, Bias: -157334242.585913, T: 3936023, Avg. loss: 153406351418034612898182518013952.000000\n",
      "Total training time: 8.43 seconds.\n",
      "-- Epoch 1170\n",
      "Norm: 5473134613054.81, NNZs: 843, Bias: 67149568.583283, T: 3939390, Avg. loss: 153340744195153253147606234693632.000000\n",
      "Total training time: 8.44 seconds.\n",
      "-- Epoch 1171\n",
      "Norm: 5454173613388.68, NNZs: 843, Bias: -157292062.844576, T: 3942757, Avg. loss: 153276640122017074952646441304064.000000\n",
      "Total training time: 8.44 seconds.\n",
      "-- Epoch 1172\n",
      "Norm: 5188494365413.21, NNZs: 843, Bias: 67094771.300549, T: 3946124, Avg. loss: 153211203696267817575994426916864.000000\n",
      "Total training time: 8.45 seconds.\n",
      "-- Epoch 1173\n",
      "Norm: 5291461021988.62, NNZs: 843, Bias: -157249577.559000, T: 3949491, Avg. loss: 153146499062478445293087368413184.000000\n",
      "Total training time: 8.46 seconds.\n",
      "-- Epoch 1174\n",
      "Norm: 5038003662503.11, NNZs: 843, Bias: 67044803.694159, T: 3952858, Avg. loss: 153081140451114715696507719254016.000000\n",
      "Total training time: 8.47 seconds.\n",
      "-- Epoch 1175\n",
      "Norm: 5177731274583.59, NNZs: 843, Bias: -157201535.462982, T: 3956225, Avg. loss: 153015679161194505532984923258880.000000\n",
      "Total training time: 8.47 seconds.\n",
      "-- Epoch 1176\n",
      "Norm: 5246334045561.44, NNZs: 843, Bias: 20592876.973589, T: 3959592, Avg. loss: 152950348470408869212615327350784.000000\n",
      "Total training time: 8.48 seconds.\n",
      "-- Epoch 1177\n",
      "Norm: 5966452466512.06, NNZs: 843, Bias: -208058624.382610, T: 3962959, Avg. loss: 152885951906639153996388146085888.000000\n",
      "Total training time: 8.49 seconds.\n",
      "-- Epoch 1178\n",
      "Norm: 5647887034147.52, NNZs: 843, Bias: 512257635.975579, T: 3966326, Avg. loss: 152822054043412626429000170864640.000000\n",
      "Total training time: 8.50 seconds.\n",
      "-- Epoch 1179\n",
      "Norm: 5253528022780.72, NNZs: 843, Bias: -159960601.330558, T: 3969693, Avg. loss: 152757625775695326551558280183808.000000\n",
      "Total training time: 8.50 seconds.\n",
      "-- Epoch 1180\n",
      "Norm: 5243111869970.06, NNZs: 843, Bias: -346925342.070722, T: 3973060, Avg. loss: 152692569695971826090027109056512.000000\n",
      "Total training time: 8.51 seconds.\n",
      "-- Epoch 1181\n",
      "Norm: 5078285996046.36, NNZs: 843, Bias: -122923521.903674, T: 3976427, Avg. loss: 152628324457504967678900006027264.000000\n",
      "Total training time: 8.52 seconds.\n",
      "-- Epoch 1182\n",
      "Norm: 5137430787815.65, NNZs: 843, Bias: -346795267.268971, T: 3979794, Avg. loss: 152563438877052623938818253258752.000000\n",
      "Total training time: 8.52 seconds.\n",
      "-- Epoch 1183\n",
      "Norm: 5059652557508.72, NNZs: 843, Bias: -122892508.617122, T: 3983161, Avg. loss: 152498661060329842909372236169216.000000\n",
      "Total training time: 8.53 seconds.\n",
      "-- Epoch 1184\n",
      "Norm: 5048282296785.29, NNZs: 843, Bias: -346671453.612104, T: 3986528, Avg. loss: 152434857565267041160307083837440.000000\n",
      "Total training time: 8.54 seconds.\n",
      "-- Epoch 1185\n",
      "Norm: 5351268634721.19, NNZs: 843, Bias: -122862234.292834, T: 3989895, Avg. loss: 152370117661567289185269945729024.000000\n",
      "Total training time: 8.55 seconds.\n",
      "-- Epoch 1186\n",
      "Norm: 5445006668991.72, NNZs: 843, Bias: 100855882.748099, T: 3993262, Avg. loss: 152305773562133935274388008992768.000000\n",
      "Total training time: 8.55 seconds.\n",
      "-- Epoch 1187\n",
      "Norm: 5544777614901.59, NNZs: 843, Bias: 274263985.057864, T: 3996629, Avg. loss: 152241965544370515156049938350080.000000\n",
      "Total training time: 8.56 seconds.\n",
      "-- Epoch 1188\n",
      "Norm: 5561918360730.35, NNZs: 843, Bias: 83647141.001936, T: 3999996, Avg. loss: 152178052194166687865309396729856.000000\n",
      "Total training time: 8.57 seconds.\n",
      "-- Epoch 1189\n",
      "Norm: 5695750927859.19, NNZs: 843, Bias: -139940771.918662, T: 4003363, Avg. loss: 152113786818403967167112857780224.000000\n",
      "Total training time: 8.58 seconds.\n",
      "-- Epoch 1190\n",
      "Norm: 5759260835596.95, NNZs: 843, Bias: 83591311.876542, T: 4006730, Avg. loss: 152050217645301771336893937483776.000000\n",
      "Total training time: 8.58 seconds.\n",
      "-- Epoch 1191\n",
      "Norm: 6001481854185.31, NNZs: 843, Bias: 307027852.697285, T: 4010097, Avg. loss: 151986001662570616088940903399424.000000\n",
      "Total training time: 8.59 seconds.\n",
      "-- Epoch 1192\n",
      "Norm: 6279671568345.90, NNZs: 843, Bias: 83534148.983063, T: 4013464, Avg. loss: 151922476415364135189814062350336.000000\n",
      "Total training time: 8.60 seconds.\n",
      "-- Epoch 1193\n",
      "Norm: 6091756386283.20, NNZs: 843, Bias: 306878706.236210, T: 4016831, Avg. loss: 151858420791333881600255492882432.000000\n",
      "Total training time: 8.60 seconds.\n",
      "-- Epoch 1194\n",
      "Norm: 5622699740846.27, NNZs: 843, Bias: -36778500.249561, T: 4020198, Avg. loss: 151795235591342083052428626755584.000000\n",
      "Total training time: 8.61 seconds.\n",
      "-- Epoch 1195\n",
      "Norm: 5724240303130.97, NNZs: 843, Bias: 186497384.749470, T: 4023565, Avg. loss: 151732612108140954159345335009280.000000\n",
      "Total training time: 8.62 seconds.\n",
      "-- Epoch 1196\n",
      "Norm: 5668401745017.82, NNZs: 843, Bias: -483248203.326579, T: 4026932, Avg. loss: 151670438148820874752771057778688.000000\n",
      "Total training time: 8.63 seconds.\n",
      "-- Epoch 1197\n",
      "Norm: 5803984655032.45, NNZs: 843, Bias: 186397920.678875, T: 4030299, Avg. loss: 151607827719457775289086249009152.000000\n",
      "Total training time: 8.63 seconds.\n",
      "-- Epoch 1198\n",
      "Norm: 5778414561395.83, NNZs: 843, Bias: -71666754.856278, T: 4033666, Avg. loss: 151544312908089322440049280679936.000000\n",
      "Total training time: 8.64 seconds.\n",
      "-- Epoch 1199\n",
      "Norm: 5396349710833.65, NNZs: 843, Bias: -294757192.312378, T: 4037033, Avg. loss: 151481747761686515548229927436288.000000\n",
      "Total training time: 8.65 seconds.\n",
      "-- Epoch 1200\n",
      "Norm: 5381452310030.15, NNZs: 843, Bias: -46961514.981995, T: 4040400, Avg. loss: 151418185782641570239045110333440.000000\n",
      "Total training time: 8.65 seconds.\n",
      "-- Epoch 1201\n",
      "Norm: 5320389063481.85, NNZs: 843, Bias: 176035409.593973, T: 4043767, Avg. loss: 151355753948928481677819575795712.000000\n",
      "Total training time: 8.66 seconds.\n",
      "-- Epoch 1202\n",
      "Norm: 5918417283372.83, NNZs: 843, Bias: 398940332.344500, T: 4047134, Avg. loss: 151293162242582054488789080866816.000000\n",
      "Total training time: 8.67 seconds.\n",
      "-- Epoch 1203\n",
      "Norm: 5587882654855.17, NNZs: 843, Bias: -28070522.115773, T: 4050501, Avg. loss: 151230024770083655648051466338304.000000\n",
      "Total training time: 8.68 seconds.\n",
      "-- Epoch 1204\n",
      "Norm: 6446072718951.75, NNZs: 843, Bias: 194783905.545378, T: 4053868, Avg. loss: 151167227014104815875651014754304.000000\n",
      "Total training time: 8.68 seconds.\n",
      "-- Epoch 1205\n",
      "Norm: 5861353652464.96, NNZs: 843, Bias: -28082435.380076, T: 4057235, Avg. loss: 151104522577935691795573122269184.000000\n",
      "Total training time: 8.69 seconds.\n",
      "-- Epoch 1206\n",
      "Norm: 5871712957273.28, NNZs: 843, Bias: -234218514.493348, T: 4060602, Avg. loss: 151042108227819222529476908810240.000000\n",
      "Total training time: 8.70 seconds.\n",
      "-- Epoch 1207\n",
      "Norm: 5980346890488.57, NNZs: 843, Bias: -11462788.701720, T: 4063969, Avg. loss: 150979245245871911737906524323840.000000\n",
      "Total training time: 8.70 seconds.\n",
      "-- Epoch 1208\n",
      "Norm: 6028996760748.95, NNZs: 843, Bias: 211198098.813976, T: 4067336, Avg. loss: 150917196076281325445217110720512.000000\n",
      "Total training time: 8.71 seconds.\n",
      "-- Epoch 1209\n",
      "Norm: 6156690419233.51, NNZs: 843, Bias: -11490586.787329, T: 4070703, Avg. loss: 150854909847410628419712836435968.000000\n",
      "Total training time: 8.72 seconds.\n",
      "-- Epoch 1210\n",
      "Norm: 6266687042667.47, NNZs: 843, Bias: -234081433.847583, T: 4074070, Avg. loss: 150793129521615927890011031076864.000000\n",
      "Total training time: 8.73 seconds.\n",
      "-- Epoch 1211\n",
      "Norm: 6134713869576.67, NNZs: 843, Bias: -11510647.774403, T: 4077437, Avg. loss: 150730817550951693796454195789824.000000\n",
      "Total training time: 8.73 seconds.\n",
      "-- Epoch 1212\n",
      "Norm: 6124221132058.43, NNZs: 843, Bias: -93600367.531395, T: 4080804, Avg. loss: 150668576010185206059028398997504.000000\n",
      "Total training time: 8.74 seconds.\n",
      "-- Epoch 1213\n",
      "Norm: 6003099256786.01, NNZs: 843, Bias: -316035566.859316, T: 4084171, Avg. loss: 150606634974404527494789674827776.000000\n",
      "Total training time: 8.75 seconds.\n",
      "-- Epoch 1214\n",
      "Norm: 5759550585317.75, NNZs: 843, Bias: -68887104.874762, T: 4087538, Avg. loss: 150544990510344932780320634175488.000000\n",
      "Total training time: 8.76 seconds.\n",
      "-- Epoch 1215\n",
      "Norm: 5687360552944.77, NNZs: 843, Bias: 153471091.030368, T: 4090905, Avg. loss: 150482733265706922574250728488960.000000\n",
      "Total training time: 8.76 seconds.\n",
      "-- Epoch 1216\n",
      "Norm: 5408846170950.52, NNZs: 843, Bias: 88236006.253127, T: 4094272, Avg. loss: 150421802244106069362858912645120.000000\n",
      "Total training time: 8.77 seconds.\n",
      "-- Epoch 1217\n",
      "Norm: 5420782714953.70, NNZs: 843, Bias: -134055927.055748, T: 4097639, Avg. loss: 150360984846596576165593534693376.000000\n",
      "Total training time: 8.78 seconds.\n",
      "-- Epoch 1218\n",
      "Norm: 5718582638190.00, NNZs: 843, Bias: 314366195.075555, T: 4101006, Avg. loss: 150300452323897012967266253275136.000000\n",
      "Total training time: 8.78 seconds.\n",
      "-- Epoch 1219\n",
      "Norm: 5653982999310.87, NNZs: 843, Bias: 168395407.464162, T: 4104373, Avg. loss: 150237786073453999470843101446144.000000\n",
      "Total training time: 8.79 seconds.\n",
      "-- Epoch 1220\n",
      "Norm: 5383794345851.18, NNZs: 843, Bias: -53776498.387721, T: 4107740, Avg. loss: 150177320434569411260050955042816.000000\n",
      "Total training time: 8.80 seconds.\n",
      "-- Epoch 1221\n",
      "Norm: 5515069225335.78, NNZs: 843, Bias: 168300008.265724, T: 4111107, Avg. loss: 150116721474194602672421794217984.000000\n",
      "Total training time: 8.81 seconds.\n",
      "-- Epoch 1222\n",
      "Norm: 5597155212301.76, NNZs: 843, Bias: -497850707.199127, T: 4114474, Avg. loss: 150055852938763736272942957330432.000000\n",
      "Total training time: 8.81 seconds.\n",
      "-- Epoch 1223\n",
      "Norm: 5752616152632.01, NNZs: 843, Bias: 389559330.659450, T: 4117841, Avg. loss: 149994673672375153989786435321856.000000\n",
      "Total training time: 8.82 seconds.\n",
      "-- Epoch 1224\n",
      "Norm: 5482285579521.43, NNZs: 843, Bias: 149164906.845634, T: 4121208, Avg. loss: 149933674982656369820377037144064.000000\n",
      "Total training time: 8.83 seconds.\n",
      "-- Epoch 1225\n",
      "Norm: 5621469584182.22, NNZs: 843, Bias: -67632540.804550, T: 4124575, Avg. loss: 149871774208376982937384458911744.000000\n",
      "Total training time: 8.83 seconds.\n",
      "-- Epoch 1226\n",
      "Norm: 5571041474890.39, NNZs: 843, Bias: 154225221.525904, T: 4127942, Avg. loss: 149810369799234128922844136996864.000000\n",
      "Total training time: 8.84 seconds.\n",
      "-- Epoch 1227\n",
      "Norm: 5526703827573.41, NNZs: 843, Bias: -511245120.676895, T: 4131309, Avg. loss: 149749771969790738753529841713152.000000\n",
      "Total training time: 8.85 seconds.\n",
      "-- Epoch 1228\n",
      "Norm: 5667842795002.04, NNZs: 843, Bias: 94613214.766507, T: 4134676, Avg. loss: 149689113722945896612826483523584.000000\n",
      "Total training time: 8.86 seconds.\n",
      "-- Epoch 1229\n",
      "Norm: 5459566486226.01, NNZs: 843, Bias: 41684561.077060, T: 4138043, Avg. loss: 149628320442845542034581439381504.000000\n",
      "Total training time: 8.86 seconds.\n",
      "-- Epoch 1230\n",
      "Norm: 5811625682257.39, NNZs: 843, Bias: -180009090.832501, T: 4141410, Avg. loss: 149566993411849253665887053938688.000000\n",
      "Total training time: 8.87 seconds.\n",
      "-- Epoch 1231\n",
      "Norm: 5764586809457.65, NNZs: 843, Bias: 484900243.046062, T: 4144777, Avg. loss: 149506992958232656426238023303168.000000\n",
      "Total training time: 8.88 seconds.\n",
      "-- Epoch 1232\n",
      "Norm: 5411082916390.88, NNZs: 843, Bias: -179960791.061427, T: 4148144, Avg. loss: 149446604948624050037950750130176.000000\n",
      "Total training time: 8.88 seconds.\n",
      "-- Epoch 1233\n",
      "Norm: 5274050795099.29, NNZs: 843, Bias: 41601547.658548, T: 4151511, Avg. loss: 149386706447716331463744736460800.000000\n",
      "Total training time: 8.89 seconds.\n",
      "-- Epoch 1234\n",
      "Norm: 5641435710911.45, NNZs: 843, Bias: 51098599.017926, T: 4154878, Avg. loss: 149325859677133125432695742005248.000000\n",
      "Total training time: 8.90 seconds.\n",
      "-- Epoch 1235\n",
      "Norm: 5737196145375.85, NNZs: 843, Bias: -170369068.779311, T: 4158245, Avg. loss: 149265744457088419733086645780480.000000\n",
      "Total training time: 8.91 seconds.\n",
      "-- Epoch 1236\n",
      "Norm: 5506627240936.42, NNZs: 843, Bias: 51060987.960584, T: 4161612, Avg. loss: 149204447468005170648561867554816.000000\n",
      "Total training time: 8.91 seconds.\n",
      "-- Epoch 1237\n",
      "Norm: 5709239560227.10, NNZs: 843, Bias: -170319201.513733, T: 4164979, Avg. loss: 149144384029354189340425165209600.000000\n",
      "Total training time: 8.92 seconds.\n",
      "-- Epoch 1238\n",
      "Norm: 5953953340817.99, NNZs: 843, Bias: 493644641.573871, T: 4168346, Avg. loss: 149083530043561262087305340911616.000000\n",
      "Total training time: 8.93 seconds.\n",
      "-- Epoch 1239\n",
      "Norm: 6003028461270.02, NNZs: 843, Bias: -170272599.724893, T: 4171713, Avg. loss: 149023435591525729846112320225280.000000\n",
      "Total training time: 8.94 seconds.\n",
      "-- Epoch 1240\n",
      "Norm: 5544246538550.96, NNZs: 843, Bias: 50976143.729773, T: 4175080, Avg. loss: 148962697353639511145849062162432.000000\n",
      "Total training time: 8.94 seconds.\n",
      "-- Epoch 1241\n",
      "Norm: 5642749256407.43, NNZs: 843, Bias: 272136087.382119, T: 4178447, Avg. loss: 148902488848477812878628624531456.000000\n",
      "Total training time: 8.95 seconds.\n",
      "-- Epoch 1242\n",
      "Norm: 5726005301082.99, NNZs: 843, Bias: 254297268.283467, T: 4181814, Avg. loss: 148843168875332294928344857706496.000000\n",
      "Total training time: 8.96 seconds.\n",
      "-- Epoch 1243\n",
      "Norm: 5573123605640.10, NNZs: 843, Bias: -434657068.658633, T: 4185181, Avg. loss: 148782450998340147608152260476928.000000\n",
      "Total training time: 8.96 seconds.\n",
      "-- Epoch 1244\n",
      "Norm: 5862471404165.45, NNZs: 843, Bias: -632563626.375270, T: 4188548, Avg. loss: 148722508409166761398204293447680.000000\n",
      "Total training time: 8.97 seconds.\n",
      "-- Epoch 1245\n",
      "Norm: 5449587825576.19, NNZs: 843, Bias: 44281898.370997, T: 4191915, Avg. loss: 148662102731889709183358916886528.000000\n",
      "Total training time: 8.98 seconds.\n",
      "-- Epoch 1246\n",
      "Norm: 5531957921874.26, NNZs: 843, Bias: -176700028.003562, T: 4195282, Avg. loss: 148602475658170650962063905521664.000000\n",
      "Total training time: 8.99 seconds.\n",
      "-- Epoch 1247\n",
      "Norm: 5511163259576.85, NNZs: 843, Bias: -235553687.986355, T: 4198649, Avg. loss: 148542319532564960942946577809408.000000\n",
      "Total training time: 8.99 seconds.\n",
      "-- Epoch 1248\n",
      "Norm: 5228300639633.56, NNZs: 843, Bias: -14652919.363476, T: 4202016, Avg. loss: 148482377012620223657794514976768.000000\n",
      "Total training time: 9.00 seconds.\n",
      "-- Epoch 1249\n",
      "Norm: 5012794740776.33, NNZs: 843, Bias: -160096126.137324, T: 4205383, Avg. loss: 148424135443240133138272466501632.000000\n",
      "Total training time: 9.01 seconds.\n",
      "-- Epoch 1250\n",
      "Norm: 6070853385789.20, NNZs: 843, Bias: 60707938.558308, T: 4208750, Avg. loss: 148365370977732342507150312996864.000000\n",
      "Total training time: 9.02 seconds.\n",
      "-- Epoch 1251\n",
      "Norm: 5835962413219.86, NNZs: 843, Bias: -160044339.522845, T: 4212117, Avg. loss: 148305543818031465451822709211136.000000\n",
      "Total training time: 9.02 seconds.\n",
      "-- Epoch 1252\n",
      "Norm: 5055576601426.58, NNZs: 843, Bias: -131263458.190621, T: 4215484, Avg. loss: 148245609720784251627952742072320.000000\n",
      "Total training time: 9.03 seconds.\n",
      "-- Epoch 1253\n",
      "Norm: 4996875083394.65, NNZs: 843, Bias: 89403112.589845, T: 4218851, Avg. loss: 148186727436404559863602382635008.000000\n",
      "Total training time: 9.04 seconds.\n",
      "-- Epoch 1254\n",
      "Norm: 4919597705535.87, NNZs: 843, Bias: -92142648.198687, T: 4222218, Avg. loss: 148126812280563508193559314432000.000000\n",
      "Total training time: 9.04 seconds.\n",
      "-- Epoch 1255\n",
      "Norm: 5255310818715.71, NNZs: 843, Bias: -312692396.073565, T: 4225585, Avg. loss: 148067931691946630063564628426752.000000\n",
      "Total training time: 9.05 seconds.\n",
      "-- Epoch 1256\n",
      "Norm: 4963728805608.77, NNZs: 843, Bias: -92116513.815273, T: 4228952, Avg. loss: 148008914489505325792886565896192.000000\n",
      "Total training time: 9.06 seconds.\n",
      "-- Epoch 1257\n",
      "Norm: 5241765139921.49, NNZs: 843, Bias: 128370096.044788, T: 4232319, Avg. loss: 147950135768986542804262516162560.000000\n",
      "Total training time: 9.07 seconds.\n",
      "-- Epoch 1258\n",
      "Norm: 5507600081278.53, NNZs: 843, Bias: -92091808.063499, T: 4235686, Avg. loss: 147891151441037273431517079535616.000000\n",
      "Total training time: 9.07 seconds.\n",
      "-- Epoch 1259\n",
      "Norm: 5110026192821.22, NNZs: 843, Bias: 68061724.296125, T: 4239053, Avg. loss: 147831803532957051284094573346816.000000\n",
      "Total training time: 9.08 seconds.\n",
      "-- Epoch 1260\n",
      "Norm: 5106124575884.01, NNZs: 843, Bias: -258300824.006339, T: 4242420, Avg. loss: 147772215123034750919511583490048.000000\n",
      "Total training time: 9.09 seconds.\n",
      "-- Epoch 1261\n",
      "Norm: 5192382092260.24, NNZs: 843, Bias: -37962944.271739, T: 4245787, Avg. loss: 147714255442120471730147941154816.000000\n",
      "Total training time: 9.09 seconds.\n",
      "-- Epoch 1262\n",
      "Norm: 5274188934417.88, NNZs: 843, Bias: 182292969.302783, T: 4249154, Avg. loss: 147655780230259684524159884328960.000000\n",
      "Total training time: 9.10 seconds.\n",
      "-- Epoch 1263\n",
      "Norm: 5359977622796.25, NNZs: 843, Bias: -37965078.802483, T: 4252521, Avg. loss: 147597627550504661292081798447104.000000\n",
      "Total training time: 9.11 seconds.\n",
      "-- Epoch 1264\n",
      "Norm: 5324534150606.48, NNZs: 843, Bias: 182196950.581661, T: 4255888, Avg. loss: 147539483799277096177529352355840.000000\n",
      "Total training time: 9.12 seconds.\n",
      "-- Epoch 1265\n",
      "Norm: 5496815403959.43, NNZs: 843, Bias: 207962761.409693, T: 4259255, Avg. loss: 147480013066606106747114742611968.000000\n",
      "Total training time: 9.12 seconds.\n",
      "-- Epoch 1266\n",
      "Norm: 5314491969194.19, NNZs: 843, Bias: -12167855.141772, T: 4262622, Avg. loss: 147423627303640349633168256532480.000000\n",
      "Total training time: 9.13 seconds.\n",
      "-- Epoch 1267\n",
      "Norm: 5509713310561.73, NNZs: 843, Bias: -232216022.292333, T: 4265989, Avg. loss: 147366419935385629994445344604160.000000\n",
      "Total training time: 9.14 seconds.\n",
      "-- Epoch 1268\n",
      "Norm: 5581969750859.11, NNZs: 843, Bias: -12187418.341194, T: 4269356, Avg. loss: 147308121705726185929057920614400.000000\n",
      "Total training time: 9.14 seconds.\n",
      "-- Epoch 1269\n",
      "Norm: 5771889335278.99, NNZs: 843, Bias: 100861340.365778, T: 4272723, Avg. loss: 147250160376032319757129000943616.000000\n",
      "Total training time: 9.15 seconds.\n",
      "-- Epoch 1270\n",
      "Norm: 5750895556200.75, NNZs: 843, Bias: -119075386.645266, T: 4276090, Avg. loss: 147191766652773816353955784949760.000000\n",
      "Total training time: 9.16 seconds.\n",
      "-- Epoch 1271\n",
      "Norm: 6075545371893.70, NNZs: 843, Bias: 100800720.368817, T: 4279457, Avg. loss: 147134093780771564104805188108288.000000\n",
      "Total training time: 9.17 seconds.\n",
      "-- Epoch 1272\n",
      "Norm: 5602120934547.10, NNZs: 843, Bias: -119044703.724409, T: 4282824, Avg. loss: 147077017458643301014561267646464.000000\n",
      "Total training time: 9.17 seconds.\n",
      "-- Epoch 1273\n",
      "Norm: 5511624193744.66, NNZs: 843, Bias: 100748355.825487, T: 4286191, Avg. loss: 147019165883743271099871407374336.000000\n",
      "Total training time: 9.18 seconds.\n",
      "-- Epoch 1274\n",
      "Norm: 5505447959637.53, NNZs: 843, Bias: -119014206.587738, T: 4289558, Avg. loss: 146960666413296846027858940788736.000000\n",
      "Total training time: 9.19 seconds.\n",
      "-- Epoch 1275\n",
      "Norm: 5377049320378.74, NNZs: 843, Bias: -338690764.211954, T: 4292925, Avg. loss: 146902608586267450202872168316928.000000\n",
      "Total training time: 9.20 seconds.\n",
      "-- Epoch 1276\n",
      "Norm: 5354271962497.80, NNZs: 843, Bias: -118986149.404835, T: 4296292, Avg. loss: 146845981527413319265850287456256.000000\n",
      "Total training time: 9.20 seconds.\n",
      "-- Epoch 1277\n",
      "Norm: 5186342797580.05, NNZs: 843, Bias: 84844185.426510, T: 4299659, Avg. loss: 146788797018139738636147272712192.000000\n",
      "Total training time: 9.21 seconds.\n",
      "-- Epoch 1278\n",
      "Norm: 5352603185769.24, NNZs: 843, Bias: -134743676.501398, T: 4303026, Avg. loss: 146730712641659172897000121171968.000000\n",
      "Total training time: 9.22 seconds.\n",
      "-- Epoch 1279\n",
      "Norm: 5780476761998.78, NNZs: 843, Bias: 362989859.093248, T: 4306393, Avg. loss: 146673215298871823929087247777792.000000\n",
      "Total training time: 9.22 seconds.\n",
      "-- Epoch 1280\n",
      "Norm: 6247954889040.63, NNZs: 843, Bias: 143436990.685300, T: 4309760, Avg. loss: 146617241104279830522404060266496.000000\n",
      "Total training time: 9.23 seconds.\n",
      "-- Epoch 1281\n",
      "Norm: 5722253768439.77, NNZs: 843, Bias: -76031345.543379, T: 4313127, Avg. loss: 146559789562318614815544856018944.000000\n",
      "Total training time: 9.24 seconds.\n",
      "-- Epoch 1282\n",
      "Norm: 5344606899985.68, NNZs: 843, Bias: -250589921.862578, T: 4316494, Avg. loss: 146503051427434582916846086782976.000000\n",
      "Total training time: 9.25 seconds.\n",
      "-- Epoch 1283\n",
      "Norm: 5442187861580.52, NNZs: 843, Bias: 16638844.409193, T: 4319861, Avg. loss: 146445555256024796073519143190528.000000\n",
      "Total training time: 9.25 seconds.\n",
      "-- Epoch 1284\n",
      "Norm: 5627812716959.77, NNZs: 843, Bias: -202680802.255512, T: 4323228, Avg. loss: 146388219769459083898579530547200.000000\n",
      "Total training time: 9.26 seconds.\n",
      "-- Epoch 1285\n",
      "Norm: 5483096807355.23, NNZs: 843, Bias: -421917218.420121, T: 4326595, Avg. loss: 146330802753266231662436904075264.000000\n",
      "Total training time: 9.27 seconds.\n",
      "-- Epoch 1286\n",
      "Norm: 5690194000695.77, NNZs: 843, Bias: 105966624.570737, T: 4329962, Avg. loss: 146274720386772596648627220250624.000000\n",
      "Total training time: 9.28 seconds.\n",
      "-- Epoch 1287\n",
      "Norm: 5220148287374.56, NNZs: 843, Bias: 325114291.565782, T: 4333329, Avg. loss: 146218654503201756519670338289664.000000\n",
      "Total training time: 9.28 seconds.\n",
      "-- Epoch 1288\n",
      "Norm: 5297546544119.64, NNZs: 843, Bias: -332363206.552089, T: 4336696, Avg. loss: 146161724694704981949200633692160.000000\n",
      "Total training time: 9.29 seconds.\n",
      "-- Epoch 1289\n",
      "Norm: 5205434861948.81, NNZs: 843, Bias: -113217545.825808, T: 4340063, Avg. loss: 146104289774433994861525281538048.000000\n",
      "Total training time: 9.30 seconds.\n",
      "-- Epoch 1290\n",
      "Norm: 5301885003327.56, NNZs: 843, Bias: 105848960.239442, T: 4343430, Avg. loss: 146047129984459664277431480483840.000000\n",
      "Total training time: 9.30 seconds.\n",
      "-- Epoch 1291\n",
      "Norm: 6007826664349.03, NNZs: 843, Bias: 324826214.842124, T: 4346797, Avg. loss: 145991043277495265178183127793664.000000\n",
      "Total training time: 9.31 seconds.\n",
      "-- Epoch 1292\n",
      "Norm: 5990397894601.50, NNZs: 843, Bias: 105793523.885717, T: 4350164, Avg. loss: 145934608234118737919706751041536.000000\n",
      "Total training time: 9.32 seconds.\n",
      "-- Epoch 1293\n",
      "Norm: 5878228529060.17, NNZs: 843, Bias: -241626152.708726, T: 4353531, Avg. loss: 145878477127017225249875585662976.000000\n",
      "Total training time: 9.33 seconds.\n",
      "-- Epoch 1294\n",
      "Norm: 5446696286964.43, NNZs: 843, Bias: -22709790.419180, T: 4356898, Avg. loss: 145822184863328908403025545003008.000000\n",
      "Total training time: 9.33 seconds.\n",
      "-- Epoch 1295\n",
      "Norm: 5831914795769.45, NNZs: 843, Bias: -370988392.854518, T: 4360265, Avg. loss: 145765487998470775552672746438656.000000\n",
      "Total training time: 9.34 seconds.\n",
      "-- Epoch 1296\n",
      "Norm: 5208402427704.59, NNZs: 843, Bias: -152128426.546933, T: 4363632, Avg. loss: 145709074950760793081386081714176.000000\n",
      "Total training time: 9.35 seconds.\n",
      "-- Epoch 1297\n",
      "Norm: 5205277176768.18, NNZs: 843, Bias: 268024406.352101, T: 4366999, Avg. loss: 145654014366485864086920388149248.000000\n",
      "Total training time: 9.35 seconds.\n",
      "-- Epoch 1298\n",
      "Norm: 5114587060506.90, NNZs: 843, Bias: 49250397.425507, T: 4370366, Avg. loss: 145597098038728858044640893337600.000000\n",
      "Total training time: 9.36 seconds.\n",
      "-- Epoch 1299\n",
      "Norm: 5859292638106.87, NNZs: 843, Bias: 106892851.523917, T: 4373733, Avg. loss: 145540260435555689118687966527488.000000\n",
      "Total training time: 9.37 seconds.\n",
      "-- Epoch 1300\n",
      "Norm: 5284838888923.55, NNZs: 843, Bias: -111759162.636313, T: 4377100, Avg. loss: 145484023179151733555122220826624.000000\n",
      "Total training time: 9.38 seconds.\n",
      "-- Epoch 1301\n",
      "Norm: 5869973131793.83, NNZs: 843, Bias: 106841992.986353, T: 4380467, Avg. loss: 145429398140618871350770846924800.000000\n",
      "Total training time: 9.38 seconds.\n",
      "-- Epoch 1302\n",
      "Norm: 5417428428819.99, NNZs: 843, Bias: -111727289.781450, T: 4383834, Avg. loss: 145373378419442652161756821979136.000000\n",
      "Total training time: 9.39 seconds.\n",
      "-- Epoch 1303\n",
      "Norm: 5624916378492.68, NNZs: 843, Bias: 48139535.395190, T: 4387201, Avg. loss: 145317843409725827981164943507456.000000\n",
      "Total training time: 9.40 seconds.\n",
      "-- Epoch 1304\n",
      "Norm: 5977406831166.02, NNZs: 843, Bias: 266577257.483435, T: 4390568, Avg. loss: 145262298513048963761714831032320.000000\n",
      "Total training time: 9.40 seconds.\n",
      "-- Epoch 1305\n",
      "Norm: 6047918336158.66, NNZs: 843, Bias: -388731429.899208, T: 4393935, Avg. loss: 145207165033791328635562215079936.000000\n",
      "Total training time: 9.41 seconds.\n",
      "-- Epoch 1306\n",
      "Norm: 5815315651512.50, NNZs: 843, Bias: -456080910.407873, T: 4397302, Avg. loss: 145151786126771634623095827857408.000000\n",
      "Total training time: 9.42 seconds.\n",
      "-- Epoch 1307\n",
      "Norm: 5769744767910.65, NNZs: 843, Bias: 198995740.396607, T: 4400669, Avg. loss: 145096166331785984135101895671808.000000\n",
      "Total training time: 9.43 seconds.\n",
      "-- Epoch 1308\n",
      "Norm: 5719847278164.05, NNZs: 843, Bias: -19342319.553168, T: 4404036, Avg. loss: 145041080002525654887079585251328.000000\n",
      "Total training time: 9.43 seconds.\n",
      "-- Epoch 1309\n",
      "Norm: 5606707111829.26, NNZs: 843, Bias: -237597611.367817, T: 4407403, Avg. loss: 144984770676666152754603465113600.000000\n",
      "Total training time: 9.44 seconds.\n",
      "-- Epoch 1310\n",
      "Norm: 5543547742344.44, NNZs: 843, Bias: -136766276.512608, T: 4410770, Avg. loss: 144929542687830250301219655385088.000000\n",
      "Total training time: 9.45 seconds.\n",
      "-- Epoch 1311\n",
      "Norm: 5866542544601.96, NNZs: 843, Bias: -354917802.600169, T: 4414137, Avg. loss: 144875078020352672339456547618816.000000\n",
      "Total training time: 9.45 seconds.\n",
      "-- Epoch 1312\n",
      "Norm: 5516446871716.09, NNZs: 843, Bias: -136733200.860019, T: 4417504, Avg. loss: 144819428547554231827103303598080.000000\n",
      "Total training time: 9.46 seconds.\n",
      "-- Epoch 1313\n",
      "Norm: 5356402284362.43, NNZs: 843, Bias: 81370465.654932, T: 4420871, Avg. loss: 144763399327705272032195835330560.000000\n",
      "Total training time: 9.47 seconds.\n",
      "-- Epoch 1314\n",
      "Norm: 5456206103688.11, NNZs: 843, Bias: -136697398.941578, T: 4424238, Avg. loss: 144708121904201971530332756770816.000000\n",
      "Total training time: 9.48 seconds.\n",
      "-- Epoch 1315\n",
      "Norm: 5166941698336.43, NNZs: 843, Bias: 94707124.660425, T: 4427605, Avg. loss: 144654115022868444758772610498560.000000\n",
      "Total training time: 9.48 seconds.\n",
      "-- Epoch 1316\n",
      "Norm: 5277742800695.03, NNZs: 843, Bias: 238156031.350645, T: 4430972, Avg. loss: 144599250163761522765692158345216.000000\n",
      "Total training time: 9.49 seconds.\n",
      "-- Epoch 1317\n",
      "Norm: 5172773555427.76, NNZs: 843, Bias: 20184394.667725, T: 4434339, Avg. loss: 144544591850560183502155448057856.000000\n",
      "Total training time: 9.50 seconds.\n",
      "-- Epoch 1318\n",
      "Norm: 5413065800261.27, NNZs: 843, Bias: -394298061.514797, T: 4437706, Avg. loss: 144489518257025835773840483942400.000000\n",
      "Total training time: 9.51 seconds.\n",
      "-- Epoch 1319\n",
      "Norm: 5046676043067.36, NNZs: 843, Bias: -176397746.446553, T: 4441073, Avg. loss: 144434302773003031289981673930752.000000\n",
      "Total training time: 9.51 seconds.\n",
      "-- Epoch 1320\n",
      "Norm: 5498425426268.31, NNZs: 843, Bias: 41424172.816159, T: 4444440, Avg. loss: 144379393151755604438613307162624.000000\n",
      "Total training time: 9.52 seconds.\n",
      "-- Epoch 1321\n",
      "Norm: 5097977102528.61, NNZs: 843, Bias: -176342605.433277, T: 4447807, Avg. loss: 144325339210521011716726538108928.000000\n",
      "Total training time: 9.53 seconds.\n",
      "-- Epoch 1322\n",
      "Norm: 5186417798217.71, NNZs: 843, Bias: 41395079.538856, T: 4451174, Avg. loss: 144270271646747902510929662705664.000000\n",
      "Total training time: 9.53 seconds.\n",
      "-- Epoch 1323\n",
      "Norm: 5234577934292.56, NNZs: 843, Bias: 253203156.669230, T: 4454541, Avg. loss: 144215947726345636519138205630464.000000\n",
      "Total training time: 9.54 seconds.\n",
      "-- Epoch 1324\n",
      "Norm: 5228008915412.51, NNZs: 843, Bias: -399741273.977812, T: 4457908, Avg. loss: 144162029500948058501746387845120.000000\n",
      "Total training time: 9.55 seconds.\n",
      "-- Epoch 1325\n",
      "Norm: 5295321295949.62, NNZs: 843, Bias: 308636499.107688, T: 4461275, Avg. loss: 144107911607876055479413020557312.000000\n",
      "Total training time: 9.56 seconds.\n",
      "-- Epoch 1326\n",
      "Norm: 5648679751997.12, NNZs: 843, Bias: 91027356.967893, T: 4464642, Avg. loss: 144053436575110580623816014168064.000000\n",
      "Total training time: 9.56 seconds.\n",
      "-- Epoch 1327\n",
      "Norm: 5582599123519.97, NNZs: 843, Bias: -126502668.646513, T: 4468009, Avg. loss: 143998343920063608308127412256768.000000\n",
      "Total training time: 9.57 seconds.\n",
      "-- Epoch 1328\n",
      "Norm: 5283848484589.84, NNZs: 843, Bias: 90976815.813362, T: 4471376, Avg. loss: 143944276398520784192381990731776.000000\n",
      "Total training time: 9.58 seconds.\n",
      "-- Epoch 1329\n",
      "Norm: 5227794850482.84, NNZs: 843, Bias: -126471215.336869, T: 4474743, Avg. loss: 143889508144969683933372274966528.000000\n",
      "Total training time: 9.58 seconds.\n",
      "-- Epoch 1330\n",
      "Norm: 5465407799018.96, NNZs: 843, Bias: 90929112.971224, T: 4478110, Avg. loss: 143835810240053427752817190240256.000000\n",
      "Total training time: 9.59 seconds.\n",
      "-- Epoch 1331\n",
      "Norm: 5385349605480.28, NNZs: 843, Bias: 37666146.592575, T: 4481477, Avg. loss: 143781843425610979031824465920000.000000\n",
      "Total training time: 9.60 seconds.\n",
      "-- Epoch 1332\n",
      "Norm: 5518009900903.29, NNZs: 843, Bias: 254951785.747177, T: 4484844, Avg. loss: 143727240631656901651571717177344.000000\n",
      "Total training time: 9.61 seconds.\n",
      "-- Epoch 1333\n",
      "Norm: 5850939429634.13, NNZs: 843, Bias: 37632074.162125, T: 4488211, Avg. loss: 143673196596717491119146347266048.000000\n",
      "Total training time: 9.61 seconds.\n",
      "-- Epoch 1334\n",
      "Norm: 5027386805579.73, NNZs: 843, Bias: -224644536.977007, T: 4491578, Avg. loss: 143619487421817760920405056421888.000000\n",
      "Total training time: 9.62 seconds.\n",
      "-- Epoch 1335\n",
      "Norm: 5096983114555.65, NNZs: 843, Bias: -7431488.836667, T: 4494945, Avg. loss: 143564861456685884659999872909312.000000\n",
      "Total training time: 9.63 seconds.\n",
      "-- Epoch 1336\n",
      "Norm: 5024868455426.59, NNZs: 843, Bias: 209700503.602923, T: 4498312, Avg. loss: 143511372811927711077129705750528.000000\n",
      "Total training time: 9.63 seconds.\n",
      "-- Epoch 1337\n",
      "Norm: 4962331635011.32, NNZs: 843, Bias: -7447115.812490, T: 4501679, Avg. loss: 143457553113250687186325915828224.000000\n",
      "Total training time: 9.64 seconds.\n",
      "-- Epoch 1338\n",
      "Norm: 5272783956942.74, NNZs: 843, Bias: 209604472.280421, T: 4505046, Avg. loss: 143404256862918392239680393314304.000000\n",
      "Total training time: 9.65 seconds.\n",
      "-- Epoch 1339\n",
      "Norm: 5271319840984.81, NNZs: 843, Bias: -441497876.457233, T: 4508413, Avg. loss: 143351509904442916690614854615040.000000\n",
      "Total training time: 9.66 seconds.\n",
      "-- Epoch 1340\n",
      "Norm: 5755623037377.91, NNZs: 843, Bias: -658397158.556060, T: 4511780, Avg. loss: 143298445064852158781361383014400.000000\n",
      "Total training time: 9.66 seconds.\n",
      "-- Epoch 1341\n",
      "Norm: 5265399406915.52, NNZs: 843, Bias: 426395030.708845, T: 4515147, Avg. loss: 143244541215967773137134354956288.000000\n",
      "Total training time: 9.67 seconds.\n",
      "-- Epoch 1342\n",
      "Norm: 5131145277068.93, NNZs: 843, Bias: 209415250.964473, T: 4518514, Avg. loss: 143190998973175600853082062192640.000000\n",
      "Total training time: 9.68 seconds.\n",
      "-- Epoch 1343\n",
      "Norm: 4964420316149.79, NNZs: 843, Bias: -7486256.483791, T: 4521881, Avg. loss: 143137453655022150457562876608512.000000\n",
      "Total training time: 9.68 seconds.\n",
      "-- Epoch 1344\n",
      "Norm: 5071729049510.75, NNZs: 843, Bias: -193998510.544470, T: 4525248, Avg. loss: 143084040942588140718020569858048.000000\n",
      "Total training time: 9.69 seconds.\n",
      "-- Epoch 1345\n",
      "Norm: 5245920183897.37, NNZs: 843, Bias: -410743941.939777, T: 4528615, Avg. loss: 143031028997887794424310484959232.000000\n",
      "Total training time: 9.70 seconds.\n",
      "-- Epoch 1346\n",
      "Norm: 5344326110437.97, NNZs: 843, Bias: -438840223.542132, T: 4531982, Avg. loss: 142977575061989845579068367437824.000000\n",
      "Total training time: 9.71 seconds.\n",
      "-- Epoch 1347\n",
      "Norm: 5417207295187.82, NNZs: 843, Bias: -222071902.832551, T: 4535349, Avg. loss: 142924708462364192256755295584256.000000\n",
      "Total training time: 9.71 seconds.\n",
      "-- Epoch 1348\n",
      "Norm: 5726504061258.00, NNZs: 843, Bias: -5381458.818425, T: 4538716, Avg. loss: 142871104594233522692073212346368.000000\n",
      "Total training time: 9.72 seconds.\n",
      "-- Epoch 1349\n",
      "Norm: 5122705384039.17, NNZs: 843, Bias: 211225800.574197, T: 4542083, Avg. loss: 142818871594127351052853380644864.000000\n",
      "Total training time: 9.73 seconds.\n",
      "-- Epoch 1350\n",
      "Norm: 5098215645991.50, NNZs: 843, Bias: -173980601.924461, T: 4545450, Avg. loss: 142765530587230944335327110627328.000000\n",
      "Total training time: 9.74 seconds.\n",
      "-- Epoch 1351\n",
      "Norm: 4971946287600.94, NNZs: 843, Bias: 42579746.418982, T: 4548817, Avg. loss: 142712888580075965260666849198080.000000\n",
      "Total training time: 9.74 seconds.\n",
      "-- Epoch 1352\n",
      "Norm: 5093758275846.68, NNZs: 843, Bias: -173929868.863684, T: 4552184, Avg. loss: 142659665810922614106814066720768.000000\n",
      "Total training time: 9.75 seconds.\n",
      "-- Epoch 1353\n",
      "Norm: 5790147621322.72, NNZs: 843, Bias: 572924380.344893, T: 4555551, Avg. loss: 142606960079985880563097635127296.000000\n",
      "Total training time: 9.76 seconds.\n",
      "-- Epoch 1354\n",
      "Norm: 5632279695829.46, NNZs: 843, Bias: -600376682.176917, T: 4558918, Avg. loss: 142553672441973514063506070044672.000000\n",
      "Total training time: 9.76 seconds.\n",
      "-- Epoch 1355\n",
      "Norm: 5620722690709.16, NNZs: 843, Bias: 48844481.407058, T: 4562285, Avg. loss: 142501949728904319771930706051072.000000\n",
      "Total training time: 9.77 seconds.\n",
      "-- Epoch 1356\n",
      "Norm: 5215172545851.15, NNZs: 843, Bias: -167506427.072746, T: 4565652, Avg. loss: 142449851171134750181502436245504.000000\n",
      "Total training time: 9.78 seconds.\n",
      "-- Epoch 1357\n",
      "Norm: 5115016481808.08, NNZs: 843, Bias: 48807938.369729, T: 4569019, Avg. loss: 142398135447531246911742711693312.000000\n",
      "Total training time: 9.79 seconds.\n",
      "-- Epoch 1358\n",
      "Norm: 5420534544254.38, NNZs: 843, Bias: 265045382.020071, T: 4572386, Avg. loss: 142345692749082457487188969914368.000000\n",
      "Total training time: 9.79 seconds.\n",
      "-- Epoch 1359\n",
      "Norm: 5321905583832.60, NNZs: 843, Bias: -4603420.452912, T: 4575753, Avg. loss: 142293277937427360215779164291072.000000\n",
      "Total training time: 9.80 seconds.\n",
      "-- Epoch 1360\n",
      "Norm: 5805713436062.96, NNZs: 843, Bias: 358309916.125957, T: 4579120, Avg. loss: 142241432788462057667671403528192.000000\n",
      "Total training time: 9.81 seconds.\n",
      "-- Epoch 1361\n",
      "Norm: 5404581443144.27, NNZs: 843, Bias: 142101779.469424, T: 4582487, Avg. loss: 142188436094231936233997003128832.000000\n",
      "Total training time: 9.81 seconds.\n",
      "-- Epoch 1362\n",
      "Norm: 5919783851639.97, NNZs: 843, Bias: 358164162.306261, T: 4585854, Avg. loss: 142136191141028498862479526330368.000000\n",
      "Total training time: 9.82 seconds.\n",
      "-- Epoch 1363\n",
      "Norm: 5473315224982.34, NNZs: 843, Bias: 142036409.951767, T: 4589221, Avg. loss: 142084522557922210258491178745856.000000\n",
      "Total training time: 9.83 seconds.\n",
      "-- Epoch 1364\n",
      "Norm: 5194822919278.44, NNZs: 843, Bias: -74011291.263560, T: 4592588, Avg. loss: 142031762521246573167759006367744.000000\n",
      "Total training time: 9.84 seconds.\n",
      "-- Epoch 1365\n",
      "Norm: 5332186002873.05, NNZs: 843, Bias: 141972217.537559, T: 4595955, Avg. loss: 141979167680552474941826749431808.000000\n",
      "Total training time: 9.84 seconds.\n",
      "-- Epoch 1366\n",
      "Norm: 5586537475301.43, NNZs: 843, Bias: 94588245.575480, T: 4599322, Avg. loss: 141926917060067022657725742972928.000000\n",
      "Total training time: 9.85 seconds.\n",
      "-- Epoch 1367\n",
      "Norm: 5467274596911.04, NNZs: 843, Bias: -121332729.273201, T: 4602689, Avg. loss: 141873739822901676407093108670464.000000\n",
      "Total training time: 9.86 seconds.\n",
      "-- Epoch 1368\n",
      "Norm: 5194545916706.80, NNZs: 843, Bias: -103505145.368506, T: 4606056, Avg. loss: 141821759154066687496880945889280.000000\n",
      "Total training time: 9.87 seconds.\n",
      "-- Epoch 1369\n",
      "Norm: 5424540989594.05, NNZs: 843, Bias: 112330005.337351, T: 4609423, Avg. loss: 141769384490033114332371504594944.000000\n",
      "Total training time: 9.87 seconds.\n",
      "-- Epoch 1370\n",
      "Norm: 5433797197878.96, NNZs: 843, Bias: -103475236.508671, T: 4612790, Avg. loss: 141717694529807496021567388778496.000000\n",
      "Total training time: 9.88 seconds.\n",
      "-- Epoch 1371\n",
      "Norm: 5337452676664.59, NNZs: 843, Bias: -319200511.964929, T: 4616157, Avg. loss: 141665996804919274977827705847808.000000\n",
      "Total training time: 9.89 seconds.\n",
      "-- Epoch 1372\n",
      "Norm: 5939929600898.99, NNZs: 843, Bias: -534846210.442091, T: 4619524, Avg. loss: 141614263078583116484846871904256.000000\n",
      "Total training time: 9.89 seconds.\n",
      "-- Epoch 1373\n",
      "Norm: 5554462526194.28, NNZs: 843, Bias: 112225576.193328, T: 4622891, Avg. loss: 141562446670764927845163966922752.000000\n",
      "Total training time: 9.90 seconds.\n",
      "-- Epoch 1374\n",
      "Norm: 6182467661398.62, NNZs: 843, Bias: 327825103.596645, T: 4626258, Avg. loss: 141510142480591727389701549588480.000000\n",
      "Total training time: 9.91 seconds.\n",
      "-- Epoch 1375\n",
      "Norm: 6089729525772.59, NNZs: 843, Bias: -250431530.743800, T: 4629625, Avg. loss: 141458910391144722799205292179456.000000\n",
      "Total training time: 9.92 seconds.\n",
      "-- Epoch 1376\n",
      "Norm: 6013398285748.01, NNZs: 843, Bias: 396238651.328787, T: 4632992, Avg. loss: 141407754961093436231466759487488.000000\n",
      "Total training time: 9.92 seconds.\n",
      "-- Epoch 1377\n",
      "Norm: 5975239842363.73, NNZs: 843, Bias: 180658898.012803, T: 4636359, Avg. loss: 141356237234845080787085265207296.000000\n",
      "Total training time: 9.93 seconds.\n",
      "-- Epoch 1378\n",
      "Norm: 6073628492709.32, NNZs: 843, Bias: -34841746.816285, T: 4639726, Avg. loss: 141304742616157414476959467962368.000000\n",
      "Total training time: 9.94 seconds.\n",
      "-- Epoch 1379\n",
      "Norm: 5849830053836.48, NNZs: 843, Bias: -250267335.665059, T: 4643093, Avg. loss: 141254499946292345576780086640640.000000\n",
      "Total training time: 9.94 seconds.\n",
      "-- Epoch 1380\n",
      "Norm: 6307254206428.04, NNZs: 843, Bias: 395936609.223991, T: 4646460, Avg. loss: 141203892314126736383148090720256.000000\n",
      "Total training time: 9.95 seconds.\n",
      "-- Epoch 1381\n",
      "Norm: 5663413297905.19, NNZs: 843, Bias: -76374052.595794, T: 4649827, Avg. loss: 141153295178429982115788857802752.000000\n",
      "Total training time: 9.96 seconds.\n",
      "-- Epoch 1382\n",
      "Norm: 5398885602246.10, NNZs: 843, Bias: 138945596.308572, T: 4653194, Avg. loss: 141102471085575187396686791049216.000000\n",
      "Total training time: 9.97 seconds.\n",
      "-- Epoch 1383\n",
      "Norm: 5327467988496.40, NNZs: 843, Bias: -76353495.842746, T: 4656561, Avg. loss: 141052089703387851539913923100672.000000\n",
      "Total training time: 9.97 seconds.\n",
      "-- Epoch 1384\n",
      "Norm: 5599398029419.47, NNZs: 843, Bias: 138886896.502320, T: 4659928, Avg. loss: 141000527724543314722158148583424.000000\n",
      "Total training time: 9.98 seconds.\n",
      "-- Epoch 1385\n",
      "Norm: 5328848978272.18, NNZs: 843, Bias: -76330669.939490, T: 4663295, Avg. loss: 140949457478764541265708593446912.000000\n",
      "Total training time: 9.99 seconds.\n",
      "-- Epoch 1386\n",
      "Norm: 5511026301876.64, NNZs: 843, Bias: 138832549.868838, T: 4666662, Avg. loss: 140899281583333307432100444504064.000000\n",
      "Total training time: 9.99 seconds.\n",
      "-- Epoch 1387\n",
      "Norm: 5453649267443.52, NNZs: 843, Bias: -76310948.641763, T: 4670029, Avg. loss: 140848307032112044950059837554688.000000\n",
      "Total training time: 10.00 seconds.\n",
      "-- Epoch 1388\n",
      "Norm: 5682655122933.04, NNZs: 843, Bias: -291376414.907945, T: 4673396, Avg. loss: 140797465736406204650835524190208.000000\n",
      "Total training time: 10.01 seconds.\n",
      "-- Epoch 1389\n",
      "Norm: 5474111130874.90, NNZs: 843, Bias: -76291285.718550, T: 4676763, Avg. loss: 140745879037733370932044069076992.000000\n",
      "Total training time: 10.02 seconds.\n",
      "-- Epoch 1390\n",
      "Norm: 5933464411306.68, NNZs: 843, Bias: 138717933.489944, T: 4680130, Avg. loss: 140695019589355794425213834231808.000000\n",
      "Total training time: 10.02 seconds.\n",
      "-- Epoch 1391\n",
      "Norm: 5817416386165.79, NNZs: 843, Bias: 353647280.368507, T: 4683497, Avg. loss: 140644010951753813278959905800192.000000\n",
      "Total training time: 10.03 seconds.\n",
      "-- Epoch 1392\n",
      "Norm: 5572064879374.36, NNZs: 843, Bias: -86279335.796942, T: 4686864, Avg. loss: 140593855414701048159964315516928.000000\n",
      "Total training time: 10.04 seconds.\n",
      "-- Epoch 1393\n",
      "Norm: 5932674505264.13, NNZs: 843, Bias: 128616272.467104, T: 4690231, Avg. loss: 140543379333451156383560590426112.000000\n",
      "Total training time: 10.05 seconds.\n",
      "-- Epoch 1394\n",
      "Norm: 5601345057590.44, NNZs: 843, Bias: 125464533.250587, T: 4693598, Avg. loss: 140493155023180392314137034620928.000000\n",
      "Total training time: 10.05 seconds.\n",
      "-- Epoch 1395\n",
      "Norm: 5475584316042.64, NNZs: 843, Bias: 340247165.309501, T: 4696965, Avg. loss: 140442031507478920246854655410176.000000\n",
      "Total training time: 10.06 seconds.\n",
      "-- Epoch 1396\n",
      "Norm: 5228422147536.51, NNZs: 843, Bias: 125414377.936305, T: 4700332, Avg. loss: 140391616167055622990323708329984.000000\n",
      "Total training time: 10.07 seconds.\n",
      "-- Epoch 1397\n",
      "Norm: 5356291274256.07, NNZs: 843, Bias: 340121247.719437, T: 4703699, Avg. loss: 140341730766738511735999634079744.000000\n",
      "Total training time: 10.07 seconds.\n",
      "-- Epoch 1398\n",
      "Norm: 5276482692110.06, NNZs: 843, Bias: -304013119.187991, T: 4707066, Avg. loss: 140291573006595551893911519625216.000000\n",
      "Total training time: 10.08 seconds.\n",
      "-- Epoch 1399\n",
      "Norm: 5258356613682.91, NNZs: 843, Bias: -89311100.851647, T: 4710433, Avg. loss: 140241089756988957474800825008128.000000\n",
      "Total training time: 10.09 seconds.\n",
      "-- Epoch 1400\n",
      "Norm: 5233793176437.11, NNZs: 843, Bias: 125313885.139058, T: 4713800, Avg. loss: 140190385102246973261133385826304.000000\n",
      "Total training time: 10.10 seconds.\n",
      "-- Epoch 1401\n",
      "Norm: 5426079267953.94, NNZs: 843, Bias: -369689588.854595, T: 4717167, Avg. loss: 140140639447230003284830242996224.000000\n",
      "Total training time: 10.10 seconds.\n",
      "-- Epoch 1402\n",
      "Norm: 6167910809820.31, NNZs: 843, Bias: -155093189.087240, T: 4720534, Avg. loss: 140090891247570941280357854478336.000000\n",
      "Total training time: 10.11 seconds.\n",
      "-- Epoch 1403\n",
      "Norm: 6372823215124.34, NNZs: 843, Bias: 59429145.147906, T: 4723901, Avg. loss: 140039893527448521587394636414976.000000\n",
      "Total training time: 10.12 seconds.\n",
      "-- Epoch 1404\n",
      "Norm: 5562900234081.79, NNZs: 843, Bias: -155047950.454449, T: 4727268, Avg. loss: 139990401039233883870341869076480.000000\n",
      "Total training time: 10.12 seconds.\n",
      "-- Epoch 1405\n",
      "Norm: 5415440515994.93, NNZs: 843, Bias: 59393204.917906, T: 4730635, Avg. loss: 139941373002884036200085153906688.000000\n",
      "Total training time: 10.13 seconds.\n",
      "-- Epoch 1406\n",
      "Norm: 5364130997561.13, NNZs: 843, Bias: -155011157.256808, T: 4734002, Avg. loss: 139891195925309799663827622559744.000000\n",
      "Total training time: 10.14 seconds.\n",
      "-- Epoch 1407\n",
      "Norm: 5225408610088.80, NNZs: 843, Bias: 59354563.817604, T: 4737369, Avg. loss: 139841575631491969824998733905920.000000\n",
      "Total training time: 10.15 seconds.\n",
      "-- Epoch 1408\n",
      "Norm: 5466318274271.04, NNZs: 843, Bias: 273646786.346959, T: 4740736, Avg. loss: 139792217139698537863142852001792.000000\n",
      "Total training time: 10.15 seconds.\n",
      "-- Epoch 1409\n",
      "Norm: 5276897033781.69, NNZs: 843, Bias: 59320784.199959, T: 4744103, Avg. loss: 139742295425630778524419768713216.000000\n",
      "Total training time: 10.16 seconds.\n",
      "-- Epoch 1410\n",
      "Norm: 5182709749891.38, NNZs: 843, Bias: -154928024.917634, T: 4747470, Avg. loss: 139692505035237115411381000077312.000000\n",
      "Total training time: 10.17 seconds.\n",
      "-- Epoch 1411\n",
      "Norm: 5322454147869.48, NNZs: 843, Bias: 59289072.785677, T: 4750837, Avg. loss: 139642079005669459969019912126464.000000\n",
      "Total training time: 10.17 seconds.\n",
      "-- Epoch 1412\n",
      "Norm: 5338039228706.14, NNZs: 843, Bias: -245562026.064624, T: 4754204, Avg. loss: 139592500842684483099050268164096.000000\n",
      "Total training time: 10.18 seconds.\n",
      "-- Epoch 1413\n",
      "Norm: 5247657386851.10, NNZs: 843, Bias: -31402568.140794, T: 4757571, Avg. loss: 139542004441168807232176318840832.000000\n",
      "Total training time: 10.19 seconds.\n",
      "-- Epoch 1414\n",
      "Norm: 5216258704387.64, NNZs: 843, Bias: -14611066.626166, T: 4760938, Avg. loss: 139493719664971322737964234047488.000000\n",
      "Total training time: 10.20 seconds.\n",
      "-- Epoch 1415\n",
      "Norm: 5419240895095.51, NNZs: 843, Bias: 199425537.431838, T: 4764305, Avg. loss: 139444379694536810478399928664064.000000\n",
      "Total training time: 10.20 seconds.\n",
      "-- Epoch 1416\n",
      "Norm: 5592897061929.93, NNZs: 843, Bias: 105043060.251183, T: 4767672, Avg. loss: 139395864450015462506807846502400.000000\n",
      "Total training time: 10.21 seconds.\n",
      "-- Epoch 1417\n",
      "Norm: 5891481969225.41, NNZs: 843, Bias: 63338680.750497, T: 4771039, Avg. loss: 139347060158833036372789419835392.000000\n",
      "Total training time: 10.22 seconds.\n",
      "-- Epoch 1418\n",
      "Norm: 5327641518439.54, NNZs: 843, Bias: 205474901.316311, T: 4774406, Avg. loss: 139298279876405976305444778409984.000000\n",
      "Total training time: 10.22 seconds.\n",
      "-- Epoch 1419\n",
      "Norm: 5577475041055.52, NNZs: 843, Bias: -8457122.961430, T: 4777773, Avg. loss: 139248892727787659323911538999296.000000\n",
      "Total training time: 10.23 seconds.\n",
      "-- Epoch 1420\n",
      "Norm: 5351967808670.80, NNZs: 843, Bias: -222318062.892015, T: 4781140, Avg. loss: 139199879052329728287329763721216.000000\n",
      "Total training time: 10.24 seconds.\n",
      "-- Epoch 1421\n",
      "Norm: 5542637997197.40, NNZs: 843, Bias: 419164800.524484, T: 4784507, Avg. loss: 139150723875574602445374739185664.000000\n",
      "Total training time: 10.25 seconds.\n",
      "-- Epoch 1422\n",
      "Norm: 5237624064951.94, NNZs: 843, Bias: -222248962.379574, T: 4787874, Avg. loss: 139101341202205879280829793828864.000000\n",
      "Total training time: 10.25 seconds.\n",
      "-- Epoch 1423\n",
      "Norm: 5642942190074.46, NNZs: 843, Bias: -435957784.447856, T: 4791241, Avg. loss: 139050997618864423213525494661120.000000\n",
      "Total training time: 10.26 seconds.\n",
      "-- Epoch 1424\n",
      "Norm: 5871527167683.34, NNZs: 843, Bias: -674223659.976142, T: 4794608, Avg. loss: 139003085524032414540869492801536.000000\n",
      "Total training time: 10.27 seconds.\n",
      "-- Epoch 1425\n",
      "Norm: 5407292156200.24, NNZs: 843, Bias: 19539718.674134, T: 4797975, Avg. loss: 138955220655641433518961896456192.000000\n",
      "Total training time: 10.28 seconds.\n",
      "-- Epoch 1426\n",
      "Norm: 5661819523816.00, NNZs: 843, Bias: 233159689.634699, T: 4801342, Avg. loss: 138907175754669370435604953694208.000000\n",
      "Total training time: 10.28 seconds.\n",
      "-- Epoch 1427\n",
      "Norm: 6188771564295.11, NNZs: 843, Bias: 19520576.334137, T: 4804709, Avg. loss: 138858064137060116503360083853312.000000\n",
      "Total training time: 10.29 seconds.\n",
      "-- Epoch 1428\n",
      "Norm: 6249145438435.92, NNZs: 843, Bias: -194045899.621759, T: 4808076, Avg. loss: 138809721666691551719867667185664.000000\n",
      "Total training time: 10.30 seconds.\n",
      "-- Epoch 1429\n",
      "Norm: 5529693802210.22, NNZs: 843, Bias: -202242499.115310, T: 4811443, Avg. loss: 138760633254107764358406649937920.000000\n",
      "Total training time: 10.30 seconds.\n",
      "-- Epoch 1430\n",
      "Norm: 5662126711364.13, NNZs: 843, Bias: -415694100.402815, T: 4814810, Avg. loss: 138712464664889038976066772795392.000000\n",
      "Total training time: 10.31 seconds.\n",
      "-- Epoch 1431\n",
      "Norm: 5575304159708.58, NNZs: 843, Bias: 353760935.812853, T: 4818177, Avg. loss: 138663676218481049887533028081664.000000\n",
      "Total training time: 10.32 seconds.\n",
      "-- Epoch 1432\n",
      "Norm: 5799205066020.44, NNZs: 843, Bias: -286518438.934453, T: 4821544, Avg. loss: 138615166131082832138459863318528.000000\n",
      "Total training time: 10.33 seconds.\n",
      "-- Epoch 1433\n",
      "Norm: 5303501592800.98, NNZs: 843, Bias: -73106868.498211, T: 4824911, Avg. loss: 138567866309487373848658868961280.000000\n",
      "Total training time: 10.33 seconds.\n",
      "-- Epoch 1434\n",
      "Norm: 5285329434734.54, NNZs: 843, Bias: 140228800.693087, T: 4828278, Avg. loss: 138520556594004348501185796243456.000000\n",
      "Total training time: 10.34 seconds.\n",
      "-- Epoch 1435\n",
      "Norm: 5439459235616.27, NNZs: 843, Bias: -73091085.127244, T: 4831645, Avg. loss: 138472393720777022362796918046720.000000\n",
      "Total training time: 10.35 seconds.\n",
      "-- Epoch 1436\n",
      "Norm: 6059102102253.55, NNZs: 843, Bias: -286340864.624981, T: 4835012, Avg. loss: 138423832465862368733816760041472.000000\n",
      "Total training time: 10.35 seconds.\n",
      "-- Epoch 1437\n",
      "Norm: 5303256974280.22, NNZs: 843, Bias: -73076696.056442, T: 4838379, Avg. loss: 138376806068482880466146270642176.000000\n",
      "Total training time: 10.36 seconds.\n",
      "-- Epoch 1438\n",
      "Norm: 5021211279367.58, NNZs: 843, Bias: 140115180.725074, T: 4841746, Avg. loss: 138328870258468808361252508139520.000000\n",
      "Total training time: 10.37 seconds.\n",
      "-- Epoch 1439\n",
      "Norm: 5385102374947.34, NNZs: 843, Bias: -264315998.352160, T: 4845113, Avg. loss: 138280671894037745655027997868032.000000\n",
      "Total training time: 10.38 seconds.\n",
      "-- Epoch 1440\n",
      "Norm: 4782205175730.18, NNZs: 843, Bias: -51168583.401622, T: 4848480, Avg. loss: 138232660725065586504684462407680.000000\n",
      "Total training time: 10.38 seconds.\n",
      "-- Epoch 1441\n",
      "Norm: 4846453499561.51, NNZs: 843, Bias: 161905503.390473, T: 4851847, Avg. loss: 138185389756830407619549379690496.000000\n",
      "Total training time: 10.39 seconds.\n",
      "-- Epoch 1442\n",
      "Norm: 5529299370920.31, NNZs: 843, Bias: 374904293.803400, T: 4855214, Avg. loss: 138137478067505085454319693594624.000000\n",
      "Total training time: 10.40 seconds.\n",
      "-- Epoch 1443\n",
      "Norm: 5135916435893.68, NNZs: 843, Bias: -135611892.296025, T: 4858581, Avg. loss: 138088567863708516138888957263872.000000\n",
      "Total training time: 10.41 seconds.\n",
      "-- Epoch 1444\n",
      "Norm: 5616423058579.40, NNZs: 843, Bias: 77363373.396587, T: 4861948, Avg. loss: 138041124208556954186882098397184.000000\n",
      "Total training time: 10.41 seconds.\n",
      "-- Epoch 1445\n",
      "Norm: 5027114382743.64, NNZs: 843, Bias: -135578725.011145, T: 4865315, Avg. loss: 137994445339319886014175145623552.000000\n",
      "Total training time: 10.42 seconds.\n",
      "-- Epoch 1446\n",
      "Norm: 5448015544579.35, NNZs: 843, Bias: 351815730.705619, T: 4868682, Avg. loss: 137946811608544724696982781689856.000000\n",
      "Total training time: 10.43 seconds.\n",
      "-- Epoch 1447\n",
      "Norm: 5269858983799.24, NNZs: 843, Bias: 269688180.557410, T: 4872049, Avg. loss: 137899869269098273359947399954432.000000\n",
      "Total training time: 10.43 seconds.\n",
      "-- Epoch 1448\n",
      "Norm: 5155016949316.17, NNZs: 843, Bias: -250871430.747889, T: 4875416, Avg. loss: 137852255843216129021173714386944.000000\n",
      "Total training time: 10.44 seconds.\n",
      "-- Epoch 1449\n",
      "Norm: 5517796442870.58, NNZs: 843, Bias: -128321300.029727, T: 4878783, Avg. loss: 137804641753646758191327376048128.000000\n",
      "Total training time: 10.45 seconds.\n",
      "-- Epoch 1450\n",
      "Norm: 5306856089274.80, NNZs: 843, Bias: 84433435.476667, T: 4882150, Avg. loss: 137757529339226115293554264768512.000000\n",
      "Total training time: 10.46 seconds.\n",
      "-- Epoch 1451\n",
      "Norm: 4932237752358.89, NNZs: 843, Bias: -128289719.404765, T: 4885517, Avg. loss: 137710430267552192138367415091200.000000\n",
      "Total training time: 10.46 seconds.\n",
      "-- Epoch 1452\n",
      "Norm: 5556949411636.35, NNZs: 843, Bias: 84392390.200972, T: 4888884, Avg. loss: 137661934702303724835636194050048.000000\n",
      "Total training time: 10.47 seconds.\n",
      "-- Epoch 1453\n",
      "Norm: 4942568984926.76, NNZs: 843, Bias: -128254648.865839, T: 4892251, Avg. loss: 137614531673949103280977981997056.000000\n",
      "Total training time: 10.48 seconds.\n",
      "-- Epoch 1454\n",
      "Norm: 5120753682022.00, NNZs: 843, Bias: -340831043.724633, T: 4895618, Avg. loss: 137566970971002010449397973778432.000000\n",
      "Total training time: 10.48 seconds.\n",
      "-- Epoch 1455\n",
      "Norm: 5564440328874.86, NNZs: 843, Bias: -128221094.238098, T: 4898985, Avg. loss: 137519556967563419514859134386176.000000\n",
      "Total training time: 10.49 seconds.\n",
      "-- Epoch 1456\n",
      "Norm: 5425720574704.72, NNZs: 843, Bias: 257289223.290572, T: 4902352, Avg. loss: 137470927356860481254761582034944.000000\n",
      "Total training time: 10.50 seconds.\n",
      "-- Epoch 1457\n",
      "Norm: 5402965288378.61, NNZs: 843, Bias: 44755097.109576, T: 4905719, Avg. loss: 137422993866600497180553949216768.000000\n",
      "Total training time: 10.51 seconds.\n",
      "-- Epoch 1458\n",
      "Norm: 5082974645260.60, NNZs: 843, Bias: -167705134.465023, T: 4909086, Avg. loss: 137375992905673715822549589295104.000000\n",
      "Total training time: 10.51 seconds.\n",
      "-- Epoch 1459\n",
      "Norm: 5160249671166.94, NNZs: 843, Bias: -380096564.898578, T: 4912453, Avg. loss: 137329681660643826924952376836096.000000\n",
      "Total training time: 10.52 seconds.\n",
      "-- Epoch 1460\n",
      "Norm: 4817732398461.90, NNZs: 843, Bias: -167663832.689051, T: 4915820, Avg. loss: 137282114559639927085366253715456.000000\n",
      "Total training time: 10.53 seconds.\n",
      "-- Epoch 1461\n",
      "Norm: 5479324767522.42, NNZs: 843, Bias: -379979109.584091, T: 4919187, Avg. loss: 137234703239943335652218709737472.000000\n",
      "Total training time: 10.53 seconds.\n",
      "-- Epoch 1462\n",
      "Norm: 5123626543007.71, NNZs: 843, Bias: -167617420.206050, T: 4922554, Avg. loss: 137188900131061866547583194759168.000000\n",
      "Total training time: 10.54 seconds.\n",
      "-- Epoch 1463\n",
      "Norm: 5287405765852.47, NNZs: 843, Bias: 44668958.344204, T: 4925921, Avg. loss: 137141505385677293380110204796928.000000\n",
      "Total training time: 10.55 seconds.\n",
      "-- Epoch 1464\n",
      "Norm: 5172615192997.75, NNZs: 843, Bias: 256882169.170278, T: 4929288, Avg. loss: 137095493476110450030447419195392.000000\n",
      "Total training time: 10.56 seconds.\n",
      "-- Epoch 1465\n",
      "Norm: 5373987138066.01, NNZs: 843, Bias: 44642833.186382, T: 4932655, Avg. loss: 137048361773469652240765835804672.000000\n",
      "Total training time: 10.56 seconds.\n",
      "-- Epoch 1466\n",
      "Norm: 5135327300849.55, NNZs: 843, Bias: 313545527.710116, T: 4936022, Avg. loss: 137001068409283436044928457637888.000000\n",
      "Total training time: 10.57 seconds.\n",
      "-- Epoch 1467\n",
      "Norm: 5082054101008.74, NNZs: 843, Bias: 101365097.248060, T: 4939389, Avg. loss: 136955211041322445203062640345088.000000\n",
      "Total training time: 10.58 seconds.\n",
      "-- Epoch 1468\n",
      "Norm: 4850828179061.12, NNZs: 843, Bias: -110743434.707088, T: 4942756, Avg. loss: 136907826349610253554206900224000.000000\n",
      "Total training time: 10.59 seconds.\n",
      "-- Epoch 1469\n",
      "Norm: 5365795999693.24, NNZs: 843, Bias: 101314820.344682, T: 4946123, Avg. loss: 136861701285506318510523311718400.000000\n",
      "Total training time: 10.59 seconds.\n",
      "-- Epoch 1470\n",
      "Norm: 5025021000798.56, NNZs: 843, Bias: -76365082.302571, T: 4949490, Avg. loss: 136816493075156793466247930970112.000000\n",
      "Total training time: 10.60 seconds.\n",
      "-- Epoch 1471\n",
      "Norm: 4964444784017.12, NNZs: 843, Bias: 135621726.571395, T: 4952857, Avg. loss: 136770188917435999654866805850112.000000\n",
      "Total training time: 10.61 seconds.\n",
      "-- Epoch 1472\n",
      "Norm: 5154846214003.74, NNZs: 843, Bias: -76344292.706645, T: 4956224, Avg. loss: 136724471389750049423163513634816.000000\n",
      "Total training time: 10.61 seconds.\n",
      "-- Epoch 1473\n",
      "Norm: 5157682698475.17, NNZs: 843, Bias: -369841497.156109, T: 4959591, Avg. loss: 136677783348356149574115675602944.000000\n",
      "Total training time: 10.62 seconds.\n",
      "-- Epoch 1474\n",
      "Norm: 5194883609970.71, NNZs: 843, Bias: 265816957.179274, T: 4962958, Avg. loss: 136631513500544801984898024865792.000000\n",
      "Total training time: 10.63 seconds.\n",
      "-- Epoch 1475\n",
      "Norm: 5630954088975.28, NNZs: 843, Bias: -369731452.847043, T: 4966325, Avg. loss: 136585247915221400543046330744832.000000\n",
      "Total training time: 10.64 seconds.\n",
      "-- Epoch 1476\n",
      "Norm: 5052759511641.09, NNZs: 843, Bias: -157880267.850852, T: 4969692, Avg. loss: 136539143775213223505516904316928.000000\n",
      "Total training time: 10.64 seconds.\n",
      "-- Epoch 1477\n",
      "Norm: 5195599463754.91, NNZs: 843, Bias: -369622299.067666, T: 4973059, Avg. loss: 136494022306133609085188186308608.000000\n",
      "Total training time: 10.65 seconds.\n",
      "-- Epoch 1478\n",
      "Norm: 5334141194763.21, NNZs: 843, Bias: -157840147.525696, T: 4976426, Avg. loss: 136447940063733534895440337567744.000000\n",
      "Total training time: 10.66 seconds.\n",
      "-- Epoch 1479\n",
      "Norm: 5603401563119.26, NNZs: 843, Bias: 477245200.175834, T: 4979793, Avg. loss: 136401994227839343729589037826048.000000\n",
      "Total training time: 10.66 seconds.\n",
      "-- Epoch 1480\n",
      "Norm: 5759583379955.34, NNZs: 843, Bias: 305681557.161209, T: 4983160, Avg. loss: 136355293052656883763879287455744.000000\n",
      "Total training time: 10.67 seconds.\n",
      "-- Epoch 1481\n",
      "Norm: 5839473459204.34, NNZs: 843, Bias: -329224419.397383, T: 4986527, Avg. loss: 136308835610634985264439900504064.000000\n",
      "Total training time: 10.68 seconds.\n",
      "-- Epoch 1482\n",
      "Norm: 5442286991035.59, NNZs: 843, Bias: -117596034.959476, T: 4989894, Avg. loss: 136263061344709958007687676428288.000000\n",
      "Total training time: 10.69 seconds.\n",
      "-- Epoch 1483\n",
      "Norm: 5518911704581.95, NNZs: 843, Bias: 93964760.792840, T: 4993261, Avg. loss: 136217095122174760507276666077184.000000\n",
      "Total training time: 10.69 seconds.\n",
      "-- Epoch 1484\n",
      "Norm: 5831735627877.37, NNZs: 843, Bias: -205847500.486741, T: 4996628, Avg. loss: 136171205362531267726838343925760.000000\n",
      "Total training time: 10.70 seconds.\n",
      "-- Epoch 1485\n",
      "Norm: 5698990904503.45, NNZs: 843, Bias: 428602168.339439, T: 4999995, Avg. loss: 136125170762171078143435289395200.000000\n",
      "Total training time: 10.71 seconds.\n",
      "-- Epoch 1486\n",
      "Norm: 5387062821749.70, NNZs: 843, Bias: 69634808.374849, T: 5003362, Avg. loss: 136079441142473160737288286633984.000000\n",
      "Total training time: 10.71 seconds.\n",
      "-- Epoch 1487\n",
      "Norm: 5553008781896.97, NNZs: 843, Bias: -141789337.056087, T: 5006729, Avg. loss: 136035071065653404785013034057728.000000\n",
      "Total training time: 10.72 seconds.\n",
      "-- Epoch 1488\n",
      "Norm: 5591013360097.17, NNZs: 843, Bias: 162407834.879414, T: 5010096, Avg. loss: 135989746774508740843193315098624.000000\n",
      "Total training time: 10.73 seconds.\n",
      "-- Epoch 1489\n",
      "Norm: 5317025441426.36, NNZs: 843, Bias: 108416590.134683, T: 5013463, Avg. loss: 135943189142676099087465153822720.000000\n",
      "Total training time: 10.74 seconds.\n",
      "-- Epoch 1490\n",
      "Norm: 5652934358596.57, NNZs: 843, Bias: 319687000.084781, T: 5016830, Avg. loss: 135897580859614940302176960905216.000000\n",
      "Total training time: 10.74 seconds.\n",
      "-- Epoch 1491\n",
      "Norm: 5607196014888.92, NNZs: 843, Bias: 108366564.115766, T: 5020197, Avg. loss: 135851768111215678262819885678592.000000\n",
      "Total training time: 10.75 seconds.\n",
      "-- Epoch 1492\n",
      "Norm: 5173774414751.46, NNZs: 843, Bias: -102884089.480098, T: 5023564, Avg. loss: 135807273673719483794250853777408.000000\n",
      "Total training time: 10.76 seconds.\n",
      "-- Epoch 1493\n",
      "Norm: 5142068239402.25, NNZs: 843, Bias: 108318535.337625, T: 5026931, Avg. loss: 135761060736332274933826249031680.000000\n",
      "Total training time: 10.77 seconds.\n",
      "-- Epoch 1494\n",
      "Norm: 5173292907252.77, NNZs: 843, Bias: -158536153.764889, T: 5030298, Avg. loss: 135715800370900018156445580132352.000000\n",
      "Total training time: 10.77 seconds.\n",
      "-- Epoch 1495\n",
      "Norm: 5759508563038.00, NNZs: 843, Bias: 474842621.041176, T: 5033665, Avg. loss: 135670818990244270203153472815104.000000\n",
      "Total training time: 10.78 seconds.\n",
      "-- Epoch 1496\n",
      "Norm: 5318739932072.97, NNZs: 843, Bias: -158495565.609478, T: 5037032, Avg. loss: 135625464919162902856344923013120.000000\n",
      "Total training time: 10.79 seconds.\n",
      "-- Epoch 1497\n",
      "Norm: 5315810064214.74, NNZs: 843, Bias: 275219319.299291, T: 5040399, Avg. loss: 135579746591192360886885656035328.000000\n",
      "Total training time: 10.79 seconds.\n",
      "-- Epoch 1498\n",
      "Norm: 5159373098571.96, NNZs: 843, Bias: 64153808.879546, T: 5043766, Avg. loss: 135534466495342454560381277831168.000000\n",
      "Total training time: 10.80 seconds.\n",
      "-- Epoch 1499\n",
      "Norm: 5146727524656.92, NNZs: 843, Bias: 275117979.716034, T: 5047133, Avg. loss: 135489508879333101783429771427840.000000\n",
      "Total training time: 10.81 seconds.\n",
      "-- Epoch 1500\n",
      "Norm: 5439401825260.64, NNZs: 843, Bias: 64119648.336014, T: 5050500, Avg. loss: 135444678642517355396741707333632.000000\n",
      "Total training time: 10.82 seconds.\n",
      "-- Epoch 1501\n",
      "Norm: 5084217883150.88, NNZs: 843, Bias: 275012277.114397, T: 5053867, Avg. loss: 135398566730517958024649253060608.000000\n",
      "Total training time: 10.82 seconds.\n",
      "-- Epoch 1502\n",
      "Norm: 5017425680094.12, NNZs: 843, Bias: 64088484.859621, T: 5057234, Avg. loss: 135353992478697060717317175902208.000000\n",
      "Total training time: 10.83 seconds.\n",
      "-- Epoch 1503\n",
      "Norm: 5090355383862.64, NNZs: 843, Bias: 226885929.374870, T: 5060601, Avg. loss: 135308448058221677378788961288192.000000\n",
      "Total training time: 10.84 seconds.\n",
      "-- Epoch 1504\n",
      "Norm: 5020680128111.47, NNZs: 843, Bias: 16033339.042025, T: 5063968, Avg. loss: 135263195209628303327370395254784.000000\n",
      "Total training time: 10.84 seconds.\n",
      "-- Epoch 1505\n",
      "Norm: 5322522774070.16, NNZs: 843, Bias: 226790380.904148, T: 5067335, Avg. loss: 135218147088856635856829991092224.000000\n",
      "Total training time: 10.85 seconds.\n",
      "-- Epoch 1506\n",
      "Norm: 5141689845282.03, NNZs: 843, Bias: 16007416.790341, T: 5070702, Avg. loss: 135173267186121018445178066698240.000000\n",
      "Total training time: 10.86 seconds.\n",
      "-- Epoch 1507\n",
      "Norm: 5229577614666.84, NNZs: 843, Bias: 331111457.795838, T: 5074069, Avg. loss: 135128345608005469490723206201344.000000\n",
      "Total training time: 10.87 seconds.\n",
      "-- Epoch 1508\n",
      "Norm: 5338198064768.61, NNZs: 843, Bias: 302719947.079598, T: 5077436, Avg. loss: 135084097262842544948799388778496.000000\n",
      "Total training time: 10.87 seconds.\n",
      "-- Epoch 1509\n",
      "Norm: 5797208210194.43, NNZs: 843, Bias: 92035929.801320, T: 5080803, Avg. loss: 135039301964337534007718992412672.000000\n",
      "Total training time: 10.88 seconds.\n",
      "-- Epoch 1510\n",
      "Norm: 5379795024581.35, NNZs: 843, Bias: -118578517.524209, T: 5084170, Avg. loss: 134995052596321025265080209506304.000000\n",
      "Total training time: 10.89 seconds.\n",
      "-- Epoch 1511\n",
      "Norm: 5242068058705.03, NNZs: 843, Bias: 142082908.617615, T: 5087537, Avg. loss: 134949896751217771012900924686336.000000\n",
      "Total training time: 10.89 seconds.\n",
      "-- Epoch 1512\n",
      "Norm: 5162806934478.12, NNZs: 843, Bias: -68469854.170976, T: 5090904, Avg. loss: 134905315129936901676620996149248.000000\n",
      "Total training time: 10.90 seconds.\n",
      "-- Epoch 1513\n",
      "Norm: 5421691027511.06, NNZs: 843, Bias: -462732751.000015, T: 5094271, Avg. loss: 134861158875481728867372231557120.000000\n",
      "Total training time: 10.91 seconds.\n",
      "-- Epoch 1514\n",
      "Norm: 5393861529762.63, NNZs: 843, Bias: 42522337.148320, T: 5097638, Avg. loss: 134817269169309753403882547970048.000000\n",
      "Total training time: 10.92 seconds.\n",
      "-- Epoch 1515\n",
      "Norm: 5128630317976.52, NNZs: 843, Bias: 102266865.705541, T: 5101005, Avg. loss: 134772902503909879757460599734272.000000\n",
      "Total training time: 10.92 seconds.\n",
      "-- Epoch 1516\n",
      "Norm: 5377698177980.95, NNZs: 843, Bias: 19240820.587168, T: 5104372, Avg. loss: 134728632021316966049107052331008.000000\n",
      "Total training time: 10.93 seconds.\n",
      "-- Epoch 1517\n",
      "Norm: 5150979390617.73, NNZs: 843, Bias: -191123244.074152, T: 5107739, Avg. loss: 134684386129957430172980497874944.000000\n",
      "Total training time: 10.94 seconds.\n",
      "-- Epoch 1518\n",
      "Norm: 5643887448577.08, NNZs: 843, Bias: 19217787.169308, T: 5111106, Avg. loss: 134639687886248086418099357614080.000000\n",
      "Total training time: 10.95 seconds.\n",
      "-- Epoch 1519\n",
      "Norm: 5464505407611.58, NNZs: 843, Bias: -196997260.638949, T: 5114473, Avg. loss: 134595631792455963760030308630528.000000\n",
      "Total training time: 10.95 seconds.\n",
      "-- Epoch 1520\n",
      "Norm: 5200345229327.88, NNZs: 843, Bias: 13273011.978236, T: 5117840, Avg. loss: 134551899881473724294399996723200.000000\n",
      "Total training time: 10.96 seconds.\n",
      "-- Epoch 1521\n",
      "Norm: 5657994458200.98, NNZs: 843, Bias: -196947686.823956, T: 5121207, Avg. loss: 134507613171260323177386009952256.000000\n",
      "Total training time: 10.97 seconds.\n",
      "-- Epoch 1522\n",
      "Norm: 5824047540773.16, NNZs: 843, Bias: -407102370.519283, T: 5124574, Avg. loss: 134463550467479939396997985861632.000000\n",
      "Total training time: 10.97 seconds.\n",
      "-- Epoch 1523\n",
      "Norm: 6144582456148.95, NNZs: 843, Bias: -196901352.039349, T: 5127941, Avg. loss: 134419839628313867587080337489920.000000\n",
      "Total training time: 10.98 seconds.\n",
      "-- Epoch 1524\n",
      "Norm: 5585387435870.70, NNZs: 843, Bias: 13230401.756461, T: 5131308, Avg. loss: 134375805499020534019832670060544.000000\n",
      "Total training time: 10.99 seconds.\n",
      "-- Epoch 1525\n",
      "Norm: 5329152327291.76, NNZs: 843, Bias: -196853229.276831, T: 5134675, Avg. loss: 134331790075604217380580758650880.000000\n",
      "Total training time: 11.00 seconds.\n",
      "-- Epoch 1526\n",
      "Norm: 5506101827609.10, NNZs: 843, Bias: 13209915.326475, T: 5138042, Avg. loss: 134288262255034853744071743111168.000000\n",
      "Total training time: 11.00 seconds.\n",
      "-- Epoch 1527\n",
      "Norm: 5655884663227.51, NNZs: 843, Bias: 223204973.550910, T: 5141409, Avg. loss: 134244710413831087390164841922560.000000\n",
      "Total training time: 11.01 seconds.\n",
      "-- Epoch 1528\n",
      "Norm: 5507553201330.89, NNZs: 843, Bias: 13193443.010512, T: 5144776, Avg. loss: 134200710373269963982167217274880.000000\n",
      "Total training time: 11.02 seconds.\n",
      "-- Epoch 1529\n",
      "Norm: 5585481879481.98, NNZs: 843, Bias: 267802683.406562, T: 5148143, Avg. loss: 134156322888980329151207898611712.000000\n",
      "Total training time: 11.02 seconds.\n",
      "-- Epoch 1530\n",
      "Norm: 5702772445117.50, NNZs: 843, Bias: -361951197.889623, T: 5151510, Avg. loss: 134112148487390649919400765816832.000000\n",
      "Total training time: 11.03 seconds.\n",
      "-- Epoch 1531\n",
      "Norm: 5317452736021.41, NNZs: 843, Bias: -152031767.068577, T: 5154877, Avg. loss: 134068588847528390670990861926400.000000\n",
      "Total training time: 11.04 seconds.\n",
      "-- Epoch 1532\n",
      "Norm: 5909157316862.11, NNZs: 843, Bias: -361848021.989655, T: 5158244, Avg. loss: 134025245654781990088713128902656.000000\n",
      "Total training time: 11.05 seconds.\n",
      "-- Epoch 1533\n",
      "Norm: 5325450767642.51, NNZs: 843, Bias: -151997037.671719, T: 5161611, Avg. loss: 133981360893257259581675937988608.000000\n",
      "Total training time: 11.05 seconds.\n",
      "-- Epoch 1534\n",
      "Norm: 5212231939276.64, NNZs: 843, Bias: 57788095.290612, T: 5164978, Avg. loss: 133938031164346978815802580402176.000000\n",
      "Total training time: 11.06 seconds.\n",
      "-- Epoch 1535\n",
      "Norm: 5477659125858.38, NNZs: 843, Bias: -151958971.392949, T: 5168345, Avg. loss: 133894698058013228323200129040384.000000\n",
      "Total training time: 11.07 seconds.\n",
      "-- Epoch 1536\n",
      "Norm: 5451286793182.70, NNZs: 843, Bias: 57754628.345680, T: 5171712, Avg. loss: 133851462496475971817946734395392.000000\n",
      "Total training time: 11.07 seconds.\n",
      "-- Epoch 1537\n",
      "Norm: 5700710729058.03, NNZs: 843, Bias: -151923503.520957, T: 5175079, Avg. loss: 133808701669973883540557254361088.000000\n",
      "Total training time: 11.08 seconds.\n",
      "-- Epoch 1538\n",
      "Norm: 5545990747007.93, NNZs: 843, Bias: 57721964.563358, T: 5178446, Avg. loss: 133765084923763698022161173184512.000000\n",
      "Total training time: 11.09 seconds.\n",
      "-- Epoch 1539\n",
      "Norm: 5481360593741.42, NNZs: 843, Bias: 225006892.995486, T: 5181813, Avg. loss: 133722412363488923023577102417920.000000\n",
      "Total training time: 11.10 seconds.\n",
      "-- Epoch 1540\n",
      "Norm: 5906469031904.02, NNZs: 843, Bias: 434524804.082509, T: 5185180, Avg. loss: 133678820680255223148408257642496.000000\n",
      "Total training time: 11.10 seconds.\n",
      "-- Epoch 1541\n",
      "Norm: 5771571051291.76, NNZs: 843, Bias: -194133312.725800, T: 5188547, Avg. loss: 133635754794473397496200963293184.000000\n",
      "Total training time: 11.11 seconds.\n",
      "-- Epoch 1542\n",
      "Norm: 5634448743491.61, NNZs: 843, Bias: -403604198.658538, T: 5191914, Avg. loss: 133592001693246064759311230304256.000000\n",
      "Total training time: 11.12 seconds.\n",
      "-- Epoch 1543\n",
      "Norm: 5419348367780.14, NNZs: 843, Bias: 224828586.329978, T: 5195281, Avg. loss: 133549302397378745171173179392000.000000\n",
      "Total training time: 11.13 seconds.\n",
      "-- Epoch 1544\n",
      "Norm: 5846203384363.68, NNZs: 843, Bias: -403490292.856771, T: 5198648, Avg. loss: 133505693339633967656696826298368.000000\n",
      "Total training time: 11.13 seconds.\n",
      "-- Epoch 1545\n",
      "Norm: 5624011358957.02, NNZs: 843, Bias: 224740251.667891, T: 5202015, Avg. loss: 133462526818712136938909096476672.000000\n",
      "Total training time: 11.14 seconds.\n",
      "-- Epoch 1546\n",
      "Norm: 5520612027458.14, NNZs: 843, Bias: 15341201.635648, T: 5205382, Avg. loss: 133419776603186346849404041822208.000000\n",
      "Total training time: 11.15 seconds.\n",
      "-- Epoch 1547\n",
      "Norm: 5554753865647.88, NNZs: 843, Bias: 224652645.555693, T: 5208749, Avg. loss: 133376665563165464656488467267584.000000\n",
      "Total training time: 11.15 seconds.\n",
      "-- Epoch 1548\n",
      "Norm: 5579667524557.13, NNZs: 843, Bias: 15318606.114515, T: 5212116, Avg. loss: 133333425696993811279882085203968.000000\n",
      "Total training time: 11.16 seconds.\n",
      "-- Epoch 1549\n",
      "Norm: 5677329889949.62, NNZs: 843, Bias: -193944181.708541, T: 5215483, Avg. loss: 133290526004845757811958793371648.000000\n",
      "Total training time: 11.17 seconds.\n",
      "-- Epoch 1550\n",
      "Norm: 5734743070029.44, NNZs: 843, Bias: 433744068.714869, T: 5218850, Avg. loss: 133247826223327710360342141861888.000000\n",
      "Total training time: 11.18 seconds.\n",
      "-- Epoch 1551\n",
      "Norm: 5906517479187.90, NNZs: 843, Bias: 224482763.276169, T: 5222217, Avg. loss: 133204825823174418260534872768512.000000\n",
      "Total training time: 11.18 seconds.\n",
      "-- Epoch 1552\n",
      "Norm: 5440452069839.70, NNZs: 843, Bias: 15285332.347338, T: 5225584, Avg. loss: 133162468575044874795673825837056.000000\n",
      "Total training time: 11.19 seconds.\n",
      "-- Epoch 1553\n",
      "Norm: 5623181120639.43, NNZs: 843, Bias: 162300569.086858, T: 5228951, Avg. loss: 133119193767094912418768098426880.000000\n",
      "Total training time: 11.20 seconds.\n",
      "-- Epoch 1554\n",
      "Norm: 5672030952332.33, NNZs: 843, Bias: -46816611.807332, T: 5232318, Avg. loss: 133076698363027383622671685124096.000000\n",
      "Total training time: 11.20 seconds.\n",
      "-- Epoch 1555\n",
      "Norm: 6176497884071.71, NNZs: 843, Bias: 162239160.157783, T: 5235685, Avg. loss: 133033442623784528223465453387776.000000\n",
      "Total training time: 11.21 seconds.\n",
      "-- Epoch 1556\n",
      "Norm: 5609939686696.93, NNZs: 843, Bias: 371224853.079771, T: 5239052, Avg. loss: 132991042405255666921385943367680.000000\n",
      "Total training time: 11.22 seconds.\n",
      "-- Epoch 1557\n",
      "Norm: 5330394002270.11, NNZs: 843, Bias: -2489259.390564, T: 5242419, Avg. loss: 132948628260181006210872612225024.000000\n",
      "Total training time: 11.23 seconds.\n",
      "-- Epoch 1558\n",
      "Norm: 5409532260270.03, NNZs: 843, Bias: -296656556.473484, T: 5245786, Avg. loss: 132906231421229055833345672347648.000000\n",
      "Total training time: 11.23 seconds.\n",
      "-- Epoch 1559\n",
      "Norm: 5608531802629.79, NNZs: 843, Bias: 168902870.936596, T: 5249153, Avg. loss: 132863611685957148885626271039488.000000\n",
      "Total training time: 11.24 seconds.\n",
      "-- Epoch 1560\n",
      "Norm: 5975246940342.89, NNZs: 843, Bias: -40012628.810524, T: 5252520, Avg. loss: 132821061979332815924026033569792.000000\n",
      "Total training time: 11.25 seconds.\n",
      "-- Epoch 1561\n",
      "Norm: 5379598112774.44, NNZs: 843, Bias: 168839470.355936, T: 5255887, Avg. loss: 132778208923275288034270708236288.000000\n",
      "Total training time: 11.25 seconds.\n",
      "-- Epoch 1562\n",
      "Norm: 5396151078354.49, NNZs: 843, Bias: -40011269.239661, T: 5259254, Avg. loss: 132735896609276845699531832033280.000000\n",
      "Total training time: 11.26 seconds.\n",
      "-- Epoch 1563\n",
      "Norm: 5760947361714.45, NNZs: 843, Bias: -666364994.976447, T: 5262621, Avg. loss: 132693137183896555022405003640832.000000\n",
      "Total training time: 11.27 seconds.\n",
      "-- Epoch 1564\n",
      "Norm: 5481570920275.38, NNZs: 843, Bias: -40008502.321924, T: 5265988, Avg. loss: 132651233237395006452669167435776.000000\n",
      "Total training time: 11.28 seconds.\n",
      "-- Epoch 1565\n",
      "Norm: 5798475987979.20, NNZs: 843, Bias: -89725440.031698, T: 5269355, Avg. loss: 132608441564891979253641212067840.000000\n",
      "Total training time: 11.28 seconds.\n",
      "-- Epoch 1566\n",
      "Norm: 6200397340859.67, NNZs: 843, Bias: 536340349.066814, T: 5272722, Avg. loss: 132566384927755306056503674273792.000000\n",
      "Total training time: 11.29 seconds.\n",
      "-- Epoch 1567\n",
      "Norm: 5850483435319.61, NNZs: 843, Bias: -89703503.440118, T: 5276089, Avg. loss: 132525004525365514894326640934912.000000\n",
      "Total training time: 11.30 seconds.\n",
      "-- Epoch 1568\n",
      "Norm: 5580085855584.84, NNZs: 843, Bias: 118927522.253821, T: 5279456, Avg. loss: 132483097174642241346359292592128.000000\n",
      "Total training time: 11.30 seconds.\n",
      "-- Epoch 1569\n",
      "Norm: 5460618547233.30, NNZs: 843, Bias: -89679192.596708, T: 5282823, Avg. loss: 132440187979344418882973547364352.000000\n",
      "Total training time: 11.31 seconds.\n",
      "-- Epoch 1570\n",
      "Norm: 5925690320984.20, NNZs: 843, Bias: -298219890.281489, T: 5286190, Avg. loss: 132397935116220112656852429307904.000000\n",
      "Total training time: 11.32 seconds.\n",
      "-- Epoch 1571\n",
      "Norm: 5469864249592.33, NNZs: 843, Bias: 327379404.783699, T: 5289557, Avg. loss: 132355236387363516173236788264960.000000\n",
      "Total training time: 11.33 seconds.\n",
      "-- Epoch 1572\n",
      "Norm: 5437203445225.38, NNZs: 843, Bias: -298132682.151889, T: 5292924, Avg. loss: 132313005951751869763197610229760.000000\n",
      "Total training time: 11.33 seconds.\n",
      "-- Epoch 1573\n",
      "Norm: 5701596664016.20, NNZs: 843, Bias: 352604213.298330, T: 5296291, Avg. loss: 132271222519682315768647798226944.000000\n",
      "Total training time: 11.34 seconds.\n",
      "-- Epoch 1574\n",
      "Norm: 5612786207738.25, NNZs: 843, Bias: 321614209.255732, T: 5299658, Avg. loss: 132229494949367548330982286295040.000000\n",
      "Total training time: 11.35 seconds.\n",
      "-- Epoch 1575\n",
      "Norm: 5355508929593.88, NNZs: 843, Bias: 6098520.694843, T: 5303025, Avg. loss: 132187483568906484941314607546368.000000\n",
      "Total training time: 11.35 seconds.\n",
      "-- Epoch 1576\n",
      "Norm: 5370791380613.38, NNZs: 843, Bias: 231993860.183596, T: 5306392, Avg. loss: 132145375256519409357566834114560.000000\n",
      "Total training time: 11.36 seconds.\n",
      "-- Epoch 1577\n",
      "Norm: 5998363272335.19, NNZs: 843, Bias: -490767625.379701, T: 5309759, Avg. loss: 132103512339382276446197389459456.000000\n",
      "Total training time: 11.37 seconds.\n",
      "-- Epoch 1578\n",
      "Norm: 5600025523265.27, NNZs: 843, Bias: -282407092.029671, T: 5313126, Avg. loss: 132061301519516995369226519707648.000000\n",
      "Total training time: 11.38 seconds.\n",
      "-- Epoch 1579\n",
      "Norm: 6001564481859.68, NNZs: 843, Bias: -74112088.076441, T: 5316493, Avg. loss: 132020066818025935446233562742784.000000\n",
      "Total training time: 11.38 seconds.\n",
      "-- Epoch 1580\n",
      "Norm: 5551717381524.69, NNZs: 843, Bias: -282322472.843137, T: 5319860, Avg. loss: 131978053480961332210668668977152.000000\n",
      "Total training time: 11.39 seconds.\n",
      "-- Epoch 1581\n",
      "Norm: 5361769442808.63, NNZs: 843, Bias: -175978506.089122, T: 5323227, Avg. loss: 131936244683721685920502748545024.000000\n",
      "Total training time: 11.40 seconds.\n",
      "-- Epoch 1582\n",
      "Norm: 5402756462953.53, NNZs: 843, Bias: -255073295.789414, T: 5326594, Avg. loss: 131895151402880024806911601803264.000000\n",
      "Total training time: 11.41 seconds.\n",
      "-- Epoch 1583\n",
      "Norm: 5689251214718.83, NNZs: 843, Bias: 472701664.574271, T: 5329961, Avg. loss: 131852568456013216726505855385600.000000\n",
      "Total training time: 11.41 seconds.\n",
      "-- Epoch 1584\n",
      "Norm: 5484369378284.77, NNZs: 843, Bias: 264531975.010045, T: 5333328, Avg. loss: 131811620524275601156521594978304.000000\n",
      "Total training time: 11.42 seconds.\n",
      "-- Epoch 1585\n",
      "Norm: 5393316316031.49, NNZs: 843, Bias: 213187982.308084, T: 5336695, Avg. loss: 131770488928618523237772796362752.000000\n",
      "Total training time: 11.43 seconds.\n",
      "-- Epoch 1586\n",
      "Norm: 5530770806978.10, NNZs: 843, Bias: 421176130.824531, T: 5340062, Avg. loss: 131729159067130600819424514539520.000000\n",
      "Total training time: 11.43 seconds.\n",
      "-- Epoch 1587\n",
      "Norm: 5642530423208.11, NNZs: 843, Bias: -202868585.508062, T: 5343429, Avg. loss: 131688058614435840470819549478912.000000\n",
      "Total training time: 11.44 seconds.\n",
      "-- Epoch 1588\n",
      "Norm: 5423681420867.11, NNZs: 843, Bias: -410802076.004651, T: 5346796, Avg. loss: 131646123523907971897375835815936.000000\n",
      "Total training time: 11.45 seconds.\n",
      "-- Epoch 1589\n",
      "Norm: 5237716559462.30, NNZs: 843, Bias: -202817001.785585, T: 5350163, Avg. loss: 131604639960975551321568427638784.000000\n",
      "Total training time: 11.46 seconds.\n",
      "-- Epoch 1590\n",
      "Norm: 5565745737409.92, NNZs: 843, Bias: 5101566.040084, T: 5353530, Avg. loss: 131563748904021631864480657833984.000000\n",
      "Total training time: 11.46 seconds.\n",
      "-- Epoch 1591\n",
      "Norm: 5683439570735.80, NNZs: 843, Bias: 26309743.421052, T: 5356897, Avg. loss: 131522622833058083041739605540864.000000\n",
      "Total training time: 11.47 seconds.\n",
      "-- Epoch 1592\n",
      "Norm: 5445059623577.93, NNZs: 843, Bias: -359658036.712866, T: 5360264, Avg. loss: 131480996764553997333801207857152.000000\n",
      "Total training time: 11.48 seconds.\n",
      "-- Epoch 1593\n",
      "Norm: 5089599710062.18, NNZs: 843, Bias: -47902233.585007, T: 5363631, Avg. loss: 131439804503220414696433320460288.000000\n",
      "Total training time: 11.48 seconds.\n",
      "-- Epoch 1594\n",
      "Norm: 5081971593688.64, NNZs: 843, Bias: -255660492.872712, T: 5366998, Avg. loss: 131398794116349243045770287906816.000000\n",
      "Total training time: 11.49 seconds.\n",
      "-- Epoch 1595\n",
      "Norm: 5231614424722.56, NNZs: 843, Bias: -47894869.068150, T: 5370365, Avg. loss: 131357339722695230233045645656064.000000\n",
      "Total training time: 11.50 seconds.\n",
      "-- Epoch 1596\n",
      "Norm: 5579440318463.39, NNZs: 843, Bias: -255589756.124300, T: 5373732, Avg. loss: 131316155294008374532381666705408.000000\n",
      "Total training time: 11.51 seconds.\n",
      "-- Epoch 1597\n",
      "Norm: 5674598880371.92, NNZs: 843, Bias: -47890665.264427, T: 5377099, Avg. loss: 131274993392306480654224455630848.000000\n",
      "Total training time: 11.51 seconds.\n",
      "-- Epoch 1598\n",
      "Norm: 5994652543933.63, NNZs: 843, Bias: -670784299.872810, T: 5380466, Avg. loss: 131233915528885986588686098104320.000000\n",
      "Total training time: 11.52 seconds.\n",
      "-- Epoch 1599\n",
      "Norm: 5416700796830.93, NNZs: 843, Bias: -47885637.637734, T: 5383833, Avg. loss: 131192330504679009909977502449664.000000\n",
      "Total training time: 11.53 seconds.\n",
      "-- Epoch 1600\n",
      "Norm: 5640293060640.51, NNZs: 843, Bias: 159683236.302773, T: 5387200, Avg. loss: 131151275315046071726844816654336.000000\n",
      "Total training time: 11.54 seconds.\n",
      "-- Epoch 1601\n",
      "Norm: 5631684383089.83, NNZs: 843, Bias: -47880792.842315, T: 5390567, Avg. loss: 131109734261929026715042040512512.000000\n",
      "Total training time: 11.54 seconds.\n",
      "-- Epoch 1602\n",
      "Norm: 5260847671290.54, NNZs: 843, Bias: -129250278.945652, T: 5393934, Avg. loss: 131068811997742946226321129209856.000000\n",
      "Total training time: 11.55 seconds.\n",
      "-- Epoch 1603\n",
      "Norm: 5780732414940.41, NNZs: 843, Bias: 40175098.139905, T: 5397301, Avg. loss: 131027249123227441045122666463232.000000\n",
      "Total training time: 11.56 seconds.\n",
      "-- Epoch 1604\n",
      "Norm: 5319562778542.96, NNZs: 843, Bias: -167275656.235118, T: 5400668, Avg. loss: 130986407174245371125025053081600.000000\n",
      "Total training time: 11.56 seconds.\n",
      "-- Epoch 1605\n",
      "Norm: 5348845434469.02, NNZs: 843, Bias: 40152598.747454, T: 5404035, Avg. loss: 130944754685202412244675017572352.000000\n",
      "Total training time: 11.57 seconds.\n",
      "-- Epoch 1606\n",
      "Norm: 5389474657582.81, NNZs: 843, Bias: 247514781.022220, T: 5407402, Avg. loss: 130904145084393707846512179937280.000000\n",
      "Total training time: 11.58 seconds.\n",
      "-- Epoch 1607\n",
      "Norm: 5578750940361.39, NNZs: 843, Bias: -208204618.287939, T: 5410769, Avg. loss: 130863266718659775253974265888768.000000\n",
      "Total training time: 11.59 seconds.\n",
      "-- Epoch 1608\n",
      "Norm: 5378939147771.13, NNZs: 843, Bias: 45464056.217445, T: 5414136, Avg. loss: 130822267425523454002773945745408.000000\n",
      "Total training time: 11.59 seconds.\n",
      "-- Epoch 1609\n",
      "Norm: 5764937802898.72, NNZs: 843, Bias: -540493784.028183, T: 5417503, Avg. loss: 130782411882378382236967436812288.000000\n",
      "Total training time: 11.60 seconds.\n",
      "-- Epoch 1610\n",
      "Norm: 5624902902161.24, NNZs: 843, Bias: -333171129.647420, T: 5420870, Avg. loss: 130742413267404266365775555592192.000000\n",
      "Total training time: 11.61 seconds.\n",
      "-- Epoch 1611\n",
      "Norm: 5762727907871.54, NNZs: 843, Bias: 288511034.659350, T: 5424237, Avg. loss: 130701530478166793429961685336064.000000\n",
      "Total training time: 11.61 seconds.\n",
      "-- Epoch 1612\n",
      "Norm: 5620379168878.29, NNZs: 843, Bias: -241432677.715835, T: 5427604, Avg. loss: 130661189655088726305849626066944.000000\n",
      "Total training time: 11.62 seconds.\n",
      "-- Epoch 1613\n",
      "Norm: 5567822584758.38, NNZs: 843, Bias: -34253301.353045, T: 5430971, Avg. loss: 130621352474502462832714826383360.000000\n",
      "Total training time: 11.63 seconds.\n",
      "-- Epoch 1614\n",
      "Norm: 5479205761859.97, NNZs: 843, Bias: 172862959.501581, T: 5434338, Avg. loss: 130580930714145374370696649506816.000000\n",
      "Total training time: 11.64 seconds.\n",
      "-- Epoch 1615\n",
      "Norm: 5887551339340.72, NNZs: 843, Bias: -508391359.999935, T: 5437705, Avg. loss: 130540403002377289262132212269056.000000\n",
      "Total training time: 11.64 seconds.\n",
      "-- Epoch 1616\n",
      "Norm: 5529928935967.37, NNZs: 843, Bias: -301261552.591574, T: 5441072, Avg. loss: 130499979123074108149699868360704.000000\n",
      "Total training time: 11.65 seconds.\n",
      "-- Epoch 1617\n",
      "Norm: 5520279266963.04, NNZs: 843, Bias: -19959584.974778, T: 5444439, Avg. loss: 130459355764301245912995642474496.000000\n",
      "Total training time: 11.66 seconds.\n",
      "-- Epoch 1618\n",
      "Norm: 5682494358493.80, NNZs: 843, Bias: 187024900.590266, T: 5447806, Avg. loss: 130419358497196221934615039836160.000000\n",
      "Total training time: 11.67 seconds.\n",
      "-- Epoch 1619\n",
      "Norm: 5849072269476.55, NNZs: 843, Bias: 53597989.793567, T: 5451173, Avg. loss: 130378647508971689077652487929856.000000\n",
      "Total training time: 11.67 seconds.\n",
      "-- Epoch 1620\n",
      "Norm: 5401546797504.32, NNZs: 843, Bias: -83287765.583482, T: 5454540, Avg. loss: 130338769427616001072554022273024.000000\n",
      "Total training time: 11.68 seconds.\n",
      "-- Epoch 1621\n",
      "Norm: 5533647874489.17, NNZs: 843, Bias: 219709266.088397, T: 5457907, Avg. loss: 130298716230302376921183054462976.000000\n",
      "Total training time: 11.69 seconds.\n",
      "-- Epoch 1622\n",
      "Norm: 5366480290684.81, NNZs: 843, Bias: 12807221.416335, T: 5461274, Avg. loss: 130259117720995813726619026063360.000000\n",
      "Total training time: 11.69 seconds.\n",
      "-- Epoch 1623\n",
      "Norm: 5657851144954.95, NNZs: 843, Bias: -194030490.631659, T: 5464641, Avg. loss: 130218641418776237369738078978048.000000\n",
      "Total training time: 11.70 seconds.\n",
      "-- Epoch 1624\n",
      "Norm: 5222537350107.33, NNZs: 843, Bias: 12786414.816030, T: 5468008, Avg. loss: 130178736280996269079222472933376.000000\n",
      "Total training time: 11.71 seconds.\n",
      "-- Epoch 1625\n",
      "Norm: 5473542158138.72, NNZs: 843, Bias: 219538440.908425, T: 5471375, Avg. loss: 130138461855198158285133496451072.000000\n",
      "Total training time: 11.72 seconds.\n",
      "-- Epoch 1626\n",
      "Norm: 5479332453202.55, NNZs: 843, Bias: 12764958.223248, T: 5474742, Avg. loss: 130098318885804177891396564287488.000000\n",
      "Total training time: 11.72 seconds.\n",
      "-- Epoch 1627\n",
      "Norm: 5524885059035.64, NNZs: 843, Bias: -193945164.148509, T: 5478109, Avg. loss: 130058578019297597734776331042816.000000\n",
      "Total training time: 11.73 seconds.\n",
      "-- Epoch 1628\n",
      "Norm: 5556272038578.92, NNZs: 843, Bias: 339178170.991225, T: 5481476, Avg. loss: 130018845510342549292041719775232.000000\n",
      "Total training time: 11.74 seconds.\n",
      "-- Epoch 1629\n",
      "Norm: 5589354725736.37, NNZs: 843, Bias: 132485443.510019, T: 5484843, Avg. loss: 129978862109430460014913354989568.000000\n",
      "Total training time: 11.74 seconds.\n",
      "-- Epoch 1630\n",
      "Norm: 5814562889261.85, NNZs: 843, Bias: -127784767.834637, T: 5488210, Avg. loss: 129939307456903967148495482126336.000000\n",
      "Total training time: 11.75 seconds.\n",
      "-- Epoch 1631\n",
      "Norm: 5815891900821.65, NNZs: 843, Bias: 78801656.380141, T: 5491577, Avg. loss: 129899372912018222950421784166400.000000\n",
      "Total training time: 11.76 seconds.\n",
      "-- Epoch 1632\n",
      "Norm: 6186706292892.64, NNZs: 843, Bias: -127757693.773289, T: 5494944, Avg. loss: 129860162378257695139240213479424.000000\n",
      "Total training time: 11.77 seconds.\n",
      "-- Epoch 1633\n",
      "Norm: 5618109818759.04, NNZs: 843, Bias: 78767686.348655, T: 5498311, Avg. loss: 129820324877332930201431870275584.000000\n",
      "Total training time: 11.77 seconds.\n",
      "-- Epoch 1634\n",
      "Norm: 5874758940131.07, NNZs: 843, Bias: -127727355.516054, T: 5501678, Avg. loss: 129780213691807845801790697111552.000000\n",
      "Total training time: 11.78 seconds.\n",
      "-- Epoch 1635\n",
      "Norm: 5714145685750.07, NNZs: 843, Bias: 78735777.616564, T: 5505045, Avg. loss: 129739906513539183176866315894784.000000\n",
      "Total training time: 11.79 seconds.\n",
      "-- Epoch 1636\n",
      "Norm: 6357289865074.24, NNZs: 843, Bias: -540530017.617651, T: 5508412, Avg. loss: 129700578414064357494102524166144.000000\n",
      "Total training time: 11.80 seconds.\n",
      "-- Epoch 1637\n",
      "Norm: 6100320851501.58, NNZs: 843, Bias: 78703034.881905, T: 5511779, Avg. loss: 129661180792640546777590333964288.000000\n",
      "Total training time: 11.80 seconds.\n",
      "-- Epoch 1638\n",
      "Norm: 6191306722219.22, NNZs: 843, Bias: 285040622.845145, T: 5515146, Avg. loss: 129621127818703410502365886808064.000000\n",
      "Total training time: 11.81 seconds.\n",
      "-- Epoch 1639\n",
      "Norm: 5641917850729.00, NNZs: 843, Bias: 258940354.149929, T: 5518513, Avg. loss: 129581845296453842856675135979520.000000\n",
      "Total training time: 11.82 seconds.\n",
      "-- Epoch 1640\n",
      "Norm: 5755541598339.63, NNZs: 843, Bias: 52606634.847607, T: 5521880, Avg. loss: 129542450235186898783833983811584.000000\n",
      "Total training time: 11.82 seconds.\n",
      "-- Epoch 1641\n",
      "Norm: 5717305316668.93, NNZs: 843, Bias: 258851618.344101, T: 5525247, Avg. loss: 129503317571743795878498859483136.000000\n",
      "Total training time: 11.83 seconds.\n",
      "-- Epoch 1642\n",
      "Norm: 5861896502570.47, NNZs: 843, Bias: 52580045.450002, T: 5528614, Avg. loss: 129464501282567744938533290246144.000000\n",
      "Total training time: 11.84 seconds.\n",
      "-- Epoch 1643\n",
      "Norm: 5769434012877.02, NNZs: 843, Bias: -153628878.019471, T: 5531981, Avg. loss: 129424988927258890712635766996992.000000\n",
      "Total training time: 11.85 seconds.\n",
      "-- Epoch 1644\n",
      "Norm: 5700177039919.97, NNZs: 843, Bias: 52553553.309613, T: 5535348, Avg. loss: 129385204304469291293506088730624.000000\n",
      "Total training time: 11.85 seconds.\n",
      "-- Epoch 1645\n",
      "Norm: 5814159006614.31, NNZs: 843, Bias: -153592946.069117, T: 5538715, Avg. loss: 129345838175839725053114312556544.000000\n",
      "Total training time: 11.86 seconds.\n",
      "-- Epoch 1646\n",
      "Norm: 5748185306849.86, NNZs: 843, Bias: 52526840.705635, T: 5542082, Avg. loss: 129306969580716876075392271646720.000000\n",
      "Total training time: 11.87 seconds.\n",
      "-- Epoch 1647\n",
      "Norm: 6060284474281.90, NNZs: 843, Bias: 258582924.783014, T: 5545449, Avg. loss: 129267736355762361807619552706560.000000\n",
      "Total training time: 11.87 seconds.\n",
      "-- Epoch 1648\n",
      "Norm: 6123281693076.07, NNZs: 843, Bias: 52499500.058928, T: 5548816, Avg. loss: 129228227520131746392638752817152.000000\n",
      "Total training time: 11.88 seconds.\n",
      "-- Epoch 1649\n",
      "Norm: 6337646393160.57, NNZs: 843, Bias: -153521264.163657, T: 5552183, Avg. loss: 129188762733205810392290748792832.000000\n",
      "Total training time: 11.89 seconds.\n",
      "-- Epoch 1650\n",
      "Norm: 6103563599347.72, NNZs: 843, Bias: 2676118.879805, T: 5555550, Avg. loss: 129149880005314200563499225055232.000000\n",
      "Total training time: 11.90 seconds.\n",
      "-- Epoch 1651\n",
      "Norm: 5620344565048.92, NNZs: 843, Bias: 208615470.574062, T: 5558917, Avg. loss: 129110869285679014729456000434176.000000\n",
      "Total training time: 11.90 seconds.\n",
      "-- Epoch 1652\n",
      "Norm: 5325727556569.25, NNZs: 843, Bias: 2665882.827268, T: 5562284, Avg. loss: 129071698599429085013264153182208.000000\n",
      "Total training time: 11.91 seconds.\n",
      "-- Epoch 1653\n",
      "Norm: 5561281023703.75, NNZs: 843, Bias: -203223386.269997, T: 5565651, Avg. loss: 129032889696430223803857640620032.000000\n",
      "Total training time: 11.92 seconds.\n",
      "-- Epoch 1654\n",
      "Norm: 5825720908208.57, NNZs: 843, Bias: -205086705.890440, T: 5569018, Avg. loss: 128993415301866629954213871878144.000000\n",
      "Total training time: 11.92 seconds.\n",
      "-- Epoch 1655\n",
      "Norm: 6234489687397.19, NNZs: 843, Bias: 763307.984806, T: 5572385, Avg. loss: 128955134165892865193349792923648.000000\n",
      "Total training time: 11.93 seconds.\n",
      "-- Epoch 1656\n",
      "Norm: 5869757888497.83, NNZs: 843, Bias: 206546775.798488, T: 5575752, Avg. loss: 128916436054298336036445336109056.000000\n",
      "Total training time: 11.94 seconds.\n",
      "-- Epoch 1657\n",
      "Norm: 5887579302140.75, NNZs: 843, Bias: 751911.769706, T: 5579119, Avg. loss: 128878172744167884768292127113216.000000\n",
      "Total training time: 11.95 seconds.\n",
      "-- Epoch 1658\n",
      "Norm: 6077935855989.85, NNZs: 843, Bias: 206471937.964770, T: 5582486, Avg. loss: 128839205561366495296468465745920.000000\n",
      "Total training time: 11.95 seconds.\n",
      "-- Epoch 1659\n",
      "Norm: 6111284049793.20, NNZs: 843, Bias: 51917889.219773, T: 5585853, Avg. loss: 128801104911760744434554638434304.000000\n",
      "Total training time: 11.96 seconds.\n",
      "-- Epoch 1660\n",
      "Norm: 5740599453846.12, NNZs: 843, Bias: -153760363.549423, T: 5589220, Avg. loss: 128762481986420101683109588631552.000000\n",
      "Total training time: 11.97 seconds.\n",
      "-- Epoch 1661\n",
      "Norm: 6083380307891.04, NNZs: 843, Bias: -359374558.289174, T: 5592587, Avg. loss: 128723849705460389955057232117760.000000\n",
      "Total training time: 11.98 seconds.\n",
      "-- Epoch 1662\n",
      "Norm: 6338829220907.11, NNZs: 843, Bias: 10452032.333541, T: 5595954, Avg. loss: 128685593605157534095093939568640.000000\n",
      "Total training time: 11.98 seconds.\n",
      "-- Epoch 1663\n",
      "Norm: 5439828595060.05, NNZs: 843, Bias: -195128913.561038, T: 5599321, Avg. loss: 128646550422437922347099889860608.000000\n",
      "Total training time: 11.99 seconds.\n",
      "-- Epoch 1664\n",
      "Norm: 5527913415973.73, NNZs: 843, Bias: -400645582.789039, T: 5602688, Avg. loss: 128608572743161638480904082948096.000000\n",
      "Total training time: 12.00 seconds.\n",
      "-- Epoch 1665\n",
      "Norm: 5987883458278.43, NNZs: 843, Bias: -195078497.110389, T: 5606055, Avg. loss: 128569981850035056509783721902080.000000\n",
      "Total training time: 12.00 seconds.\n",
      "-- Epoch 1666\n",
      "Norm: 5285193836188.38, NNZs: 843, Bias: 10423400.194159, T: 5609422, Avg. loss: 128531859938641668006467136913408.000000\n",
      "Total training time: 12.01 seconds.\n",
      "-- Epoch 1667\n",
      "Norm: 6047865442125.54, NNZs: 843, Bias: 261606405.874825, T: 5612789, Avg. loss: 128492802506783469584686166048768.000000\n",
      "Total training time: 12.02 seconds.\n",
      "-- Epoch 1668\n",
      "Norm: 5450597601636.67, NNZs: 843, Bias: 56141647.557841, T: 5616156, Avg. loss: 128454441040975750865685732917248.000000\n",
      "Total training time: 12.03 seconds.\n",
      "-- Epoch 1669\n",
      "Norm: 5720174699121.18, NNZs: 843, Bias: 261516508.960455, T: 5619523, Avg. loss: 128415631545976917198576572956672.000000\n",
      "Total training time: 12.03 seconds.\n",
      "-- Epoch 1670\n",
      "Norm: 6084063882123.09, NNZs: 843, Bias: -354601575.300663, T: 5622890, Avg. loss: 128376590703709613109490281873408.000000\n",
      "Total training time: 12.04 seconds.\n",
      "-- Epoch 1671\n",
      "Norm: 5660427338081.71, NNZs: 843, Bias: -149228816.427207, T: 5626257, Avg. loss: 128337936303848009312386567634944.000000\n",
      "Total training time: 12.05 seconds.\n",
      "-- Epoch 1672\n",
      "Norm: 5875599708891.09, NNZs: 843, Bias: -291542084.431599, T: 5629624, Avg. loss: 128299241325269750624553546022912.000000\n",
      "Total training time: 12.06 seconds.\n",
      "-- Epoch 1673\n",
      "Norm: 6038257042633.51, NNZs: 843, Bias: -86234741.582601, T: 5632991, Avg. loss: 128260772537094528106856877916160.000000\n",
      "Total training time: 12.06 seconds.\n",
      "-- Epoch 1674\n",
      "Norm: 5658553628746.59, NNZs: 843, Bias: -291459228.713487, T: 5636358, Avg. loss: 128223011818535142835252652670976.000000\n",
      "Total training time: 12.07 seconds.\n",
      "-- Epoch 1675\n",
      "Norm: 5772813496160.44, NNZs: 843, Bias: -276775611.108912, T: 5639725, Avg. loss: 128184380648063749808984816615424.000000\n",
      "Total training time: 12.08 seconds.\n",
      "-- Epoch 1676\n",
      "Norm: 5916651746280.07, NNZs: 843, Bias: -72142234.441294, T: 5643092, Avg. loss: 128145689438636469964201453944832.000000\n",
      "Total training time: 12.08 seconds.\n",
      "-- Epoch 1677\n",
      "Norm: 5653890110117.55, NNZs: 843, Bias: 133007127.265635, T: 5646459, Avg. loss: 128107895659304279703604630126592.000000\n",
      "Total training time: 12.09 seconds.\n",
      "-- Epoch 1678\n",
      "Norm: 5838193327813.27, NNZs: 843, Bias: -46790247.600890, T: 5649826, Avg. loss: 128070282804287100670861021020160.000000\n",
      "Total training time: 12.10 seconds.\n",
      "-- Epoch 1679\n",
      "Norm: 5782095874010.56, NNZs: 843, Bias: -251869481.084236, T: 5653193, Avg. loss: 128031780614598490768373956214784.000000\n",
      "Total training time: 12.11 seconds.\n",
      "-- Epoch 1680\n",
      "Norm: 6019659553138.36, NNZs: 843, Bias: -46786265.943412, T: 5656560, Avg. loss: 127994147276309666673125534728192.000000\n",
      "Total training time: 12.11 seconds.\n",
      "-- Epoch 1681\n",
      "Norm: 5962324264136.83, NNZs: 843, Bias: 158240187.801686, T: 5659927, Avg. loss: 127955918262940978975779940466688.000000\n",
      "Total training time: 12.12 seconds.\n",
      "-- Epoch 1682\n",
      "Norm: 5736684766430.13, NNZs: 843, Bias: -46776548.700054, T: 5663294, Avg. loss: 127918516445117916058500556914688.000000\n",
      "Total training time: 12.13 seconds.\n",
      "-- Epoch 1683\n",
      "Norm: 5725226237209.94, NNZs: 843, Bias: 129361564.964745, T: 5666661, Avg. loss: 127880855533080952182203056390144.000000\n",
      "Total training time: 12.13 seconds.\n",
      "-- Epoch 1684\n",
      "Norm: 5699530404081.94, NNZs: 843, Bias: -75590208.886103, T: 5670028, Avg. loss: 127843480954289497540542748164096.000000\n",
      "Total training time: 12.14 seconds.\n",
      "-- Epoch 1685\n",
      "Norm: 6046668176607.83, NNZs: 843, Bias: -280478984.134446, T: 5673395, Avg. loss: 127806171229074363125117086597120.000000\n",
      "Total training time: 12.15 seconds.\n",
      "-- Epoch 1686\n",
      "Norm: 6308020190054.56, NNZs: 843, Bias: 334161884.336428, T: 5676762, Avg. loss: 127767658182694320373306998390784.000000\n",
      "Total training time: 12.16 seconds.\n",
      "-- Epoch 1687\n",
      "Norm: 6590125851380.79, NNZs: 843, Bias: 129271800.254698, T: 5680129, Avg. loss: 127730630884901843550550330703872.000000\n",
      "Total training time: 12.16 seconds.\n",
      "-- Epoch 1688\n",
      "Norm: 5823610259159.45, NNZs: 843, Bias: -75557944.362204, T: 5683496, Avg. loss: 127693255718715761401339425325056.000000\n",
      "Total training time: 12.17 seconds.\n",
      "-- Epoch 1689\n",
      "Norm: 6315433906206.13, NNZs: 843, Bias: -280328983.919915, T: 5686863, Avg. loss: 127656066401903474813990726533120.000000\n",
      "Total training time: 12.18 seconds.\n",
      "-- Epoch 1690\n",
      "Norm: 6073280225280.48, NNZs: 843, Bias: 52183778.909574, T: 5690230, Avg. loss: 127618297123790658187012998168576.000000\n",
      "Total training time: 12.19 seconds.\n",
      "-- Epoch 1691\n",
      "Norm: 5968053570557.84, NNZs: 843, Bias: -152544713.247869, T: 5693597, Avg. loss: 127580794385423691192977667915776.000000\n",
      "Total training time: 12.19 seconds.\n",
      "-- Epoch 1692\n",
      "Norm: 6459897849553.51, NNZs: 843, Bias: -357215303.490485, T: 5696964, Avg. loss: 127543048849174207745819542552576.000000\n",
      "Total training time: 12.20 seconds.\n",
      "-- Epoch 1693\n",
      "Norm: 6510497988071.31, NNZs: 843, Bias: 256800803.069993, T: 5700331, Avg. loss: 127505497702675569959358026481664.000000\n",
      "Total training time: 12.21 seconds.\n",
      "-- Epoch 1694\n",
      "Norm: 6298800876722.47, NNZs: 843, Bias: 461384596.486392, T: 5703698, Avg. loss: 127467403696933760300076836585472.000000\n",
      "Total training time: 12.21 seconds.\n",
      "-- Epoch 1695\n",
      "Norm: 6411319434794.69, NNZs: 843, Bias: -152476625.290610, T: 5707065, Avg. loss: 127429925890085079323768950620160.000000\n",
      "Total training time: 12.22 seconds.\n",
      "-- Epoch 1696\n",
      "Norm: 6390549252740.04, NNZs: 843, Bias: 461235023.905593, T: 5710432, Avg. loss: 127392973647360666986435265429504.000000\n",
      "Total training time: 12.23 seconds.\n",
      "-- Epoch 1697\n",
      "Norm: 6112585557228.85, NNZs: 843, Bias: 65884492.284791, T: 5713799, Avg. loss: 127354730910005535084019651182592.000000\n",
      "Total training time: 12.24 seconds.\n",
      "-- Epoch 1698\n",
      "Norm: 6563866568229.75, NNZs: 843, Bias: -138638531.392496, T: 5717166, Avg. loss: 127317046804992557497562098040832.000000\n",
      "Total training time: 12.24 seconds.\n",
      "-- Epoch 1699\n",
      "Norm: 6245209963109.86, NNZs: 843, Bias: 65851305.269742, T: 5720533, Avg. loss: 127280227387829162316031691063296.000000\n",
      "Total training time: 12.25 seconds.\n",
      "-- Epoch 1700\n",
      "Norm: 5974089027836.73, NNZs: 843, Bias: 229866015.180303, T: 5723900, Avg. loss: 127242960423562456283868792094720.000000\n",
      "Total training time: 12.26 seconds.\n",
      "-- Epoch 1701\n",
      "Norm: 6039248598017.24, NNZs: 843, Bias: 25408909.304586, T: 5727267, Avg. loss: 127205205765639367284168922759168.000000\n",
      "Total training time: 12.26 seconds.\n",
      "-- Epoch 1702\n",
      "Norm: 6078051396671.07, NNZs: 843, Bias: 229785092.167302, T: 5730634, Avg. loss: 127168535690866503032240095625216.000000\n",
      "Total training time: 12.27 seconds.\n",
      "-- Epoch 1703\n",
      "Norm: 5706277201849.64, NNZs: 843, Bias: 499305.964353, T: 5734001, Avg. loss: 127131004558637203158508284936192.000000\n",
      "Total training time: 12.28 seconds.\n",
      "-- Epoch 1704\n",
      "Norm: 5842579363803.97, NNZs: 843, Bias: -203832424.654060, T: 5737368, Avg. loss: 127094310057669891033370170753024.000000\n",
      "Total training time: 12.29 seconds.\n",
      "-- Epoch 1705\n",
      "Norm: 6012197417507.40, NNZs: 843, Bias: 486012.995595, T: 5740735, Avg. loss: 127057472292825563115367111327744.000000\n",
      "Total training time: 12.29 seconds.\n",
      "-- Epoch 1706\n",
      "Norm: 5881164206187.48, NNZs: 843, Bias: -203784943.628528, T: 5744102, Avg. loss: 127020117943022394482911082446848.000000\n",
      "Total training time: 12.30 seconds.\n",
      "-- Epoch 1707\n",
      "Norm: 5965282095860.56, NNZs: 843, Bias: 473860.356966, T: 5747469, Avg. loss: 126982665769873329499186890014720.000000\n",
      "Total training time: 12.31 seconds.\n",
      "-- Epoch 1708\n",
      "Norm: 5864384105916.78, NNZs: 843, Bias: -203736341.749000, T: 5750836, Avg. loss: 126945587769611410551962433748992.000000\n",
      "Total training time: 12.31 seconds.\n",
      "-- Epoch 1709\n",
      "Norm: 5830470947688.54, NNZs: 843, Bias: -307361946.952742, T: 5754203, Avg. loss: 126908479566789468746959391031296.000000\n",
      "Total training time: 12.32 seconds.\n",
      "-- Epoch 1710\n",
      "Norm: 6008783464725.21, NNZs: 843, Bias: -511466863.023351, T: 5757570, Avg. loss: 126871169524674223095966502748160.000000\n",
      "Total training time: 12.33 seconds.\n",
      "-- Epoch 1711\n",
      "Norm: 5809714403644.89, NNZs: 843, Bias: 100950593.143678, T: 5760937, Avg. loss: 126834408662490933780058292092928.000000\n",
      "Total training time: 12.34 seconds.\n",
      "-- Epoch 1712\n",
      "Norm: 5738213381243.08, NNZs: 843, Bias: -103154590.181784, T: 5764304, Avg. loss: 126797145766231519818662820184064.000000\n",
      "Total training time: 12.34 seconds.\n",
      "-- Epoch 1713\n",
      "Norm: 6117360320609.95, NNZs: 843, Bias: -307201495.285261, T: 5767671, Avg. loss: 126759997619024138004530414485504.000000\n",
      "Total training time: 12.35 seconds.\n",
      "-- Epoch 1714\n",
      "Norm: 6049107714768.86, NNZs: 843, Bias: 304917941.922492, T: 5771038, Avg. loss: 126723583907353413609620057882624.000000\n",
      "Total training time: 12.36 seconds.\n",
      "-- Epoch 1715\n",
      "Norm: 6675081818067.35, NNZs: 843, Bias: -887260008.757773, T: 5774405, Avg. loss: 126686859342029967932462282571776.000000\n",
      "Total training time: 12.37 seconds.\n",
      "-- Epoch 1716\n",
      "Norm: 6176444154880.68, NNZs: 843, Bias: 30391856.308042, T: 5777772, Avg. loss: 126650847778789349049648294133760.000000\n",
      "Total training time: 12.37 seconds.\n",
      "-- Epoch 1717\n",
      "Norm: 5697903843575.29, NNZs: 843, Bias: -173557177.694669, T: 5781139, Avg. loss: 126613863791241618983573218394112.000000\n",
      "Total training time: 12.38 seconds.\n",
      "-- Epoch 1718\n",
      "Norm: 6050126974951.48, NNZs: 843, Bias: -377445646.688736, T: 5784506, Avg. loss: 126577400696759437844803770908672.000000\n",
      "Total training time: 12.39 seconds.\n",
      "-- Epoch 1719\n",
      "Norm: 6213565111113.59, NNZs: 843, Bias: -581275608.430889, T: 5787873, Avg. loss: 126540647647143774685351716585472.000000\n",
      "Total training time: 12.39 seconds.\n",
      "-- Epoch 1720\n",
      "Norm: 5911845922874.67, NNZs: 843, Bias: -125148093.303733, T: 5791240, Avg. loss: 126503926668965876944040477327360.000000\n",
      "Total training time: 12.40 seconds.\n",
      "-- Epoch 1721\n",
      "Norm: 6464682094481.15, NNZs: 843, Bias: 78684379.587229, T: 5794607, Avg. loss: 126466622884929995433239258333184.000000\n",
      "Total training time: 12.41 seconds.\n",
      "-- Epoch 1722\n",
      "Norm: 5877561198201.63, NNZs: 843, Bias: 282456813.779770, T: 5797974, Avg. loss: 126430906863782561639989287845888.000000\n",
      "Total training time: 12.42 seconds.\n",
      "-- Epoch 1723\n",
      "Norm: 6150379624122.63, NNZs: 843, Bias: 78649416.272678, T: 5801341, Avg. loss: 126394059564988022615763782205440.000000\n",
      "Total training time: 12.42 seconds.\n",
      "-- Epoch 1724\n",
      "Norm: 5791327710226.67, NNZs: 843, Bias: -125097074.855743, T: 5804708, Avg. loss: 126357250355321867753324739035136.000000\n",
      "Total training time: 12.43 seconds.\n",
      "-- Epoch 1725\n",
      "Norm: 5703106008598.08, NNZs: 843, Bias: -41039740.611162, T: 5808075, Avg. loss: 126319999933034415124461354221568.000000\n",
      "Total training time: 12.44 seconds.\n",
      "-- Epoch 1726\n",
      "Norm: 5796612447113.89, NNZs: 843, Bias: -244710248.266409, T: 5811442, Avg. loss: 126283845540303826387437082378240.000000\n",
      "Total training time: 12.44 seconds.\n",
      "-- Epoch 1727\n",
      "Norm: 5843737734958.11, NNZs: 843, Bias: -41037087.038939, T: 5814809, Avg. loss: 126246989172646800567458273951744.000000\n",
      "Total training time: 12.45 seconds.\n",
      "-- Epoch 1728\n",
      "Norm: 5897491782597.23, NNZs: 843, Bias: 162576721.043085, T: 5818176, Avg. loss: 126209701700884559854684176646144.000000\n",
      "Total training time: 12.46 seconds.\n",
      "-- Epoch 1729\n",
      "Norm: 5760754869101.03, NNZs: 843, Bias: -41034627.274836, T: 5821543, Avg. loss: 126173192156240693939525495291904.000000\n",
      "Total training time: 12.47 seconds.\n",
      "-- Epoch 1730\n",
      "Norm: 5556953129780.07, NNZs: 843, Bias: 162518714.581919, T: 5824910, Avg. loss: 126136976289479026588239477604352.000000\n",
      "Total training time: 12.47 seconds.\n",
      "-- Epoch 1731\n",
      "Norm: 5419670811089.49, NNZs: 843, Bias: -11108485.435463, T: 5828277, Avg. loss: 126100766856983827824958741938176.000000\n",
      "Total training time: 12.48 seconds.\n",
      "-- Epoch 1732\n",
      "Norm: 5605910372691.53, NNZs: 843, Bias: 93135175.504399, T: 5831644, Avg. loss: 126064719475319524722208988463104.000000\n",
      "Total training time: 12.49 seconds.\n",
      "-- Epoch 1733\n",
      "Norm: 5785841088756.44, NNZs: 843, Bias: -110347438.874229, T: 5835011, Avg. loss: 126028994054479264775030731440128.000000\n",
      "Total training time: 12.49 seconds.\n",
      "-- Epoch 1734\n",
      "Norm: 5289971616999.27, NNZs: 843, Bias: 93099739.470110, T: 5838378, Avg. loss: 125992415367224160079577792643072.000000\n",
      "Total training time: 12.50 seconds.\n",
      "-- Epoch 1735\n",
      "Norm: 5613743089538.57, NNZs: 843, Bias: -188758733.282363, T: 5841745, Avg. loss: 125956562223487711885815229448192.000000\n",
      "Total training time: 12.51 seconds.\n",
      "-- Epoch 1736\n",
      "Norm: 5518587971967.85, NNZs: 843, Bias: 252730342.261768, T: 5845112, Avg. loss: 125920098857212932947266158723072.000000\n",
      "Total training time: 12.52 seconds.\n",
      "-- Epoch 1737\n",
      "Norm: 5646562052823.92, NNZs: 843, Bias: 132623756.492693, T: 5848479, Avg. loss: 125884237617211301427088570974208.000000\n",
      "Total training time: 12.52 seconds.\n",
      "-- Epoch 1738\n",
      "Norm: 5751465992828.42, NNZs: 843, Bias: 335921303.054964, T: 5851846, Avg. loss: 125847554282201823932056659296256.000000\n",
      "Total training time: 12.53 seconds.\n",
      "-- Epoch 1739\n",
      "Norm: 5715164981313.40, NNZs: 843, Bias: 132578547.135999, T: 5855213, Avg. loss: 125811782114378832724372674838528.000000\n",
      "Total training time: 12.54 seconds.\n",
      "-- Epoch 1740\n",
      "Norm: 5699609790715.36, NNZs: 843, Bias: -104014228.447162, T: 5858580, Avg. loss: 125775643624436556237543527415808.000000\n",
      "Total training time: 12.55 seconds.\n",
      "-- Epoch 1741\n",
      "Norm: 5558592424560.23, NNZs: 843, Bias: 99227020.313696, T: 5861947, Avg. loss: 125740032336425240980955210973184.000000\n",
      "Total training time: 12.55 seconds.\n",
      "-- Epoch 1742\n",
      "Norm: 5411297692298.41, NNZs: 843, Bias: -103994232.205225, T: 5865314, Avg. loss: 125704049282459982052521024684032.000000\n",
      "Total training time: 12.56 seconds.\n",
      "-- Epoch 1743\n",
      "Norm: 5379749573606.15, NNZs: 843, Bias: -123457254.992935, T: 5868681, Avg. loss: 125668295170964283073618000216064.000000\n",
      "Total training time: 12.57 seconds.\n",
      "-- Epoch 1744\n",
      "Norm: 5536207998710.68, NNZs: 843, Bias: 103322786.257337, T: 5872048, Avg. loss: 125632433913610534390761024126976.000000\n",
      "Total training time: 12.57 seconds.\n",
      "-- Epoch 1745\n",
      "Norm: 5266386093438.55, NNZs: 843, Bias: -99808317.023805, T: 5875415, Avg. loss: 125596698636317636294930782486528.000000\n",
      "Total training time: 12.58 seconds.\n",
      "-- Epoch 1746\n",
      "Norm: 5316977928785.46, NNZs: 843, Bias: 103287257.289381, T: 5878782, Avg. loss: 125560583027727319318151544438784.000000\n",
      "Total training time: 12.59 seconds.\n",
      "-- Epoch 1747\n",
      "Norm: 5270958498281.68, NNZs: 843, Bias: -99788081.924450, T: 5882149, Avg. loss: 125524781519637248891850906927104.000000\n",
      "Total training time: 12.60 seconds.\n",
      "-- Epoch 1748\n",
      "Norm: 5288900258969.37, NNZs: 843, Bias: 10118774.303452, T: 5885516, Avg. loss: 125488596589301469119609565085696.000000\n",
      "Total training time: 12.60 seconds.\n",
      "-- Epoch 1749\n",
      "Norm: 5399974401169.68, NNZs: 843, Bias: 213110493.719464, T: 5888883, Avg. loss: 125452875602163150871887752462336.000000\n",
      "Total training time: 12.61 seconds.\n",
      "-- Epoch 1750\n",
      "Norm: 5417600359242.55, NNZs: 843, Bias: -395834138.911397, T: 5892250, Avg. loss: 125416915925801908724624147873792.000000\n",
      "Total training time: 12.62 seconds.\n",
      "-- Epoch 1751\n",
      "Norm: 5315809869998.08, NNZs: 843, Bias: -192843980.671970, T: 5895617, Avg. loss: 125381479335563741781472825049088.000000\n",
      "Total training time: 12.62 seconds.\n",
      "-- Epoch 1752\n",
      "Norm: 5318388883075.79, NNZs: 843, Bias: 10091765.558579, T: 5898984, Avg. loss: 125345188139661744056804784996352.000000\n",
      "Total training time: 12.63 seconds.\n",
      "-- Epoch 1753\n",
      "Norm: 5375525663740.65, NNZs: 843, Bias: -192797070.971402, T: 5902351, Avg. loss: 125309018503265995597491214483456.000000\n",
      "Total training time: 12.64 seconds.\n",
      "-- Epoch 1754\n",
      "Norm: 5491070490425.12, NNZs: 843, Bias: 10077800.845182, T: 5905718, Avg. loss: 125273714953191474935442018664448.000000\n",
      "Total training time: 12.65 seconds.\n",
      "-- Epoch 1755\n",
      "Norm: 5623165913722.34, NNZs: 843, Bias: 212895890.476758, T: 5909085, Avg. loss: 125238442199345839495589486657536.000000\n",
      "Total training time: 12.65 seconds.\n",
      "-- Epoch 1756\n",
      "Norm: 6366679504344.47, NNZs: 843, Bias: 107973698.726858, T: 5912452, Avg. loss: 125202491395450706872388303192064.000000\n",
      "Total training time: 12.66 seconds.\n",
      "-- Epoch 1757\n",
      "Norm: 5824407176076.83, NNZs: 843, Bias: -500342065.346277, T: 5915819, Avg. loss: 125166922567102283146739845169152.000000\n",
      "Total training time: 12.67 seconds.\n",
      "-- Epoch 1758\n",
      "Norm: 5821298572848.84, NNZs: 843, Bias: -54002472.196702, T: 5919186, Avg. loss: 125131459520233089372655068381184.000000\n",
      "Total training time: 12.68 seconds.\n",
      "-- Epoch 1759\n",
      "Norm: 5726428053474.23, NNZs: 843, Bias: 148711392.113677, T: 5922553, Avg. loss: 125095949295192232581196748423168.000000\n",
      "Total training time: 12.68 seconds.\n",
      "-- Epoch 1760\n",
      "Norm: 6066788943147.69, NNZs: 843, Bias: -67653986.372217, T: 5925920, Avg. loss: 125060952841114964464802541338624.000000\n",
      "Total training time: 12.69 seconds.\n",
      "-- Epoch 1761\n",
      "Norm: 6121268123235.31, NNZs: 843, Bias: 135002410.877553, T: 5929287, Avg. loss: 125025213254794552176307469287424.000000\n",
      "Total training time: 12.70 seconds.\n",
      "-- Epoch 1762\n",
      "Norm: 5911905650764.27, NNZs: 843, Bias: -67643758.631461, T: 5932654, Avg. loss: 124989321623014267540770962538496.000000\n",
      "Total training time: 12.70 seconds.\n",
      "-- Epoch 1763\n",
      "Norm: 6028236581567.24, NNZs: 843, Bias: 134955799.009614, T: 5936021, Avg. loss: 124953873537788015380398440710144.000000\n",
      "Total training time: 12.71 seconds.\n",
      "-- Epoch 1764\n",
      "Norm: 5919457076496.87, NNZs: 843, Bias: -67631719.770848, T: 5939388, Avg. loss: 124918428224398080557260673646592.000000\n",
      "Total training time: 12.72 seconds.\n",
      "-- Epoch 1765\n",
      "Norm: 6216587904902.07, NNZs: 843, Bias: -270159806.497712, T: 5942755, Avg. loss: 124882623619226303362249468674048.000000\n",
      "Total training time: 12.73 seconds.\n",
      "-- Epoch 1766\n",
      "Norm: 5730364320753.90, NNZs: 843, Bias: 81534094.771833, T: 5946122, Avg. loss: 124847186750569050213996631687168.000000\n",
      "Total training time: 12.73 seconds.\n",
      "-- Epoch 1767\n",
      "Norm: 5945249084290.06, NNZs: 843, Bias: -120960381.516078, T: 5949489, Avg. loss: 124812140795410214675456169345024.000000\n",
      "Total training time: 12.74 seconds.\n",
      "-- Epoch 1768\n",
      "Norm: 5978699739202.26, NNZs: 843, Bias: 81502265.311280, T: 5952856, Avg. loss: 124776045302725823483557649055744.000000\n",
      "Total training time: 12.75 seconds.\n",
      "-- Epoch 1769\n",
      "Norm: 5849472700802.95, NNZs: 843, Bias: -120935240.471198, T: 5956223, Avg. loss: 124740664331201671995374195179520.000000\n",
      "Total training time: 12.76 seconds.\n",
      "-- Epoch 1770\n",
      "Norm: 6168985929762.41, NNZs: 843, Bias: -532458423.067380, T: 5959590, Avg. loss: 124705924567708312656035402743808.000000\n",
      "Total training time: 12.76 seconds.\n",
      "-- Epoch 1771\n",
      "Norm: 5827223160292.58, NNZs: 843, Bias: -331040208.748086, T: 5962957, Avg. loss: 124670794339197434549908279394304.000000\n",
      "Total training time: 12.77 seconds.\n",
      "-- Epoch 1772\n",
      "Norm: 5786613240408.83, NNZs: 843, Bias: -128660766.174324, T: 5966324, Avg. loss: 124635528921439161151465730342912.000000\n",
      "Total training time: 12.78 seconds.\n",
      "-- Epoch 1773\n",
      "Norm: 5898872989560.54, NNZs: 843, Bias: 73658734.270319, T: 5969691, Avg. loss: 124600383420639018710618343473152.000000\n",
      "Total training time: 12.78 seconds.\n",
      "-- Epoch 1774\n",
      "Norm: 5994785432215.46, NNZs: 843, Bias: 275923930.124804, T: 5973058, Avg. loss: 124565299870003748072546008825856.000000\n",
      "Total training time: 12.79 seconds.\n",
      "-- Epoch 1775\n",
      "Norm: 6427459326790.12, NNZs: 843, Bias: -330868461.159825, T: 5976425, Avg. loss: 124530930911162921492288467107840.000000\n",
      "Total training time: 12.80 seconds.\n",
      "-- Epoch 1776\n",
      "Norm: 5990772980034.14, NNZs: 843, Bias: -128602843.871995, T: 5979792, Avg. loss: 124495950174774976106048448888832.000000\n",
      "Total training time: 12.81 seconds.\n",
      "-- Epoch 1777\n",
      "Norm: 6403932923457.23, NNZs: 843, Bias: 477992613.362127, T: 5983159, Avg. loss: 124460590203176283623441563648000.000000\n",
      "Total training time: 12.81 seconds.\n",
      "-- Epoch 1778\n",
      "Norm: 6222835838751.93, NNZs: 843, Bias: -128571879.905034, T: 5986526, Avg. loss: 124425559987181735278208763297792.000000\n",
      "Total training time: 12.82 seconds.\n",
      "-- Epoch 1779\n",
      "Norm: 6168081284355.31, NNZs: 843, Bias: 73580166.664295, T: 5989893, Avg. loss: 124390553966195905176178289278976.000000\n",
      "Total training time: 12.83 seconds.\n",
      "-- Epoch 1780\n",
      "Norm: 5865221105454.02, NNZs: 843, Bias: 170472119.548439, T: 5993260, Avg. loss: 124356265268436414109874258968576.000000\n",
      "Total training time: 12.83 seconds.\n",
      "-- Epoch 1781\n",
      "Norm: 5826356570192.63, NNZs: 843, Bias: 183345747.467708, T: 5996627, Avg. loss: 124321895224623475366211801317376.000000\n",
      "Total training time: 12.84 seconds.\n",
      "-- Epoch 1782\n",
      "Norm: 6170332485176.74, NNZs: 843, Bias: -18735506.135172, T: 5999994, Avg. loss: 124287053850907541073586160664576.000000\n",
      "Total training time: 12.85 seconds.\n",
      "-- Epoch 1783\n",
      "Norm: 5686565743776.60, NNZs: 843, Bias: 183284148.188375, T: 6003361, Avg. loss: 124252447690503822286643884195840.000000\n",
      "Total training time: 12.86 seconds.\n",
      "-- Epoch 1784\n",
      "Norm: 5704980643269.00, NNZs: 843, Bias: -18738696.121403, T: 6006728, Avg. loss: 124217999293378701632300365381632.000000\n",
      "Total training time: 12.86 seconds.\n",
      "-- Epoch 1785\n",
      "Norm: 5700899632311.31, NNZs: 843, Bias: -220704834.811053, T: 6010095, Avg. loss: 124183086370971614208925234102272.000000\n",
      "Total training time: 12.87 seconds.\n",
      "-- Epoch 1786\n",
      "Norm: 5627628211615.47, NNZs: 843, Bias: 20917910.524986, T: 6013462, Avg. loss: 124148233913949508929164077432832.000000\n",
      "Total training time: 12.88 seconds.\n",
      "-- Epoch 1787\n",
      "Norm: 6028872664250.81, NNZs: 843, Bias: 208342341.373193, T: 6016829, Avg. loss: 124112893675280916141399844323328.000000\n",
      "Total training time: 12.89 seconds.\n",
      "-- Epoch 1788\n",
      "Norm: 5907971851806.75, NNZs: 843, Bias: -194078187.890727, T: 6020196, Avg. loss: 124078618270661319181369232850944.000000\n",
      "Total training time: 12.89 seconds.\n",
      "-- Epoch 1789\n",
      "Norm: 5572500823541.06, NNZs: 843, Bias: 7797843.433663, T: 6023563, Avg. loss: 124043885924500079017219076390912.000000\n",
      "Total training time: 12.90 seconds.\n",
      "-- Epoch 1790\n",
      "Norm: 5603873669486.75, NNZs: 843, Bias: -210452797.833981, T: 6026930, Avg. loss: 124009229019301440289434748583936.000000\n",
      "Total training time: 12.91 seconds.\n",
      "-- Epoch 1791\n",
      "Norm: 5562806391758.76, NNZs: 843, Bias: 35999720.351972, T: 6030297, Avg. loss: 123974711097864854323215325986816.000000\n",
      "Total training time: 12.91 seconds.\n",
      "-- Epoch 1792\n",
      "Norm: 5778681035527.06, NNZs: 843, Bias: -165777512.364079, T: 6033664, Avg. loss: 123939902622811666042228706377728.000000\n",
      "Total training time: 12.92 seconds.\n",
      "-- Epoch 1793\n",
      "Norm: 5808970223707.74, NNZs: 843, Bias: 35982925.708161, T: 6037031, Avg. loss: 123904756044101127711287379230720.000000\n",
      "Total training time: 12.93 seconds.\n",
      "-- Epoch 1794\n",
      "Norm: 5514042633682.64, NNZs: 843, Bias: 237683411.929555, T: 6040398, Avg. loss: 123870066511067021213114552025088.000000\n",
      "Total training time: 12.94 seconds.\n",
      "-- Epoch 1795\n",
      "Norm: 5758907373469.21, NNZs: 843, Bias: 124513632.008343, T: 6043765, Avg. loss: 123835484234989741208941667287040.000000\n",
      "Total training time: 12.94 seconds.\n",
      "-- Epoch 1796\n",
      "Norm: 5674794976853.44, NNZs: 843, Bias: 75466117.919136, T: 6047132, Avg. loss: 123800757417968073764931958210560.000000\n",
      "Total training time: 12.95 seconds.\n",
      "-- Epoch 1797\n",
      "Norm: 5584671629975.75, NNZs: 843, Bias: -126176539.559833, T: 6050499, Avg. loss: 123766652664408036632921978699776.000000\n",
      "Total training time: 12.96 seconds.\n",
      "-- Epoch 1798\n",
      "Norm: 5634622195442.00, NNZs: 843, Bias: 75438050.362784, T: 6053866, Avg. loss: 123732486565449021673982511808512.000000\n",
      "Total training time: 12.96 seconds.\n",
      "-- Epoch 1799\n",
      "Norm: 5563005100111.39, NNZs: 843, Bias: 276995979.667921, T: 6057233, Avg. loss: 123698146349344160936287627378688.000000\n",
      "Total training time: 12.97 seconds.\n",
      "-- Epoch 1800\n",
      "Norm: 5453468702902.40, NNZs: 843, Bias: 75410320.740262, T: 6060600, Avg. loss: 123663646100655216485171772522496.000000\n",
      "Total training time: 12.98 seconds.\n",
      "-- Epoch 1801\n",
      "Norm: 5773016949974.13, NNZs: 843, Bias: -126120998.276166, T: 6063967, Avg. loss: 123628889383812050376285669556224.000000\n",
      "Total training time: 12.99 seconds.\n",
      "-- Epoch 1802\n",
      "Norm: 5665922687174.99, NNZs: 843, Bias: 75381783.590961, T: 6067334, Avg. loss: 123594669153533645521995192860672.000000\n",
      "Total training time: 12.99 seconds.\n",
      "-- Epoch 1803\n",
      "Norm: 5511101791932.33, NNZs: 843, Bias: -24781136.608749, T: 6070701, Avg. loss: 123560171883263943561766223478784.000000\n",
      "Total training time: 13.00 seconds.\n",
      "-- Epoch 1804\n",
      "Norm: 5755628628545.67, NNZs: 843, Bias: -226213698.561988, T: 6074068, Avg. loss: 123525984664087098571085595017216.000000\n",
      "Total training time: 13.01 seconds.\n",
      "-- Epoch 1805\n",
      "Norm: 5466116042324.94, NNZs: 843, Bias: 112123572.424588, T: 6077435, Avg. loss: 123492480879874968153070732574720.000000\n",
      "Total training time: 13.02 seconds.\n",
      "-- Epoch 1806\n",
      "Norm: 5710449989038.07, NNZs: 843, Bias: -89273009.492077, T: 6080802, Avg. loss: 123458344949741997230271775113216.000000\n",
      "Total training time: 13.02 seconds.\n",
      "-- Epoch 1807\n",
      "Norm: 5806158731424.36, NNZs: 843, Bias: -290609632.567618, T: 6084169, Avg. loss: 123424068675248493335130945880064.000000\n",
      "Total training time: 13.03 seconds.\n",
      "-- Epoch 1808\n",
      "Norm: 5781884871384.03, NNZs: 843, Bias: -89250827.327968, T: 6087536, Avg. loss: 123389691671425494802882992537600.000000\n",
      "Total training time: 13.04 seconds.\n",
      "-- Epoch 1809\n",
      "Norm: 6165501119325.02, NNZs: 843, Bias: 112051097.305555, T: 6090903, Avg. loss: 123355753699767843920793640632320.000000\n",
      "Total training time: 13.04 seconds.\n",
      "-- Epoch 1810\n",
      "Norm: 5682513099757.83, NNZs: 843, Bias: 313296465.528006, T: 6094270, Avg. loss: 123321327039656858915436125749248.000000\n",
      "Total training time: 13.05 seconds.\n",
      "-- Epoch 1811\n",
      "Norm: 5834204200894.67, NNZs: 843, Bias: -290464284.945197, T: 6097637, Avg. loss: 123287133443324809104143062925312.000000\n",
      "Total training time: 13.06 seconds.\n",
      "-- Epoch 1812\n",
      "Norm: 5969945562878.50, NNZs: 843, Bias: -491639011.194610, T: 6101004, Avg. loss: 123253301870258648326320039133184.000000\n",
      "Total training time: 13.07 seconds.\n",
      "-- Epoch 1813\n",
      "Norm: 5565921368868.63, NNZs: 843, Bias: -290392793.057208, T: 6104371, Avg. loss: 123219671958702160878770407342080.000000\n",
      "Total training time: 13.07 seconds.\n",
      "-- Epoch 1814\n",
      "Norm: 5374698187370.68, NNZs: 843, Bias: -89201037.118654, T: 6107738, Avg. loss: 123185206638994439000389576556544.000000\n",
      "Total training time: 13.08 seconds.\n",
      "-- Epoch 1815\n",
      "Norm: 5580098092022.24, NNZs: 843, Bias: 235965650.839810, T: 6111105, Avg. loss: 123150436461860134500186019856384.000000\n",
      "Total training time: 13.09 seconds.\n",
      "-- Epoch 1816\n",
      "Norm: 5941519460797.69, NNZs: 843, Bias: 34830603.291735, T: 6114472, Avg. loss: 123116697324301131979007232835584.000000\n",
      "Total training time: 13.09 seconds.\n",
      "-- Epoch 1817\n",
      "Norm: 5688848723489.41, NNZs: 843, Bias: -166251626.218252, T: 6117839, Avg. loss: 123083096386963883824208332980224.000000\n",
      "Total training time: 13.10 seconds.\n",
      "-- Epoch 1818\n",
      "Norm: 6156163731322.28, NNZs: 843, Bias: 34810661.066685, T: 6121206, Avg. loss: 123049388920717635667662221606912.000000\n",
      "Total training time: 13.11 seconds.\n",
      "-- Epoch 1819\n",
      "Norm: 6171985832628.76, NNZs: 843, Bias: -166216326.781697, T: 6124573, Avg. loss: 123015198573757018427771313979392.000000\n",
      "Total training time: 13.12 seconds.\n",
      "-- Epoch 1820\n",
      "Norm: 5898720410851.36, NNZs: 843, Bias: 34789597.859724, T: 6127940, Avg. loss: 122981705915718211762047844089856.000000\n",
      "Total training time: 13.12 seconds.\n",
      "-- Epoch 1821\n",
      "Norm: 5770743195957.32, NNZs: 843, Bias: -58006810.315062, T: 6131307, Avg. loss: 122947732720180752403241872916480.000000\n",
      "Total training time: 13.13 seconds.\n",
      "-- Epoch 1822\n",
      "Norm: 5696599209645.42, NNZs: 843, Bias: 152513115.496449, T: 6134674, Avg. loss: 122913708154380368389448608514048.000000\n",
      "Total training time: 13.14 seconds.\n",
      "-- Epoch 1823\n",
      "Norm: 5720961640827.38, NNZs: 843, Bias: -48415599.656016, T: 6138041, Avg. loss: 122880254037106156699363961733120.000000\n",
      "Total training time: 13.14 seconds.\n",
      "-- Epoch 1824\n",
      "Norm: 5929257987244.59, NNZs: 843, Bias: -249288994.273499, T: 6141408, Avg. loss: 122846587415703668930891715969024.000000\n",
      "Total training time: 13.15 seconds.\n",
      "-- Epoch 1825\n",
      "Norm: 5923280288540.54, NNZs: 843, Bias: 305728755.507617, T: 6144775, Avg. loss: 122812944408301520284870638567424.000000\n",
      "Total training time: 13.16 seconds.\n",
      "-- Epoch 1826\n",
      "Norm: 6043282784336.39, NNZs: 843, Bias: 104860355.762105, T: 6148142, Avg. loss: 122779152894980017582512476782592.000000\n",
      "Total training time: 13.17 seconds.\n",
      "-- Epoch 1827\n",
      "Norm: 6224302334393.94, NNZs: 843, Bias: 305637418.878922, T: 6151509, Avg. loss: 122745122393649894195051924291584.000000\n",
      "Total training time: 13.17 seconds.\n",
      "-- Epoch 1828\n",
      "Norm: 6069191387073.42, NNZs: 843, Bias: 104821814.479004, T: 6154876, Avg. loss: 122711490780583099268696392073216.000000\n",
      "Total training time: 13.18 seconds.\n",
      "-- Epoch 1829\n",
      "Norm: 6347404067099.15, NNZs: 843, Bias: -108551718.086642, T: 6158243, Avg. loss: 122677690915140577340729990316032.000000\n",
      "Total training time: 13.19 seconds.\n",
      "-- Epoch 1830\n",
      "Norm: 6623232670645.82, NNZs: 843, Bias: -309252292.021541, T: 6161610, Avg. loss: 122643739425539029495133818060800.000000\n",
      "Total training time: 13.20 seconds.\n",
      "-- Epoch 1831\n",
      "Norm: 6505627079696.37, NNZs: 843, Bias: 292842381.813325, T: 6164977, Avg. loss: 122610146820461223647697401020416.000000\n",
      "Total training time: 13.20 seconds.\n",
      "-- Epoch 1832\n",
      "Norm: 6470734155892.96, NNZs: 843, Bias: 92140865.174645, T: 6168344, Avg. loss: 122576486890086444035510217211904.000000\n",
      "Total training time: 13.21 seconds.\n",
      "-- Epoch 1833\n",
      "Norm: 6608179358071.28, NNZs: 843, Bias: -108506810.914309, T: 6171711, Avg. loss: 122542779355925427109457270145024.000000\n",
      "Total training time: 13.22 seconds.\n",
      "-- Epoch 1834\n",
      "Norm: 6927761495644.22, NNZs: 843, Bias: 493318551.537500, T: 6175078, Avg. loss: 122508760190684126280201860546560.000000\n",
      "Total training time: 13.22 seconds.\n",
      "-- Epoch 1835\n",
      "Norm: 7015435643493.53, NNZs: 843, Bias: -108482226.917548, T: 6178445, Avg. loss: 122475307543461087912737745403904.000000\n",
      "Total training time: 13.23 seconds.\n",
      "-- Epoch 1836\n",
      "Norm: 6837860443765.05, NNZs: 843, Bias: 136038377.813149, T: 6181812, Avg. loss: 122442315010122259855792236658688.000000\n",
      "Total training time: 13.24 seconds.\n",
      "-- Epoch 1837\n",
      "Norm: 6273854284597.12, NNZs: 843, Bias: -64504536.094502, T: 6185179, Avg. loss: 122409410518905196479428371152896.000000\n",
      "Total training time: 13.25 seconds.\n",
      "-- Epoch 1838\n",
      "Norm: 6061688044093.33, NNZs: 843, Bias: -264993046.487280, T: 6188546, Avg. loss: 122376223478923657684662536372224.000000\n",
      "Total training time: 13.25 seconds.\n",
      "-- Epoch 1839\n",
      "Norm: 6365594507276.50, NNZs: 843, Bias: -470816180.761576, T: 6191913, Avg. loss: 122343072010681583880576285278208.000000\n",
      "Total training time: 13.26 seconds.\n",
      "-- Epoch 1840\n",
      "Norm: 6172711056217.63, NNZs: 843, Bias: 130565202.882164, T: 6195280, Avg. loss: 122310392138255311838128977739776.000000\n",
      "Total training time: 13.27 seconds.\n",
      "-- Epoch 1841\n",
      "Norm: 6004327293081.42, NNZs: 843, Bias: 7559528.259667, T: 6198647, Avg. loss: 122277645697556989274407898710016.000000\n",
      "Total training time: 13.27 seconds.\n",
      "-- Epoch 1842\n",
      "Norm: 5952464960735.31, NNZs: 843, Bias: -192832038.521917, T: 6202014, Avg. loss: 122244778368772648755773315219456.000000\n",
      "Total training time: 13.28 seconds.\n",
      "-- Epoch 1843\n",
      "Norm: 5870217368259.13, NNZs: 843, Bias: -210835557.987466, T: 6205381, Avg. loss: 122211709810746142745691370291200.000000\n",
      "Total training time: 13.29 seconds.\n",
      "-- Epoch 1844\n",
      "Norm: 5760350681238.62, NNZs: 843, Bias: 67072170.622331, T: 6208748, Avg. loss: 122179239701276262846410310811648.000000\n",
      "Total training time: 13.30 seconds.\n",
      "-- Epoch 1845\n",
      "Norm: 5709388237252.65, NNZs: 843, Bias: -162032344.119750, T: 6212115, Avg. loss: 122146193899021182608958226956288.000000\n",
      "Total training time: 13.30 seconds.\n",
      "-- Epoch 1846\n",
      "Norm: 5885144020473.81, NNZs: 843, Bias: 38262041.678000, T: 6215482, Avg. loss: 122113205967516982981708285476864.000000\n",
      "Total training time: 13.31 seconds.\n",
      "-- Epoch 1847\n",
      "Norm: 5894281449263.45, NNZs: 843, Bias: 238504128.642636, T: 6218849, Avg. loss: 122080157665351507770169746259968.000000\n",
      "Total training time: 13.32 seconds.\n",
      "-- Epoch 1848\n",
      "Norm: 5897114072094.55, NNZs: 843, Bias: -189379501.442533, T: 6222216, Avg. loss: 122046725454488693205878842589184.000000\n",
      "Total training time: 13.32 seconds.\n",
      "-- Epoch 1849\n",
      "Norm: 5498936139437.05, NNZs: 843, Bias: 10837005.080455, T: 6225583, Avg. loss: 122013643911544598347095591616512.000000\n",
      "Total training time: 13.33 seconds.\n",
      "-- Epoch 1850\n",
      "Norm: 5476215192999.76, NNZs: 843, Bias: 210998615.711293, T: 6228950, Avg. loss: 121981160345971549407582940561408.000000\n",
      "Total training time: 13.34 seconds.\n",
      "-- Epoch 1851\n",
      "Norm: 5722184239573.32, NNZs: 843, Bias: 10824463.708706, T: 6232317, Avg. loss: 121947765634571001946271790399488.000000\n",
      "Total training time: 13.35 seconds.\n",
      "-- Epoch 1852\n",
      "Norm: 5586631091900.12, NNZs: 843, Bias: -189296432.246555, T: 6235684, Avg. loss: 121914512270737870485192821440512.000000\n",
      "Total training time: 13.35 seconds.\n",
      "-- Epoch 1853\n",
      "Norm: 5691031884165.45, NNZs: 843, Bias: 10813704.357258, T: 6239051, Avg. loss: 121881437348427519447117357645824.000000\n",
      "Total training time: 13.36 seconds.\n",
      "-- Epoch 1854\n",
      "Norm: 5778128285357.51, NNZs: 843, Bias: -189255806.018340, T: 6242418, Avg. loss: 121848458180185443017908180484096.000000\n",
      "Total training time: 13.37 seconds.\n",
      "-- Epoch 1855\n",
      "Norm: 6162255646599.01, NNZs: 843, Bias: -4074115.246122, T: 6245785, Avg. loss: 121815834690059926388578587770880.000000\n",
      "Total training time: 13.38 seconds.\n",
      "-- Epoch 1856\n",
      "Norm: 5839037765080.74, NNZs: 843, Bias: 195931556.851533, T: 6249152, Avg. loss: 121783638071704593061014990225408.000000\n",
      "Total training time: 13.38 seconds.\n",
      "-- Epoch 1857\n",
      "Norm: 5867242820481.04, NNZs: 843, Bias: -404033830.216914, T: 6252519, Avg. loss: 121750911226014658424630163800064.000000\n",
      "Total training time: 13.39 seconds.\n",
      "-- Epoch 1858\n",
      "Norm: 5704391633043.69, NNZs: 843, Bias: -347941319.448778, T: 6255886, Avg. loss: 121717549429303834983279928279040.000000\n",
      "Total training time: 13.40 seconds.\n",
      "-- Epoch 1859\n",
      "Norm: 5876181138997.05, NNZs: 843, Bias: -147969831.626008, T: 6259253, Avg. loss: 121685441876551886512249009864704.000000\n",
      "Total training time: 13.40 seconds.\n",
      "-- Epoch 1860\n",
      "Norm: 5463691561108.93, NNZs: 843, Bias: 51946521.877103, T: 6262620, Avg. loss: 121652748320811592830680349802496.000000\n",
      "Total training time: 13.41 seconds.\n",
      "-- Epoch 1861\n",
      "Norm: 5845750700321.09, NNZs: 843, Bias: 251806904.326139, T: 6265987, Avg. loss: 121620079985248920175858663030784.000000\n",
      "Total training time: 13.42 seconds.\n",
      "-- Epoch 1862\n",
      "Norm: 5502842756551.69, NNZs: 843, Bias: -347765106.650866, T: 6269354, Avg. loss: 121587397841729791946876849750016.000000\n",
      "Total training time: 13.43 seconds.\n",
      "-- Epoch 1863\n",
      "Norm: 5540024872739.58, NNZs: 843, Bias: 251734807.709274, T: 6272721, Avg. loss: 121555172842362444714776612306944.000000\n",
      "Total training time: 13.43 seconds.\n",
      "-- Epoch 1864\n",
      "Norm: 5262823725391.18, NNZs: 843, Bias: -311998118.956730, T: 6276088, Avg. loss: 121522038743781684240869167726592.000000\n",
      "Total training time: 13.44 seconds.\n",
      "-- Epoch 1865\n",
      "Norm: 5392870955340.83, NNZs: 843, Bias: 27723466.264967, T: 6279455, Avg. loss: 121489373599034281514408257519616.000000\n",
      "Total training time: 13.45 seconds.\n",
      "-- Epoch 1866\n",
      "Norm: 5770350914131.95, NNZs: 843, Bias: 31286160.432720, T: 6282822, Avg. loss: 121456810536086865114744223694848.000000\n",
      "Total training time: 13.45 seconds.\n",
      "-- Epoch 1867\n",
      "Norm: 5303126063556.67, NNZs: 843, Bias: 230990024.419035, T: 6286189, Avg. loss: 121423814186314900634092315869184.000000\n",
      "Total training time: 13.46 seconds.\n",
      "-- Epoch 1868\n",
      "Norm: 5257552896606.60, NNZs: 843, Bias: 31272141.415831, T: 6289556, Avg. loss: 121390822548195553667954816057344.000000\n",
      "Total training time: 13.47 seconds.\n",
      "-- Epoch 1869\n",
      "Norm: 5455783106106.22, NNZs: 843, Bias: -168392184.943757, T: 6292923, Avg. loss: 121358566078237644871393202929664.000000\n",
      "Total training time: 13.48 seconds.\n",
      "-- Epoch 1870\n",
      "Norm: 5602620159738.92, NNZs: 843, Bias: -368004468.111867, T: 6296290, Avg. loss: 121326239593213325211401950593024.000000\n",
      "Total training time: 13.48 seconds.\n",
      "-- Epoch 1871\n",
      "Norm: 5485032618450.26, NNZs: 843, Bias: 230854868.578745, T: 6299657, Avg. loss: 121293801143691311973888591659008.000000\n",
      "Total training time: 13.49 seconds.\n",
      "-- Epoch 1872\n",
      "Norm: 5615607037361.55, NNZs: 843, Bias: 31241200.155591, T: 6303024, Avg. loss: 121261080550106932744770585362432.000000\n",
      "Total training time: 13.50 seconds.\n",
      "-- Epoch 1873\n",
      "Norm: 5658021826120.64, NNZs: 843, Bias: -168318389.059933, T: 6306391, Avg. loss: 121228835550077981960431653093376.000000\n",
      "Total training time: 13.50 seconds.\n",
      "-- Epoch 1874\n",
      "Norm: 5853068300973.68, NNZs: 843, Bias: 430275342.482850, T: 6309758, Avg. loss: 121196351128802485018857014558720.000000\n",
      "Total training time: 13.51 seconds.\n",
      "-- Epoch 1875\n",
      "Norm: 5977122702317.34, NNZs: 843, Bias: 230715197.083401, T: 6313125, Avg. loss: 121163532360374887419089673781248.000000\n",
      "Total training time: 13.52 seconds.\n",
      "-- Epoch 1876\n",
      "Norm: 5643338398067.70, NNZs: 843, Bias: 31209056.583273, T: 6316492, Avg. loss: 121131498902029590056563485179904.000000\n",
      "Total training time: 13.53 seconds.\n",
      "-- Epoch 1877\n",
      "Norm: 5587610584627.98, NNZs: 843, Bias: 230647308.683241, T: 6319859, Avg. loss: 121100146352719768007025436393472.000000\n",
      "Total training time: 13.53 seconds.\n",
      "-- Epoch 1878\n",
      "Norm: 5800674413544.28, NNZs: 843, Bias: 31987751.503513, T: 6323226, Avg. loss: 121067466866068093050244843962368.000000\n",
      "Total training time: 13.54 seconds.\n",
      "-- Epoch 1879\n",
      "Norm: 5766638901215.86, NNZs: 843, Bias: 231373455.234056, T: 6326593, Avg. loss: 121035075756956082339244626935808.000000\n",
      "Total training time: 13.55 seconds.\n",
      "-- Epoch 1880\n",
      "Norm: 5575876641813.42, NNZs: 843, Bias: 31972947.830170, T: 6329960, Avg. loss: 121003288470526965760244083326976.000000\n",
      "Total training time: 13.56 seconds.\n",
      "-- Epoch 1881\n",
      "Norm: 5606741567619.55, NNZs: 843, Bias: 2730224.371364, T: 6333327, Avg. loss: 120971488810833419146532950638592.000000\n",
      "Total training time: 13.56 seconds.\n",
      "-- Epoch 1882\n",
      "Norm: 5939306137034.34, NNZs: 843, Bias: -478805392.914373, T: 6336694, Avg. loss: 120939123817343528404869655298048.000000\n",
      "Total training time: 13.57 seconds.\n",
      "-- Epoch 1883\n",
      "Norm: 6265940756018.99, NNZs: 843, Bias: 319589429.870240, T: 6340061, Avg. loss: 120907198928190462212699188101120.000000\n",
      "Total training time: 13.58 seconds.\n",
      "-- Epoch 1884\n",
      "Norm: 5934272173875.41, NNZs: 843, Bias: 120286106.026894, T: 6343428, Avg. loss: 120875610954760528578201306267648.000000\n",
      "Total training time: 13.58 seconds.\n",
      "-- Epoch 1885\n",
      "Norm: 5478133477760.82, NNZs: 843, Bias: -86533267.465855, T: 6346795, Avg. loss: 120843446031929787600773926879232.000000\n",
      "Total training time: 13.59 seconds.\n",
      "-- Epoch 1886\n",
      "Norm: 5475390135927.81, NNZs: 843, Bias: -285731076.289189, T: 6350162, Avg. loss: 120811081874135932178910626709504.000000\n",
      "Total training time: 13.60 seconds.\n",
      "-- Epoch 1887\n",
      "Norm: 5653917310642.46, NNZs: 843, Bias: -86515049.535687, T: 6353529, Avg. loss: 120779196088926107172768856408064.000000\n",
      "Total training time: 13.61 seconds.\n",
      "-- Epoch 1888\n",
      "Norm: 5521923462291.64, NNZs: 843, Bias: -285660108.655850, T: 6356896, Avg. loss: 120746286722567923544989953425408.000000\n",
      "Total training time: 13.61 seconds.\n",
      "-- Epoch 1889\n",
      "Norm: 5636962337513.60, NNZs: 843, Bias: -86497262.644844, T: 6360263, Avg. loss: 120714149586613526999104929923072.000000\n",
      "Total training time: 13.62 seconds.\n",
      "-- Epoch 1890\n",
      "Norm: 5592297244990.50, NNZs: 843, Bias: -285590785.053176, T: 6363630, Avg. loss: 120682061540749041352968299872256.000000\n",
      "Total training time: 13.63 seconds.\n",
      "-- Epoch 1891\n",
      "Norm: 6167066610612.83, NNZs: 843, Bias: 311670177.388927, T: 6366997, Avg. loss: 120650005984346168321524281376768.000000\n",
      "Total training time: 13.64 seconds.\n",
      "-- Epoch 1892\n",
      "Norm: 5667458164832.68, NNZs: 843, Bias: -285518410.209639, T: 6370364, Avg. loss: 120618422756516535832696869355520.000000\n",
      "Total training time: 13.64 seconds.\n",
      "-- Epoch 1893\n",
      "Norm: 5571804558800.47, NNZs: 843, Bias: -86460431.968275, T: 6373731, Avg. loss: 120586751139148420179875992698880.000000\n",
      "Total training time: 13.65 seconds.\n",
      "-- Epoch 1894\n",
      "Norm: 5633967970328.57, NNZs: 843, Bias: 158954129.394287, T: 6377098, Avg. loss: 120555067728532795162329324978176.000000\n",
      "Total training time: 13.66 seconds.\n",
      "-- Epoch 1895\n",
      "Norm: 5363752063932.74, NNZs: 843, Bias: -17969043.417974, T: 6380465, Avg. loss: 120523437992847682525118712512512.000000\n",
      "Total training time: 13.66 seconds.\n",
      "-- Epoch 1896\n",
      "Norm: 5546453807373.74, NNZs: 843, Bias: -216914087.398183, T: 6383832, Avg. loss: 120491665597054765444554609917952.000000\n",
      "Total training time: 13.67 seconds.\n",
      "-- Epoch 1897\n",
      "Norm: 5510896399548.35, NNZs: 843, Bias: -88679937.778243, T: 6387199, Avg. loss: 120460457752840042860962572664832.000000\n",
      "Total training time: 13.68 seconds.\n",
      "-- Epoch 1898\n",
      "Norm: 5907064137202.73, NNZs: 843, Bias: 110218279.020391, T: 6390566, Avg. loss: 120428444849646146456783526297600.000000\n",
      "Total training time: 13.69 seconds.\n",
      "-- Epoch 1899\n",
      "Norm: 5345490782833.28, NNZs: 843, Bias: -88663715.897184, T: 6393933, Avg. loss: 120397281887380198826498687238144.000000\n",
      "Total training time: 13.69 seconds.\n",
      "-- Epoch 1900\n",
      "Norm: 5564689936051.60, NNZs: 843, Bias: -287494001.403198, T: 6397300, Avg. loss: 120365598293832049896856915804160.000000\n",
      "Total training time: 13.70 seconds.\n",
      "-- Epoch 1901\n",
      "Norm: 5772368722811.76, NNZs: 843, Bias: -88646147.560549, T: 6400667, Avg. loss: 120333627761536969865590237822976.000000\n",
      "Total training time: 13.71 seconds.\n",
      "-- Epoch 1902\n",
      "Norm: 5705411738154.92, NNZs: 843, Bias: -287425798.134489, T: 6404034, Avg. loss: 120301655927190312865754098171904.000000\n",
      "Total training time: 13.71 seconds.\n",
      "-- Epoch 1903\n",
      "Norm: 6229363778068.69, NNZs: 843, Bias: -338792633.078102, T: 6407401, Avg. loss: 120270086385984847742165353234432.000000\n",
      "Total training time: 13.72 seconds.\n",
      "-- Epoch 1904\n",
      "Norm: 5884421301797.58, NNZs: 843, Bias: -140021719.865795, T: 6410768, Avg. loss: 120238005037125824138512210853888.000000\n",
      "Total training time: 13.73 seconds.\n",
      "-- Epoch 1905\n",
      "Norm: 5670309638914.64, NNZs: 843, Bias: 58699856.044695, T: 6414135, Avg. loss: 120206038694905383903715322232832.000000\n",
      "Total training time: 13.74 seconds.\n",
      "-- Epoch 1906\n",
      "Norm: 5580064307842.92, NNZs: 843, Bias: -139996679.590759, T: 6417502, Avg. loss: 120174251654028948140394868113408.000000\n",
      "Total training time: 13.74 seconds.\n",
      "-- Epoch 1907\n",
      "Norm: 5714749074883.20, NNZs: 843, Bias: -338640646.498626, T: 6420869, Avg. loss: 120142748818134377649035091640320.000000\n",
      "Total training time: 13.75 seconds.\n",
      "-- Epoch 1908\n",
      "Norm: 5552304946635.91, NNZs: 843, Bias: -87965467.655561, T: 6424236, Avg. loss: 120111131623174534784363060527104.000000\n",
      "Total training time: 13.76 seconds.\n",
      "-- Epoch 1909\n",
      "Norm: 5458776230232.23, NNZs: 843, Bias: 110646105.360420, T: 6427603, Avg. loss: 120079669797902091655713562558464.000000\n",
      "Total training time: 13.77 seconds.\n",
      "-- Epoch 1910\n",
      "Norm: 5446889171362.98, NNZs: 843, Bias: -87951447.387456, T: 6430970, Avg. loss: 120048645789682095365714884952064.000000\n",
      "Total training time: 13.77 seconds.\n",
      "-- Epoch 1911\n",
      "Norm: 5272727971210.59, NNZs: 843, Bias: 110611049.439699, T: 6434337, Avg. loss: 120016854447300814112101201084416.000000\n",
      "Total training time: 13.78 seconds.\n",
      "-- Epoch 1912\n",
      "Norm: 5465313441858.59, NNZs: 843, Bias: -87931566.233045, T: 6437704, Avg. loss: 119985590083522068627805125476352.000000\n",
      "Total training time: 13.79 seconds.\n",
      "-- Epoch 1913\n",
      "Norm: 5953452851346.70, NNZs: 843, Bias: -286421046.430642, T: 6441071, Avg. loss: 119954303308045828038074514276352.000000\n",
      "Total training time: 13.79 seconds.\n",
      "-- Epoch 1914\n",
      "Norm: 5185787971157.62, NNZs: 843, Bias: -87912804.975951, T: 6444438, Avg. loss: 119923171081271996076169185198080.000000\n",
      "Total training time: 13.80 seconds.\n",
      "-- Epoch 1915\n",
      "Norm: 5282289777656.37, NNZs: 843, Bias: 86505966.421411, T: 6447805, Avg. loss: 119891546997534429850721715224576.000000\n",
      "Total training time: 13.81 seconds.\n",
      "-- Epoch 1916\n",
      "Norm: 5268559984758.30, NNZs: 843, Bias: -97238231.189927, T: 6451172, Avg. loss: 119859674136341583185020358492160.000000\n",
      "Total training time: 13.82 seconds.\n",
      "-- Epoch 1917\n",
      "Norm: 5268812029205.10, NNZs: 843, Bias: 101168402.919462, T: 6454539, Avg. loss: 119828480430855577112968861581312.000000\n",
      "Total training time: 13.82 seconds.\n",
      "-- Epoch 1918\n",
      "Norm: 5476675479059.03, NNZs: 843, Bias: -294945164.721982, T: 6457906, Avg. loss: 119797267598802828138545611276288.000000\n",
      "Total training time: 13.83 seconds.\n",
      "-- Epoch 1919\n",
      "Norm: 5282247182191.89, NNZs: 843, Bias: -96565476.021174, T: 6461273, Avg. loss: 119765821739293305468748791021568.000000\n",
      "Total training time: 13.84 seconds.\n",
      "-- Epoch 1920\n",
      "Norm: 5250940171461.59, NNZs: 843, Bias: -294872757.791375, T: 6464640, Avg. loss: 119734317557519747731115739185152.000000\n",
      "Total training time: 13.84 seconds.\n",
      "-- Epoch 1921\n",
      "Norm: 5503202500991.40, NNZs: 843, Bias: -96543717.373921, T: 6468007, Avg. loss: 119702778901544060270556798255104.000000\n",
      "Total training time: 13.85 seconds.\n",
      "-- Epoch 1922\n",
      "Norm: 5213605137636.93, NNZs: 843, Bias: 101733635.558133, T: 6471374, Avg. loss: 119671568639343887505577979412480.000000\n",
      "Total training time: 13.86 seconds.\n",
      "-- Epoch 1923\n",
      "Norm: 5695854429045.35, NNZs: 843, Bias: -493005195.651273, T: 6474741, Avg. loss: 119641005270260476108121567133696.000000\n",
      "Total training time: 13.87 seconds.\n",
      "-- Epoch 1924\n",
      "Norm: 5583172335722.39, NNZs: 843, Bias: -294728791.111739, T: 6478108, Avg. loss: 119609701241052638240944433397760.000000\n",
      "Total training time: 13.87 seconds.\n",
      "-- Epoch 1925\n",
      "Norm: 5329295586384.92, NNZs: 843, Bias: -203405265.211321, T: 6481475, Avg. loss: 119578428031742299394414076755968.000000\n",
      "Total training time: 13.88 seconds.\n",
      "-- Epoch 1926\n",
      "Norm: 5521226865683.67, NNZs: 843, Bias: 45610463.605084, T: 6484842, Avg. loss: 119547304082918461401455134769152.000000\n",
      "Total training time: 13.89 seconds.\n",
      "-- Epoch 1927\n",
      "Norm: 5436928716046.42, NNZs: 843, Bias: -152539951.988081, T: 6488209, Avg. loss: 119516383504820227151209384902656.000000\n",
      "Total training time: 13.89 seconds.\n",
      "-- Epoch 1928\n",
      "Norm: 5587029208784.60, NNZs: 843, Bias: 110661606.665964, T: 6491576, Avg. loss: 119485283067167172896057173475328.000000\n",
      "Total training time: 13.90 seconds.\n",
      "-- Epoch 1929\n",
      "Norm: 5717231311792.75, NNZs: 843, Bias: 256174923.962551, T: 6494943, Avg. loss: 119454292059879914755840425328640.000000\n",
      "Total training time: 13.91 seconds.\n",
      "-- Epoch 1930\n",
      "Norm: 5911922722743.69, NNZs: 843, Bias: 58075413.386389, T: 6498310, Avg. loss: 119423644447566120365720063705088.000000\n",
      "Total training time: 13.92 seconds.\n",
      "-- Epoch 1931\n",
      "Norm: 5621562243884.29, NNZs: 843, Bias: -139971475.155065, T: 6501677, Avg. loss: 119392897085575113696099285598208.000000\n",
      "Total training time: 13.92 seconds.\n",
      "-- Epoch 1932\n",
      "Norm: 5515377603664.37, NNZs: 843, Bias: 58053064.075743, T: 6505044, Avg. loss: 119361880125566158284104205860864.000000\n",
      "Total training time: 13.93 seconds.\n",
      "-- Epoch 1933\n",
      "Norm: 5735007307030.76, NNZs: 843, Bias: -139941470.021877, T: 6508411, Avg. loss: 119331135656747287982747500412928.000000\n",
      "Total training time: 13.94 seconds.\n",
      "-- Epoch 1934\n",
      "Norm: 5608138562297.00, NNZs: 843, Bias: 58028539.080924, T: 6511778, Avg. loss: 119300337894102557803514402177024.000000\n",
      "Total training time: 13.94 seconds.\n",
      "-- Epoch 1935\n",
      "Norm: 5567221097635.67, NNZs: 843, Bias: 255951033.929532, T: 6515145, Avg. loss: 119269234565709707295645394534400.000000\n",
      "Total training time: 13.95 seconds.\n",
      "-- Epoch 1936\n",
      "Norm: 5416532063735.15, NNZs: 843, Bias: -176724080.693305, T: 6518512, Avg. loss: 119238341594108168020821117239296.000000\n",
      "Total training time: 13.96 seconds.\n",
      "-- Epoch 1937\n",
      "Norm: 5604781385677.01, NNZs: 843, Bias: 21177863.662018, T: 6521879, Avg. loss: 119207679941042385746929339334656.000000\n",
      "Total training time: 13.97 seconds.\n",
      "-- Epoch 1938\n",
      "Norm: 5441868583508.33, NNZs: 843, Bias: -176686024.872711, T: 6525246, Avg. loss: 119176980421856745083442940608512.000000\n",
      "Total training time: 13.97 seconds.\n",
      "-- Epoch 1939\n",
      "Norm: 5346878333568.08, NNZs: 843, Bias: -11562211.308163, T: 6528613, Avg. loss: 119146043803733051973254491668480.000000\n",
      "Total training time: 13.98 seconds.\n",
      "-- Epoch 1940\n",
      "Norm: 5260529248580.64, NNZs: 843, Bias: -209371048.548948, T: 6531980, Avg. loss: 119115521992750361444726417129472.000000\n",
      "Total training time: 13.99 seconds.\n",
      "-- Epoch 1941\n",
      "Norm: 5237554406220.30, NNZs: 843, Bias: -11568366.871509, T: 6535347, Avg. loss: 119084645352603545982479336734720.000000\n",
      "Total training time: 14.00 seconds.\n",
      "-- Epoch 1942\n",
      "Norm: 5545322731423.84, NNZs: 843, Bias: -168394290.994835, T: 6538714, Avg. loss: 119054368517881126968090427392000.000000\n",
      "Total training time: 14.00 seconds.\n",
      "-- Epoch 1943\n",
      "Norm: 5582056618666.51, NNZs: 843, Bias: -206237116.248514, T: 6542081, Avg. loss: 119024534887741270211711819841536.000000\n",
      "Total training time: 14.01 seconds.\n",
      "-- Epoch 1944\n",
      "Norm: 5809903106515.77, NNZs: 843, Bias: -403918764.030489, T: 6545448, Avg. loss: 118993883819117621956351054839808.000000\n",
      "Total training time: 14.02 seconds.\n",
      "-- Epoch 1945\n",
      "Norm: 5407789786421.76, NNZs: 843, Bias: 127486019.638046, T: 6548815, Avg. loss: 118963800511686237590103489773568.000000\n",
      "Total training time: 14.02 seconds.\n",
      "-- Epoch 1946\n",
      "Norm: 6287036282749.85, NNZs: 843, Bias: 720427482.617449, T: 6552182, Avg. loss: 118933067601028885630974689280000.000000\n",
      "Total training time: 14.03 seconds.\n",
      "-- Epoch 1947\n",
      "Norm: 5542620479333.19, NNZs: 843, Bias: 127450793.027513, T: 6555549, Avg. loss: 118902916376090939501217377681408.000000\n",
      "Total training time: 14.04 seconds.\n",
      "-- Epoch 1948\n",
      "Norm: 5463301526057.93, NNZs: 843, Bias: 109608419.214610, T: 6558916, Avg. loss: 118872050908188641475314032050176.000000\n",
      "Total training time: 14.05 seconds.\n",
      "-- Epoch 1949\n",
      "Norm: 6144738921356.87, NNZs: 843, Bias: 307168292.590330, T: 6562283, Avg. loss: 118841166248456605281323373821952.000000\n",
      "Total training time: 14.05 seconds.\n",
      "-- Epoch 1950\n",
      "Norm: 5903871913360.08, NNZs: 843, Bias: 504680042.742709, T: 6565650, Avg. loss: 118810542083627147463250008866816.000000\n",
      "Total training time: 14.06 seconds.\n",
      "-- Epoch 1951\n",
      "Norm: 5415001708633.54, NNZs: 843, Bias: -190915675.848767, T: 6569017, Avg. loss: 118779790181513401309069077118976.000000\n",
      "Total training time: 14.07 seconds.\n",
      "-- Epoch 1952\n",
      "Norm: 5406629679279.61, NNZs: 843, Bias: 6602752.430181, T: 6572384, Avg. loss: 118749472679623720794386866372608.000000\n",
      "Total training time: 14.07 seconds.\n",
      "-- Epoch 1953\n",
      "Norm: 5341533170860.72, NNZs: 843, Bias: -110517135.590306, T: 6575751, Avg. loss: 118718888094337170435049363865600.000000\n",
      "Total training time: 14.08 seconds.\n",
      "-- Epoch 1954\n",
      "Norm: 5602207368806.26, NNZs: 843, Bias: 86947515.524329, T: 6579118, Avg. loss: 118689069603482202563822389035008.000000\n",
      "Total training time: 14.09 seconds.\n",
      "-- Epoch 1955\n",
      "Norm: 5713558661349.13, NNZs: 843, Bias: -69919698.058700, T: 6582485, Avg. loss: 118658629849374769831792458334208.000000\n",
      "Total training time: 14.10 seconds.\n",
      "-- Epoch 1956\n",
      "Norm: 5676550063371.12, NNZs: 843, Bias: -277067046.564137, T: 6585852, Avg. loss: 118627907459433364962323019595776.000000\n",
      "Total training time: 14.10 seconds.\n",
      "-- Epoch 1957\n",
      "Norm: 5426963590456.98, NNZs: 843, Bias: -79656488.516171, T: 6589219, Avg. loss: 118597490291964187608264216674304.000000\n",
      "Total training time: 14.11 seconds.\n",
      "-- Epoch 1958\n",
      "Norm: 5446840315817.36, NNZs: 843, Bias: 117702534.528425, T: 6592586, Avg. loss: 118567357665761146090680783732736.000000\n",
      "Total training time: 14.12 seconds.\n",
      "-- Epoch 1959\n",
      "Norm: 5337320348578.23, NNZs: 843, Bias: -79637816.961299, T: 6595953, Avg. loss: 118537831189531356309029413453824.000000\n",
      "Total training time: 14.12 seconds.\n",
      "-- Epoch 1960\n",
      "Norm: 5355997754458.74, NNZs: 843, Bias: 72094365.463319, T: 6599320, Avg. loss: 118507570611312450265508692885504.000000\n",
      "Total training time: 14.13 seconds.\n",
      "-- Epoch 1961\n",
      "Norm: 5580140248922.53, NNZs: 843, Bias: 420323227.907302, T: 6602687, Avg. loss: 118477000202096032584069103484928.000000\n",
      "Total training time: 14.14 seconds.\n",
      "-- Epoch 1962\n",
      "Norm: 5423233015781.28, NNZs: 843, Bias: 56418993.122580, T: 6606054, Avg. loss: 118446683430017680089326853029888.000000\n",
      "Total training time: 14.15 seconds.\n",
      "-- Epoch 1963\n",
      "Norm: 5415188372870.41, NNZs: 843, Bias: -140815957.412761, T: 6609421, Avg. loss: 118416568232456785975426133524480.000000\n",
      "Total training time: 14.15 seconds.\n",
      "-- Epoch 1964\n",
      "Norm: 5322609704645.23, NNZs: 843, Bias: 56396426.756554, T: 6612788, Avg. loss: 118386823611792582423609545326592.000000\n",
      "Total training time: 14.16 seconds.\n",
      "-- Epoch 1965\n",
      "Norm: 5308517665403.43, NNZs: 843, Bias: -82572912.069339, T: 6616155, Avg. loss: 118356722043865054190387551272960.000000\n",
      "Total training time: 14.17 seconds.\n",
      "-- Epoch 1966\n",
      "Norm: 5722887421910.69, NNZs: 843, Bias: -279711825.254389, T: 6619522, Avg. loss: 118326116949649401331452783624192.000000\n",
      "Total training time: 14.18 seconds.\n",
      "-- Epoch 1967\n",
      "Norm: 5339051743115.02, NNZs: 843, Bias: -82554260.466174, T: 6622889, Avg. loss: 118296080979660507625251393241088.000000\n",
      "Total training time: 14.18 seconds.\n",
      "-- Epoch 1968\n",
      "Norm: 5443400014362.96, NNZs: 843, Bias: -279644505.938595, T: 6626256, Avg. loss: 118266215665862377004112973660160.000000\n",
      "Total training time: 14.19 seconds.\n",
      "-- Epoch 1969\n",
      "Norm: 5335890258537.33, NNZs: 843, Bias: -82536807.778400, T: 6629623, Avg. loss: 118236721506933349485006844592128.000000\n",
      "Total training time: 14.20 seconds.\n",
      "-- Epoch 1970\n",
      "Norm: 5606587801383.17, NNZs: 843, Bias: -279577881.301720, T: 6632990, Avg. loss: 118206529535236926360362929881088.000000\n",
      "Total training time: 14.20 seconds.\n",
      "-- Epoch 1971\n",
      "Norm: 5688527730344.28, NNZs: 843, Bias: 194276265.988443, T: 6636357, Avg. loss: 118176663281646954949150079188992.000000\n",
      "Total training time: 14.21 seconds.\n",
      "-- Epoch 1972\n",
      "Norm: 5605398323372.48, NNZs: 843, Bias: -2751519.715463, T: 6639724, Avg. loss: 118147207954711827534862021033984.000000\n",
      "Total training time: 14.22 seconds.\n",
      "-- Epoch 1973\n",
      "Norm: 5489140927311.04, NNZs: 843, Bias: -199728871.393757, T: 6643091, Avg. loss: 118117745461653584734659155066880.000000\n",
      "Total training time: 14.23 seconds.\n",
      "-- Epoch 1974\n",
      "Norm: 5648645419936.28, NNZs: 843, Bias: -2758391.711196, T: 6646458, Avg. loss: 118087507282022699684455721730048.000000\n",
      "Total training time: 14.23 seconds.\n",
      "-- Epoch 1975\n",
      "Norm: 5556007003431.15, NNZs: 843, Bias: -199685542.111292, T: 6649825, Avg. loss: 118057807357499288883767713726464.000000\n",
      "Total training time: 14.24 seconds.\n",
      "-- Epoch 1976\n",
      "Norm: 5983562774469.70, NNZs: 843, Bias: 4530838.902890, T: 6653192, Avg. loss: 118027892184983177981852521594880.000000\n",
      "Total training time: 14.25 seconds.\n",
      "-- Epoch 1977\n",
      "Norm: 5334756445449.23, NNZs: 843, Bias: -22686841.059436, T: 6656559, Avg. loss: 117997977076655233258584646090752.000000\n",
      "Total training time: 14.25 seconds.\n",
      "-- Epoch 1978\n",
      "Norm: 5518696116934.30, NNZs: 843, Bias: 174160624.653279, T: 6659926, Avg. loss: 117968490723227269749424647045120.000000\n",
      "Total training time: 14.26 seconds.\n",
      "-- Epoch 1979\n",
      "Norm: 5366039193871.19, NNZs: 843, Bias: 90738017.006908, T: 6663293, Avg. loss: 117938384399157450391580345106432.000000\n",
      "Total training time: 14.27 seconds.\n",
      "-- Epoch 1980\n",
      "Norm: 5617499587186.64, NNZs: 843, Bias: 295279960.458019, T: 6666660, Avg. loss: 117908364716374027050355798310912.000000\n",
      "Total training time: 14.28 seconds.\n",
      "-- Epoch 1981\n",
      "Norm: 5493913052651.52, NNZs: 843, Bias: 98467147.806029, T: 6670027, Avg. loss: 117878921069743480220933360189440.000000\n",
      "Total training time: 14.28 seconds.\n",
      "-- Epoch 1982\n",
      "Norm: 5658952756377.54, NNZs: 843, Bias: -98297177.751569, T: 6673394, Avg. loss: 117848440783032271600377405636608.000000\n",
      "Total training time: 14.29 seconds.\n",
      "-- Epoch 1983\n",
      "Norm: 5904689375707.93, NNZs: 843, Bias: -406791087.991727, T: 6676761, Avg. loss: 117818570726796483664690192318464.000000\n",
      "Total training time: 14.30 seconds.\n",
      "-- Epoch 1984\n",
      "Norm: 5569966124460.74, NNZs: 843, Bias: 176446942.972613, T: 6680128, Avg. loss: 117789056243304013031503741059072.000000\n",
      "Total training time: 14.30 seconds.\n",
      "-- Epoch 1985\n",
      "Norm: 5675176448537.00, NNZs: 843, Bias: -441222188.008458, T: 6683495, Avg. loss: 117758967572455001823620443930624.000000\n",
      "Total training time: 14.31 seconds.\n",
      "-- Epoch 1986\n",
      "Norm: 5444151946953.37, NNZs: 843, Bias: -172580692.352544, T: 6686862, Avg. loss: 117729799765010823333581802176512.000000\n",
      "Total training time: 14.32 seconds.\n",
      "-- Epoch 1987\n",
      "Norm: 5387097888588.23, NNZs: 843, Bias: 24062136.548902, T: 6690229, Avg. loss: 117699692798851365958472364457984.000000\n",
      "Total training time: 14.33 seconds.\n",
      "-- Epoch 1988\n",
      "Norm: 5440270275947.11, NNZs: 843, Bias: -172544844.001703, T: 6693596, Avg. loss: 117669826097675541094279082409984.000000\n",
      "Total training time: 14.33 seconds.\n",
      "-- Epoch 1989\n",
      "Norm: 5462162913350.04, NNZs: 843, Bias: -20912383.086920, T: 6696963, Avg. loss: 117640571287546030427502742077440.000000\n",
      "Total training time: 14.34 seconds.\n",
      "-- Epoch 1990\n",
      "Norm: 5648460567701.04, NNZs: 843, Bias: 2000375.027910, T: 6700330, Avg. loss: 117611417874339320932115666698240.000000\n",
      "Total training time: 14.35 seconds.\n",
      "-- Epoch 1991\n",
      "Norm: 5546744456385.34, NNZs: 843, Bias: 198523302.153811, T: 6703697, Avg. loss: 117581806012823478371910166773760.000000\n",
      "Total training time: 14.36 seconds.\n",
      "-- Epoch 1992\n",
      "Norm: 5652292374725.80, NNZs: 843, Bias: 394996197.434962, T: 6707064, Avg. loss: 117552985123808819968706611249152.000000\n",
      "Total training time: 14.36 seconds.\n",
      "-- Epoch 1993\n",
      "Norm: 5755801013235.72, NNZs: 843, Bias: 51370207.556442, T: 6710431, Avg. loss: 117523657213958856827120438476800.000000\n",
      "Total training time: 14.37 seconds.\n",
      "-- Epoch 1994\n",
      "Norm: 5759548274064.46, NNZs: 843, Bias: -145090459.131418, T: 6713798, Avg. loss: 117494138928299302309849108316160.000000\n",
      "Total training time: 14.38 seconds.\n",
      "-- Epoch 1995\n",
      "Norm: 5685886783274.17, NNZs: 843, Bias: 51352833.987883, T: 6717165, Avg. loss: 117464903197555545826817032060928.000000\n",
      "Total training time: 14.38 seconds.\n",
      "-- Epoch 1996\n",
      "Norm: 6041155982038.67, NNZs: 843, Bias: -488994198.926178, T: 6720532, Avg. loss: 117435731130807123852391694204928.000000\n",
      "Total training time: 14.39 seconds.\n",
      "-- Epoch 1997\n",
      "Norm: 5793858760780.60, NNZs: 843, Bias: 100199726.443060, T: 6723899, Avg. loss: 117406246551894709293556831354880.000000\n",
      "Total training time: 14.40 seconds.\n",
      "-- Epoch 1998\n",
      "Norm: 5822973880504.70, NNZs: 843, Bias: 296538091.522041, T: 6727266, Avg. loss: 117377264088818064052185896845312.000000\n",
      "Total training time: 14.41 seconds.\n",
      "-- Epoch 1999\n",
      "Norm: 5701509151511.88, NNZs: 843, Bias: 100168575.455315, T: 6730633, Avg. loss: 117347566483779570139655338721280.000000\n",
      "Total training time: 14.41 seconds.\n",
      "-- Epoch 2000\n",
      "Norm: 5778679270752.69, NNZs: 843, Bias: -96150285.222076, T: 6734000, Avg. loss: 117317764559802294442021816893440.000000\n",
      "Total training time: 14.42 seconds.\n",
      "-- Epoch 2001\n",
      "Norm: 6379360680293.78, NNZs: 843, Bias: 100138485.608284, T: 6737367, Avg. loss: 117288248458854213386512646537216.000000\n",
      "Total training time: 14.43 seconds.\n",
      "-- Epoch 2002\n",
      "Norm: 5965050756269.57, NNZs: 843, Bias: 296379698.819908, T: 6740734, Avg. loss: 117258640356454050677281167245312.000000\n",
      "Total training time: 14.43 seconds.\n",
      "-- Epoch 2003\n",
      "Norm: 5982175843142.14, NNZs: 843, Bias: -292353580.580654, T: 6744101, Avg. loss: 117229178797862790626133462220800.000000\n",
      "Total training time: 14.44 seconds.\n",
      "-- Epoch 2004\n",
      "Norm: 5838427331818.93, NNZs: 843, Bias: -96112611.793388, T: 6747468, Avg. loss: 117199663047469085914425137823744.000000\n",
      "Total training time: 14.45 seconds.\n",
      "-- Epoch 2005\n",
      "Norm: 6175658031672.75, NNZs: 843, Bias: -79911963.537203, T: 6750835, Avg. loss: 117170076582397682542184681701376.000000\n",
      "Total training time: 14.46 seconds.\n",
      "-- Epoch 2006\n",
      "Norm: 5708244391713.33, NNZs: 843, Bias: -74855315.210752, T: 6754202, Avg. loss: 117141405439742676634427715485696.000000\n",
      "Total training time: 14.46 seconds.\n",
      "-- Epoch 2007\n",
      "Norm: 6081127131394.32, NNZs: 843, Bias: -36275757.836112, T: 6757569, Avg. loss: 117112283243984003493408467320832.000000\n",
      "Total training time: 14.47 seconds.\n",
      "-- Epoch 2008\n",
      "Norm: 5930041072854.48, NNZs: 843, Bias: 159835366.105631, T: 6760936, Avg. loss: 117083569865201354824922228064256.000000\n",
      "Total training time: 14.48 seconds.\n",
      "-- Epoch 2009\n",
      "Norm: 6187097897017.95, NNZs: 843, Bias: -104405340.874515, T: 6764303, Avg. loss: 117054712564373625215840933642240.000000\n",
      "Total training time: 14.48 seconds.\n",
      "-- Epoch 2010\n",
      "Norm: 6158497199031.73, NNZs: 843, Bias: 91664948.852110, T: 6767670, Avg. loss: 117025207451879926214739485523968.000000\n",
      "Total training time: 14.49 seconds.\n",
      "-- Epoch 2011\n",
      "Norm: 6267201053273.23, NNZs: 843, Bias: -104384924.442824, T: 6771037, Avg. loss: 116996196716081527073655622729728.000000\n",
      "Total training time: 14.50 seconds.\n",
      "-- Epoch 2012\n",
      "Norm: 5678949738524.65, NNZs: 843, Bias: 81829884.426736, T: 6774404, Avg. loss: 116967341833156345384020473806848.000000\n",
      "Total training time: 14.51 seconds.\n",
      "-- Epoch 2013\n",
      "Norm: 5805973816625.66, NNZs: 843, Bias: -114171825.365548, T: 6777771, Avg. loss: 116937821774313712930974861361152.000000\n",
      "Total training time: 14.51 seconds.\n",
      "-- Epoch 2014\n",
      "Norm: 5864699468512.96, NNZs: 843, Bias: 81802680.961027, T: 6781138, Avg. loss: 116908992986210271928507732852736.000000\n",
      "Total training time: 14.52 seconds.\n",
      "-- Epoch 2015\n",
      "Norm: 5901652515507.85, NNZs: 843, Bias: -114147774.588023, T: 6784505, Avg. loss: 116880835622076165086520214028288.000000\n",
      "Total training time: 14.53 seconds.\n",
      "-- Epoch 2016\n",
      "Norm: 6028160144482.36, NNZs: 843, Bias: 81778761.160171, T: 6787872, Avg. loss: 116851814246102235458307239182336.000000\n",
      "Total training time: 14.54 seconds.\n",
      "-- Epoch 2017\n",
      "Norm: 5752899683121.33, NNZs: 843, Bias: 277655005.601015, T: 6791239, Avg. loss: 116822394249526243086596816502784.000000\n",
      "Total training time: 14.54 seconds.\n",
      "-- Epoch 2018\n",
      "Norm: 5749514193470.44, NNZs: 843, Bias: 81752360.607600, T: 6794606, Avg. loss: 116793203874801019007364327538688.000000\n",
      "Total training time: 14.55 seconds.\n",
      "-- Epoch 2019\n",
      "Norm: 5696279252399.02, NNZs: 843, Bias: 277581403.420958, T: 6797973, Avg. loss: 116764213330613839214477106479104.000000\n",
      "Total training time: 14.56 seconds.\n",
      "-- Epoch 2020\n",
      "Norm: 5818392033599.41, NNZs: 843, Bias: -309909154.717584, T: 6801340, Avg. loss: 116735001048995837252480496828416.000000\n",
      "Total training time: 14.56 seconds.\n",
      "-- Epoch 2021\n",
      "Norm: 6051098535393.35, NNZs: 843, Bias: -505666459.353750, T: 6804707, Avg. loss: 116706291579012072902924408193024.000000\n",
      "Total training time: 14.57 seconds.\n",
      "-- Epoch 2022\n",
      "Norm: 5778999952750.38, NNZs: 843, Bias: 81700190.763115, T: 6808074, Avg. loss: 116677222877467033146022829228032.000000\n",
      "Total training time: 14.58 seconds.\n",
      "-- Epoch 2023\n",
      "Norm: 5667448723052.42, NNZs: 843, Bias: -114059327.002509, T: 6811441, Avg. loss: 116648398356416350844427373117440.000000\n",
      "Total training time: 14.59 seconds.\n",
      "-- Epoch 2024\n",
      "Norm: 5680567176518.42, NNZs: 843, Bias: 81673127.303299, T: 6814808, Avg. loss: 116619554902867806595928953978880.000000\n",
      "Total training time: 14.59 seconds.\n",
      "-- Epoch 2025\n",
      "Norm: 5621121157453.93, NNZs: 843, Bias: 7140480.932561, T: 6818175, Avg. loss: 116590305520613007890385209393152.000000\n",
      "Total training time: 14.60 seconds.\n",
      "-- Epoch 2026\n",
      "Norm: 6024101764742.75, NNZs: 843, Bias: -579883226.963036, T: 6821542, Avg. loss: 116561341079841013334441218539520.000000\n",
      "Total training time: 14.61 seconds.\n",
      "-- Epoch 2027\n",
      "Norm: 5634330995246.49, NNZs: 843, Bias: 254722359.949441, T: 6824909, Avg. loss: 116532104082675620104405208006656.000000\n",
      "Total training time: 14.61 seconds.\n",
      "-- Epoch 2028\n",
      "Norm: 5518402603484.93, NNZs: 843, Bias: 59065136.723336, T: 6828276, Avg. loss: 116503635999087684180504142151680.000000\n",
      "Total training time: 14.62 seconds.\n",
      "-- Epoch 2029\n",
      "Norm: 5431006817383.87, NNZs: 843, Bias: -136543728.117832, T: 6831643, Avg. loss: 116474784271071779554904940478464.000000\n",
      "Total training time: 14.63 seconds.\n",
      "-- Epoch 2030\n",
      "Norm: 5720254719534.50, NNZs: 843, Bias: -188920920.703525, T: 6835010, Avg. loss: 116445805379763285436912059285504.000000\n",
      "Total training time: 14.64 seconds.\n",
      "-- Epoch 2031\n",
      "Norm: 5356446265342.57, NNZs: 843, Bias: -384453398.575746, T: 6838377, Avg. loss: 116417724560786318253824621412352.000000\n",
      "Total training time: 14.64 seconds.\n",
      "-- Epoch 2032\n",
      "Norm: 5404367723742.24, NNZs: 843, Bias: 202173294.619408, T: 6841744, Avg. loss: 116389244720302289847068045344768.000000\n",
      "Total training time: 14.65 seconds.\n",
      "-- Epoch 2033\n",
      "Norm: 5378670147538.89, NNZs: 843, Bias: 6639879.149402, T: 6845111, Avg. loss: 116360723670679065784577070989312.000000\n",
      "Total training time: 14.66 seconds.\n",
      "-- Epoch 2034\n",
      "Norm: 5646057733695.08, NNZs: 843, Bias: 202114608.816911, T: 6848478, Avg. loss: 116331581873212179694567578140672.000000\n",
      "Total training time: 14.67 seconds.\n",
      "-- Epoch 2035\n",
      "Norm: 5568056752370.27, NNZs: 843, Bias: 6630766.839287, T: 6851845, Avg. loss: 116303443214035184520689117822976.000000\n",
      "Total training time: 14.67 seconds.\n",
      "-- Epoch 2036\n",
      "Norm: 5654591387297.03, NNZs: 843, Bias: -146726476.111469, T: 6855212, Avg. loss: 116274687216001601135236920377344.000000\n",
      "Total training time: 14.68 seconds.\n",
      "-- Epoch 2037\n",
      "Norm: 6126277210609.32, NNZs: 843, Bias: -342119597.267799, T: 6858579, Avg. loss: 116245989447678432760341815361536.000000\n",
      "Total training time: 14.69 seconds.\n",
      "-- Epoch 2038\n",
      "Norm: 5544769562285.64, NNZs: 843, Bias: -45172782.616563, T: 6861946, Avg. loss: 116218072488003700298595824566272.000000\n",
      "Total training time: 14.69 seconds.\n",
      "-- Epoch 2039\n",
      "Norm: 5897158675890.80, NNZs: 843, Bias: -240528020.529296, T: 6865313, Avg. loss: 116189677014272587707403005329408.000000\n",
      "Total training time: 14.70 seconds.\n",
      "-- Epoch 2040\n",
      "Norm: 6050036228633.83, NNZs: 843, Bias: -315003811.580313, T: 6868680, Avg. loss: 116161342998000856340020322107392.000000\n",
      "Total training time: 14.71 seconds.\n",
      "-- Epoch 2041\n",
      "Norm: 5772099754284.78, NNZs: 843, Bias: -119653289.162464, T: 6872047, Avg. loss: 116133019651120533063234198962176.000000\n",
      "Total training time: 14.72 seconds.\n",
      "-- Epoch 2042\n",
      "Norm: 5452911016973.27, NNZs: 843, Bias: -314930903.271888, T: 6875414, Avg. loss: 116105386432098401783424462356480.000000\n",
      "Total training time: 14.72 seconds.\n",
      "-- Epoch 2043\n",
      "Norm: 5573053525230.52, NNZs: 843, Bias: -119630545.647325, T: 6878781, Avg. loss: 116076960549061995421433474318336.000000\n",
      "Total training time: 14.73 seconds.\n",
      "-- Epoch 2044\n",
      "Norm: 5399705204794.30, NNZs: 843, Bias: 75621568.920172, T: 6882148, Avg. loss: 116048708170815516323953785176064.000000\n",
      "Total training time: 14.74 seconds.\n",
      "-- Epoch 2045\n",
      "Norm: 5561357761883.87, NNZs: 843, Bias: 270827157.948617, T: 6885515, Avg. loss: 116019939540510174086153476505600.000000\n",
      "Total training time: 14.74 seconds.\n",
      "-- Epoch 2046\n",
      "Norm: 5438497198276.02, NNZs: 843, Bias: -99086317.803658, T: 6888882, Avg. loss: 115991694371237966353868051709952.000000\n",
      "Total training time: 14.75 seconds.\n",
      "-- Epoch 2047\n",
      "Norm: 5769402494503.96, NNZs: 843, Bias: -12956708.602604, T: 6892249, Avg. loss: 115963919014384408564563550666752.000000\n",
      "Total training time: 14.76 seconds.\n",
      "-- Epoch 2048\n",
      "Norm: 5592252094196.62, NNZs: 843, Bias: -208103625.177750, T: 6895616, Avg. loss: 115935591607142680405186457894912.000000\n",
      "Total training time: 14.77 seconds.\n",
      "-- Epoch 2049\n",
      "Norm: 5840502272584.97, NNZs: 843, Bias: -403201307.356559, T: 6898983, Avg. loss: 115907351709665913932190391992320.000000\n",
      "Total training time: 14.77 seconds.\n",
      "-- Epoch 2050\n",
      "Norm: 5659007551730.21, NNZs: 843, Bias: -208059242.486423, T: 6902350, Avg. loss: 115879787011095252918404980408320.000000\n",
      "Total training time: 14.78 seconds.\n",
      "-- Epoch 2051\n",
      "Norm: 5577896108608.10, NNZs: 843, Bias: 43919472.778436, T: 6905717, Avg. loss: 115851510620438880771828424376320.000000\n",
      "Total training time: 14.79 seconds.\n",
      "-- Epoch 2052\n",
      "Norm: 5640065645584.46, NNZs: 843, Bias: 186035793.553988, T: 6909084, Avg. loss: 115822939173189216840446558339072.000000\n",
      "Total training time: 14.79 seconds.\n",
      "-- Epoch 2053\n",
      "Norm: 5458415410033.48, NNZs: 843, Bias: -159172915.154471, T: 6912451, Avg. loss: 115795141029794001314052604690432.000000\n",
      "Total training time: 14.80 seconds.\n",
      "-- Epoch 2054\n",
      "Norm: 5716773363335.46, NNZs: 843, Bias: -354159979.505250, T: 6915818, Avg. loss: 115766795564072044131229341057024.000000\n",
      "Total training time: 14.81 seconds.\n",
      "-- Epoch 2055\n",
      "Norm: 5658435083559.88, NNZs: 843, Bias: 230815300.930072, T: 6919185, Avg. loss: 115739231337618722043080151662592.000000\n",
      "Total training time: 14.82 seconds.\n",
      "-- Epoch 2056\n",
      "Norm: 5612015962244.68, NNZs: 843, Bias: 35829711.601433, T: 6922552, Avg. loss: 115710929967984482487199523143680.000000\n",
      "Total training time: 14.82 seconds.\n",
      "-- Epoch 2057\n",
      "Norm: 5533984567922.07, NNZs: 843, Bias: -159109398.255971, T: 6925919, Avg. loss: 115682912718266623246429356818432.000000\n",
      "Total training time: 14.83 seconds.\n",
      "-- Epoch 2058\n",
      "Norm: 5429207584951.54, NNZs: 843, Bias: 35813915.023530, T: 6929286, Avg. loss: 115654787861198591876134661521408.000000\n",
      "Total training time: 14.84 seconds.\n",
      "-- Epoch 2059\n",
      "Norm: 5533984248620.66, NNZs: 843, Bias: -159077212.710811, T: 6932653, Avg. loss: 115626941070218501918561428045824.000000\n",
      "Total training time: 14.84 seconds.\n",
      "-- Epoch 2060\n",
      "Norm: 5699657446772.01, NNZs: 843, Bias: -231407031.607106, T: 6936020, Avg. loss: 115598557849499296185417579102208.000000\n",
      "Total training time: 14.85 seconds.\n",
      "-- Epoch 2061\n",
      "Norm: 5526352831013.43, NNZs: 843, Bias: -36542482.123124, T: 6939387, Avg. loss: 115570533200826093919357500391424.000000\n",
      "Total training time: 14.86 seconds.\n",
      "-- Epoch 2062\n",
      "Norm: 5689388146526.77, NNZs: 843, Bias: 111327086.166550, T: 6942754, Avg. loss: 115542505403630583552001292894208.000000\n",
      "Total training time: 14.87 seconds.\n",
      "-- Epoch 2063\n",
      "Norm: 5651716447643.64, NNZs: 843, Bias: -83478478.701784, T: 6946121, Avg. loss: 115514464411419769889106618744832.000000\n",
      "Total training time: 14.87 seconds.\n",
      "-- Epoch 2064\n",
      "Norm: 5837365820440.41, NNZs: 843, Bias: 111294759.306761, T: 6949488, Avg. loss: 115486435585771908350679502028800.000000\n",
      "Total training time: 14.88 seconds.\n",
      "-- Epoch 2065\n",
      "Norm: 5635476788454.45, NNZs: 843, Bias: -83460732.428618, T: 6952855, Avg. loss: 115458517063839015565153292779520.000000\n",
      "Total training time: 14.89 seconds.\n",
      "-- Epoch 2066\n",
      "Norm: 5822991843669.36, NNZs: 843, Bias: 111265973.736099, T: 6956222, Avg. loss: 115431097430177843684537800851456.000000\n",
      "Total training time: 14.89 seconds.\n",
      "-- Epoch 2067\n",
      "Norm: 5806481996504.60, NNZs: 843, Bias: 46144164.444300, T: 6959589, Avg. loss: 115402624864100615413063783284736.000000\n",
      "Total training time: 14.90 seconds.\n",
      "-- Epoch 2068\n",
      "Norm: 5831844786186.70, NNZs: 843, Bias: -36961760.048086, T: 6962956, Avg. loss: 115375273275785310419286503718912.000000\n",
      "Total training time: 14.91 seconds.\n",
      "-- Epoch 2069\n",
      "Norm: 5805364049204.34, NNZs: 843, Bias: -114903395.693828, T: 6966323, Avg. loss: 115347251678068181725588995702784.000000\n",
      "Total training time: 14.92 seconds.\n",
      "-- Epoch 2070\n",
      "Norm: 6076189852047.28, NNZs: 843, Bias: 79731109.982974, T: 6969690, Avg. loss: 115319441448722087518454620880896.000000\n",
      "Total training time: 14.92 seconds.\n",
      "-- Epoch 2071\n",
      "Norm: 6177022359396.35, NNZs: 843, Bias: -272054153.837281, T: 6973057, Avg. loss: 115291655781482898999882350067712.000000\n",
      "Total training time: 14.93 seconds.\n",
      "-- Epoch 2072\n",
      "Norm: 6018352897925.06, NNZs: 843, Bias: -242542335.222897, T: 6976424, Avg. loss: 115263817048464409163576826134528.000000\n",
      "Total training time: 14.94 seconds.\n",
      "-- Epoch 2073\n",
      "Norm: 6134160222081.03, NNZs: 843, Bias: -47961069.429850, T: 6979791, Avg. loss: 115236200222027926289548906070016.000000\n",
      "Total training time: 14.94 seconds.\n",
      "-- Epoch 2074\n",
      "Norm: 6260377724475.65, NNZs: 843, Bias: -62506644.749697, T: 6983158, Avg. loss: 115208015153815091315713540882432.000000\n",
      "Total training time: 14.95 seconds.\n",
      "-- Epoch 2075\n",
      "Norm: 6096475383618.75, NNZs: 843, Bias: 51395278.922169, T: 6986525, Avg. loss: 115180059875511005653976410488832.000000\n",
      "Total training time: 14.96 seconds.\n",
      "-- Epoch 2076\n",
      "Norm: 6463430173249.72, NNZs: 843, Bias: -532062707.902602, T: 6989892, Avg. loss: 115152623322581863860669782163456.000000\n",
      "Total training time: 14.97 seconds.\n",
      "-- Epoch 2077\n",
      "Norm: 6475031292464.78, NNZs: 843, Bias: -397252145.282848, T: 6993259, Avg. loss: 115125198057642524144499096027136.000000\n",
      "Total training time: 14.97 seconds.\n",
      "-- Epoch 2078\n",
      "Norm: 6158754519690.90, NNZs: 843, Bias: -202770434.122863, T: 6996626, Avg. loss: 115097493238384785393947143831552.000000\n",
      "Total training time: 14.98 seconds.\n",
      "-- Epoch 2079\n",
      "Norm: 6219886750410.25, NNZs: 843, Bias: 234893297.598924, T: 6999993, Avg. loss: 115070111432402348765159062765568.000000\n",
      "Total training time: 14.99 seconds.\n",
      "-- Epoch 2080\n",
      "Norm: 6322265795391.10, NNZs: 843, Bias: -348306646.376259, T: 7003360, Avg. loss: 115042085073329255773420499501056.000000\n",
      "Total training time: 15.00 seconds.\n",
      "-- Epoch 2081\n",
      "Norm: 6017801786392.75, NNZs: 843, Bias: -153901428.919355, T: 7006727, Avg. loss: 115014117166488174372719572287488.000000\n",
      "Total training time: 15.00 seconds.\n",
      "-- Epoch 2082\n",
      "Norm: 5923597994195.49, NNZs: 843, Bias: -348229092.289026, T: 7010094, Avg. loss: 114986754929144829277073110466560.000000\n",
      "Total training time: 15.01 seconds.\n",
      "-- Epoch 2083\n",
      "Norm: 5685195655122.01, NNZs: 843, Bias: -153871882.765431, T: 7013461, Avg. loss: 114958644249773612755684041949184.000000\n",
      "Total training time: 15.02 seconds.\n",
      "-- Epoch 2084\n",
      "Norm: 5978271750248.25, NNZs: 843, Bias: -298069943.375601, T: 7016828, Avg. loss: 114930941239044210622826555637760.000000\n",
      "Total training time: 15.02 seconds.\n",
      "-- Epoch 2085\n",
      "Norm: 5884260863771.74, NNZs: 843, Bias: 284782006.935208, T: 7020195, Avg. loss: 114903841937967440038134603055104.000000\n",
      "Total training time: 15.03 seconds.\n",
      "-- Epoch 2086\n",
      "Norm: 5728937890552.60, NNZs: 843, Bias: -298006369.889758, T: 7023562, Avg. loss: 114876479014534612824986430210048.000000\n",
      "Total training time: 15.04 seconds.\n",
      "-- Epoch 2087\n",
      "Norm: 5726407840211.61, NNZs: 843, Bias: -103747527.582452, T: 7026929, Avg. loss: 114848768363362009774033985339392.000000\n",
      "Total training time: 15.05 seconds.\n",
      "-- Epoch 2088\n",
      "Norm: 5930740329146.99, NNZs: 843, Bias: 478871874.103465, T: 7030296, Avg. loss: 114821043951439664428620443549696.000000\n",
      "Total training time: 15.05 seconds.\n",
      "-- Epoch 2089\n",
      "Norm: 6231786162098.23, NNZs: 843, Bias: 284630923.958382, T: 7033663, Avg. loss: 114793713195907941642600628355072.000000\n",
      "Total training time: 15.06 seconds.\n",
      "-- Epoch 2090\n",
      "Norm: 5784447096481.88, NNZs: 843, Bias: 90436976.640070, T: 7037030, Avg. loss: 114766656657624490021324507316224.000000\n",
      "Total training time: 15.07 seconds.\n",
      "-- Epoch 2091\n",
      "Norm: 6210472908348.43, NNZs: 843, Bias: 284556918.302997, T: 7040397, Avg. loss: 114739821109620286875156174864384.000000\n",
      "Total training time: 15.07 seconds.\n",
      "-- Epoch 2092\n",
      "Norm: 6016738792028.46, NNZs: 843, Bias: 90409767.316763, T: 7043764, Avg. loss: 114712608462016243099382264627200.000000\n",
      "Total training time: 15.08 seconds.\n",
      "-- Epoch 2093\n",
      "Norm: 5934122066822.75, NNZs: 843, Bias: 284479902.318393, T: 7047131, Avg. loss: 114684789396639043059139745939456.000000\n",
      "Total training time: 15.09 seconds.\n",
      "-- Epoch 2094\n",
      "Norm: 5758619263299.87, NNZs: 843, Bias: 90379670.743443, T: 7050498, Avg. loss: 114657706025031728371435193761792.000000\n",
      "Total training time: 15.10 seconds.\n",
      "-- Epoch 2095\n",
      "Norm: 6141669141736.41, NNZs: 843, Bias: -491758163.971716, T: 7053865, Avg. loss: 114630854491845227288380680699904.000000\n",
      "Total training time: 15.10 seconds.\n",
      "-- Epoch 2096\n",
      "Norm: 5921771750949.03, NNZs: 843, Bias: 478386113.580000, T: 7057232, Avg. loss: 114603544624350633645619357417472.000000\n",
      "Total training time: 15.11 seconds.\n",
      "-- Epoch 2097\n",
      "Norm: 5809147027786.95, NNZs: 843, Bias: -103657117.024471, T: 7060599, Avg. loss: 114576477839527660074824739323904.000000\n",
      "Total training time: 15.12 seconds.\n",
      "-- Epoch 2098\n",
      "Norm: 5598298026227.00, NNZs: 843, Bias: 279477784.994709, T: 7063966, Avg. loss: 114548996867626360277353862529024.000000\n",
      "Total training time: 15.12 seconds.\n",
      "-- Epoch 2099\n",
      "Norm: 5653968417609.81, NNZs: 843, Bias: -302404539.157926, T: 7067333, Avg. loss: 114521899166805556821289496215552.000000\n",
      "Total training time: 15.13 seconds.\n",
      "-- Epoch 2100\n",
      "Norm: 5665966633846.81, NNZs: 843, Bias: 3645733.145457, T: 7070700, Avg. loss: 114495032606899279653004155813888.000000\n",
      "Total training time: 15.14 seconds.\n",
      "-- Epoch 2101\n",
      "Norm: 5954300534719.11, NNZs: 843, Bias: -186504887.098387, T: 7074067, Avg. loss: 114467692812262658652277118599168.000000\n",
      "Total training time: 15.15 seconds.\n",
      "-- Epoch 2102\n",
      "Norm: 5825013913995.61, NNZs: 843, Bias: 395150486.820405, T: 7077434, Avg. loss: 114440355529247142736688037494784.000000\n",
      "Total training time: 15.15 seconds.\n",
      "-- Epoch 2103\n",
      "Norm: 5767576954940.49, NNZs: 843, Bias: 201242610.171756, T: 7080801, Avg. loss: 114413763045690797813527226613760.000000\n",
      "Total training time: 15.16 seconds.\n",
      "-- Epoch 2104\n",
      "Norm: 5713958888728.52, NNZs: 843, Bias: 171979423.020037, T: 7084168, Avg. loss: 114386613106908671603362941108224.000000\n",
      "Total training time: 15.17 seconds.\n",
      "-- Epoch 2105\n",
      "Norm: 5470367009202.00, NNZs: 843, Bias: -21854935.129020, T: 7087535, Avg. loss: 114359819763806096956775886487552.000000\n",
      "Total training time: 15.17 seconds.\n",
      "-- Epoch 2106\n",
      "Norm: 5422324240213.66, NNZs: 843, Bias: 88751750.192118, T: 7090902, Avg. loss: 114332832305391229302344559624192.000000\n",
      "Total training time: 15.18 seconds.\n",
      "-- Epoch 2107\n",
      "Norm: 5566210587585.66, NNZs: 843, Bias: -88663185.368374, T: 7094269, Avg. loss: 114305576212259890427217297014784.000000\n",
      "Total training time: 15.19 seconds.\n",
      "-- Epoch 2108\n",
      "Norm: 5487593781590.41, NNZs: 843, Bias: -282395442.087171, T: 7097636, Avg. loss: 114278522836868562382273441169408.000000\n",
      "Total training time: 15.20 seconds.\n",
      "-- Epoch 2109\n",
      "Norm: 5594836467656.04, NNZs: 843, Bias: -88647019.156897, T: 7101003, Avg. loss: 114251624229495793503655205797888.000000\n",
      "Total training time: 15.20 seconds.\n",
      "-- Epoch 2110\n",
      "Norm: 5595536294045.93, NNZs: 843, Bias: -282331109.098930, T: 7104370, Avg. loss: 114224722114891258003494520487936.000000\n",
      "Total training time: 15.21 seconds.\n",
      "-- Epoch 2111\n",
      "Norm: 5561684073047.17, NNZs: 843, Bias: -249461474.124084, T: 7107737, Avg. loss: 114197755638900302756324220862464.000000\n",
      "Total training time: 15.22 seconds.\n",
      "-- Epoch 2112\n",
      "Norm: 5376723693484.92, NNZs: 843, Bias: 223136377.471096, T: 7111104, Avg. loss: 114170989030457703317252283039744.000000\n",
      "Total training time: 15.23 seconds.\n",
      "-- Epoch 2113\n",
      "Norm: 5366739327767.79, NNZs: 843, Bias: 29480083.511234, T: 7114471, Avg. loss: 114144208593394223687402377969664.000000\n",
      "Total training time: 15.23 seconds.\n",
      "-- Epoch 2114\n",
      "Norm: 5813990203098.10, NNZs: 843, Bias: -551335122.925815, T: 7117838, Avg. loss: 114117289209548216210687956877312.000000\n",
      "Total training time: 15.24 seconds.\n",
      "-- Epoch 2115\n",
      "Norm: 5459522655142.18, NNZs: 843, Bias: -123945778.449204, T: 7121205, Avg. loss: 114090192202419655590270059151360.000000\n",
      "Total training time: 15.25 seconds.\n",
      "-- Epoch 2116\n",
      "Norm: 5472846743806.57, NNZs: 843, Bias: 69624526.391041, T: 7124572, Avg. loss: 114063237729261594661053043572736.000000\n",
      "Total training time: 15.25 seconds.\n",
      "-- Epoch 2117\n",
      "Norm: 5365137459687.34, NNZs: 843, Bias: -157047074.814576, T: 7127939, Avg. loss: 114036434758505383837299696992256.000000\n",
      "Total training time: 15.26 seconds.\n",
      "-- Epoch 2118\n",
      "Norm: 5290900934641.47, NNZs: 843, Bias: 36479568.494994, T: 7131306, Avg. loss: 114009622318530838661784406065152.000000\n",
      "Total training time: 15.27 seconds.\n",
      "-- Epoch 2119\n",
      "Norm: 5718897187922.38, NNZs: 843, Bias: 229959346.835810, T: 7134673, Avg. loss: 113982587041494340525047450435584.000000\n",
      "Total training time: 15.28 seconds.\n",
      "-- Epoch 2120\n",
      "Norm: 5688793993072.22, NNZs: 843, Bias: -350470617.413171, T: 7138040, Avg. loss: 113955930422061779162479139487744.000000\n",
      "Total training time: 15.28 seconds.\n",
      "-- Epoch 2121\n",
      "Norm: 5549060567482.90, NNZs: 843, Bias: -156989689.794501, T: 7141407, Avg. loss: 113928755342655417183291270234112.000000\n",
      "Total training time: 15.29 seconds.\n",
      "-- Epoch 2122\n",
      "Norm: 5369122093103.11, NNZs: 843, Bias: 36443399.693562, T: 7144774, Avg. loss: 113902114572816198510689600405504.000000\n",
      "Total training time: 15.30 seconds.\n",
      "-- Epoch 2123\n",
      "Norm: 5700373098154.96, NNZs: 843, Bias: -24786587.365406, T: 7148141, Avg. loss: 113874897755503981403716909531136.000000\n",
      "Total training time: 15.30 seconds.\n",
      "-- Epoch 2124\n",
      "Norm: 5583655934898.62, NNZs: 843, Bias: -218161496.093926, T: 7151508, Avg. loss: 113848047910045331787833820577792.000000\n",
      "Total training time: 15.31 seconds.\n",
      "-- Epoch 2125\n",
      "Norm: 5786251335117.23, NNZs: 843, Bias: -24785755.839197, T: 7154875, Avg. loss: 113821751797115196037907229966336.000000\n",
      "Total training time: 15.32 seconds.\n",
      "-- Epoch 2126\n",
      "Norm: 6166968519019.15, NNZs: 843, Bias: 168544212.228475, T: 7158242, Avg. loss: 113794882231473236954107336458240.000000\n",
      "Total training time: 15.33 seconds.\n",
      "-- Epoch 2127\n",
      "Norm: 5346101817472.75, NNZs: 843, Bias: -170034405.132611, T: 7161609, Avg. loss: 113768122195716537686965557919744.000000\n",
      "Total training time: 15.33 seconds.\n",
      "-- Epoch 2128\n",
      "Norm: 5508140791642.47, NNZs: 843, Bias: -116337509.353639, T: 7164976, Avg. loss: 113741146187020364428961621475328.000000\n",
      "Total training time: 15.34 seconds.\n",
      "-- Epoch 2129\n",
      "Norm: 5786693968467.38, NNZs: 843, Bias: 76933890.349447, T: 7168343, Avg. loss: 113713954729707548799029915680768.000000\n",
      "Total training time: 15.35 seconds.\n",
      "-- Epoch 2130\n",
      "Norm: 5310414283380.82, NNZs: 843, Bias: -116316244.979956, T: 7171710, Avg. loss: 113687603548022321573530051805184.000000\n",
      "Total training time: 15.35 seconds.\n",
      "-- Epoch 2131\n",
      "Norm: 5247591862379.79, NNZs: 843, Bias: 76910333.663205, T: 7175077, Avg. loss: 113661244649642241174225226301440.000000\n",
      "Total training time: 15.36 seconds.\n",
      "-- Epoch 2132\n",
      "Norm: 5366523415955.36, NNZs: 843, Bias: -116294641.654739, T: 7178444, Avg. loss: 113634308755628444492518941261824.000000\n",
      "Total training time: 15.37 seconds.\n",
      "-- Epoch 2133\n",
      "Norm: 5771182093797.40, NNZs: 843, Bias: -309455008.517699, T: 7181811, Avg. loss: 113608120022280605056915426246656.000000\n",
      "Total training time: 15.38 seconds.\n",
      "-- Epoch 2134\n",
      "Norm: 5406763280135.43, NNZs: 843, Bias: -66629695.650508, T: 7185178, Avg. loss: 113581345944684821013549031620608.000000\n",
      "Total training time: 15.38 seconds.\n",
      "-- Epoch 2135\n",
      "Norm: 6065668877028.42, NNZs: 843, Bias: 126501606.607795, T: 7188545, Avg. loss: 113554533166518064298582792470528.000000\n",
      "Total training time: 15.39 seconds.\n",
      "-- Epoch 2136\n",
      "Norm: 5801235016245.71, NNZs: 843, Bias: -117546044.299316, T: 7191912, Avg. loss: 113527991607822982353508005576704.000000\n",
      "Total training time: 15.40 seconds.\n",
      "-- Epoch 2137\n",
      "Norm: 6077341043647.58, NNZs: 843, Bias: 75544598.918583, T: 7195279, Avg. loss: 113502023909432009222769604558848.000000\n",
      "Total training time: 15.41 seconds.\n",
      "-- Epoch 2138\n",
      "Norm: 6130665927543.86, NNZs: 843, Bias: -66013212.423973, T: 7198646, Avg. loss: 113475310126409725491222562209792.000000\n",
      "Total training time: 15.41 seconds.\n",
      "-- Epoch 2139\n",
      "Norm: 6157367939759.07, NNZs: 843, Bias: -259044256.493908, T: 7202013, Avg. loss: 113449146420058213555857916952576.000000\n",
      "Total training time: 15.42 seconds.\n",
      "-- Epoch 2140\n",
      "Norm: 5995638135237.11, NNZs: 843, Bias: -66002739.691239, T: 7205380, Avg. loss: 113422495093601288312921784320000.000000\n",
      "Total training time: 15.43 seconds.\n",
      "-- Epoch 2141\n",
      "Norm: 6173459626750.33, NNZs: 843, Bias: 126992733.102066, T: 7208747, Avg. loss: 113396093097527178234410179756032.000000\n",
      "Total training time: 15.43 seconds.\n",
      "-- Epoch 2142\n",
      "Norm: 6249885945104.52, NNZs: 843, Bias: -65991020.436701, T: 7212114, Avg. loss: 113369625760687824620656189767680.000000\n",
      "Total training time: 15.44 seconds.\n",
      "-- Epoch 2143\n",
      "Norm: 5778763870423.84, NNZs: 843, Bias: -271607735.182557, T: 7215481, Avg. loss: 113343107621046161044813119488000.000000\n",
      "Total training time: 15.45 seconds.\n",
      "-- Epoch 2144\n",
      "Norm: 5858787444790.79, NNZs: 843, Bias: -4326760.130574, T: 7218848, Avg. loss: 113316946744854401075380609875968.000000\n",
      "Total training time: 15.46 seconds.\n",
      "-- Epoch 2145\n",
      "Norm: 5941971810178.77, NNZs: 843, Bias: 46301350.322139, T: 7222215, Avg. loss: 113290935581039499907543647387648.000000\n",
      "Total training time: 15.46 seconds.\n",
      "-- Epoch 2146\n",
      "Norm: 6036807288922.88, NNZs: 843, Bias: 415508849.273387, T: 7225582, Avg. loss: 113264578202189768439334006947840.000000\n",
      "Total training time: 15.47 seconds.\n",
      "-- Epoch 2147\n",
      "Norm: 6049367053674.98, NNZs: 843, Bias: -163109377.621421, T: 7228949, Avg. loss: 113238406453142237823621780209664.000000\n",
      "Total training time: 15.48 seconds.\n",
      "-- Epoch 2148\n",
      "Norm: 6537298967163.31, NNZs: 843, Bias: 29738357.332040, T: 7232316, Avg. loss: 113211685640225106941782475669504.000000\n",
      "Total training time: 15.48 seconds.\n",
      "-- Epoch 2149\n",
      "Norm: 6619872693724.36, NNZs: 843, Bias: 293001881.663275, T: 7235683, Avg. loss: 113185482096030567986729689546752.000000\n",
      "Total training time: 15.49 seconds.\n",
      "-- Epoch 2150\n",
      "Norm: 6167333189036.83, NNZs: 843, Bias: 100177366.401760, T: 7239050, Avg. loss: 113159228710445792432396190089216.000000\n",
      "Total training time: 15.50 seconds.\n",
      "-- Epoch 2151\n",
      "Norm: 6244637216956.43, NNZs: 843, Bias: -92602671.884153, T: 7242417, Avg. loss: 113133076730317797069747365347328.000000\n",
      "Total training time: 15.51 seconds.\n",
      "-- Epoch 2152\n",
      "Norm: 6167810484900.35, NNZs: 843, Bias: -285338990.567247, T: 7245784, Avg. loss: 113107108429963754198456682414080.000000\n",
      "Total training time: 15.51 seconds.\n",
      "-- Epoch 2153\n",
      "Norm: 5783278229327.55, NNZs: 843, Bias: -92591372.354967, T: 7249151, Avg. loss: 113080298121770614654557000564736.000000\n",
      "Total training time: 15.52 seconds.\n",
      "-- Epoch 2154\n",
      "Norm: 5787382761113.75, NNZs: 843, Bias: 100112835.878852, T: 7252518, Avg. loss: 113054157187206950062617553534976.000000\n",
      "Total training time: 15.53 seconds.\n",
      "-- Epoch 2155\n",
      "Norm: 5745412051795.91, NNZs: 843, Bias: -141239160.139229, T: 7255885, Avg. loss: 113027913118445795142841149685760.000000\n",
      "Total training time: 15.53 seconds.\n",
      "-- Epoch 2156\n",
      "Norm: 5678854356268.10, NNZs: 843, Bias: 51426827.087268, T: 7259252, Avg. loss: 113001985445359589045936972627968.000000\n",
      "Total training time: 15.54 seconds.\n",
      "-- Epoch 2157\n",
      "Norm: 5809397893427.13, NNZs: 843, Bias: 244047392.313474, T: 7262619, Avg. loss: 112976227064700806221180457451520.000000\n",
      "Total training time: 15.55 seconds.\n",
      "-- Epoch 2158\n",
      "Norm: 5681018421542.55, NNZs: 843, Bias: -235521338.586689, T: 7265986, Avg. loss: 112949952811185960650322934235136.000000\n",
      "Total training time: 15.56 seconds.\n",
      "-- Epoch 2159\n",
      "Norm: 5885975767257.97, NNZs: 843, Bias: -428080593.830225, T: 7269353, Avg. loss: 112923976483468440058864542416896.000000\n",
      "Total training time: 15.56 seconds.\n",
      "-- Epoch 2160\n",
      "Norm: 5931025968035.29, NNZs: 843, Bias: -427799293.497681, T: 7272720, Avg. loss: 112897854010218840647750211600384.000000\n",
      "Total training time: 15.57 seconds.\n",
      "-- Epoch 2161\n",
      "Norm: 5751819274966.69, NNZs: 843, Bias: 55432029.070491, T: 7276087, Avg. loss: 112871797211253941481954682601472.000000\n",
      "Total training time: 15.58 seconds.\n",
      "-- Epoch 2162\n",
      "Norm: 6004238755737.91, NNZs: 843, Bias: -399134448.842803, T: 7279454, Avg. loss: 112845216808309728097264212115456.000000\n",
      "Total training time: 15.58 seconds.\n",
      "-- Epoch 2163\n",
      "Norm: 5919623513575.21, NNZs: 843, Bias: 219274259.713142, T: 7282821, Avg. loss: 112819500451102742943715295232000.000000\n",
      "Total training time: 15.59 seconds.\n",
      "-- Epoch 2164\n",
      "Norm: 5823100713267.39, NNZs: 843, Bias: -171924665.598894, T: 7286188, Avg. loss: 112793447290066521543951299641344.000000\n",
      "Total training time: 15.60 seconds.\n",
      "-- Epoch 2165\n",
      "Norm: 6246115417556.47, NNZs: 843, Bias: -379966119.100581, T: 7289555, Avg. loss: 112766897323040146747327166545920.000000\n",
      "Total training time: 15.61 seconds.\n",
      "-- Epoch 2166\n",
      "Norm: 5827075263078.16, NNZs: 843, Bias: 197367266.918109, T: 7292922, Avg. loss: 112740997002240072756920745721856.000000\n",
      "Total training time: 15.61 seconds.\n",
      "-- Epoch 2167\n",
      "Norm: 5691659495200.55, NNZs: 843, Bias: 4930666.728583, T: 7296289, Avg. loss: 112715237057311785986003329613824.000000\n",
      "Total training time: 15.62 seconds.\n",
      "-- Epoch 2168\n",
      "Norm: 5634385120009.37, NNZs: 843, Bias: 197312105.543059, T: 7299656, Avg. loss: 112689239244307407779950558707712.000000\n",
      "Total training time: 15.63 seconds.\n",
      "-- Epoch 2169\n",
      "Norm: 5455253166160.82, NNZs: 843, Bias: 77785172.236320, T: 7303023, Avg. loss: 112663248354866592485279956729856.000000\n",
      "Total training time: 15.63 seconds.\n",
      "-- Epoch 2170\n",
      "Norm: 5799849128487.35, NNZs: 843, Bias: -499254687.147779, T: 7306390, Avg. loss: 112637430693946618697727304794112.000000\n",
      "Total training time: 15.64 seconds.\n",
      "-- Epoch 2171\n",
      "Norm: 5766630520176.83, NNZs: 843, Bias: -358130071.370855, T: 7309757, Avg. loss: 112611403221379243122120607137792.000000\n",
      "Total training time: 15.65 seconds.\n",
      "-- Epoch 2172\n",
      "Norm: 5731732945263.61, NNZs: 843, Bias: 218800092.298822, T: 7313124, Avg. loss: 112585337302815871011286092349440.000000\n",
      "Total training time: 15.66 seconds.\n",
      "-- Epoch 2173\n",
      "Norm: 6303188109337.96, NNZs: 843, Bias: -742606819.494823, T: 7316491, Avg. loss: 112559412577978915199145148940288.000000\n",
      "Total training time: 15.66 seconds.\n",
      "-- Epoch 2174\n",
      "Norm: 6121192953848.97, NNZs: 843, Bias: -165765339.334005, T: 7319858, Avg. loss: 112533486395923126669831051411456.000000\n",
      "Total training time: 15.67 seconds.\n",
      "-- Epoch 2175\n",
      "Norm: 5958513713225.47, NNZs: 843, Bias: 26482393.749424, T: 7323225, Avg. loss: 112507330752983813398349570113536.000000\n",
      "Total training time: 15.68 seconds.\n",
      "-- Epoch 2176\n",
      "Norm: 6587994982759.08, NNZs: 843, Bias: 148204893.469165, T: 7326592, Avg. loss: 112481419452728666301729265942528.000000\n",
      "Total training time: 15.69 seconds.\n",
      "-- Epoch 2177\n",
      "Norm: 6141331213122.97, NNZs: 843, Bias: -539165420.409678, T: 7329959, Avg. loss: 112456155466552204108485638488064.000000\n",
      "Total training time: 15.69 seconds.\n",
      "-- Epoch 2178\n",
      "Norm: 6032585417136.13, NNZs: 843, Bias: 37388586.104869, T: 7333326, Avg. loss: 112430670041081936055765284421632.000000\n",
      "Total training time: 15.70 seconds.\n",
      "-- Epoch 2179\n",
      "Norm: 5944355978950.03, NNZs: 843, Bias: 3375697.637350, T: 7336693, Avg. loss: 112405301614353036978177634729984.000000\n",
      "Total training time: 15.71 seconds.\n",
      "-- Epoch 2180\n",
      "Norm: 6291751624789.04, NNZs: 843, Bias: -188747675.030375, T: 7340060, Avg. loss: 112379416909321754896467935363072.000000\n",
      "Total training time: 15.71 seconds.\n",
      "-- Epoch 2181\n",
      "Norm: 5655537678021.68, NNZs: 843, Bias: 3366858.699397, T: 7343427, Avg. loss: 112353975200002011092434552881152.000000\n",
      "Total training time: 15.72 seconds.\n",
      "-- Epoch 2182\n",
      "Norm: 5815205275899.68, NNZs: 843, Bias: 195438043.353780, T: 7346794, Avg. loss: 112328212356788254979532131401728.000000\n",
      "Total training time: 15.73 seconds.\n",
      "-- Epoch 2183\n",
      "Norm: 5855365628872.10, NNZs: 843, Bias: 3355779.544030, T: 7350161, Avg. loss: 112302438419906427842441647226880.000000\n",
      "Total training time: 15.74 seconds.\n",
      "-- Epoch 2184\n",
      "Norm: 5695498498659.39, NNZs: 843, Bias: -188680630.864425, T: 7353528, Avg. loss: 112276961404019705539258769997824.000000\n",
      "Total training time: 15.74 seconds.\n",
      "-- Epoch 2185\n",
      "Norm: 5751663447054.01, NNZs: 843, Bias: 3348226.115499, T: 7356895, Avg. loss: 112250861636295645477337584107520.000000\n",
      "Total training time: 15.75 seconds.\n",
      "-- Epoch 2186\n",
      "Norm: 5834904306533.20, NNZs: 843, Bias: 195331079.533678, T: 7360262, Avg. loss: 112224970634267593435476811317248.000000\n",
      "Total training time: 15.76 seconds.\n",
      "-- Epoch 2187\n",
      "Norm: 5787222408633.71, NNZs: 843, Bias: 19976646.442291, T: 7363629, Avg. loss: 112199324927365943539053256245248.000000\n",
      "Total training time: 15.77 seconds.\n",
      "-- Epoch 2188\n",
      "Norm: 5735968473002.36, NNZs: 843, Bias: -171973910.637952, T: 7366996, Avg. loss: 112173596663880213659424896581632.000000\n",
      "Total training time: 15.77 seconds.\n",
      "-- Epoch 2189\n",
      "Norm: 5781003420546.64, NNZs: 843, Bias: -363882196.166299, T: 7370363, Avg. loss: 112147951264045371456979751403520.000000\n",
      "Total training time: 15.78 seconds.\n",
      "-- Epoch 2190\n",
      "Norm: 5594231612058.13, NNZs: 843, Bias: -171944869.040107, T: 7373730, Avg. loss: 112122549112852856034023463452672.000000\n",
      "Total training time: 15.79 seconds.\n",
      "-- Epoch 2191\n",
      "Norm: 5596148910398.97, NNZs: 843, Bias: 19951270.938634, T: 7377097, Avg. loss: 112097051508279524824008991178752.000000\n",
      "Total training time: 15.79 seconds.\n",
      "-- Epoch 2192\n",
      "Norm: 5574878452671.66, NNZs: 843, Bias: -171912336.559676, T: 7380464, Avg. loss: 112071662732293117213137153556480.000000\n",
      "Total training time: 15.80 seconds.\n",
      "-- Epoch 2193\n",
      "Norm: 5721566860927.43, NNZs: 843, Bias: 19937448.621311, T: 7383831, Avg. loss: 112045971547203427533264376561664.000000\n",
      "Total training time: 15.81 seconds.\n",
      "-- Epoch 2194\n",
      "Norm: 5714734030504.82, NNZs: 843, Bias: -171882998.742654, T: 7387198, Avg. loss: 112020172815229308652109067976704.000000\n",
      "Total training time: 15.82 seconds.\n",
      "-- Epoch 2195\n",
      "Norm: 5720388424765.48, NNZs: 843, Bias: 210611123.332248, T: 7390565, Avg. loss: 111994402605276637390717539319808.000000\n",
      "Total training time: 15.82 seconds.\n",
      "-- Epoch 2196\n",
      "Norm: 5854289443441.08, NNZs: 843, Bias: 18813812.194360, T: 7393932, Avg. loss: 111969347539470438922129023959040.000000\n",
      "Total training time: 15.83 seconds.\n",
      "-- Epoch 2197\n",
      "Norm: 5753028373621.88, NNZs: 843, Bias: 46382724.437278, T: 7397299, Avg. loss: 111943723809290779850774604152832.000000\n",
      "Total training time: 15.84 seconds.\n",
      "-- Epoch 2198\n",
      "Norm: 5754708748399.11, NNZs: 843, Bias: -145352324.396311, T: 7400666, Avg. loss: 111917989039692026023751607910400.000000\n",
      "Total training time: 15.84 seconds.\n",
      "-- Epoch 2199\n",
      "Norm: 5820615512833.56, NNZs: 843, Bias: -337044057.952916, T: 7404033, Avg. loss: 111892840511588164404346169065472.000000\n",
      "Total training time: 15.85 seconds.\n",
      "-- Epoch 2200\n",
      "Norm: 5717647798159.45, NNZs: 843, Bias: -145326865.582222, T: 7407400, Avg. loss: 111867784935514940811440621092864.000000\n",
      "Total training time: 15.86 seconds.\n",
      "-- Epoch 2201\n",
      "Norm: 5693780618400.23, NNZs: 843, Bias: 46347615.280975, T: 7410767, Avg. loss: 111842192490037282697836046057472.000000\n",
      "Total training time: 15.87 seconds.\n",
      "-- Epoch 2202\n",
      "Norm: 5533338549031.83, NNZs: 843, Bias: -145301759.252840, T: 7414134, Avg. loss: 111816881338041234228403463258112.000000\n",
      "Total training time: 15.87 seconds.\n",
      "-- Epoch 2203\n",
      "Norm: 5451038240526.19, NNZs: 843, Bias: 46329669.826982, T: 7417501, Avg. loss: 111791337990385068696433344905216.000000\n",
      "Total training time: 15.88 seconds.\n",
      "-- Epoch 2204\n",
      "Norm: 5451653743438.34, NNZs: 843, Bias: -145274511.303385, T: 7420868, Avg. loss: 111765698523689761430221889732608.000000\n",
      "Total training time: 15.89 seconds.\n",
      "-- Epoch 2205\n",
      "Norm: 5748935693716.30, NNZs: 843, Bias: 46312197.530156, T: 7424235, Avg. loss: 111740599121260084431908834902016.000000\n",
      "Total training time: 15.89 seconds.\n",
      "-- Epoch 2206\n",
      "Norm: 5543841978612.48, NNZs: 843, Bias: 270145785.160356, T: 7427602, Avg. loss: 111715283203560800987329985511424.000000\n",
      "Total training time: 15.90 seconds.\n",
      "-- Epoch 2207\n",
      "Norm: 5459368862191.01, NNZs: 843, Bias: -304480918.569840, T: 7430969, Avg. loss: 111690005664777298348220889432064.000000\n",
      "Total training time: 15.91 seconds.\n",
      "-- Epoch 2208\n",
      "Norm: 5449099495683.45, NNZs: 843, Bias: -112939173.808594, T: 7434336, Avg. loss: 111664914323980609930603500929024.000000\n",
      "Total training time: 15.92 seconds.\n",
      "-- Epoch 2209\n",
      "Norm: 5413654207136.18, NNZs: 843, Bias: 78556036.615840, T: 7437703, Avg. loss: 111639415332627320234477287899136.000000\n",
      "Total training time: 15.92 seconds.\n",
      "-- Epoch 2210\n",
      "Norm: 5814630531531.00, NNZs: 843, Bias: -112921763.868699, T: 7441070, Avg. loss: 111613887863102907511195169718272.000000\n",
      "Total training time: 15.93 seconds.\n",
      "-- Epoch 2211\n",
      "Norm: 5403794524537.21, NNZs: 843, Bias: 78532319.130412, T: 7444437, Avg. loss: 111589009750648749575009288060928.000000\n",
      "Total training time: 15.94 seconds.\n",
      "-- Epoch 2212\n",
      "Norm: 5304722955743.82, NNZs: 843, Bias: -112902418.526174, T: 7447804, Avg. loss: 111563778173205867869525600370688.000000\n",
      "Total training time: 15.95 seconds.\n",
      "-- Epoch 2213\n",
      "Norm: 5537599077174.10, NNZs: 843, Bias: -264754006.913092, T: 7451171, Avg. loss: 111538947730913025376106892492800.000000\n",
      "Total training time: 15.95 seconds.\n",
      "-- Epoch 2214\n",
      "Norm: 5746657800124.65, NNZs: 843, Bias: 309411185.250119, T: 7454538, Avg. loss: 111513656111904456896843494719488.000000\n",
      "Total training time: 15.96 seconds.\n",
      "-- Epoch 2215\n",
      "Norm: 5626982154334.10, NNZs: 843, Bias: -264698008.590417, T: 7457905, Avg. loss: 111488569953685477982823690272768.000000\n",
      "Total training time: 15.97 seconds.\n",
      "-- Epoch 2216\n",
      "Norm: 6023153229519.48, NNZs: 843, Bias: 309337065.012277, T: 7461272, Avg. loss: 111463992777891188728553046802432.000000\n",
      "Total training time: 15.97 seconds.\n",
      "-- Epoch 2217\n",
      "Norm: 5659997624001.42, NNZs: 843, Bias: -278964549.795477, T: 7464639, Avg. loss: 111438541282763513524298434215936.000000\n",
      "Total training time: 15.98 seconds.\n",
      "-- Epoch 2218\n",
      "Norm: 5604225254966.53, NNZs: 843, Bias: -87643722.261566, T: 7468006, Avg. loss: 111414020002271288422397896556544.000000\n",
      "Total training time: 15.99 seconds.\n",
      "-- Epoch 2219\n",
      "Norm: 5519195052886.81, NNZs: 843, Bias: -60970443.642585, T: 7471373, Avg. loss: 111388598339392418526809395560448.000000\n",
      "Total training time: 16.00 seconds.\n",
      "-- Epoch 2220\n",
      "Norm: 5825622785321.32, NNZs: 843, Bias: -252215407.561757, T: 7474740, Avg. loss: 111363706352970275463881832792064.000000\n",
      "Total training time: 16.00 seconds.\n",
      "-- Epoch 2221\n",
      "Norm: 5494353012522.80, NNZs: 843, Bias: 66980520.974697, T: 7478107, Avg. loss: 111338585871599429954581328560128.000000\n",
      "Total training time: 16.01 seconds.\n",
      "-- Epoch 2222\n",
      "Norm: 5817461025513.04, NNZs: 843, Bias: 258176381.197261, T: 7481474, Avg. loss: 111313471272138844957843686162432.000000\n",
      "Total training time: 16.02 seconds.\n",
      "-- Epoch 2223\n",
      "Norm: 5655921733162.24, NNZs: 843, Bias: -315412106.322017, T: 7484841, Avg. loss: 111288674812662646659168243548160.000000\n",
      "Total training time: 16.02 seconds.\n",
      "-- Epoch 2224\n",
      "Norm: 5589714061474.12, NNZs: 843, Bias: 258112133.748989, T: 7488208, Avg. loss: 111263872870877880624737626882048.000000\n",
      "Total training time: 16.03 seconds.\n",
      "-- Epoch 2225\n",
      "Norm: 5496915356763.24, NNZs: 843, Bias: 66937378.595674, T: 7491575, Avg. loss: 111239243736071752756783256436736.000000\n",
      "Total training time: 16.04 seconds.\n",
      "-- Epoch 2226\n",
      "Norm: 5543704403059.94, NNZs: 843, Bias: -124194729.625497, T: 7494942, Avg. loss: 111214615778702167822492495773696.000000\n",
      "Total training time: 16.05 seconds.\n",
      "-- Epoch 2227\n",
      "Norm: 5724355588323.43, NNZs: 843, Bias: 66915926.384150, T: 7498309, Avg. loss: 111189582793055897830079779045376.000000\n",
      "Total training time: 16.05 seconds.\n",
      "-- Epoch 2228\n",
      "Norm: 5554395719107.12, NNZs: 843, Bias: -124170958.249324, T: 7501676, Avg. loss: 111164930261849223531386663600128.000000\n",
      "Total training time: 16.06 seconds.\n",
      "-- Epoch 2229\n",
      "Norm: 5640369929586.66, NNZs: 843, Bias: -193822153.282535, T: 7505043, Avg. loss: 111139962725628220622845671112704.000000\n",
      "Total training time: 16.07 seconds.\n",
      "-- Epoch 2230\n",
      "Norm: 5615145205877.93, NNZs: 843, Bias: 199311306.754739, T: 7508410, Avg. loss: 111115119972154438052305382146048.000000\n",
      "Total training time: 16.07 seconds.\n",
      "-- Epoch 2231\n",
      "Norm: 5885300452119.27, NNZs: 843, Bias: 390299176.518973, T: 7511777, Avg. loss: 111090131549269453526891161976832.000000\n",
      "Total training time: 16.08 seconds.\n",
      "-- Epoch 2232\n",
      "Norm: 5483460666340.66, NNZs: 843, Bias: 199259604.151007, T: 7515144, Avg. loss: 111065265901351044767993061965824.000000\n",
      "Total training time: 16.09 seconds.\n",
      "-- Epoch 2233\n",
      "Norm: 5415660768872.74, NNZs: 843, Bias: 8264057.771376, T: 7518511, Avg. loss: 111040205498460631170132559790080.000000\n",
      "Total training time: 16.10 seconds.\n",
      "-- Epoch 2234\n",
      "Norm: 5496490154887.93, NNZs: 843, Bias: -202136322.632785, T: 7521878, Avg. loss: 111015623960806704177746588729344.000000\n",
      "Total training time: 16.10 seconds.\n",
      "-- Epoch 2235\n",
      "Norm: 5286587800351.60, NNZs: 843, Bias: -86749535.771865, T: 7525245, Avg. loss: 110990397459736186396930180382720.000000\n",
      "Total training time: 16.11 seconds.\n",
      "-- Epoch 2236\n",
      "Norm: 5420019039772.83, NNZs: 843, Bias: -277650185.995170, T: 7528612, Avg. loss: 110965432280428490058907789557760.000000\n",
      "Total training time: 16.12 seconds.\n",
      "-- Epoch 2237\n",
      "Norm: 5232939537814.13, NNZs: 843, Bias: -86734416.835330, T: 7531979, Avg. loss: 110940893847730016640915690487808.000000\n",
      "Total training time: 16.12 seconds.\n",
      "-- Epoch 2238\n",
      "Norm: 5376333857966.40, NNZs: 843, Bias: 104135622.380464, T: 7535346, Avg. loss: 110916040288959537895032798314496.000000\n",
      "Total training time: 16.13 seconds.\n",
      "-- Epoch 2239\n",
      "Norm: 5254548550448.08, NNZs: 843, Bias: -86718339.897077, T: 7538713, Avg. loss: 110891356850315529925971301892096.000000\n",
      "Total training time: 16.14 seconds.\n",
      "-- Epoch 2240\n",
      "Norm: 5428985901693.42, NNZs: 843, Bias: -277533366.218667, T: 7542080, Avg. loss: 110866827431612333158665828696064.000000\n",
      "Total training time: 16.15 seconds.\n",
      "-- Epoch 2241\n",
      "Norm: 5328230887971.92, NNZs: 843, Bias: -100931803.660285, T: 7545447, Avg. loss: 110842006623239849695102438998016.000000\n",
      "Total training time: 16.15 seconds.\n",
      "-- Epoch 2242\n",
      "Norm: 5668625908416.59, NNZs: 843, Bias: -291702598.308817, T: 7548814, Avg. loss: 110816813164043988031718478053376.000000\n",
      "Total training time: 16.16 seconds.\n",
      "-- Epoch 2243\n",
      "Norm: 5356522739644.26, NNZs: 843, Bias: -350299639.349412, T: 7552181, Avg. loss: 110792068651686990825128003108864.000000\n",
      "Total training time: 16.17 seconds.\n",
      "-- Epoch 2244\n",
      "Norm: 5464380225044.73, NNZs: 843, Bias: 221945112.259997, T: 7555548, Avg. loss: 110767323987099084826979399106560.000000\n",
      "Total training time: 16.18 seconds.\n",
      "-- Epoch 2245\n",
      "Norm: 5484081133850.06, NNZs: 843, Bias: 31200996.968542, T: 7558915, Avg. loss: 110742635746980524661703694090240.000000\n",
      "Total training time: 16.18 seconds.\n",
      "-- Epoch 2246\n",
      "Norm: 5230950127352.21, NNZs: 843, Bias: -159500443.262786, T: 7562282, Avg. loss: 110718244968274072664032245972992.000000\n",
      "Total training time: 16.19 seconds.\n",
      "-- Epoch 2247\n",
      "Norm: 5508920571796.46, NNZs: 843, Bias: 31186012.177428, T: 7565649, Avg. loss: 110693720488977588211582880448512.000000\n",
      "Total training time: 16.20 seconds.\n",
      "-- Epoch 2248\n",
      "Norm: 5267848032333.68, NNZs: 843, Bias: 260600915.736465, T: 7569016, Avg. loss: 110669040624405844992856221024256.000000\n",
      "Total training time: 16.20 seconds.\n",
      "-- Epoch 2249\n",
      "Norm: 5295920257632.89, NNZs: 843, Bias: -75423164.321610, T: 7572383, Avg. loss: 110644744287019374987731819036672.000000\n",
      "Total training time: 16.21 seconds.\n",
      "-- Epoch 2250\n",
      "Norm: 5601517869532.41, NNZs: 843, Bias: 115191675.509072, T: 7575750, Avg. loss: 110619871349799861405911456153600.000000\n",
      "Total training time: 16.22 seconds.\n",
      "-- Epoch 2251\n",
      "Norm: 5518997362353.42, NNZs: 843, Bias: -144287889.102210, T: 7579117, Avg. loss: 110595424476709543583972031725568.000000\n",
      "Total training time: 16.23 seconds.\n",
      "-- Epoch 2252\n",
      "Norm: 5609783183280.58, NNZs: 843, Bias: 46291787.520285, T: 7582484, Avg. loss: 110570713698586469912751353364480.000000\n",
      "Total training time: 16.23 seconds.\n",
      "-- Epoch 2253\n",
      "Norm: 5689902336508.73, NNZs: 843, Bias: -441586556.417172, T: 7585851, Avg. loss: 110545764903954119451012145086464.000000\n",
      "Total training time: 16.24 seconds.\n",
      "-- Epoch 2254\n",
      "Norm: 5728386104118.70, NNZs: 843, Bias: 130034408.687630, T: 7589218, Avg. loss: 110520891657537595551467127701504.000000\n",
      "Total training time: 16.25 seconds.\n",
      "-- Epoch 2255\n",
      "Norm: 5898796021641.45, NNZs: 843, Bias: -60484957.747096, T: 7592585, Avg. loss: 110496948929993429572001560788992.000000\n",
      "Total training time: 16.25 seconds.\n",
      "-- Epoch 2256\n",
      "Norm: 5462251885256.79, NNZs: 843, Bias: 130002863.565828, T: 7595952, Avg. loss: 110472511315323818229698016575488.000000\n",
      "Total training time: 16.26 seconds.\n",
      "-- Epoch 2257\n",
      "Norm: 5474287840544.30, NNZs: 843, Bias: -245068587.546462, T: 7599319, Avg. loss: 110448291171906833374470519914496.000000\n",
      "Total training time: 16.27 seconds.\n",
      "-- Epoch 2258\n",
      "Norm: 5498950664661.46, NNZs: 843, Bias: -54601856.175768, T: 7602686, Avg. loss: 110424109438702061869634336849920.000000\n",
      "Total training time: 16.28 seconds.\n",
      "-- Epoch 2259\n",
      "Norm: 5637496140644.76, NNZs: 843, Bias: 219369985.079591, T: 7606053, Avg. loss: 110399859160704224400806487523328.000000\n",
      "Total training time: 16.28 seconds.\n",
      "-- Epoch 2260\n",
      "Norm: 5592348114949.05, NNZs: 843, Bias: 246982837.477379, T: 7609420, Avg. loss: 110375504223916249490302926061568.000000\n",
      "Total training time: 16.29 seconds.\n",
      "-- Epoch 2261\n",
      "Norm: 5350571827461.82, NNZs: 843, Bias: 56576126.063204, T: 7612787, Avg. loss: 110351252538611082462605634174976.000000\n",
      "Total training time: 16.30 seconds.\n",
      "-- Epoch 2262\n",
      "Norm: 5535461162135.04, NNZs: 843, Bias: -133788823.717803, T: 7616154, Avg. loss: 110326930928757767813164020269056.000000\n",
      "Total training time: 16.30 seconds.\n",
      "-- Epoch 2263\n",
      "Norm: 5648525546441.13, NNZs: 843, Bias: 56556100.734480, T: 7619521, Avg. loss: 110302617981341248912088169971712.000000\n",
      "Total training time: 16.31 seconds.\n",
      "-- Epoch 2264\n",
      "Norm: 5547322900320.70, NNZs: 843, Bias: -133765686.674812, T: 7622888, Avg. loss: 110278306363184696041949576036352.000000\n",
      "Total training time: 16.32 seconds.\n",
      "-- Epoch 2265\n",
      "Norm: 5666272059140.74, NNZs: 843, Bias: 56537358.420650, T: 7626255, Avg. loss: 110253534119865147059088796942336.000000\n",
      "Total training time: 16.33 seconds.\n",
      "-- Epoch 2266\n",
      "Norm: 5568559659684.13, NNZs: 843, Bias: -133743397.994470, T: 7629622, Avg. loss: 110229313968016414895107200778240.000000\n",
      "Total training time: 16.33 seconds.\n",
      "-- Epoch 2267\n",
      "Norm: 5788826088366.58, NNZs: 843, Bias: 56519907.766958, T: 7632989, Avg. loss: 110205151698401733584805980798976.000000\n",
      "Total training time: 16.34 seconds.\n",
      "-- Epoch 2268\n",
      "Norm: 5463577615626.66, NNZs: 843, Bias: 187036719.670967, T: 7636356, Avg. loss: 110181012376694738301709166051328.000000\n",
      "Total training time: 16.35 seconds.\n",
      "-- Epoch 2269\n",
      "Norm: 5534755762265.21, NNZs: 843, Bias: -58876156.387413, T: 7639723, Avg. loss: 110157012379244337521132549701632.000000\n",
      "Total training time: 16.36 seconds.\n",
      "-- Epoch 2270\n",
      "Norm: 5711742769795.27, NNZs: 843, Bias: -249057959.114425, T: 7643090, Avg. loss: 110132525946994998629086940626944.000000\n",
      "Total training time: 16.36 seconds.\n",
      "-- Epoch 2271\n",
      "Norm: 5917955203865.87, NNZs: 843, Bias: -439197032.801206, T: 7646457, Avg. loss: 110108405336917954466785972453376.000000\n",
      "Total training time: 16.37 seconds.\n",
      "-- Epoch 2272\n",
      "Norm: 5591572165407.74, NNZs: 843, Bias: -238476930.794922, T: 7649824, Avg. loss: 110084241747785782069851504771072.000000\n",
      "Total training time: 16.38 seconds.\n",
      "-- Epoch 2273\n",
      "Norm: 5463285763904.90, NNZs: 843, Bias: -81246425.231494, T: 7653191, Avg. loss: 110059995248141596937040695394304.000000\n",
      "Total training time: 16.38 seconds.\n",
      "-- Epoch 2274\n",
      "Norm: 5813700767731.67, NNZs: 843, Bias: 108865657.255011, T: 7656558, Avg. loss: 110035975610920342582390840885248.000000\n",
      "Total training time: 16.39 seconds.\n",
      "-- Epoch 2275\n",
      "Norm: 5667869161784.77, NNZs: 843, Bias: -81231496.414509, T: 7659925, Avg. loss: 110011852416817083535377563648000.000000\n",
      "Total training time: 16.40 seconds.\n",
      "-- Epoch 2276\n",
      "Norm: 5783668910775.71, NNZs: 843, Bias: 108837324.326198, T: 7663292, Avg. loss: 109987471834915101362321277059072.000000\n",
      "Total training time: 16.41 seconds.\n",
      "-- Epoch 2277\n",
      "Norm: 5612125117331.45, NNZs: 843, Bias: 32199967.305549, T: 7666659, Avg. loss: 109963261709846100017264826777600.000000\n",
      "Total training time: 16.41 seconds.\n",
      "-- Epoch 2278\n",
      "Norm: 6053673435571.33, NNZs: 843, Bias: 222215118.517359, T: 7670026, Avg. loss: 109938797211447023527911068008448.000000\n",
      "Total training time: 16.42 seconds.\n",
      "-- Epoch 2279\n",
      "Norm: 6009536450132.58, NNZs: 843, Bias: 32188453.436905, T: 7673393, Avg. loss: 109914379982526705793947876196352.000000\n",
      "Total training time: 16.43 seconds.\n",
      "-- Epoch 2280\n",
      "Norm: 5587416880090.99, NNZs: 843, Bias: -157794734.635280, T: 7676760, Avg. loss: 109890196964073629803833232195584.000000\n",
      "Total training time: 16.43 seconds.\n",
      "-- Epoch 2281\n",
      "Norm: 5694675144366.98, NNZs: 843, Bias: 412094487.148395, T: 7680127, Avg. loss: 109866309118735532800676269654016.000000\n",
      "Total training time: 16.44 seconds.\n",
      "-- Epoch 2282\n",
      "Norm: 5454837425959.79, NNZs: 843, Bias: -157765208.622795, T: 7683494, Avg. loss: 109842123422547736794525796925440.000000\n",
      "Total training time: 16.45 seconds.\n",
      "-- Epoch 2283\n",
      "Norm: 5696585372571.16, NNZs: 843, Bias: 32165972.142998, T: 7686861, Avg. loss: 109818101579668731082416586752000.000000\n",
      "Total training time: 16.46 seconds.\n",
      "-- Epoch 2284\n",
      "Norm: 5997068758754.76, NNZs: 843, Bias: 222056680.684568, T: 7690228, Avg. loss: 109794037157794287980186634813440.000000\n",
      "Total training time: 16.46 seconds.\n",
      "-- Epoch 2285\n",
      "Norm: 5741750821893.37, NNZs: 843, Bias: -5713314.394533, T: 7693595, Avg. loss: 109769570201807934446915941302272.000000\n",
      "Total training time: 16.47 seconds.\n",
      "-- Epoch 2286\n",
      "Norm: 5656174110268.87, NNZs: 843, Bias: 132358737.438350, T: 7696962, Avg. loss: 109745911849910892698504209104896.000000\n",
      "Total training time: 16.48 seconds.\n",
      "-- Epoch 2287\n",
      "Norm: 5860132874224.71, NNZs: 843, Bias: -437155298.954653, T: 7700329, Avg. loss: 109722187098294997263464666234880.000000\n",
      "Total training time: 16.48 seconds.\n",
      "-- Epoch 2288\n",
      "Norm: 6039760298015.64, NNZs: 843, Bias: 132327817.710513, T: 7703696, Avg. loss: 109698041664361458767459667410944.000000\n",
      "Total training time: 16.49 seconds.\n",
      "-- Epoch 2289\n",
      "Norm: 6036144257036.20, NNZs: 843, Bias: -301948645.658083, T: 7707063, Avg. loss: 109673726793276962685785928105984.000000\n",
      "Total training time: 16.50 seconds.\n",
      "-- Epoch 2290\n",
      "Norm: 5843203315814.90, NNZs: 843, Bias: -491688138.298917, T: 7710430, Avg. loss: 109650080551059455406796833816576.000000\n",
      "Total training time: 16.51 seconds.\n",
      "-- Epoch 2291\n",
      "Norm: 5502720820212.53, NNZs: 843, Bias: 77613416.644795, T: 7713797, Avg. loss: 109625944900134572912226150121472.000000\n",
      "Total training time: 16.51 seconds.\n",
      "-- Epoch 2292\n",
      "Norm: 5426550826020.04, NNZs: 843, Bias: -112125152.698281, T: 7717164, Avg. loss: 109601779743374147995834884554752.000000\n",
      "Total training time: 16.52 seconds.\n",
      "-- Epoch 2293\n",
      "Norm: 5600382657394.07, NNZs: 843, Bias: 77594897.736647, T: 7720531, Avg. loss: 109578517572670942058862314782720.000000\n",
      "Total training time: 16.53 seconds.\n",
      "-- Epoch 2294\n",
      "Norm: 5803096251625.22, NNZs: 843, Bias: -112103093.477629, T: 7723898, Avg. loss: 109554734087022736299446400712704.000000\n",
      "Total training time: 16.53 seconds.\n",
      "-- Epoch 2295\n",
      "Norm: 5939068431007.90, NNZs: 843, Bias: -154033644.966118, T: 7727265, Avg. loss: 109530546942536873746815271305216.000000\n",
      "Total training time: 16.54 seconds.\n",
      "-- Epoch 2296\n",
      "Norm: 6068384963896.95, NNZs: 843, Bias: 414921178.184149, T: 7730632, Avg. loss: 109506649797493518436721563271168.000000\n",
      "Total training time: 16.55 seconds.\n",
      "-- Epoch 2297\n",
      "Norm: 5534711235097.40, NNZs: 843, Bias: -154003907.126560, T: 7733999, Avg. loss: 109483359302704716619499406622720.000000\n",
      "Total training time: 16.56 seconds.\n",
      "-- Epoch 2298\n",
      "Norm: 5750906648190.58, NNZs: 843, Bias: 414828761.545195, T: 7737366, Avg. loss: 109459462806011903528641297907712.000000\n",
      "Total training time: 16.56 seconds.\n",
      "-- Epoch 2299\n",
      "Norm: 5562813703407.28, NNZs: 843, Bias: 321924917.882925, T: 7740733, Avg. loss: 109435525926856030696158609276928.000000\n",
      "Total training time: 16.57 seconds.\n",
      "-- Epoch 2300\n",
      "Norm: 5270195591501.55, NNZs: 843, Bias: -64103778.373909, T: 7744100, Avg. loss: 109411843366881274945878108405760.000000\n",
      "Total training time: 16.58 seconds.\n",
      "-- Epoch 2301\n",
      "Norm: 5477663844106.59, NNZs: 843, Bias: 125443958.188328, T: 7747467, Avg. loss: 109388237547016794575202911518720.000000\n",
      "Total training time: 16.58 seconds.\n",
      "-- Epoch 2302\n",
      "Norm: 5771510977864.86, NNZs: 843, Bias: -443141651.637011, T: 7750834, Avg. loss: 109364494067216994811447751999488.000000\n",
      "Total training time: 16.59 seconds.\n",
      "-- Epoch 2303\n",
      "Norm: 5343560279563.43, NNZs: 843, Bias: 52515654.141214, T: 7754201, Avg. loss: 109340971994062119379749889376256.000000\n",
      "Total training time: 16.60 seconds.\n",
      "-- Epoch 2304\n",
      "Norm: 5870788339367.92, NNZs: 843, Bias: -136976429.514905, T: 7757568, Avg. loss: 109317159444896256080967498727424.000000\n",
      "Total training time: 16.61 seconds.\n",
      "-- Epoch 2305\n",
      "Norm: 5414429960453.45, NNZs: 843, Bias: -326424449.924396, T: 7760935, Avg. loss: 109293321297881159205727472975872.000000\n",
      "Total training time: 16.61 seconds.\n",
      "-- Epoch 2306\n",
      "Norm: 5562402553290.79, NNZs: 843, Bias: -81314009.674385, T: 7764302, Avg. loss: 109269949907461724684596071628800.000000\n",
      "Total training time: 16.62 seconds.\n",
      "-- Epoch 2307\n",
      "Norm: 5145030883366.71, NNZs: 843, Bias: -270727457.901586, T: 7767669, Avg. loss: 109246450760122577667999383158784.000000\n",
      "Total training time: 16.63 seconds.\n",
      "-- Epoch 2308\n",
      "Norm: 5475728176678.40, NNZs: 843, Bias: -460099641.846820, T: 7771036, Avg. loss: 109222822743030249027391503466496.000000\n",
      "Total training time: 16.64 seconds.\n",
      "-- Epoch 2309\n",
      "Norm: 5313369589864.90, NNZs: 843, Bias: -331771874.550673, T: 7774403, Avg. loss: 109198969780261312464320804683776.000000\n",
      "Total training time: 16.64 seconds.\n",
      "-- Epoch 2310\n",
      "Norm: 5426797129076.12, NNZs: 843, Bias: -142379166.197185, T: 7777770, Avg. loss: 109175367540449444803564235915264.000000\n",
      "Total training time: 16.65 seconds.\n",
      "-- Epoch 2311\n",
      "Norm: 5530821742794.75, NNZs: 843, Bias: -331706778.272046, T: 7781137, Avg. loss: 109151952072443169996720744431616.000000\n",
      "Total training time: 16.66 seconds.\n",
      "-- Epoch 2312\n",
      "Norm: 5438122027609.01, NNZs: 843, Bias: -142355727.627281, T: 7784504, Avg. loss: 109128258999326260722122392338432.000000\n",
      "Total training time: 16.66 seconds.\n",
      "-- Epoch 2313\n",
      "Norm: 5627280492173.80, NNZs: 843, Bias: 46954112.366424, T: 7787871, Avg. loss: 109104481164055520443578699481088.000000\n",
      "Total training time: 16.67 seconds.\n",
      "-- Epoch 2314\n",
      "Norm: 5411410928422.94, NNZs: 843, Bias: -186533402.224487, T: 7791238, Avg. loss: 109081231964144597311240498315264.000000\n",
      "Total training time: 16.68 seconds.\n",
      "-- Epoch 2315\n",
      "Norm: 5371916955542.36, NNZs: 843, Bias: 90360654.970998, T: 7794605, Avg. loss: 109057587851112224521883450605568.000000\n",
      "Total training time: 16.69 seconds.\n",
      "-- Epoch 2316\n",
      "Norm: 5680801087334.94, NNZs: 843, Bias: -98886280.669164, T: 7797972, Avg. loss: 109034399089499257473549809483776.000000\n",
      "Total training time: 16.69 seconds.\n",
      "-- Epoch 2317\n",
      "Norm: 5824942784883.00, NNZs: 843, Bias: 468770002.754217, T: 7801339, Avg. loss: 109010165832214286575024711139328.000000\n",
      "Total training time: 16.70 seconds.\n",
      "-- Epoch 2318\n",
      "Norm: 5473045913495.24, NNZs: 843, Bias: -98869927.325735, T: 7804706, Avg. loss: 108986940357951125876188452487168.000000\n",
      "Total training time: 16.71 seconds.\n",
      "-- Epoch 2319\n",
      "Norm: 5539837690065.88, NNZs: 843, Bias: -288037280.898178, T: 7808073, Avg. loss: 108963644900816941373264284876800.000000\n",
      "Total training time: 16.71 seconds.\n",
      "-- Epoch 2320\n",
      "Norm: 5827330461566.26, NNZs: 843, Bias: -8569816.504608, T: 7811440, Avg. loss: 108939660774859735349052424847360.000000\n",
      "Total training time: 16.72 seconds.\n",
      "-- Epoch 2321\n",
      "Norm: 5320260037986.06, NNZs: 843, Bias: 180560991.223518, T: 7814807, Avg. loss: 108916365985307148021379234791424.000000\n",
      "Total training time: 16.73 seconds.\n",
      "-- Epoch 2322\n",
      "Norm: 5545169380594.81, NNZs: 843, Bias: 108361966.948777, T: 7818174, Avg. loss: 108892967725829156484921237176320.000000\n",
      "Total training time: 16.74 seconds.\n",
      "-- Epoch 2323\n",
      "Norm: 5516474277522.74, NNZs: 843, Bias: -80745896.922246, T: 7821541, Avg. loss: 108869514085530576437304967036928.000000\n",
      "Total training time: 16.74 seconds.\n",
      "-- Epoch 2324\n",
      "Norm: 5507089603986.75, NNZs: 843, Bias: 162707392.771513, T: 7824908, Avg. loss: 108846353994406037375722069164032.000000\n",
      "Total training time: 16.75 seconds.\n",
      "-- Epoch 2325\n",
      "Norm: 5456946276403.67, NNZs: 843, Bias: 235919801.586887, T: 7828275, Avg. loss: 108823242506343228382779933720576.000000\n",
      "Total training time: 16.76 seconds.\n",
      "-- Epoch 2326\n",
      "Norm: 5698928055518.20, NNZs: 843, Bias: 156181058.751722, T: 7831642, Avg. loss: 108799462509044457015237643599872.000000\n",
      "Total training time: 16.77 seconds.\n",
      "-- Epoch 2327\n",
      "Norm: 5672466671579.60, NNZs: 843, Bias: 74177998.124986, T: 7835009, Avg. loss: 108776171976581116774420773339136.000000\n",
      "Total training time: 16.77 seconds.\n",
      "-- Epoch 2328\n",
      "Norm: 5441900662659.85, NNZs: 843, Bias: -114823666.915181, T: 7838376, Avg. loss: 108752762096547170405473694253056.000000\n",
      "Total training time: 16.78 seconds.\n",
      "-- Epoch 2329\n",
      "Norm: 5617685877254.78, NNZs: 843, Bias: -303785958.224285, T: 7841743, Avg. loss: 108729508910377106688095683608576.000000\n",
      "Total training time: 16.79 seconds.\n",
      "-- Epoch 2330\n",
      "Norm: 5361277230577.84, NNZs: 843, Bias: -128271674.894059, T: 7845110, Avg. loss: 108705995301417921282506219323392.000000\n",
      "Total training time: 16.79 seconds.\n",
      "-- Epoch 2331\n",
      "Norm: 5364684519585.50, NNZs: 843, Bias: 60671075.553412, T: 7848477, Avg. loss: 108682541514281925342672216653824.000000\n",
      "Total training time: 16.80 seconds.\n",
      "-- Epoch 2332\n",
      "Norm: 5734400511405.74, NNZs: 843, Bias: -506070923.681436, T: 7851844, Avg. loss: 108659078259339149275083749457920.000000\n",
      "Total training time: 16.81 seconds.\n",
      "-- Epoch 2333\n",
      "Norm: 5327328116771.97, NNZs: 843, Bias: 60652411.112260, T: 7855211, Avg. loss: 108635697574728249499602319835136.000000\n",
      "Total training time: 16.82 seconds.\n",
      "-- Epoch 2334\n",
      "Norm: 5555564690305.72, NNZs: 843, Bias: -128227826.155360, T: 7858578, Avg. loss: 108612566953226979192056961826816.000000\n",
      "Total training time: 16.82 seconds.\n",
      "-- Epoch 2335\n",
      "Norm: 5686453671885.92, NNZs: 843, Bias: -317066857.618639, T: 7861945, Avg. loss: 108589092814956860783010504507392.000000\n",
      "Total training time: 16.83 seconds.\n",
      "-- Epoch 2336\n",
      "Norm: 5932459068329.31, NNZs: 843, Bias: 249454715.094008, T: 7865312, Avg. loss: 108565802091511370742527562874880.000000\n",
      "Total training time: 16.84 seconds.\n",
      "-- Epoch 2337\n",
      "Norm: 5493093714816.79, NNZs: 843, Bias: -99429882.673319, T: 7868679, Avg. loss: 108542420263871339114500565499904.000000\n",
      "Total training time: 16.84 seconds.\n",
      "-- Epoch 2338\n",
      "Norm: 5480118081734.60, NNZs: 843, Bias: 89368766.758291, T: 7872046, Avg. loss: 108519330232468833165524554743808.000000\n",
      "Total training time: 16.85 seconds.\n",
      "-- Epoch 2339\n",
      "Norm: 5585776017308.31, NNZs: 843, Bias: -476951542.657509, T: 7875413, Avg. loss: 108495876032778215459813014372352.000000\n",
      "Total training time: 16.86 seconds.\n",
      "-- Epoch 2340\n",
      "Norm: 5291579075506.36, NNZs: 843, Bias: 89346156.864779, T: 7878780, Avg. loss: 108472592962666219262877412360192.000000\n",
      "Total training time: 16.87 seconds.\n",
      "-- Epoch 2341\n",
      "Norm: 5197219668915.92, NNZs: 843, Bias: -99394648.406923, T: 7882147, Avg. loss: 108449264753251138516625666342912.000000\n",
      "Total training time: 16.87 seconds.\n",
      "-- Epoch 2342\n",
      "Norm: 5406272976612.37, NNZs: 843, Bias: 112463970.997905, T: 7885514, Avg. loss: 108426251245635957807496109752320.000000\n",
      "Total training time: 16.88 seconds.\n",
      "-- Epoch 2343\n",
      "Norm: 5314441969744.30, NNZs: 843, Bias: -76239117.610275, T: 7888881, Avg. loss: 108403100804121292056812297650176.000000\n",
      "Total training time: 16.89 seconds.\n",
      "-- Epoch 2344\n",
      "Norm: 5458926179180.08, NNZs: 843, Bias: -264901996.381634, T: 7892248, Avg. loss: 108380075348922674696768832143360.000000\n",
      "Total training time: 16.89 seconds.\n",
      "-- Epoch 2345\n",
      "Norm: 5340461170108.28, NNZs: 843, Bias: -76227708.595745, T: 7895615, Avg. loss: 108356679089989358181702223003648.000000\n",
      "Total training time: 16.90 seconds.\n",
      "-- Epoch 2346\n",
      "Norm: 5603574629463.70, NNZs: 843, Bias: -264850179.002181, T: 7898982, Avg. loss: 108333469313567400050460979101696.000000\n",
      "Total training time: 16.91 seconds.\n",
      "-- Epoch 2347\n",
      "Norm: 5253725058362.17, NNZs: 843, Bias: -76218066.514588, T: 7902349, Avg. loss: 108310364148452813438437115822080.000000\n",
      "Total training time: 16.92 seconds.\n",
      "-- Epoch 2348\n",
      "Norm: 5803794131742.68, NNZs: 843, Bias: 112376744.700036, T: 7905716, Avg. loss: 108287395195759069979378005835776.000000\n",
      "Total training time: 16.92 seconds.\n",
      "-- Epoch 2349\n",
      "Norm: 5672894969991.65, NNZs: 843, Bias: -76204714.089934, T: 7909083, Avg. loss: 108264205325254706613929893691392.000000\n",
      "Total training time: 16.93 seconds.\n",
      "-- Epoch 2350\n",
      "Norm: 5387485908854.86, NNZs: 843, Bias: -295508755.179657, T: 7912450, Avg. loss: 108241197623898463991981682982912.000000\n",
      "Total training time: 16.94 seconds.\n",
      "-- Epoch 2351\n",
      "Norm: 5363258053936.92, NNZs: 843, Bias: -106950489.055032, T: 7915817, Avg. loss: 108218245078852189518083766878208.000000\n",
      "Total training time: 16.94 seconds.\n",
      "-- Epoch 2352\n",
      "Norm: 5607927133046.42, NNZs: 843, Bias: -295451374.108332, T: 7919184, Avg. loss: 108195371785855826103373771309056.000000\n",
      "Total training time: 16.95 seconds.\n",
      "-- Epoch 2353\n",
      "Norm: 5437009854039.36, NNZs: 843, Bias: -106935463.026668, T: 7922551, Avg. loss: 108172481192938767488920351408128.000000\n",
      "Total training time: 16.96 seconds.\n",
      "-- Epoch 2354\n",
      "Norm: 5540732849648.00, NNZs: 843, Bias: 81539826.385666, T: 7925918, Avg. loss: 108149550155250249409862461030400.000000\n",
      "Total training time: 16.97 seconds.\n",
      "-- Epoch 2355\n",
      "Norm: 5799749484607.29, NNZs: 843, Bias: -141224624.033784, T: 7929285, Avg. loss: 108126894796574858748581745000448.000000\n",
      "Total training time: 16.97 seconds.\n",
      "-- Epoch 2356\n",
      "Norm: 5696988004996.48, NNZs: 843, Bias: -162710443.320054, T: 7932652, Avg. loss: 108103927649742816524655501770752.000000\n",
      "Total training time: 16.98 seconds.\n",
      "-- Epoch 2357\n",
      "Norm: 5654068779133.28, NNZs: 843, Bias: -351104361.290375, T: 7936019, Avg. loss: 108080440706037425504474305134592.000000\n",
      "Total training time: 16.99 seconds.\n",
      "-- Epoch 2358\n",
      "Norm: 5538737819818.88, NNZs: 843, Bias: 214090925.274460, T: 7939386, Avg. loss: 108057593773423139268371566034944.000000\n",
      "Total training time: 17.00 seconds.\n",
      "-- Epoch 2359\n",
      "Norm: 5987432429282.86, NNZs: 843, Bias: -351041878.773832, T: 7942753, Avg. loss: 108034584491505152673888438583296.000000\n",
      "Total training time: 17.00 seconds.\n",
      "-- Epoch 2360\n",
      "Norm: 5724833076168.75, NNZs: 843, Bias: -271425231.741031, T: 7946120, Avg. loss: 108011558837535968404995422814208.000000\n",
      "Total training time: 17.01 seconds.\n",
      "-- Epoch 2361\n",
      "Norm: 5580062288514.96, NNZs: 843, Bias: -83071596.763328, T: 7949487, Avg. loss: 107988983270137010611100160360448.000000\n",
      "Total training time: 17.02 seconds.\n",
      "-- Epoch 2362\n",
      "Norm: 5321681671222.27, NNZs: 843, Bias: -85835619.489242, T: 7952854, Avg. loss: 107966552062501294182436966498304.000000\n",
      "Total training time: 17.02 seconds.\n",
      "-- Epoch 2363\n",
      "Norm: 5388711013030.55, NNZs: 843, Bias: 102458242.350927, T: 7956221, Avg. loss: 107943862471033431187105746255872.000000\n",
      "Total training time: 17.03 seconds.\n",
      "-- Epoch 2364\n",
      "Norm: 5618229975767.73, NNZs: 843, Bias: -462362771.713056, T: 7959588, Avg. loss: 107921000262764430686741319385088.000000\n",
      "Total training time: 17.04 seconds.\n",
      "-- Epoch 2365\n",
      "Norm: 5505400160819.34, NNZs: 843, Bias: -274069789.638959, T: 7962955, Avg. loss: 107898133026505435216934456000512.000000\n",
      "Total training time: 17.05 seconds.\n",
      "-- Epoch 2366\n",
      "Norm: 5316981882294.24, NNZs: 843, Bias: -85814439.032761, T: 7966322, Avg. loss: 107875118927051600988754663178240.000000\n",
      "Total training time: 17.05 seconds.\n",
      "-- Epoch 2367\n",
      "Norm: 5885425656327.34, NNZs: 843, Bias: 102398431.398271, T: 7969689, Avg. loss: 107852045008561819075401989750784.000000\n",
      "Total training time: 17.06 seconds.\n",
      "-- Epoch 2368\n",
      "Norm: 5306777312144.89, NNZs: 843, Bias: 81094947.826913, T: 7973056, Avg. loss: 107829163247238703768389187272704.000000\n",
      "Total training time: 17.07 seconds.\n",
      "-- Epoch 2369\n",
      "Norm: 5508167063383.87, NNZs: 843, Bias: -117565721.526650, T: 7976423, Avg. loss: 107806406470198081476273857626112.000000\n",
      "Total training time: 17.07 seconds.\n",
      "-- Epoch 2370\n",
      "Norm: 5357079464364.23, NNZs: 843, Bias: -61904589.914213, T: 7979790, Avg. loss: 107783189015244485183189083488256.000000\n",
      "Total training time: 17.08 seconds.\n",
      "-- Epoch 2371\n",
      "Norm: 5457945562093.67, NNZs: 843, Bias: 126226567.228669, T: 7983157, Avg. loss: 107761052037901686963730665963520.000000\n",
      "Total training time: 17.09 seconds.\n",
      "-- Epoch 2372\n",
      "Norm: 5369625592607.11, NNZs: 843, Bias: 90000116.208947, T: 7986524, Avg. loss: 107737933837742610181723908997120.000000\n",
      "Total training time: 17.10 seconds.\n",
      "-- Epoch 2373\n",
      "Norm: 5461076730606.45, NNZs: 843, Bias: 278074947.789962, T: 7989891, Avg. loss: 107715236180232878685689382699008.000000\n",
      "Total training time: 17.10 seconds.\n",
      "-- Epoch 2374\n",
      "Norm: 5665401751192.04, NNZs: 843, Bias: 466111760.762769, T: 7993258, Avg. loss: 107692591330237547095521868382208.000000\n",
      "Total training time: 17.11 seconds.\n",
      "-- Epoch 2375\n",
      "Norm: 5470784728531.74, NNZs: 843, Bias: -98091097.066741, T: 7996625, Avg. loss: 107670079011017561007033470156800.000000\n",
      "Total training time: 17.12 seconds.\n",
      "-- Epoch 2376\n",
      "Norm: 5355065170989.76, NNZs: 843, Bias: -72513110.140669, T: 7999992, Avg. loss: 107647957645936439536851783516160.000000\n",
      "Total training time: 17.12 seconds.\n",
      "-- Epoch 2377\n",
      "Norm: 5502890383761.39, NNZs: 843, Bias: -260518345.523749, T: 8003359, Avg. loss: 107625502817722246226014980014080.000000\n",
      "Total training time: 17.13 seconds.\n",
      "-- Epoch 2378\n",
      "Norm: 5767927649549.27, NNZs: 843, Bias: 44774259.360434, T: 8006726, Avg. loss: 107602740373911668298948146102272.000000\n",
      "Total training time: 17.14 seconds.\n",
      "-- Epoch 2379\n",
      "Norm: 5691662428336.25, NNZs: 843, Bias: -143205737.951715, T: 8010093, Avg. loss: 107579980497189231029256166834176.000000\n",
      "Total training time: 17.15 seconds.\n",
      "-- Epoch 2380\n",
      "Norm: 5873690476848.41, NNZs: 843, Bias: -331144210.790228, T: 8013460, Avg. loss: 107556876533463329767486763565056.000000\n",
      "Total training time: 17.15 seconds.\n",
      "-- Epoch 2381\n",
      "Norm: 5662865243429.16, NNZs: 843, Bias: 232678948.366283, T: 8016827, Avg. loss: 107534117341543822302546310987776.000000\n",
      "Total training time: 17.16 seconds.\n",
      "-- Epoch 2382\n",
      "Norm: 5621669036212.56, NNZs: 843, Bias: -331083748.487863, T: 8020194, Avg. loss: 107511215373794986745094167592960.000000\n",
      "Total training time: 17.17 seconds.\n",
      "-- Epoch 2383\n",
      "Norm: 5553577418007.00, NNZs: 843, Bias: -143162848.706721, T: 8023561, Avg. loss: 107488690928781316504473854541824.000000\n",
      "Total training time: 17.18 seconds.\n",
      "-- Epoch 2384\n",
      "Norm: 5625594147569.75, NNZs: 843, Bias: 44721648.988566, T: 8026928, Avg. loss: 107465885790657642338312154775552.000000\n",
      "Total training time: 17.18 seconds.\n",
      "-- Epoch 2385\n",
      "Norm: 5860560224057.29, NNZs: 843, Bias: -143139460.835579, T: 8030295, Avg. loss: 107443393873510030534786357919744.000000\n",
      "Total training time: 17.19 seconds.\n",
      "-- Epoch 2386\n",
      "Norm: 5818041666224.02, NNZs: 843, Bias: 44703139.630139, T: 8033662, Avg. loss: 107420635180842049750389365932032.000000\n",
      "Total training time: 17.20 seconds.\n",
      "-- Epoch 2387\n",
      "Norm: 5707285618214.51, NNZs: 843, Bias: -143120422.773813, T: 8037029, Avg. loss: 107398263380976373281592837144576.000000\n",
      "Total training time: 17.20 seconds.\n",
      "-- Epoch 2388\n",
      "Norm: 6099998844742.80, NNZs: 843, Bias: 420270377.989780, T: 8040396, Avg. loss: 107375446509961729395182863908864.000000\n",
      "Total training time: 17.21 seconds.\n",
      "-- Epoch 2389\n",
      "Norm: 5699953825623.82, NNZs: 843, Bias: -143100330.557330, T: 8043763, Avg. loss: 107352736364987378183803810873344.000000\n",
      "Total training time: 17.22 seconds.\n",
      "-- Epoch 2390\n",
      "Norm: 5832595472179.44, NNZs: 843, Bias: 44663328.940236, T: 8047130, Avg. loss: 107330021687380115941753419202560.000000\n",
      "Total training time: 17.23 seconds.\n",
      "-- Epoch 2391\n",
      "Norm: 5858596668587.70, NNZs: 843, Bias: -235552152.532142, T: 8050497, Avg. loss: 107307076220911807956488409841664.000000\n",
      "Total training time: 17.23 seconds.\n",
      "-- Epoch 2392\n",
      "Norm: 5701623123417.57, NNZs: 843, Bias: -47815650.272727, T: 8053864, Avg. loss: 107284529979697533634392449089536.000000\n",
      "Total training time: 17.24 seconds.\n",
      "-- Epoch 2393\n",
      "Norm: 5856102720110.75, NNZs: 843, Bias: -235511334.792559, T: 8057231, Avg. loss: 107262183962749239611846257803264.000000\n",
      "Total training time: 17.25 seconds.\n",
      "-- Epoch 2394\n",
      "Norm: 5636781184422.92, NNZs: 843, Bias: -47815365.078934, T: 8060598, Avg. loss: 107240063020564734034241677950976.000000\n",
      "Total training time: 17.25 seconds.\n",
      "-- Epoch 2395\n",
      "Norm: 5575029205462.59, NNZs: 843, Bias: 51448859.331203, T: 8063965, Avg. loss: 107217846177039562289143126949888.000000\n",
      "Total training time: 17.26 seconds.\n",
      "-- Epoch 2396\n",
      "Norm: 5761595002910.61, NNZs: 843, Bias: -264894136.360229, T: 8067332, Avg. loss: 107195413004413468601312757153792.000000\n",
      "Total training time: 17.27 seconds.\n",
      "-- Epoch 2397\n",
      "Norm: 5661044132259.47, NNZs: 843, Bias: 297979224.549570, T: 8070699, Avg. loss: 107172567242070300133343564398592.000000\n",
      "Total training time: 17.28 seconds.\n",
      "-- Epoch 2398\n",
      "Norm: 5592740379940.78, NNZs: 843, Bias: 110348560.001887, T: 8074066, Avg. loss: 107150375701455567923247889514496.000000\n",
      "Total training time: 17.28 seconds.\n",
      "-- Epoch 2399\n",
      "Norm: 6046354438717.54, NNZs: 843, Bias: -452401624.299614, T: 8077433, Avg. loss: 107128108622181629545839706767360.000000\n",
      "Total training time: 17.29 seconds.\n",
      "-- Epoch 2400\n",
      "Norm: 5675127962309.55, NNZs: 843, Bias: 110316507.060802, T: 8080800, Avg. loss: 107105504848303968454456933089280.000000\n",
      "Total training time: 17.30 seconds.\n",
      "-- Epoch 2401\n",
      "Norm: 5878909454840.44, NNZs: 843, Bias: 294511544.874702, T: 8084167, Avg. loss: 107083620483513891063467366940672.000000\n",
      "Total training time: 17.30 seconds.\n",
      "-- Epoch 2402\n",
      "Norm: 5696197443954.04, NNZs: 843, Bias: -268081539.562460, T: 8087534, Avg. loss: 107060987407149379653117784621056.000000\n",
      "Total training time: 17.31 seconds.\n",
      "-- Epoch 2403\n",
      "Norm: 6058219385801.88, NNZs: 843, Bias: -80557964.048372, T: 8090901, Avg. loss: 107038534225102829656159446630400.000000\n",
      "Total training time: 17.32 seconds.\n",
      "-- Epoch 2404\n",
      "Norm: 6036371675136.75, NNZs: 843, Bias: -268034304.625298, T: 8094268, Avg. loss: 107016789950627932417318495191040.000000\n",
      "Total training time: 17.33 seconds.\n",
      "-- Epoch 2405\n",
      "Norm: 5638578484235.53, NNZs: 843, Bias: -80549029.525933, T: 8097635, Avg. loss: 106994903052893466713487338110976.000000\n",
      "Total training time: 17.33 seconds.\n",
      "-- Epoch 2406\n",
      "Norm: 5723021377216.18, NNZs: 843, Bias: -267986239.310864, T: 8101002, Avg. loss: 106972965475977816673509303123968.000000\n",
      "Total training time: 17.34 seconds.\n",
      "-- Epoch 2407\n",
      "Norm: 5948464955753.00, NNZs: 843, Bias: -17775185.227477, T: 8104369, Avg. loss: 106950976467690403037911980179456.000000\n",
      "Total training time: 17.35 seconds.\n",
      "-- Epoch 2408\n",
      "Norm: 5981841750934.00, NNZs: 843, Bias: -205181281.118708, T: 8107736, Avg. loss: 106928628589429801339436671172608.000000\n",
      "Total training time: 17.35 seconds.\n",
      "-- Epoch 2409\n",
      "Norm: 5851963939753.30, NNZs: 843, Bias: -17782275.458829, T: 8111103, Avg. loss: 106906603744423316386470699204608.000000\n",
      "Total training time: 17.36 seconds.\n",
      "-- Epoch 2410\n",
      "Norm: 5691385206979.00, NNZs: 843, Bias: 169576563.401321, T: 8114470, Avg. loss: 106884934388679732689606084132864.000000\n",
      "Total training time: 17.37 seconds.\n",
      "-- Epoch 2411\n",
      "Norm: 5720777008160.75, NNZs: 843, Bias: -17789578.435714, T: 8117837, Avg. loss: 106862384149423802278960017965056.000000\n",
      "Total training time: 17.38 seconds.\n",
      "-- Epoch 2412\n",
      "Norm: 5713438700671.69, NNZs: 843, Bias: 169531555.499059, T: 8121204, Avg. loss: 106840343377518930427475442794496.000000\n",
      "Total training time: 17.38 seconds.\n",
      "-- Epoch 2413\n",
      "Norm: 5678814252306.96, NNZs: 843, Bias: -17795696.806068, T: 8124571, Avg. loss: 106818563797543056721283593011200.000000\n",
      "Total training time: 17.39 seconds.\n",
      "-- Epoch 2414\n",
      "Norm: 5929658964490.25, NNZs: 843, Bias: -205082735.907422, T: 8127938, Avg. loss: 106796434026448807257346739798016.000000\n",
      "Total training time: 17.40 seconds.\n",
      "-- Epoch 2415\n",
      "Norm: 5620918250126.61, NNZs: 843, Bias: -17799204.510240, T: 8131305, Avg. loss: 106774631878442633099154668126208.000000\n",
      "Total training time: 17.41 seconds.\n",
      "-- Epoch 2416\n",
      "Norm: 5557862346945.51, NNZs: 843, Bias: -205048645.088677, T: 8134672, Avg. loss: 106752477632028148609094382518272.000000\n",
      "Total training time: 17.41 seconds.\n",
      "-- Epoch 2417\n",
      "Norm: 5825472344480.24, NNZs: 843, Bias: -392260844.293852, T: 8138039, Avg. loss: 106730837535945209193852752101376.000000\n",
      "Total training time: 17.42 seconds.\n",
      "-- Epoch 2418\n",
      "Norm: 5508550118720.58, NNZs: 843, Bias: -104288364.073065, T: 8141406, Avg. loss: 106708782956279698280006119587840.000000\n",
      "Total training time: 17.43 seconds.\n",
      "-- Epoch 2419\n",
      "Norm: 5629016426580.41, NNZs: 843, Bias: 82909197.660760, T: 8144773, Avg. loss: 106686953104885359942428683927552.000000\n",
      "Total training time: 17.43 seconds.\n",
      "-- Epoch 2420\n",
      "Norm: 5749117389433.34, NNZs: 843, Bias: -104272878.890601, T: 8148140, Avg. loss: 106665193394013414654406377340928.000000\n",
      "Total training time: 17.44 seconds.\n",
      "-- Epoch 2421\n",
      "Norm: 5755808250932.43, NNZs: 843, Bias: -291415019.038508, T: 8151507, Avg. loss: 106643188034386011353946131529728.000000\n",
      "Total training time: 17.45 seconds.\n",
      "-- Epoch 2422\n",
      "Norm: 5621727788788.39, NNZs: 843, Bias: -178677491.873717, T: 8154874, Avg. loss: 106621191285156490251570662866944.000000\n",
      "Total training time: 17.46 seconds.\n",
      "-- Epoch 2423\n",
      "Norm: 5861247397457.36, NNZs: 843, Bias: 141855815.346270, T: 8158241, Avg. loss: 106599227022817476736621989593088.000000\n",
      "Total training time: 17.46 seconds.\n",
      "-- Epoch 2424\n",
      "Norm: 5779829740433.76, NNZs: 843, Bias: -45255346.845153, T: 8161608, Avg. loss: 106577282444977753243699128565760.000000\n",
      "Total training time: 17.47 seconds.\n",
      "-- Epoch 2425\n",
      "Norm: 5910838786257.56, NNZs: 843, Bias: 141818513.185662, T: 8164975, Avg. loss: 106555020457808747484439654694912.000000\n",
      "Total training time: 17.48 seconds.\n",
      "-- Epoch 2426\n",
      "Norm: 5685372766668.79, NNZs: 843, Bias: -45252225.611583, T: 8168342, Avg. loss: 106533078674578423348498293325824.000000\n",
      "Total training time: 17.48 seconds.\n",
      "-- Epoch 2427\n",
      "Norm: 5695328612651.28, NNZs: 843, Bias: -232287292.398117, T: 8171709, Avg. loss: 106511020801338775621397653749760.000000\n",
      "Total training time: 17.49 seconds.\n",
      "-- Epoch 2428\n",
      "Norm: 5710786284971.07, NNZs: 843, Bias: 32190017.239674, T: 8175076, Avg. loss: 106489114370795118429445654839296.000000\n",
      "Total training time: 17.50 seconds.\n",
      "-- Epoch 2429\n",
      "Norm: 5819737550677.86, NNZs: 843, Bias: 219179058.518694, T: 8178443, Avg. loss: 106467326855441735348932948525056.000000\n",
      "Total training time: 17.51 seconds.\n",
      "-- Epoch 2430\n",
      "Norm: 6204214449880.54, NNZs: 843, Bias: 18707245.584810, T: 8181810, Avg. loss: 106445878561724191747619965370368.000000\n",
      "Total training time: 17.51 seconds.\n",
      "-- Epoch 2431\n",
      "Norm: 5809914532688.55, NNZs: 843, Bias: -169091644.904721, T: 8185177, Avg. loss: 106424202464344041355466019700736.000000\n",
      "Total training time: 17.52 seconds.\n",
      "-- Epoch 2432\n",
      "Norm: 5959765585113.02, NNZs: 843, Bias: -356016837.394450, T: 8188544, Avg. loss: 106402083613142255924375422238720.000000\n",
      "Total training time: 17.53 seconds.\n",
      "-- Epoch 2433\n",
      "Norm: 6007944347202.09, NNZs: 843, Bias: -169065237.229547, T: 8191911, Avg. loss: 106380333517738431200953490210816.000000\n",
      "Total training time: 17.53 seconds.\n",
      "-- Epoch 2434\n",
      "Norm: 5883290219122.68, NNZs: 843, Bias: 17849305.560386, T: 8195278, Avg. loss: 106357978402759276748056052105216.000000\n",
      "Total training time: 17.54 seconds.\n",
      "-- Epoch 2435\n",
      "Norm: 5981749207337.68, NNZs: 843, Bias: -67626213.673411, T: 8198645, Avg. loss: 106335698792962595251548447571968.000000\n",
      "Total training time: 17.55 seconds.\n",
      "-- Epoch 2436\n",
      "Norm: 6048251294244.46, NNZs: 843, Bias: -101054257.987688, T: 8202012, Avg. loss: 106313495151485854224864790970368.000000\n",
      "Total training time: 17.56 seconds.\n",
      "-- Epoch 2437\n",
      "Norm: 6219279410014.31, NNZs: 843, Bias: -349482597.627606, T: 8205379, Avg. loss: 106291735557328404246241841512448.000000\n",
      "Total training time: 17.56 seconds.\n",
      "-- Epoch 2438\n",
      "Norm: 6109216426219.36, NNZs: 843, Bias: -271278694.677491, T: 8208746, Avg. loss: 106270214184252677280537325338624.000000\n",
      "Total training time: 17.57 seconds.\n",
      "-- Epoch 2439\n",
      "Norm: 6063892291796.69, NNZs: 843, Bias: -391895582.302702, T: 8212113, Avg. loss: 106248232262596296771259187855360.000000\n",
      "Total training time: 17.58 seconds.\n",
      "-- Epoch 2440\n",
      "Norm: 5951203170450.65, NNZs: 843, Bias: -67062469.905215, T: 8215480, Avg. loss: 106226338474754699139848349941760.000000\n",
      "Total training time: 17.58 seconds.\n",
      "-- Epoch 2441\n",
      "Norm: 6281759565893.53, NNZs: 843, Bias: -253824049.258444, T: 8218847, Avg. loss: 106204918819504990174681883475968.000000\n",
      "Total training time: 17.59 seconds.\n",
      "-- Epoch 2442\n",
      "Norm: 5994096049652.33, NNZs: 843, Bias: 306438459.375562, T: 8222214, Avg. loss: 106183444545881157259653799739392.000000\n",
      "Total training time: 17.60 seconds.\n",
      "-- Epoch 2443\n",
      "Norm: 5790326859451.08, NNZs: 843, Bias: 119676079.399952, T: 8225581, Avg. loss: 106161653881311551447259718615040.000000\n",
      "Total training time: 17.61 seconds.\n",
      "-- Epoch 2444\n",
      "Norm: 5874427564711.12, NNZs: 843, Bias: -113584280.993866, T: 8228948, Avg. loss: 106139684393050978650199672487936.000000\n",
      "Total training time: 17.61 seconds.\n",
      "-- Epoch 2445\n",
      "Norm: 6260452349684.75, NNZs: 843, Bias: 73114585.042260, T: 8232315, Avg. loss: 106118037235018047117720366874624.000000\n",
      "Total training time: 17.62 seconds.\n",
      "-- Epoch 2446\n",
      "Norm: 5823117807059.05, NNZs: 843, Bias: -113566359.687520, T: 8235682, Avg. loss: 106095730499412882079870340825088.000000\n",
      "Total training time: 17.63 seconds.\n",
      "-- Epoch 2447\n",
      "Norm: 5734650564207.22, NNZs: 843, Bias: 73093964.380412, T: 8239049, Avg. loss: 106074551870741171123019677958144.000000\n",
      "Total training time: 17.64 seconds.\n",
      "-- Epoch 2448\n",
      "Norm: 5726518288665.83, NNZs: 843, Bias: -79596028.347727, T: 8242416, Avg. loss: 106053186339719069748780920209408.000000\n",
      "Total training time: 17.64 seconds.\n",
      "-- Epoch 2449\n",
      "Norm: 5726746777326.26, NNZs: 843, Bias: 107021583.425406, T: 8245783, Avg. loss: 106031587131463686717117927784448.000000\n",
      "Total training time: 17.65 seconds.\n",
      "-- Epoch 2450\n",
      "Norm: 5806019657531.42, NNZs: 843, Bias: 82129343.450364, T: 8249150, Avg. loss: 106009570163383608467538732646400.000000\n",
      "Total training time: 17.66 seconds.\n",
      "-- Epoch 2451\n",
      "Norm: 6156978122362.76, NNZs: 843, Bias: -116737115.668946, T: 8252517, Avg. loss: 105988077402585789109745011916800.000000\n",
      "Total training time: 17.66 seconds.\n",
      "-- Epoch 2452\n",
      "Norm: 5793377998904.46, NNZs: 843, Bias: 69826004.998790, T: 8255884, Avg. loss: 105966676695375125815198201413632.000000\n",
      "Total training time: 17.67 seconds.\n",
      "-- Epoch 2453\n",
      "Norm: 6074991601918.01, NNZs: 843, Bias: -116721879.928864, T: 8259251, Avg. loss: 105945255220055110992124086059008.000000\n",
      "Total training time: 17.68 seconds.\n",
      "-- Epoch 2454\n",
      "Norm: 5803250765227.84, NNZs: 843, Bias: 213415867.521064, T: 8262618, Avg. loss: 105923849361081051391591985446912.000000\n",
      "Total training time: 17.69 seconds.\n",
      "-- Epoch 2455\n",
      "Norm: 5644672448199.56, NNZs: 843, Bias: 26891045.414359, T: 8265985, Avg. loss: 105902541416692015554768796647424.000000\n",
      "Total training time: 17.69 seconds.\n",
      "-- Epoch 2456\n",
      "Norm: 5633351093462.22, NNZs: 843, Bias: -63201013.846909, T: 8269352, Avg. loss: 105881276800445787998534949142528.000000\n",
      "Total training time: 17.70 seconds.\n",
      "-- Epoch 2457\n",
      "Norm: 5655617136204.61, NNZs: 843, Bias: -249660588.366335, T: 8272719, Avg. loss: 105859751406190315181934458699776.000000\n",
      "Total training time: 17.71 seconds.\n",
      "-- Epoch 2458\n",
      "Norm: 5566998390164.16, NNZs: 843, Bias: -106266788.339903, T: 8276086, Avg. loss: 105837729870585111327878703742976.000000\n",
      "Total training time: 17.71 seconds.\n",
      "-- Epoch 2459\n",
      "Norm: 5516450714932.12, NNZs: 843, Bias: 80164220.304205, T: 8279453, Avg. loss: 105816256731798452615201802944512.000000\n",
      "Total training time: 17.72 seconds.\n",
      "-- Epoch 2460\n",
      "Norm: 5664187193554.32, NNZs: 843, Bias: 244431138.028648, T: 8282820, Avg. loss: 105794757657243627212005160517632.000000\n",
      "Total training time: 17.73 seconds.\n",
      "-- Epoch 2461\n",
      "Norm: 5533442976887.06, NNZs: 843, Bias: 58017786.938298, T: 8286187, Avg. loss: 105773563009385387490190991294464.000000\n",
      "Total training time: 17.74 seconds.\n",
      "-- Epoch 2462\n",
      "Norm: 5616099623116.24, NNZs: 843, Bias: -128359518.948961, T: 8289554, Avg. loss: 105752159247843617033733141430272.000000\n",
      "Total training time: 17.74 seconds.\n",
      "-- Epoch 2463\n",
      "Norm: 5491361610154.26, NNZs: 843, Bias: 44995759.155558, T: 8292921, Avg. loss: 105731237762989074153249869660160.000000\n",
      "Total training time: 17.75 seconds.\n",
      "-- Epoch 2464\n",
      "Norm: 5448918828195.06, NNZs: 843, Bias: 41863389.877962, T: 8296288, Avg. loss: 105709522164334920903161857179648.000000\n",
      "Total training time: 17.76 seconds.\n",
      "-- Epoch 2465\n",
      "Norm: 5515962510361.00, NNZs: 843, Bias: -144455900.899010, T: 8299655, Avg. loss: 105688009775924039424554722918400.000000\n",
      "Total training time: 17.76 seconds.\n",
      "-- Epoch 2466\n",
      "Norm: 5521763452460.03, NNZs: 843, Bias: 41844292.355845, T: 8303022, Avg. loss: 105666512758962498732126338285568.000000\n",
      "Total training time: 17.77 seconds.\n",
      "-- Epoch 2467\n",
      "Norm: 5498063990626.08, NNZs: 843, Bias: -144434928.891774, T: 8306389, Avg. loss: 105645005870918808847097140346880.000000\n",
      "Total training time: 17.78 seconds.\n",
      "-- Epoch 2468\n",
      "Norm: 5554746719731.79, NNZs: 843, Bias: 67903214.175620, T: 8309756, Avg. loss: 105623830379022000550727290716160.000000\n",
      "Total training time: 17.79 seconds.\n",
      "-- Epoch 2469\n",
      "Norm: 5832999692750.20, NNZs: 843, Bias: -118340372.074334, T: 8313123, Avg. loss: 105602317776286884461386842243072.000000\n",
      "Total training time: 17.79 seconds.\n",
      "-- Epoch 2470\n",
      "Norm: 5657179964692.62, NNZs: 843, Bias: 67882363.888660, T: 8316490, Avg. loss: 105581040464809061650925105446912.000000\n",
      "Total training time: 17.80 seconds.\n",
      "-- Epoch 2471\n",
      "Norm: 5671746894187.98, NNZs: 843, Bias: -131062608.476051, T: 8319857, Avg. loss: 105559373149934249131821202669568.000000\n",
      "Total training time: 17.81 seconds.\n",
      "-- Epoch 2472\n",
      "Norm: 5556464365235.64, NNZs: 843, Bias: 55122912.995018, T: 8323224, Avg. loss: 105537778641678563694039535714304.000000\n",
      "Total training time: 17.82 seconds.\n",
      "-- Epoch 2473\n",
      "Norm: 5607687757204.10, NNZs: 843, Bias: 241272130.238974, T: 8326591, Avg. loss: 105516721188187330931400225325056.000000\n",
      "Total training time: 17.82 seconds.\n",
      "-- Epoch 2474\n",
      "Norm: 5519016579019.35, NNZs: 843, Bias: 88968518.063528, T: 8329958, Avg. loss: 105495211820418847976520381628416.000000\n",
      "Total training time: 17.83 seconds.\n",
      "-- Epoch 2475\n",
      "Norm: 5603839926337.81, NNZs: 843, Bias: -97165563.498709, T: 8333325, Avg. loss: 105473774528846539224828881141760.000000\n",
      "Total training time: 17.84 seconds.\n",
      "-- Epoch 2476\n",
      "Norm: 6379902878651.40, NNZs: 843, Bias: -283262444.125083, T: 8336692, Avg. loss: 105452175700002599350922320543744.000000\n",
      "Total training time: 17.84 seconds.\n",
      "-- Epoch 2477\n",
      "Norm: 5804406830906.36, NNZs: 843, Bias: 216732422.494336, T: 8340059, Avg. loss: 105430751715715414674322663407616.000000\n",
      "Total training time: 17.85 seconds.\n",
      "-- Epoch 2478\n",
      "Norm: 5748401680691.01, NNZs: 843, Bias: -341488332.906788, T: 8343426, Avg. loss: 105409566429477542634166119563264.000000\n",
      "Total training time: 17.86 seconds.\n",
      "-- Epoch 2479\n",
      "Norm: 5715822841471.09, NNZs: 843, Bias: -155412268.391973, T: 8346793, Avg. loss: 105388197618276721106331610644480.000000\n",
      "Total training time: 17.87 seconds.\n",
      "-- Epoch 2480\n",
      "Norm: 5685338565808.14, NNZs: 843, Bias: 21314583.302834, T: 8350160, Avg. loss: 105366876658984535626623540527104.000000\n",
      "Total training time: 17.87 seconds.\n",
      "-- Epoch 2481\n",
      "Norm: 5809602033354.50, NNZs: 843, Bias: -211396132.862701, T: 8353527, Avg. loss: 105345719177585489030987009490944.000000\n",
      "Total training time: 17.88 seconds.\n",
      "-- Epoch 2482\n",
      "Norm: 5824924838924.40, NNZs: 843, Bias: -25392218.986910, T: 8356894, Avg. loss: 105324475649947947017846665510912.000000\n",
      "Total training time: 17.89 seconds.\n",
      "-- Epoch 2483\n",
      "Norm: 6293880104610.69, NNZs: 843, Bias: -583309236.015676, T: 8360261, Avg. loss: 105303465001807844083090130468864.000000\n",
      "Total training time: 17.89 seconds.\n",
      "-- Epoch 2484\n",
      "Norm: 6209589117138.16, NNZs: 843, Bias: -397304667.800631, T: 8363628, Avg. loss: 105282323904345138693682302550016.000000\n",
      "Total training time: 17.90 seconds.\n",
      "-- Epoch 2485\n",
      "Norm: 5963988266395.27, NNZs: 843, Bias: -211338007.660015, T: 8366995, Avg. loss: 105261518205528886094234378043392.000000\n",
      "Total training time: 17.91 seconds.\n",
      "-- Epoch 2486\n",
      "Norm: 6032774728127.69, NNZs: 843, Bias: -90281443.508048, T: 8370362, Avg. loss: 105240311031237027599785121546240.000000\n",
      "Total training time: 17.92 seconds.\n",
      "-- Epoch 2487\n",
      "Norm: 6434625578854.00, NNZs: 843, Bias: 95618957.215331, T: 8373729, Avg. loss: 105218697852535098265652393148416.000000\n",
      "Total training time: 17.92 seconds.\n",
      "-- Epoch 2488\n",
      "Norm: 5944668601141.30, NNZs: 843, Bias: -205614037.883018, T: 8377096, Avg. loss: 105197782708406250906372670488576.000000\n",
      "Total training time: 17.93 seconds.\n",
      "-- Epoch 2489\n",
      "Norm: 6114314403923.89, NNZs: 843, Bias: -259464122.666140, T: 8380463, Avg. loss: 105176537367057467694773308489728.000000\n",
      "Total training time: 17.94 seconds.\n",
      "-- Epoch 2490\n",
      "Norm: 6036308496524.96, NNZs: 843, Bias: -73602762.777046, T: 8383830, Avg. loss: 105155494137908560503972762222592.000000\n",
      "Total training time: 17.94 seconds.\n",
      "-- Epoch 2491\n",
      "Norm: 6122381882534.55, NNZs: 843, Bias: 112222746.640343, T: 8387197, Avg. loss: 105134509559422101954801872928768.000000\n",
      "Total training time: 17.95 seconds.\n",
      "-- Epoch 2492\n",
      "Norm: 6203190488506.12, NNZs: 843, Bias: 143634602.226046, T: 8390564, Avg. loss: 105113125218681703618833394171904.000000\n",
      "Total training time: 17.96 seconds.\n",
      "-- Epoch 2493\n",
      "Norm: 6273942203994.75, NNZs: 843, Bias: -42169197.592831, T: 8393931, Avg. loss: 105092049796620014721579947655168.000000\n",
      "Total training time: 17.97 seconds.\n",
      "-- Epoch 2494\n",
      "Norm: 6247219432073.92, NNZs: 843, Bias: 143594834.607528, T: 8397298, Avg. loss: 105071158975241967020075342888960.000000\n",
      "Total training time: 17.97 seconds.\n",
      "-- Epoch 2495\n",
      "Norm: 6134037435726.14, NNZs: 843, Bias: -21334049.602227, T: 8400665, Avg. loss: 105049764686525103568970277978112.000000\n",
      "Total training time: 17.98 seconds.\n",
      "-- Epoch 2496\n",
      "Norm: 6108868101122.67, NNZs: 843, Bias: -101298138.376974, T: 8404032, Avg. loss: 105028696977085842846475748376576.000000\n",
      "Total training time: 17.99 seconds.\n",
      "-- Epoch 2497\n",
      "Norm: 6099220457877.83, NNZs: 843, Bias: 84415774.652966, T: 8407399, Avg. loss: 105007609166230375650615749509120.000000\n",
      "Total training time: 18.00 seconds.\n",
      "-- Epoch 2498\n",
      "Norm: 6046511104483.49, NNZs: 843, Bias: -101286813.080814, T: 8410766, Avg. loss: 104986565998618185393844414054400.000000\n",
      "Total training time: 18.00 seconds.\n",
      "-- Epoch 2499\n",
      "Norm: 6119123629444.16, NNZs: 843, Bias: 20471987.685950, T: 8414133, Avg. loss: 104965787074468860672521499836416.000000\n",
      "Total training time: 18.01 seconds.\n",
      "-- Epoch 2500\n",
      "Norm: 6053548975492.82, NNZs: 843, Bias: -165188630.044625, T: 8417500, Avg. loss: 104945111927305983246438768836608.000000\n",
      "Total training time: 18.02 seconds.\n",
      "-- Epoch 2501\n",
      "Norm: 5962572417799.98, NNZs: 843, Bias: 20459542.605230, T: 8420867, Avg. loss: 104923924898933947979909220532224.000000\n",
      "Total training time: 18.02 seconds.\n",
      "-- Epoch 2502\n",
      "Norm: 6111951131848.90, NNZs: 843, Bias: -165164705.483944, T: 8424234, Avg. loss: 104902908108360470181168958930944.000000\n",
      "Total training time: 18.03 seconds.\n",
      "-- Epoch 2503\n",
      "Norm: 6112013284380.79, NNZs: 843, Bias: -350752542.950464, T: 8427601, Avg. loss: 104881894470197510243612908060672.000000\n",
      "Total training time: 18.04 seconds.\n",
      "-- Epoch 2504\n",
      "Norm: 6039338501039.07, NNZs: 843, Bias: -536302763.676827, T: 8430968, Avg. loss: 104860668273211768121112985075712.000000\n",
      "Total training time: 18.05 seconds.\n",
      "-- Epoch 2505\n",
      "Norm: 5981580260247.57, NNZs: 843, Bias: -272496090.271768, T: 8434335, Avg. loss: 104839685063176518294581391917056.000000\n",
      "Total training time: 18.05 seconds.\n",
      "-- Epoch 2506\n",
      "Norm: 5860139892548.70, NNZs: 843, Bias: -86929781.718747, T: 8437702, Avg. loss: 104818690114846753814092837814272.000000\n",
      "Total training time: 18.06 seconds.\n",
      "-- Epoch 2507\n",
      "Norm: 5952712793871.95, NNZs: 843, Bias: -272449422.355401, T: 8441069, Avg. loss: 104797910007391002491182666219520.000000\n",
      "Total training time: 18.07 seconds.\n",
      "-- Epoch 2508\n",
      "Norm: 5856456454310.08, NNZs: 843, Bias: -83376327.682032, T: 8444436, Avg. loss: 104776951070849079916407732305920.000000\n",
      "Total training time: 18.07 seconds.\n",
      "-- Epoch 2509\n",
      "Norm: 5983901891617.81, NNZs: 843, Bias: 102116525.293055, T: 8447803, Avg. loss: 104755777723402879464199000424448.000000\n",
      "Total training time: 18.08 seconds.\n",
      "-- Epoch 2510\n",
      "Norm: 5892755773387.38, NNZs: 843, Bias: -170846857.271114, T: 8451170, Avg. loss: 104734749841096581444347306704896.000000\n",
      "Total training time: 18.09 seconds.\n",
      "-- Epoch 2511\n",
      "Norm: 5998365739851.08, NNZs: 843, Bias: 14615352.270019, T: 8454537, Avg. loss: 104713797961557104522571877974016.000000\n",
      "Total training time: 18.10 seconds.\n",
      "-- Epoch 2512\n",
      "Norm: 6164884801180.15, NNZs: 843, Bias: -170823304.241541, T: 8457904, Avg. loss: 104692777970140551793175146004480.000000\n",
      "Total training time: 18.10 seconds.\n",
      "-- Epoch 2513\n",
      "Norm: 5952471608523.31, NNZs: 843, Bias: 14602805.923717, T: 8461271, Avg. loss: 104671760031025662298777395920896.000000\n",
      "Total training time: 18.11 seconds.\n",
      "-- Epoch 2514\n",
      "Norm: 6194245516697.28, NNZs: 843, Bias: 199990171.325823, T: 8464638, Avg. loss: 104651140507536236294916961992704.000000\n",
      "Total training time: 18.12 seconds.\n",
      "-- Epoch 2515\n",
      "Norm: 6143091618221.42, NNZs: 843, Bias: 14588892.171507, T: 8468005, Avg. loss: 104630590987898933823160461557760.000000\n",
      "Total training time: 18.12 seconds.\n",
      "-- Epoch 2516\n",
      "Norm: 6303725731593.29, NNZs: 843, Bias: 5701877.488150, T: 8471372, Avg. loss: 104610127053529971540149437202432.000000\n",
      "Total training time: 18.13 seconds.\n",
      "-- Epoch 2517\n",
      "Norm: 6259174783798.07, NNZs: 843, Bias: 125513241.554214, T: 8474739, Avg. loss: 104589504332512588305738464493568.000000\n",
      "Total training time: 18.14 seconds.\n",
      "-- Epoch 2518\n",
      "Norm: 6177653060986.69, NNZs: 843, Bias: -59824998.760811, T: 8478106, Avg. loss: 104568838024193602910432903823360.000000\n",
      "Total training time: 18.15 seconds.\n",
      "-- Epoch 2519\n",
      "Norm: 6417261688382.29, NNZs: 843, Bias: -132216350.564736, T: 8481473, Avg. loss: 104548060876051167134441101328384.000000\n",
      "Total training time: 18.15 seconds.\n",
      "-- Epoch 2520\n",
      "Norm: 6532152344475.19, NNZs: 843, Bias: 423646476.117269, T: 8484840, Avg. loss: 104527101024173931330049572601856.000000\n",
      "Total training time: 18.16 seconds.\n",
      "-- Epoch 2521\n",
      "Norm: 6529664745486.53, NNZs: 843, Bias: 200089220.385382, T: 8488207, Avg. loss: 104506608346436625759199035392000.000000\n",
      "Total training time: 18.17 seconds.\n",
      "-- Epoch 2522\n",
      "Norm: 6529366678446.89, NNZs: 843, Bias: 14815025.446588, T: 8491574, Avg. loss: 104486104031420365032326667698176.000000\n",
      "Total training time: 18.17 seconds.\n",
      "-- Epoch 2523\n",
      "Norm: 6949272937640.84, NNZs: 843, Bias: 200038057.115422, T: 8494941, Avg. loss: 104465416375641730255352384978944.000000\n",
      "Total training time: 18.18 seconds.\n",
      "-- Epoch 2524\n",
      "Norm: 6507654667197.82, NNZs: 843, Bias: -134032496.315726, T: 8498308, Avg. loss: 104444492880451326475604076789760.000000\n",
      "Total training time: 18.19 seconds.\n",
      "-- Epoch 2525\n",
      "Norm: 6433080950237.17, NNZs: 843, Bias: 51168559.068108, T: 8501675, Avg. loss: 104423414939182489136884715880448.000000\n",
      "Total training time: 18.20 seconds.\n",
      "-- Epoch 2526\n",
      "Norm: 6321147109205.62, NNZs: 843, Bias: -134016578.389138, T: 8505042, Avg. loss: 104402713095538307869169435017216.000000\n",
      "Total training time: 18.20 seconds.\n",
      "-- Epoch 2527\n",
      "Norm: 6364989648864.06, NNZs: 843, Bias: -279364608.232065, T: 8508409, Avg. loss: 104381920728188987328001589903360.000000\n",
      "Total training time: 18.21 seconds.\n",
      "-- Epoch 2528\n",
      "Norm: 6311395556397.11, NNZs: 843, Bias: -94203009.056027, T: 8511776, Avg. loss: 104360897105753071294494082596864.000000\n",
      "Total training time: 18.22 seconds.\n",
      "-- Epoch 2529\n",
      "Norm: 6241959228292.08, NNZs: 843, Bias: 90920105.381650, T: 8515143, Avg. loss: 104339946175151363300455597735936.000000\n",
      "Total training time: 18.22 seconds.\n",
      "-- Epoch 2530\n",
      "Norm: 6560967258192.08, NNZs: 843, Bias: -464396541.900325, T: 8518510, Avg. loss: 104319339669342961232837695504384.000000\n",
      "Total training time: 18.23 seconds.\n",
      "-- Epoch 2531\n",
      "Norm: 6503922985479.20, NNZs: 843, Bias: 176672522.364646, T: 8521877, Avg. loss: 104298825858093235559045311496192.000000\n",
      "Total training time: 18.24 seconds.\n",
      "-- Epoch 2532\n",
      "Norm: 6188771881853.00, NNZs: 843, Bias: -136830718.140857, T: 8525244, Avg. loss: 104278262622045282045794854109184.000000\n",
      "Total training time: 18.25 seconds.\n",
      "-- Epoch 2533\n",
      "Norm: 6128879707011.72, NNZs: 843, Bias: 237369483.496006, T: 8528611, Avg. loss: 104257693815165862406006677438464.000000\n",
      "Total training time: 18.25 seconds.\n",
      "-- Epoch 2534\n",
      "Norm: 6007176619670.85, NNZs: 843, Bias: 52313653.360676, T: 8531978, Avg. loss: 104236779395482610355429630279680.000000\n",
      "Total training time: 18.26 seconds.\n",
      "-- Epoch 2535\n",
      "Norm: 6130973503224.26, NNZs: 843, Bias: -132704833.908505, T: 8535345, Avg. loss: 104216428168738895033691245903872.000000\n",
      "Total training time: 18.27 seconds.\n",
      "-- Epoch 2536\n",
      "Norm: 5965513034480.54, NNZs: 843, Bias: -149951028.041064, T: 8538712, Avg. loss: 104196041311309540350501614256128.000000\n",
      "Total training time: 18.28 seconds.\n",
      "-- Epoch 2537\n",
      "Norm: 6282714918248.90, NNZs: 843, Bias: 35031789.703494, T: 8542079, Avg. loss: 104175606449918587056104077787136.000000\n",
      "Total training time: 18.28 seconds.\n",
      "-- Epoch 2538\n",
      "Norm: 6139576964972.58, NNZs: 843, Bias: -149931772.020903, T: 8545446, Avg. loss: 104155238416568391653621091008512.000000\n",
      "Total training time: 18.29 seconds.\n",
      "-- Epoch 2539\n",
      "Norm: 6061060738805.79, NNZs: 843, Bias: -334858008.755305, T: 8548813, Avg. loss: 104134950837432384574007722115072.000000\n",
      "Total training time: 18.30 seconds.\n",
      "-- Epoch 2540\n",
      "Norm: 5851938741099.88, NNZs: 843, Bias: 70020000.854827, T: 8552180, Avg. loss: 104114236084678986409178315096064.000000\n",
      "Total training time: 18.30 seconds.\n",
      "-- Epoch 2541\n",
      "Norm: 6220463994027.26, NNZs: 843, Bias: -174971820.799135, T: 8555547, Avg. loss: 104093300822632473325869983596544.000000\n",
      "Total training time: 18.31 seconds.\n",
      "-- Epoch 2542\n",
      "Norm: 6040408592924.40, NNZs: 843, Bias: 363888248.958591, T: 8558914, Avg. loss: 104073191484315482598764019974144.000000\n",
      "Total training time: 18.32 seconds.\n",
      "-- Epoch 2543\n",
      "Norm: 5940655971503.17, NNZs: 843, Bias: -190744267.134936, T: 8562281, Avg. loss: 104052796408146939831503955165184.000000\n",
      "Total training time: 18.33 seconds.\n",
      "-- Epoch 2544\n",
      "Norm: 5784072491342.79, NNZs: 843, Bias: -5885110.380240, T: 8565648, Avg. loss: 104032528212921590942331314372608.000000\n",
      "Total training time: 18.33 seconds.\n",
      "-- Epoch 2545\n",
      "Norm: 5802761857010.55, NNZs: 843, Bias: -190717427.185903, T: 8569015, Avg. loss: 104011867412951508745793910603776.000000\n",
      "Total training time: 18.34 seconds.\n",
      "-- Epoch 2546\n",
      "Norm: 6124807103059.53, NNZs: 843, Bias: -416882539.495823, T: 8572382, Avg. loss: 103991468751706407740298334568448.000000\n",
      "Total training time: 18.35 seconds.\n",
      "-- Epoch 2547\n",
      "Norm: 6092957402498.14, NNZs: 843, Bias: 262574108.399252, T: 8575749, Avg. loss: 103971197627917132609591445553152.000000\n",
      "Total training time: 18.35 seconds.\n",
      "-- Epoch 2548\n",
      "Norm: 6014074222982.65, NNZs: 843, Bias: 77769556.123093, T: 8579116, Avg. loss: 103950956250944873051408646012928.000000\n",
      "Total training time: 18.36 seconds.\n",
      "-- Epoch 2549\n",
      "Norm: 6184896375479.21, NNZs: 843, Bias: -106996174.454564, T: 8582483, Avg. loss: 103930379241615875251741667622912.000000\n",
      "Total training time: 18.37 seconds.\n",
      "-- Epoch 2550\n",
      "Norm: 5906364628772.50, NNZs: 843, Bias: 149326050.255781, T: 8585850, Avg. loss: 103909765752185184664422835027968.000000\n",
      "Total training time: 18.38 seconds.\n",
      "-- Epoch 2551\n",
      "Norm: 6087464527004.73, NNZs: 843, Bias: 334024840.298472, T: 8589217, Avg. loss: 103889176440017768780570910785536.000000\n",
      "Total training time: 18.38 seconds.\n",
      "-- Epoch 2552\n",
      "Norm: 5949894510160.81, NNZs: 843, Bias: 149286753.291585, T: 8592584, Avg. loss: 103869042136348965647936028409856.000000\n",
      "Total training time: 18.39 seconds.\n",
      "-- Epoch 2553\n",
      "Norm: 5924929252283.38, NNZs: 843, Bias: -35415443.027708, T: 8595951, Avg. loss: 103848786099780842010953719480320.000000\n",
      "Total training time: 18.40 seconds.\n",
      "-- Epoch 2554\n",
      "Norm: 6267592870996.94, NNZs: 843, Bias: 87725887.196251, T: 8599318, Avg. loss: 103828399580371301073479293468672.000000\n",
      "Total training time: 18.41 seconds.\n",
      "-- Epoch 2555\n",
      "Norm: 6161645992904.54, NNZs: 843, Bias: 272360728.707787, T: 8602685, Avg. loss: 103807971919809012541446533677056.000000\n",
      "Total training time: 18.41 seconds.\n",
      "-- Epoch 2556\n",
      "Norm: 6393742738069.48, NNZs: 843, Bias: -145300365.462512, T: 8606052, Avg. loss: 103787784595646800100906892263424.000000\n",
      "Total training time: 18.42 seconds.\n",
      "-- Epoch 2557\n",
      "Norm: 6287968092782.60, NNZs: 843, Bias: -135172145.913648, T: 8609419, Avg. loss: 103767551598669220567655297056768.000000\n",
      "Total training time: 18.43 seconds.\n",
      "-- Epoch 2558\n",
      "Norm: 6167150140961.85, NNZs: 843, Bias: -319755168.882470, T: 8612786, Avg. loss: 103747462224838579313941971730432.000000\n",
      "Total training time: 18.43 seconds.\n",
      "-- Epoch 2559\n",
      "Norm: 6103680150906.52, NNZs: 843, Bias: -135154565.576745, T: 8616153, Avg. loss: 103727144609469761030574816886784.000000\n",
      "Total training time: 18.44 seconds.\n",
      "-- Epoch 2560\n",
      "Norm: 6106945385578.66, NNZs: 843, Bias: 46705710.146749, T: 8619520, Avg. loss: 103707309928519782623824620552192.000000\n",
      "Total training time: 18.45 seconds.\n",
      "-- Epoch 2561\n",
      "Norm: 6154518345888.81, NNZs: 843, Bias: 97402833.019008, T: 8622887, Avg. loss: 103686790587801883586680328290304.000000\n",
      "Total training time: 18.46 seconds.\n",
      "-- Epoch 2562\n",
      "Norm: 6166760166485.84, NNZs: 843, Bias: 281910337.650931, T: 8626254, Avg. loss: 103666435940898042075859661094912.000000\n",
      "Total training time: 18.46 seconds.\n",
      "-- Epoch 2563\n",
      "Norm: 6185024072051.54, NNZs: 843, Bias: 66723946.462057, T: 8629621, Avg. loss: 103646148678102600999710499012608.000000\n",
      "Total training time: 18.47 seconds.\n",
      "-- Epoch 2564\n",
      "Norm: 6272772963780.79, NNZs: 843, Bias: -117770202.549958, T: 8632988, Avg. loss: 103625707701609375986343621951488.000000\n",
      "Total training time: 18.48 seconds.\n",
      "-- Epoch 2565\n",
      "Norm: 6347319898708.31, NNZs: 843, Bias: 66703031.087807, T: 8636355, Avg. loss: 103605100735469609261113614008320.000000\n",
      "Total training time: 18.49 seconds.\n",
      "-- Epoch 2566\n",
      "Norm: 6467780917928.83, NNZs: 843, Bias: -95893912.088551, T: 8639722, Avg. loss: 103584741362751707308301580500992.000000\n",
      "Total training time: 18.49 seconds.\n",
      "-- Epoch 2567\n",
      "Norm: 6802573832441.41, NNZs: 843, Bias: -280319245.818237, T: 8643089, Avg. loss: 103564315933321331065986649948160.000000\n",
      "Total training time: 18.50 seconds.\n",
      "-- Epoch 2568\n",
      "Norm: 6509779663864.02, NNZs: 843, Bias: 272941899.291507, T: 8646456, Avg. loss: 103544032885282615630502337445888.000000\n",
      "Total training time: 18.51 seconds.\n",
      "-- Epoch 2569\n",
      "Norm: 6724020347454.01, NNZs: 843, Bias: 88515833.597171, T: 8649823, Avg. loss: 103523994885510893085694349017088.000000\n",
      "Total training time: 18.51 seconds.\n",
      "-- Epoch 2570\n",
      "Norm: 6395034323930.56, NNZs: 843, Bias: -11568672.176236, T: 8653190, Avg. loss: 103504167319357495903635439091712.000000\n",
      "Total training time: 18.52 seconds.\n",
      "-- Epoch 2571\n",
      "Norm: 6607541433864.50, NNZs: 843, Bias: -195932911.581089, T: 8656557, Avg. loss: 103484073833935562854110993580032.000000\n",
      "Total training time: 18.53 seconds.\n",
      "-- Epoch 2572\n",
      "Norm: 6567820059980.31, NNZs: 843, Bias: -11580859.353512, T: 8659924, Avg. loss: 103464165265693751622899163201536.000000\n",
      "Total training time: 18.54 seconds.\n",
      "-- Epoch 2573\n",
      "Norm: 6687705208268.08, NNZs: 843, Bias: -195908586.956359, T: 8663291, Avg. loss: 103443986212475535412584760999936.000000\n",
      "Total training time: 18.54 seconds.\n",
      "-- Epoch 2574\n",
      "Norm: 6541445740716.77, NNZs: 843, Bias: -11590534.623392, T: 8666658, Avg. loss: 103423838702046167257018724777984.000000\n",
      "Total training time: 18.55 seconds.\n",
      "-- Epoch 2575\n",
      "Norm: 6485353679004.20, NNZs: 843, Bias: -195882838.931423, T: 8670025, Avg. loss: 103403852759552161064139282710528.000000\n",
      "Total training time: 18.56 seconds.\n",
      "-- Epoch 2576\n",
      "Norm: 6607985297023.40, NNZs: 843, Bias: -380138892.672851, T: 8673392, Avg. loss: 103383992584233418726919001604096.000000\n",
      "Total training time: 18.57 seconds.\n",
      "-- Epoch 2577\n",
      "Norm: 6718250682557.83, NNZs: 843, Bias: 172645541.680042, T: 8676759, Avg. loss: 103363870700191099791381146632192.000000\n",
      "Total training time: 18.57 seconds.\n",
      "-- Epoch 2578\n",
      "Norm: 6848722638533.79, NNZs: 843, Bias: -164856766.629003, T: 8680126, Avg. loss: 103343682026739612337136704946176.000000\n",
      "Total training time: 18.58 seconds.\n",
      "-- Epoch 2579\n",
      "Norm: 6577504146574.52, NNZs: 843, Bias: 19368665.641452, T: 8683493, Avg. loss: 103323957358307056938680385011712.000000\n",
      "Total training time: 18.59 seconds.\n",
      "-- Epoch 2580\n",
      "Norm: 6974618724223.46, NNZs: 843, Bias: 203560842.224455, T: 8686860, Avg. loss: 103304244588611518330308875255808.000000\n",
      "Total training time: 18.59 seconds.\n",
      "-- Epoch 2581\n",
      "Norm: 6395934207087.88, NNZs: 843, Bias: 19356118.237766, T: 8690227, Avg. loss: 103284398941793052273985674280960.000000\n",
      "Total training time: 18.60 seconds.\n",
      "-- Epoch 2582\n",
      "Norm: 6425433665735.17, NNZs: 843, Bias: 203509993.032675, T: 8693594, Avg. loss: 103264453870317169057103626633216.000000\n",
      "Total training time: 18.61 seconds.\n",
      "-- Epoch 2583\n",
      "Norm: 6697749150251.01, NNZs: 843, Bias: 28279868.108636, T: 8696961, Avg. loss: 103244560637286232008854717071360.000000\n",
      "Total training time: 18.62 seconds.\n",
      "-- Epoch 2584\n",
      "Norm: 6518553890760.10, NNZs: 843, Bias: -155854637.154560, T: 8700328, Avg. loss: 103224583001491578510189055180800.000000\n",
      "Total training time: 18.62 seconds.\n",
      "-- Epoch 2585\n",
      "Norm: 6714124699844.88, NNZs: 843, Bias: 223854840.169063, T: 8703695, Avg. loss: 103204215304706372044194018492416.000000\n",
      "Total training time: 18.63 seconds.\n",
      "-- Epoch 2586\n",
      "Norm: 6531071004389.96, NNZs: 843, Bias: 39736455.564233, T: 8707062, Avg. loss: 103183666663753458530565184552960.000000\n",
      "Total training time: 18.64 seconds.\n",
      "-- Epoch 2587\n",
      "Norm: 6678536759587.07, NNZs: 843, Bias: 223801935.082876, T: 8710429, Avg. loss: 103163780936410779761285181472768.000000\n",
      "Total training time: 18.64 seconds.\n",
      "-- Epoch 2588\n",
      "Norm: 6346106674287.53, NNZs: 843, Bias: -349644420.494942, T: 8713796, Avg. loss: 103143758988622059475848315011072.000000\n",
      "Total training time: 18.65 seconds.\n",
      "-- Epoch 2589\n",
      "Norm: 6226323242804.14, NNZs: 843, Bias: 202497447.467506, T: 8717163, Avg. loss: 103124033573082443962107325054976.000000\n",
      "Total training time: 18.66 seconds.\n",
      "-- Epoch 2590\n",
      "Norm: 6315815554507.93, NNZs: 843, Bias: 18452640.956328, T: 8720530, Avg. loss: 103104047040572553148961071824896.000000\n",
      "Total training time: 18.67 seconds.\n",
      "-- Epoch 2591\n",
      "Norm: 6117959859769.76, NNZs: 843, Bias: 155545614.309908, T: 8723897, Avg. loss: 103084038968830013045515256594432.000000\n",
      "Total training time: 18.67 seconds.\n",
      "-- Epoch 2592\n",
      "Norm: 6067800833065.93, NNZs: 843, Bias: -28458272.198373, T: 8727264, Avg. loss: 103064701403952155680128154730496.000000\n",
      "Total training time: 18.68 seconds.\n",
      "-- Epoch 2593\n",
      "Norm: 6092104838195.79, NNZs: 843, Bias: -8236867.686159, T: 8730631, Avg. loss: 103044945457708322926580962689024.000000\n",
      "Total training time: 18.69 seconds.\n",
      "-- Epoch 2594\n",
      "Norm: 6040751960624.60, NNZs: 843, Bias: -166018335.419105, T: 8733998, Avg. loss: 103025522552167933373673082191872.000000\n",
      "Total training time: 18.69 seconds.\n",
      "-- Epoch 2595\n",
      "Norm: 6377298486675.95, NNZs: 843, Bias: -237385920.301766, T: 8737365, Avg. loss: 103005912871053572985849116622848.000000\n",
      "Total training time: 18.70 seconds.\n",
      "-- Epoch 2596\n",
      "Norm: 6323474307510.48, NNZs: 843, Bias: -53452363.916113, T: 8740732, Avg. loss: 102986054118542451246598446907392.000000\n",
      "Total training time: 18.71 seconds.\n",
      "-- Epoch 2597\n",
      "Norm: 6367126020959.07, NNZs: 843, Bias: -237345915.952008, T: 8744099, Avg. loss: 102966156678893891962652252438528.000000\n",
      "Total training time: 18.72 seconds.\n",
      "-- Epoch 2598\n",
      "Norm: 6107762572423.88, NNZs: 843, Bias: -53448037.076952, T: 8747466, Avg. loss: 102946406113053588506120921546752.000000\n",
      "Total training time: 18.72 seconds.\n",
      "-- Epoch 2599\n",
      "Norm: 6034415663940.53, NNZs: 843, Bias: 130414100.573658, T: 8750833, Avg. loss: 102927034378155241060906215407616.000000\n",
      "Total training time: 18.73 seconds.\n",
      "-- Epoch 2600\n",
      "Norm: 6498674781713.33, NNZs: 843, Bias: -53444932.511725, T: 8754200, Avg. loss: 102907590376893522987007550160896.000000\n",
      "Total training time: 18.74 seconds.\n",
      "-- Epoch 2601\n",
      "Norm: 6017474583379.79, NNZs: 843, Bias: 130381985.069919, T: 8757567, Avg. loss: 102887861461118645002430761140224.000000\n",
      "Total training time: 18.74 seconds.\n",
      "-- Epoch 2602\n",
      "Norm: 5992554315099.20, NNZs: 843, Bias: 26274183.115401, T: 8760934, Avg. loss: 102868128706678016558482306629632.000000\n",
      "Total training time: 18.75 seconds.\n",
      "-- Epoch 2603\n",
      "Norm: 6203967187177.34, NNZs: 843, Bias: 210057514.424614, T: 8764301, Avg. loss: 102848543774723501127990853500928.000000\n",
      "Total training time: 18.76 seconds.\n",
      "-- Epoch 2604\n",
      "Norm: 6193788822334.93, NNZs: 843, Bias: 26260547.591567, T: 8767668, Avg. loss: 102828399698630834183732023263232.000000\n",
      "Total training time: 18.77 seconds.\n",
      "-- Epoch 2605\n",
      "Norm: 6107009722627.55, NNZs: 843, Bias: -279765442.937027, T: 8771035, Avg. loss: 102808721078588202297209521176576.000000\n",
      "Total training time: 18.77 seconds.\n",
      "-- Epoch 2606\n",
      "Norm: 6166551490491.06, NNZs: 843, Bias: -96006806.852981, T: 8774402, Avg. loss: 102789254669533664038002068815872.000000\n",
      "Total training time: 18.78 seconds.\n",
      "-- Epoch 2607\n",
      "Norm: 6521991828655.74, NNZs: 843, Bias: -279720259.797355, T: 8777769, Avg. loss: 102769651682433402762568069545984.000000\n",
      "Total training time: 18.79 seconds.\n",
      "-- Epoch 2608\n",
      "Norm: 5997556378956.18, NNZs: 843, Bias: -95996661.007851, T: 8781136, Avg. loss: 102750162872577432379581788061696.000000\n",
      "Total training time: 18.80 seconds.\n",
      "-- Epoch 2609\n",
      "Norm: 6237322425999.81, NNZs: 843, Bias: 87693374.348747, T: 8784503, Avg. loss: 102730191239388446557496221368320.000000\n",
      "Total training time: 18.80 seconds.\n",
      "-- Epoch 2610\n",
      "Norm: 6013816226382.27, NNZs: 843, Bias: -138477228.882141, T: 8787870, Avg. loss: 102710647769156611163887960588288.000000\n",
      "Total training time: 18.81 seconds.\n",
      "-- Epoch 2611\n",
      "Norm: 5985709794933.04, NNZs: 843, Bias: -263443164.946192, T: 8791237, Avg. loss: 102691352899849273067956076544000.000000\n",
      "Total training time: 18.82 seconds.\n",
      "-- Epoch 2612\n",
      "Norm: 6232328637925.42, NNZs: 843, Bias: 287471730.885032, T: 8794604, Avg. loss: 102671716906149196073236248395776.000000\n",
      "Total training time: 18.82 seconds.\n",
      "-- Epoch 2613\n",
      "Norm: 5943446013580.39, NNZs: 843, Bias: -77406450.842236, T: 8797971, Avg. loss: 102651993940912519393321039167488.000000\n",
      "Total training time: 18.83 seconds.\n",
      "-- Epoch 2614\n",
      "Norm: 5956197496591.76, NNZs: 843, Bias: -61689243.829526, T: 8801338, Avg. loss: 102632422729475595590913755185152.000000\n",
      "Total training time: 18.84 seconds.\n",
      "-- Epoch 2615\n",
      "Norm: 5899429131275.88, NNZs: 843, Bias: 1138935.034148, T: 8804705, Avg. loss: 102613038374744467492945266737152.000000\n",
      "Total training time: 18.85 seconds.\n",
      "-- Epoch 2616\n",
      "Norm: 5868933152304.58, NNZs: 843, Bias: 184696141.060795, T: 8808072, Avg. loss: 102593498642718315053211927445504.000000\n",
      "Total training time: 18.85 seconds.\n",
      "-- Epoch 2617\n",
      "Norm: 5932713366279.69, NNZs: 843, Bias: 1131666.909169, T: 8811439, Avg. loss: 102574325853979848028119870996480.000000\n",
      "Total training time: 18.86 seconds.\n",
      "-- Epoch 2618\n",
      "Norm: 6043669175005.37, NNZs: 843, Bias: -285526879.943535, T: 8814806, Avg. loss: 102554940001778335039406588559360.000000\n",
      "Total training time: 18.87 seconds.\n",
      "-- Epoch 2619\n",
      "Norm: 5760920076199.02, NNZs: 843, Bias: -101995863.978997, T: 8818173, Avg. loss: 102535239644386751893200850386944.000000\n",
      "Total training time: 18.87 seconds.\n",
      "-- Epoch 2620\n",
      "Norm: 5969949393333.75, NNZs: 843, Bias: 359238642.838184, T: 8821540, Avg. loss: 102516067464932009846147345547264.000000\n",
      "Total training time: 18.88 seconds.\n",
      "-- Epoch 2621\n",
      "Norm: 5865252822802.46, NNZs: 843, Bias: -191219509.205829, T: 8824907, Avg. loss: 102496426995232976085241791774720.000000\n",
      "Total training time: 18.89 seconds.\n",
      "-- Epoch 2622\n",
      "Norm: 5904604480068.37, NNZs: 843, Bias: -372057666.886193, T: 8828274, Avg. loss: 102477125014362857469852751233024.000000\n",
      "Total training time: 18.90 seconds.\n",
      "-- Epoch 2623\n",
      "Norm: 5799556927514.86, NNZs: 843, Bias: -188587886.915986, T: 8831641, Avg. loss: 102457967875063264103582360666112.000000\n",
      "Total training time: 18.90 seconds.\n",
      "-- Epoch 2624\n",
      "Norm: 5988949650253.87, NNZs: 843, Bias: -5155120.929441, T: 8835008, Avg. loss: 102438506976126552904781648625664.000000\n",
      "Total training time: 18.91 seconds.\n",
      "-- Epoch 2625\n",
      "Norm: 5860676611116.62, NNZs: 843, Bias: -188561495.398171, T: 8838375, Avg. loss: 102418676793311317907893978660864.000000\n",
      "Total training time: 18.92 seconds.\n",
      "-- Epoch 2626\n",
      "Norm: 5824374846113.81, NNZs: 843, Bias: -84738081.694547, T: 8841742, Avg. loss: 102399246721679608382440526053376.000000\n",
      "Total training time: 18.92 seconds.\n",
      "-- Epoch 2627\n",
      "Norm: 5918389662050.77, NNZs: 843, Bias: -268101438.618975, T: 8845109, Avg. loss: 102379894159199818178601443917824.000000\n",
      "Total training time: 18.93 seconds.\n",
      "-- Epoch 2628\n",
      "Norm: 6100643486426.71, NNZs: 843, Bias: -84727617.978069, T: 8848476, Avg. loss: 102360354673106270219113169158144.000000\n",
      "Total training time: 18.94 seconds.\n",
      "-- Epoch 2629\n",
      "Norm: 5886200777741.86, NNZs: 843, Bias: -184969282.060382, T: 8851843, Avg. loss: 102341266891535900670995145424896.000000\n",
      "Total training time: 18.95 seconds.\n",
      "-- Epoch 2630\n",
      "Norm: 6155017939604.83, NNZs: 843, Bias: -1639244.227237, T: 8855210, Avg. loss: 102321951876776879521209461506048.000000\n",
      "Total training time: 18.95 seconds.\n",
      "-- Epoch 2631\n",
      "Norm: 6211589951638.02, NNZs: 843, Bias: 548253145.726644, T: 8858577, Avg. loss: 102302211651513852307077949554688.000000\n",
      "Total training time: 18.96 seconds.\n",
      "-- Epoch 2632\n",
      "Norm: 6385378992236.86, NNZs: 843, Bias: -1646501.672502, T: 8861944, Avg. loss: 102282548679474652762946060419072.000000\n",
      "Total training time: 18.97 seconds.\n",
      "-- Epoch 2633\n",
      "Norm: 5951719309300.88, NNZs: 843, Bias: -39945589.190535, T: 8865311, Avg. loss: 102262969140513497402183199490048.000000\n",
      "Total training time: 18.98 seconds.\n",
      "-- Epoch 2634\n",
      "Norm: 5875823108015.05, NNZs: 843, Bias: -289610135.672844, T: 8868678, Avg. loss: 102243645466873632697810948194304.000000\n",
      "Total training time: 18.98 seconds.\n",
      "-- Epoch 2635\n",
      "Norm: 5760986341686.41, NNZs: 843, Bias: -106356402.992033, T: 8872045, Avg. loss: 102224186965702758591775306153984.000000\n",
      "Total training time: 18.99 seconds.\n",
      "-- Epoch 2636\n",
      "Norm: 6265723049470.00, NNZs: 843, Bias: 347876550.319341, T: 8875412, Avg. loss: 102204798171360785939255187734528.000000\n",
      "Total training time: 19.00 seconds.\n",
      "-- Epoch 2637\n",
      "Norm: 5763218593943.58, NNZs: 843, Bias: -201741378.043675, T: 8878779, Avg. loss: 102185237925539049328979082215424.000000\n",
      "Total training time: 19.00 seconds.\n",
      "-- Epoch 2638\n",
      "Norm: 6055084545773.98, NNZs: 843, Bias: 377803378.691143, T: 8882146, Avg. loss: 102165920037517260928420738498560.000000\n",
      "Total training time: 19.01 seconds.\n",
      "-- Epoch 2639\n",
      "Norm: 5786058309881.68, NNZs: 843, Bias: -34001127.096009, T: 8885513, Avg. loss: 102146888128164818893862850265088.000000\n",
      "Total training time: 19.02 seconds.\n",
      "-- Epoch 2640\n",
      "Norm: 5771766920776.38, NNZs: 843, Bias: -276449710.642464, T: 8888880, Avg. loss: 102127360312236897678140997894144.000000\n",
      "Total training time: 19.03 seconds.\n",
      "-- Epoch 2641\n",
      "Norm: 5806774057046.85, NNZs: 843, Bias: 272949657.269535, T: 8892247, Avg. loss: 102108072306126134497978265632768.000000\n",
      "Total training time: 19.03 seconds.\n",
      "-- Epoch 2642\n",
      "Norm: 5764595030656.49, NNZs: 843, Bias: 89812871.924245, T: 8895614, Avg. loss: 102088250099115511362457994002432.000000\n",
      "Total training time: 19.04 seconds.\n",
      "-- Epoch 2643\n",
      "Norm: 5973472605469.09, NNZs: 843, Bias: -93286831.543154, T: 8898981, Avg. loss: 102069141748092127893713590419456.000000\n",
      "Total training time: 19.05 seconds.\n",
      "-- Epoch 2644\n",
      "Norm: 6263455338190.10, NNZs: 843, Bias: -642499531.086349, T: 8902348, Avg. loss: 102049697605543248122767101394944.000000\n",
      "Total training time: 19.05 seconds.\n",
      "-- Epoch 2645\n",
      "Norm: 5747772734675.56, NNZs: 843, Bias: 272836713.540039, T: 8905715, Avg. loss: 102030781338518980058272900317184.000000\n",
      "Total training time: 19.06 seconds.\n",
      "-- Epoch 2646\n",
      "Norm: 6069158243433.07, NNZs: 843, Bias: -276305839.857327, T: 8909082, Avg. loss: 102011090983117802172470301556736.000000\n",
      "Total training time: 19.07 seconds.\n",
      "-- Epoch 2647\n",
      "Norm: 5796558904791.32, NNZs: 843, Bias: -93260666.262108, T: 8912449, Avg. loss: 101992086196802148324585936781312.000000\n",
      "Total training time: 19.08 seconds.\n",
      "-- Epoch 2648\n",
      "Norm: 5829839046518.85, NNZs: 843, Bias: 89749763.029177, T: 8915816, Avg. loss: 101973062977281808499695991914496.000000\n",
      "Total training time: 19.08 seconds.\n",
      "-- Epoch 2649\n",
      "Norm: 5702081288419.08, NNZs: 843, Bias: -93246406.672799, T: 8919183, Avg. loss: 101953985111900471076018553094144.000000\n",
      "Total training time: 19.09 seconds.\n",
      "-- Epoch 2650\n",
      "Norm: 5602979096946.73, NNZs: 843, Bias: 89729180.141604, T: 8922550, Avg. loss: 101934865989837544415081999106048.000000\n",
      "Total training time: 19.10 seconds.\n",
      "-- Epoch 2651\n",
      "Norm: 5597129844888.55, NNZs: 843, Bias: -93232990.631305, T: 8925917, Avg. loss: 101915823590097674203871842402304.000000\n",
      "Total training time: 19.10 seconds.\n",
      "-- Epoch 2652\n",
      "Norm: 5846991338825.16, NNZs: 843, Bias: -276160602.276659, T: 8929284, Avg. loss: 101896599147249674220566170566656.000000\n",
      "Total training time: 19.11 seconds.\n",
      "-- Epoch 2653\n",
      "Norm: 5465834354226.64, NNZs: 843, Bias: -3335681.388366, T: 8932651, Avg. loss: 101877125609385262222532453335040.000000\n",
      "Total training time: 19.12 seconds.\n",
      "-- Epoch 2654\n",
      "Norm: 5713589419102.16, NNZs: 843, Bias: -186238335.273280, T: 8936018, Avg. loss: 101857896192675923104759279517696.000000\n",
      "Total training time: 19.13 seconds.\n",
      "-- Epoch 2655\n",
      "Norm: 5667189448445.52, NNZs: 843, Bias: 362424261.264001, T: 8939385, Avg. loss: 101838550033672283470127815458816.000000\n",
      "Total training time: 19.13 seconds.\n",
      "-- Epoch 2656\n",
      "Norm: 5481923274452.01, NNZs: 843, Bias: 179521730.650336, T: 8942752, Avg. loss: 101819473806869231354675139182592.000000\n",
      "Total training time: 19.14 seconds.\n",
      "-- Epoch 2657\n",
      "Norm: 5819267059730.33, NNZs: 843, Bias: -369044324.434851, T: 8946119, Avg. loss: 101800370527189006077916363096064.000000\n",
      "Total training time: 19.15 seconds.\n",
      "-- Epoch 2658\n",
      "Norm: 5968157094263.91, NNZs: 843, Bias: 179480857.717302, T: 8949486, Avg. loss: 101781281011463410691245095256064.000000\n",
      "Total training time: 19.16 seconds.\n",
      "-- Epoch 2659\n",
      "Norm: 5567025615174.51, NNZs: 843, Bias: -157565471.753251, T: 8952853, Avg. loss: 101762346499295805918279505018880.000000\n",
      "Total training time: 19.16 seconds.\n",
      "-- Epoch 2660\n",
      "Norm: 5593099444856.65, NNZs: 843, Bias: -340349254.079105, T: 8956220, Avg. loss: 101742805488917224713900389302272.000000\n",
      "Total training time: 19.17 seconds.\n",
      "-- Epoch 2661\n",
      "Norm: 5750843046362.34, NNZs: 843, Bias: -157539647.859193, T: 8959587, Avg. loss: 101723338060878703140205758513152.000000\n",
      "Total training time: 19.18 seconds.\n",
      "-- Epoch 2662\n",
      "Norm: 5526581537377.16, NNZs: 843, Bias: 25233216.968027, T: 8962954, Avg. loss: 101704313237880719171650978840576.000000\n",
      "Total training time: 19.18 seconds.\n",
      "-- Epoch 2663\n",
      "Norm: 5851970610301.47, NNZs: 843, Bias: -349738006.044838, T: 8966321, Avg. loss: 101685237687303563428216757551104.000000\n",
      "Total training time: 19.19 seconds.\n",
      "-- Epoch 2664\n",
      "Norm: 5825409488420.53, NNZs: 843, Bias: 198475259.620543, T: 8969688, Avg. loss: 101666136219135705839052609552384.000000\n",
      "Total training time: 19.20 seconds.\n",
      "-- Epoch 2665\n",
      "Norm: 5503807088012.69, NNZs: 843, Bias: 15741441.417681, T: 8973055, Avg. loss: 101647345847082069635581408706560.000000\n",
      "Total training time: 19.21 seconds.\n",
      "-- Epoch 2666\n",
      "Norm: 5761203441023.21, NNZs: 843, Bias: -166958954.605113, T: 8976422, Avg. loss: 101628109125383426464933396611072.000000\n",
      "Total training time: 19.21 seconds.\n",
      "-- Epoch 2667\n",
      "Norm: 5749266692636.13, NNZs: 843, Bias: -148536984.818045, T: 8979789, Avg. loss: 101609354871811402132893705699328.000000\n",
      "Total training time: 19.22 seconds.\n",
      "-- Epoch 2668\n",
      "Norm: 5586949588973.21, NNZs: 843, Bias: 34133520.162661, T: 8983156, Avg. loss: 101590764247561658642022168264704.000000\n",
      "Total training time: 19.23 seconds.\n",
      "-- Epoch 2669\n",
      "Norm: 5441716503715.01, NNZs: 843, Bias: -60201972.500652, T: 8986523, Avg. loss: 101571715514777108174878409752576.000000\n",
      "Total training time: 19.23 seconds.\n",
      "-- Epoch 2670\n",
      "Norm: 5468941161790.63, NNZs: 843, Bias: -242823642.190772, T: 8989890, Avg. loss: 101552569115544398240289529004032.000000\n",
      "Total training time: 19.24 seconds.\n",
      "-- Epoch 2671\n",
      "Norm: 5568889136194.12, NNZs: 843, Bias: -60194430.914012, T: 8993257, Avg. loss: 101533818632179248705534274043904.000000\n",
      "Total training time: 19.25 seconds.\n",
      "-- Epoch 2672\n",
      "Norm: 5516665422322.34, NNZs: 843, Bias: -78999629.085652, T: 8996624, Avg. loss: 101514541766218244332447512657920.000000\n",
      "Total training time: 19.26 seconds.\n",
      "-- Epoch 2673\n",
      "Norm: 5545978393849.78, NNZs: 843, Bias: -9059945.732306, T: 8999991, Avg. loss: 101495800437886520114830137884672.000000\n",
      "Total training time: 19.26 seconds.\n",
      "-- Epoch 2674\n",
      "Norm: 5634958605269.35, NNZs: 843, Bias: -191619189.195933, T: 9003358, Avg. loss: 101476995708470204118473631596544.000000\n",
      "Total training time: 19.27 seconds.\n",
      "-- Epoch 2675\n",
      "Norm: 5849545571988.28, NNZs: 843, Bias: -9065333.738199, T: 9006725, Avg. loss: 101457774122212393363835910619136.000000\n",
      "Total training time: 19.28 seconds.\n",
      "-- Epoch 2676\n",
      "Norm: 5733369976605.71, NNZs: 843, Bias: -191591083.982723, T: 9010092, Avg. loss: 101439072153400396953753911033856.000000\n",
      "Total training time: 19.28 seconds.\n",
      "-- Epoch 2677\n",
      "Norm: 6260027791482.83, NNZs: 843, Bias: -222221419.568027, T: 9013459, Avg. loss: 101420238444053592789997523566592.000000\n",
      "Total training time: 19.29 seconds.\n",
      "-- Epoch 2678\n",
      "Norm: 5558161794605.64, NNZs: 843, Bias: -39714060.996035, T: 9016826, Avg. loss: 101401473499052930497929847767040.000000\n",
      "Total training time: 19.30 seconds.\n",
      "-- Epoch 2679\n",
      "Norm: 5695735469338.41, NNZs: 843, Bias: 87945357.448592, T: 9020193, Avg. loss: 101382345298706995427097444352000.000000\n",
      "Total training time: 19.31 seconds.\n",
      "-- Epoch 2680\n",
      "Norm: 6046749399990.82, NNZs: 843, Bias: 74257322.139215, T: 9023560, Avg. loss: 101363484222639373832774519619584.000000\n",
      "Total training time: 19.31 seconds.\n",
      "-- Epoch 2681\n",
      "Norm: 5893397605877.36, NNZs: 843, Bias: 11195934.444870, T: 9026927, Avg. loss: 101344538952132430859997106143232.000000\n",
      "Total training time: 19.32 seconds.\n",
      "-- Epoch 2682\n",
      "Norm: 5602995502766.92, NNZs: 843, Bias: -17237487.173681, T: 9030294, Avg. loss: 101325805435983184958703162884096.000000\n",
      "Total training time: 19.33 seconds.\n",
      "-- Epoch 2683\n",
      "Norm: 5431198388215.88, NNZs: 843, Bias: 165165339.552647, T: 9033661, Avg. loss: 101306977827473608780611126820864.000000\n",
      "Total training time: 19.34 seconds.\n",
      "-- Epoch 2684\n",
      "Norm: 5659379710978.68, NNZs: 843, Bias: 264337032.471169, T: 9037028, Avg. loss: 101288102559949090208696875614208.000000\n",
      "Total training time: 19.34 seconds.\n",
      "-- Epoch 2685\n",
      "Norm: 5727695214099.81, NNZs: 843, Bias: 250996882.183199, T: 9040395, Avg. loss: 101269041610091700183492385046528.000000\n",
      "Total training time: 19.35 seconds.\n",
      "-- Epoch 2686\n",
      "Norm: 5511613441254.04, NNZs: 843, Bias: 68618859.235605, T: 9043762, Avg. loss: 101250071299124017904526529921024.000000\n",
      "Total training time: 19.36 seconds.\n",
      "-- Epoch 2687\n",
      "Norm: 5609853226619.60, NNZs: 843, Bias: -168703041.441223, T: 9047129, Avg. loss: 101231203013152347794739447201792.000000\n",
      "Total training time: 19.36 seconds.\n",
      "-- Epoch 2688\n",
      "Norm: 5479038320245.76, NNZs: 843, Bias: -44705570.625652, T: 9050496, Avg. loss: 101212213512223398775488260341760.000000\n",
      "Total training time: 19.37 seconds.\n",
      "-- Epoch 2689\n",
      "Norm: 5468528132996.24, NNZs: 843, Bias: 137598243.214229, T: 9053863, Avg. loss: 101193580338172244032571427520512.000000\n",
      "Total training time: 19.38 seconds.\n",
      "-- Epoch 2690\n",
      "Norm: 5490069318130.07, NNZs: 843, Bias: -113984272.193766, T: 9057230, Avg. loss: 101174754668738251993846164488192.000000\n",
      "Total training time: 19.39 seconds.\n",
      "-- Epoch 2691\n",
      "Norm: 5504212907453.72, NNZs: 843, Bias: 68291865.509868, T: 9060597, Avg. loss: 101155942271751789112688272998400.000000\n",
      "Total training time: 19.39 seconds.\n",
      "-- Epoch 2692\n",
      "Norm: 5925700211078.42, NNZs: 843, Bias: 151888434.481721, T: 9063964, Avg. loss: 101137218356588240295875796008960.000000\n",
      "Total training time: 19.40 seconds.\n",
      "-- Epoch 2693\n",
      "Norm: 5667436682488.64, NNZs: 843, Bias: -346063135.834535, T: 9067331, Avg. loss: 101118843098676265496716561612800.000000\n",
      "Total training time: 19.41 seconds.\n",
      "-- Epoch 2694\n",
      "Norm: 6053769319092.01, NNZs: 843, Bias: -163816595.004328, T: 9070698, Avg. loss: 101099878409262416608300940394496.000000\n",
      "Total training time: 19.41 seconds.\n",
      "-- Epoch 2695\n",
      "Norm: 5496302528227.21, NNZs: 843, Bias: 18395091.812109, T: 9074065, Avg. loss: 101081408558655433112620529352704.000000\n",
      "Total training time: 19.42 seconds.\n",
      "-- Epoch 2696\n",
      "Norm: 5516252790158.69, NNZs: 843, Bias: 200572280.113632, T: 9077432, Avg. loss: 101062690906798684609879713251328.000000\n",
      "Total training time: 19.43 seconds.\n",
      "-- Epoch 2697\n",
      "Norm: 5557757163501.51, NNZs: 843, Bias: 18382713.305748, T: 9080799, Avg. loss: 101043856248539691964269846331392.000000\n",
      "Total training time: 19.44 seconds.\n",
      "-- Epoch 2698\n",
      "Norm: 5634230334139.35, NNZs: 843, Bias: -163772772.606389, T: 9084166, Avg. loss: 101025313337864991546497552089088.000000\n",
      "Total training time: 19.44 seconds.\n",
      "-- Epoch 2699\n",
      "Norm: 5756759392665.74, NNZs: 843, Bias: 74415180.234510, T: 9087533, Avg. loss: 101006742801171878877337915228160.000000\n",
      "Total training time: 19.45 seconds.\n",
      "-- Epoch 2700\n",
      "Norm: 5541108902087.47, NNZs: 843, Bias: -107709474.946156, T: 9090900, Avg. loss: 100988013174886763165642704027648.000000\n",
      "Total training time: 19.46 seconds.\n",
      "-- Epoch 2701\n",
      "Norm: 5711061370827.09, NNZs: 843, Bias: 74398227.367874, T: 9094267, Avg. loss: 100969466469477971890376930230272.000000\n",
      "Total training time: 19.46 seconds.\n",
      "-- Epoch 2702\n",
      "Norm: 5542024356329.48, NNZs: 843, Bias: 256471608.425393, T: 9097634, Avg. loss: 100950723627183933148163137863680.000000\n",
      "Total training time: 19.47 seconds.\n",
      "-- Epoch 2703\n",
      "Norm: 5630928622322.48, NNZs: 843, Bias: 74381560.217778, T: 9101001, Avg. loss: 100932324316652901006028207816704.000000\n",
      "Total training time: 19.48 seconds.\n",
      "-- Epoch 2704\n",
      "Norm: 5836658971367.05, NNZs: 843, Bias: -107674701.304311, T: 9104368, Avg. loss: 100913720858063728452362559291392.000000\n",
      "Total training time: 19.49 seconds.\n",
      "-- Epoch 2705\n",
      "Norm: 5565957684435.65, NNZs: 843, Bias: 126039992.835437, T: 9107735, Avg. loss: 100895068199839447950758762250240.000000\n",
      "Total training time: 19.49 seconds.\n",
      "-- Epoch 2706\n",
      "Norm: 5581347464810.83, NNZs: 843, Bias: -55989048.280793, T: 9111102, Avg. loss: 100876172703311187015719096156160.000000\n",
      "Total training time: 19.50 seconds.\n",
      "-- Epoch 2707\n",
      "Norm: 5525487034535.89, NNZs: 843, Bias: -237984993.423746, T: 9114469, Avg. loss: 100857893264214639789822690983936.000000\n",
      "Total training time: 19.51 seconds.\n",
      "-- Epoch 2708\n",
      "Norm: 5538641402959.04, NNZs: 843, Bias: 90237952.734874, T: 9117836, Avg. loss: 100839244492815334348478448926720.000000\n",
      "Total training time: 19.51 seconds.\n",
      "-- Epoch 2709\n",
      "Norm: 5502105262132.66, NNZs: 843, Bias: -91736665.443949, T: 9121203, Avg. loss: 100820739934577104410283868160000.000000\n",
      "Total training time: 19.52 seconds.\n",
      "-- Epoch 2710\n",
      "Norm: 5704194149241.79, NNZs: 843, Bias: 90217998.148121, T: 9124570, Avg. loss: 100802358851305867057911220404224.000000\n",
      "Total training time: 19.53 seconds.\n",
      "-- Epoch 2711\n",
      "Norm: 5545683030850.01, NNZs: 843, Bias: -91723529.547184, T: 9127937, Avg. loss: 100783705425815711947370543448064.000000\n",
      "Total training time: 19.54 seconds.\n",
      "-- Epoch 2712\n",
      "Norm: 6033249987211.71, NNZs: 843, Bias: -273630325.420501, T: 9131304, Avg. loss: 100764993332671279002710460858368.000000\n",
      "Total training time: 19.54 seconds.\n",
      "-- Epoch 2713\n",
      "Norm: 5764879562093.39, NNZs: 843, Bias: 170236964.626030, T: 9134671, Avg. loss: 100746546870945109582354239717376.000000\n",
      "Total training time: 19.55 seconds.\n",
      "-- Epoch 2714\n",
      "Norm: 5614895928634.46, NNZs: 843, Bias: -191521065.149535, T: 9138038, Avg. loss: 100728299830775284439640877039616.000000\n",
      "Total training time: 19.56 seconds.\n",
      "-- Epoch 2715\n",
      "Norm: 5766660451659.95, NNZs: 843, Bias: -373369847.009050, T: 9141405, Avg. loss: 100709564036491281916861882564608.000000\n",
      "Total training time: 19.57 seconds.\n",
      "-- Epoch 2716\n",
      "Norm: 5686310113157.68, NNZs: 843, Bias: 172204520.081420, T: 9144772, Avg. loss: 100691551014218705590509725810688.000000\n",
      "Total training time: 19.57 seconds.\n",
      "-- Epoch 2717\n",
      "Norm: 5769812124837.22, NNZs: 843, Bias: -9644745.179759, T: 9148139, Avg. loss: 100673153096964542771329723531264.000000\n",
      "Total training time: 19.58 seconds.\n",
      "-- Epoch 2718\n",
      "Norm: 5564923050024.25, NNZs: 843, Bias: 87981897.798687, T: 9151506, Avg. loss: 100654512700663585825640973074432.000000\n",
      "Total training time: 19.59 seconds.\n",
      "-- Epoch 2719\n",
      "Norm: 6070609410680.22, NNZs: 843, Bias: -324917006.520272, T: 9154873, Avg. loss: 100635877154569551780031999508480.000000\n",
      "Total training time: 19.60 seconds.\n",
      "-- Epoch 2720\n",
      "Norm: 5583807083116.69, NNZs: 843, Bias: -143108156.786608, T: 9158240, Avg. loss: 100617280183147245930173066903552.000000\n",
      "Total training time: 19.60 seconds.\n",
      "-- Epoch 2721\n",
      "Norm: 6047571788767.79, NNZs: 843, Bias: -324862853.119648, T: 9161607, Avg. loss: 100598791323866624878401627881472.000000\n",
      "Total training time: 19.61 seconds.\n",
      "-- Epoch 2722\n",
      "Norm: 5696355423142.41, NNZs: 843, Bias: 220404724.758799, T: 9164974, Avg. loss: 100580286614283200080164618764288.000000\n",
      "Total training time: 19.62 seconds.\n",
      "-- Epoch 2723\n",
      "Norm: 5504027532573.53, NNZs: 843, Bias: 38652059.847163, T: 9168341, Avg. loss: 100561917093894694631384914329600.000000\n",
      "Total training time: 19.63 seconds.\n",
      "-- Epoch 2724\n",
      "Norm: 5545019347842.26, NNZs: 843, Bias: -143066851.380914, T: 9171708, Avg. loss: 100543479790402856941722389184512.000000\n",
      "Total training time: 19.63 seconds.\n",
      "-- Epoch 2725\n",
      "Norm: 5617825662883.33, NNZs: 843, Bias: -98615236.567721, T: 9175075, Avg. loss: 100524530126827303590671622864896.000000\n",
      "Total training time: 19.64 seconds.\n",
      "-- Epoch 2726\n",
      "Norm: 5678523469475.01, NNZs: 843, Bias: -280288914.998962, T: 9178442, Avg. loss: 100506001722286616595589295505408.000000\n",
      "Total training time: 19.65 seconds.\n",
      "-- Epoch 2727\n",
      "Norm: 5644877222760.69, NNZs: 843, Bias: -98602939.211388, T: 9181809, Avg. loss: 100487588888152121924622099677184.000000\n",
      "Total training time: 19.65 seconds.\n",
      "-- Epoch 2728\n",
      "Norm: 5666288346467.61, NNZs: 843, Bias: 83049859.180349, T: 9185176, Avg. loss: 100469067638193085478399662620672.000000\n",
      "Total training time: 19.66 seconds.\n",
      "-- Epoch 2729\n",
      "Norm: 5428608392864.67, NNZs: 843, Bias: -98590139.395399, T: 9188543, Avg. loss: 100450571622320965450463937822720.000000\n",
      "Total training time: 19.67 seconds.\n",
      "-- Epoch 2730\n",
      "Norm: 5845775914525.14, NNZs: 843, Bias: 148401141.131704, T: 9191910, Avg. loss: 100432225981284889168805026070528.000000\n",
      "Total training time: 19.68 seconds.\n",
      "-- Epoch 2731\n",
      "Norm: 5533572589176.13, NNZs: 843, Bias: -33211620.714840, T: 9195277, Avg. loss: 100414231480268280257262756298752.000000\n",
      "Total training time: 19.68 seconds.\n",
      "-- Epoch 2732\n",
      "Norm: 5553484572337.81, NNZs: 843, Bias: -214791762.138208, T: 9198644, Avg. loss: 100395756970391289506511330476032.000000\n",
      "Total training time: 19.69 seconds.\n",
      "-- Epoch 2733\n",
      "Norm: 5577780712446.10, NNZs: 843, Bias: -215155068.462665, T: 9202011, Avg. loss: 100377170716837030049873920000000.000000\n",
      "Total training time: 19.70 seconds.\n",
      "-- Epoch 2734\n",
      "Norm: 5917689887456.84, NNZs: 843, Bias: -33591208.350908, T: 9205378, Avg. loss: 100358658349169552391552586743808.000000\n",
      "Total training time: 19.71 seconds.\n",
      "-- Epoch 2735\n",
      "Norm: 6082380222061.25, NNZs: 843, Bias: -728285262.895605, T: 9208745, Avg. loss: 100340620301621221781000694005760.000000\n",
      "Total training time: 19.71 seconds.\n",
      "-- Epoch 2736\n",
      "Norm: 5931184671987.13, NNZs: 843, Bias: -183680122.893583, T: 9212112, Avg. loss: 100322745320316343583368120631296.000000\n",
      "Total training time: 19.72 seconds.\n",
      "-- Epoch 2737\n",
      "Norm: 5610531406750.14, NNZs: 843, Bias: -94168336.889538, T: 9215479, Avg. loss: 100304341399973078689207564632064.000000\n",
      "Total training time: 19.73 seconds.\n",
      "-- Epoch 2738\n",
      "Norm: 5636384213762.22, NNZs: 843, Bias: 87319412.998698, T: 9218846, Avg. loss: 100286133628841899830867830767616.000000\n",
      "Total training time: 19.74 seconds.\n",
      "-- Epoch 2739\n",
      "Norm: 5961617218609.05, NNZs: 843, Bias: 410644304.928755, T: 9222213, Avg. loss: 100267985359198777666131288129536.000000\n",
      "Total training time: 19.74 seconds.\n",
      "-- Epoch 2740\n",
      "Norm: 6395086519928.07, NNZs: 843, Bias: -204588339.172836, T: 9225580, Avg. loss: 100250065158590852722409122824192.000000\n",
      "Total training time: 19.75 seconds.\n",
      "-- Epoch 2741\n",
      "Norm: 6008492424527.36, NNZs: 843, Bias: -23143433.551245, T: 9228947, Avg. loss: 100231812665505815301523450626048.000000\n",
      "Total training time: 19.76 seconds.\n",
      "-- Epoch 2742\n",
      "Norm: 5878706655035.94, NNZs: 843, Bias: 158270632.954429, T: 9232314, Avg. loss: 100213336537152212064127395299328.000000\n",
      "Total training time: 19.76 seconds.\n",
      "-- Epoch 2743\n",
      "Norm: 5870421317759.40, NNZs: 843, Bias: -6814967.695922, T: 9235681, Avg. loss: 100195061563805581774026073702400.000000\n",
      "Total training time: 19.77 seconds.\n",
      "-- Epoch 2744\n",
      "Norm: 6289592779499.68, NNZs: 843, Bias: 99698655.102471, T: 9239048, Avg. loss: 100176766681777245425235214204928.000000\n",
      "Total training time: 19.78 seconds.\n",
      "-- Epoch 2745\n",
      "Norm: 6035959263533.15, NNZs: 843, Bias: -81678229.605664, T: 9242415, Avg. loss: 100158526363695590377652769259520.000000\n",
      "Total training time: 19.79 seconds.\n",
      "-- Epoch 2746\n",
      "Norm: 5994678892691.35, NNZs: 843, Bias: 99675082.980745, T: 9245782, Avg. loss: 100140333845458870015566729445376.000000\n",
      "Total training time: 19.79 seconds.\n",
      "-- Epoch 2747\n",
      "Norm: 6250022738235.15, NNZs: 843, Bias: -81668770.687429, T: 9249149, Avg. loss: 100122209415868559704815896100864.000000\n",
      "Total training time: 19.80 seconds.\n",
      "-- Epoch 2748\n",
      "Norm: 6256369384870.74, NNZs: 843, Bias: 99650032.584137, T: 9252516, Avg. loss: 100103895666108825808720780853248.000000\n",
      "Total training time: 19.81 seconds.\n",
      "-- Epoch 2749\n",
      "Norm: 6111435877290.56, NNZs: 843, Bias: -81660721.621259, T: 9255883, Avg. loss: 100085904799539197820086932996096.000000\n",
      "Total training time: 19.82 seconds.\n",
      "-- Epoch 2750\n",
      "Norm: 6195807040064.14, NNZs: 843, Bias: 99626278.149374, T: 9259250, Avg. loss: 100067659118676544774193849303040.000000\n",
      "Total training time: 19.82 seconds.\n",
      "-- Epoch 2751\n",
      "Norm: 6229161722942.91, NNZs: 843, Bias: 280879613.360055, T: 9262617, Avg. loss: 100049498744510870001344076316672.000000\n",
      "Total training time: 19.83 seconds.\n",
      "-- Epoch 2752\n",
      "Norm: 6017559519538.53, NNZs: 843, Bias: 135974982.811148, T: 9265984, Avg. loss: 100031391620435392160275674169344.000000\n",
      "Total training time: 19.84 seconds.\n",
      "-- Epoch 2753\n",
      "Norm: 6397279079460.81, NNZs: 843, Bias: -198469054.600555, T: 9269351, Avg. loss: 100013012122282478753824825671680.000000\n",
      "Total training time: 19.84 seconds.\n",
      "-- Epoch 2754\n",
      "Norm: 6188470236408.27, NNZs: 843, Bias: -17238531.309079, T: 9272718, Avg. loss: 99994776801609161434822483116032.000000\n",
      "Total training time: 19.85 seconds.\n",
      "-- Epoch 2755\n",
      "Norm: 6141407311051.07, NNZs: 843, Bias: 163958374.286128, T: 9276085, Avg. loss: 99976885298932691080804858593280.000000\n",
      "Total training time: 19.86 seconds.\n",
      "-- Epoch 2756\n",
      "Norm: 6238451141380.34, NNZs: 843, Bias: -17245939.668464, T: 9279452, Avg. loss: 99958823151463185593072024027136.000000\n",
      "Total training time: 19.87 seconds.\n",
      "-- Epoch 2757\n",
      "Norm: 6039691079728.44, NNZs: 843, Bias: 163917971.501483, T: 9282819, Avg. loss: 99940482178281067528244839841792.000000\n",
      "Total training time: 19.87 seconds.\n",
      "-- Epoch 2758\n",
      "Norm: 6559867990399.82, NNZs: 843, Bias: -103929148.241280, T: 9286186, Avg. loss: 99922499613343504062694075400192.000000\n",
      "Total training time: 19.88 seconds.\n",
      "-- Epoch 2759\n",
      "Norm: 6154640329944.21, NNZs: 843, Bias: -267618205.904427, T: 9289553, Avg. loss: 99904560269661792502399077711872.000000\n",
      "Total training time: 19.89 seconds.\n",
      "-- Epoch 2760\n",
      "Norm: 6348430722570.75, NNZs: 843, Bias: -86478422.026894, T: 9292920, Avg. loss: 99886525088022409397477618745344.000000\n",
      "Total training time: 19.89 seconds.\n",
      "-- Epoch 2761\n",
      "Norm: 6435263163490.28, NNZs: 843, Bias: 456831669.484053, T: 9296287, Avg. loss: 99868333070738099194403309486080.000000\n",
      "Total training time: 19.90 seconds.\n",
      "-- Epoch 2762\n",
      "Norm: 6456703345681.82, NNZs: 843, Bias: -86469942.655228, T: 9299654, Avg. loss: 99850416050538562188911197552640.000000\n",
      "Total training time: 19.91 seconds.\n",
      "-- Epoch 2763\n",
      "Norm: 6382913148530.38, NNZs: 843, Bias: 94603788.691487, T: 9303021, Avg. loss: 99832464362447703982408757936128.000000\n",
      "Total training time: 19.92 seconds.\n",
      "-- Epoch 2764\n",
      "Norm: 6358098971017.33, NNZs: 843, Bias: -86462015.811544, T: 9306388, Avg. loss: 99814607219846636387804752379904.000000\n",
      "Total training time: 19.92 seconds.\n",
      "-- Epoch 2765\n",
      "Norm: 6268209749719.86, NNZs: 843, Bias: -230169595.093314, T: 9309755, Avg. loss: 99796422088925453612490445291520.000000\n",
      "Total training time: 19.93 seconds.\n",
      "-- Epoch 2766\n",
      "Norm: 6205579320533.78, NNZs: 843, Bias: 312907320.369414, T: 9313122, Avg. loss: 99778264508299158953955961077760.000000\n",
      "Total training time: 19.94 seconds.\n",
      "-- Epoch 2767\n",
      "Norm: 6069062670905.18, NNZs: 843, Bias: -230133874.900113, T: 9316489, Avg. loss: 99760186934684383436005854674944.000000\n",
      "Total training time: 19.95 seconds.\n",
      "-- Epoch 2768\n",
      "Norm: 6020663421977.46, NNZs: 843, Bias: -170595343.089954, T: 9319856, Avg. loss: 99742304125847714556857649463296.000000\n",
      "Total training time: 19.95 seconds.\n",
      "-- Epoch 2769\n",
      "Norm: 5878866090056.54, NNZs: 843, Bias: 10386586.155302, T: 9323223, Avg. loss: 99724132183852680150500746723328.000000\n",
      "Total training time: 19.96 seconds.\n",
      "-- Epoch 2770\n",
      "Norm: 6070356687484.48, NNZs: 843, Bias: -170573171.260058, T: 9326590, Avg. loss: 99706004510831761637601745829888.000000\n",
      "Total training time: 19.97 seconds.\n",
      "-- Epoch 2771\n",
      "Norm: 5778916145746.20, NNZs: 843, Bias: 10375463.927405, T: 9329957, Avg. loss: 99688345072576930339242016505856.000000\n",
      "Total training time: 19.97 seconds.\n",
      "-- Epoch 2772\n",
      "Norm: 5957124210503.71, NNZs: 843, Bias: -170552454.524280, T: 9333324, Avg. loss: 99670049978262778643058276696064.000000\n",
      "Total training time: 19.98 seconds.\n",
      "-- Epoch 2773\n",
      "Norm: 6040502784592.33, NNZs: 843, Bias: -255124526.480970, T: 9336691, Avg. loss: 99652189365835059506398722260992.000000\n",
      "Total training time: 19.99 seconds.\n",
      "-- Epoch 2774\n",
      "Norm: 6072523573401.23, NNZs: 843, Bias: -74215779.266457, T: 9340058, Avg. loss: 99634242698872890179445730574336.000000\n",
      "Total training time: 20.00 seconds.\n",
      "-- Epoch 2775\n",
      "Norm: 5658221161226.20, NNZs: 843, Bias: 106659563.444162, T: 9343425, Avg. loss: 99616111022325435635017625632768.000000\n",
      "Total training time: 20.00 seconds.\n",
      "-- Epoch 2776\n",
      "Norm: 6138046257935.17, NNZs: 843, Bias: 195564129.884901, T: 9346792, Avg. loss: 99598061446783885534901588459520.000000\n",
      "Total training time: 20.01 seconds.\n",
      "-- Epoch 2777\n",
      "Norm: 5812627375146.23, NNZs: 843, Bias: 72052914.109630, T: 9350159, Avg. loss: 99580213838936561292035473014784.000000\n",
      "Total training time: 20.02 seconds.\n",
      "-- Epoch 2778\n",
      "Norm: 5881794881098.46, NNZs: 843, Bias: -108781772.301184, T: 9353526, Avg. loss: 99562332072215372219865844678656.000000\n",
      "Total training time: 20.02 seconds.\n",
      "-- Epoch 2779\n",
      "Norm: 5913625809477.78, NNZs: 843, Bias: -289583253.600001, T: 9356893, Avg. loss: 99544091901876933050354905382912.000000\n",
      "Total training time: 20.03 seconds.\n",
      "-- Epoch 2780\n",
      "Norm: 5845894134884.50, NNZs: 843, Bias: -108768849.958754, T: 9360260, Avg. loss: 99526280783700550083377474043904.000000\n",
      "Total training time: 20.04 seconds.\n",
      "-- Epoch 2781\n",
      "Norm: 5933383554051.48, NNZs: 843, Bias: -289536671.514613, T: 9363627, Avg. loss: 99508438796896026774144607584256.000000\n",
      "Total training time: 20.05 seconds.\n",
      "-- Epoch 2782\n",
      "Norm: 6097139645298.01, NNZs: 843, Bias: 252764279.362705, T: 9366994, Avg. loss: 99490228781773933710845633626112.000000\n",
      "Total training time: 20.05 seconds.\n",
      "-- Epoch 2783\n",
      "Norm: 5651488466473.75, NNZs: 843, Bias: 50782939.527736, T: 9370361, Avg. loss: 99472269093976923671798080339968.000000\n",
      "Total training time: 20.06 seconds.\n",
      "-- Epoch 2784\n",
      "Norm: 5810351187629.45, NNZs: 843, Bias: 231501443.493514, T: 9373728, Avg. loss: 99454363304498965521835946409984.000000\n",
      "Total training time: 20.07 seconds.\n",
      "-- Epoch 2785\n",
      "Norm: 6131586794074.83, NNZs: 843, Bias: -310652494.280852, T: 9377095, Avg. loss: 99436559919085900896752707305472.000000\n",
      "Total training time: 20.07 seconds.\n",
      "-- Epoch 2786\n",
      "Norm: 5852605986393.17, NNZs: 843, Bias: -129935155.845118, T: 9380462, Avg. loss: 99418839243117896648700043722752.000000\n",
      "Total training time: 20.08 seconds.\n",
      "-- Epoch 2787\n",
      "Norm: 5890556505021.34, NNZs: 843, Bias: -310604939.516436, T: 9383829, Avg. loss: 99401075006897035702529557528576.000000\n",
      "Total training time: 20.09 seconds.\n",
      "-- Epoch 2788\n",
      "Norm: 5963758224404.33, NNZs: 843, Bias: 94909048.047660, T: 9387196, Avg. loss: 99383074004665824043095463821312.000000\n",
      "Total training time: 20.10 seconds.\n",
      "-- Epoch 2789\n",
      "Norm: 6081194795312.23, NNZs: 843, Bias: -395354500.352150, T: 9390563, Avg. loss: 99365142518475793440623661416448.000000\n",
      "Total training time: 20.10 seconds.\n",
      "-- Epoch 2790\n",
      "Norm: 5946246201908.02, NNZs: 843, Bias: 146566525.640961, T: 9393930, Avg. loss: 99347423657729357562064985391104.000000\n",
      "Total training time: 20.11 seconds.\n",
      "-- Epoch 2791\n",
      "Norm: 6183013611663.51, NNZs: 843, Bias: 327162286.913099, T: 9397297, Avg. loss: 99329798867950180952775034142720.000000\n",
      "Total training time: 20.12 seconds.\n",
      "-- Epoch 2792\n",
      "Norm: 6114347393330.43, NNZs: 843, Bias: 43448228.812434, T: 9400664, Avg. loss: 99312470814947303168525044023296.000000\n",
      "Total training time: 20.12 seconds.\n",
      "-- Epoch 2793\n",
      "Norm: 6365520204277.82, NNZs: 843, Bias: -137140808.973733, T: 9404031, Avg. loss: 99294504192515891542100621131776.000000\n",
      "Total training time: 20.13 seconds.\n",
      "-- Epoch 2794\n",
      "Norm: 6248802189957.41, NNZs: 843, Bias: 43431796.222510, T: 9407398, Avg. loss: 99276710957971589348770015346688.000000\n",
      "Total training time: 20.14 seconds.\n",
      "-- Epoch 2795\n",
      "Norm: 6330944664411.11, NNZs: 843, Bias: 98441226.385963, T: 9410765, Avg. loss: 99258888084695725891084731023360.000000\n",
      "Total training time: 20.15 seconds.\n",
      "-- Epoch 2796\n",
      "Norm: 6157896338894.92, NNZs: 843, Bias: -264630948.960971, T: 9414132, Avg. loss: 99240675976181360779838992941056.000000\n",
      "Total training time: 20.15 seconds.\n",
      "-- Epoch 2797\n",
      "Norm: 6174948758926.01, NNZs: 843, Bias: 122090369.828188, T: 9417499, Avg. loss: 99222784581470188695932534521856.000000\n",
      "Total training time: 20.16 seconds.\n",
      "-- Epoch 2798\n",
      "Norm: 6259784484582.97, NNZs: 843, Bias: -58423382.884467, T: 9420866, Avg. loss: 99205245650833729995720439627776.000000\n",
      "Total training time: 20.17 seconds.\n",
      "-- Epoch 2799\n",
      "Norm: 6185151703996.66, NNZs: 843, Bias: 18630146.854380, T: 9424233, Avg. loss: 99187563121246876777139067158528.000000\n",
      "Total training time: 20.17 seconds.\n",
      "-- Epoch 2800\n",
      "Norm: 6207306964981.02, NNZs: 843, Bias: 237742529.453547, T: 9427600, Avg. loss: 99169925832159748339626282057728.000000\n",
      "Total training time: 20.18 seconds.\n",
      "-- Epoch 2801\n",
      "Norm: 6057672216523.22, NNZs: 843, Bias: -303636962.650062, T: 9430967, Avg. loss: 99152202436745855262735372124160.000000\n",
      "Total training time: 20.19 seconds.\n",
      "-- Epoch 2802\n",
      "Norm: 6065567056231.80, NNZs: 843, Bias: -123177395.417552, T: 9434334, Avg. loss: 99134657597084890234203173552128.000000\n",
      "Total training time: 20.20 seconds.\n",
      "-- Epoch 2803\n",
      "Norm: 6080181723769.37, NNZs: 843, Bias: 57250587.557475, T: 9437701, Avg. loss: 99116563312768869564273745985536.000000\n",
      "Total training time: 20.20 seconds.\n",
      "-- Epoch 2804\n",
      "Norm: 6151222427364.46, NNZs: 843, Bias: -123161857.768243, T: 9441068, Avg. loss: 99098467843700676740378584416256.000000\n",
      "Total training time: 20.21 seconds.\n",
      "-- Epoch 2805\n",
      "Norm: 6145684857755.69, NNZs: 843, Bias: 57233174.610526, T: 9444435, Avg. loss: 99080830215843236700847064743936.000000\n",
      "Total training time: 20.22 seconds.\n",
      "-- Epoch 2806\n",
      "Norm: 6047345714090.96, NNZs: 843, Bias: 237596929.919087, T: 9447802, Avg. loss: 99062932606166938731860868464640.000000\n",
      "Total training time: 20.22 seconds.\n",
      "-- Epoch 2807\n",
      "Norm: 6235206997562.11, NNZs: 843, Bias: 44315380.900089, T: 9451169, Avg. loss: 99044821055954251514227263012864.000000\n",
      "Total training time: 20.23 seconds.\n",
      "-- Epoch 2808\n",
      "Norm: 5762747143763.01, NNZs: 843, Bias: -106449169.832266, T: 9454536, Avg. loss: 99027352433359536367757086949376.000000\n",
      "Total training time: 20.24 seconds.\n",
      "-- Epoch 2809\n",
      "Norm: 5910996562290.06, NNZs: 843, Bias: -286765986.932470, T: 9457903, Avg. loss: 99010086254676066788163945758720.000000\n",
      "Total training time: 20.25 seconds.\n",
      "-- Epoch 2810\n",
      "Norm: 5955422132375.48, NNZs: 843, Bias: -106436819.892790, T: 9461270, Avg. loss: 98992411875804069312411501330432.000000\n",
      "Total training time: 20.25 seconds.\n",
      "-- Epoch 2811\n",
      "Norm: 5961489132064.07, NNZs: 843, Bias: -140579802.925348, T: 9464637, Avg. loss: 98975011447101951161967977693184.000000\n",
      "Total training time: 20.26 seconds.\n",
      "-- Epoch 2812\n",
      "Norm: 6499126371397.75, NNZs: 843, Bias: -320845847.940085, T: 9468004, Avg. loss: 98957499602359221024059939618816.000000\n",
      "Total training time: 20.27 seconds.\n",
      "-- Epoch 2813\n",
      "Norm: 6221479428255.29, NNZs: 843, Bias: 177318150.169526, T: 9471371, Avg. loss: 98939704136871148919044534435840.000000\n",
      "Total training time: 20.27 seconds.\n",
      "-- Epoch 2814\n",
      "Norm: 6225524867150.29, NNZs: 843, Bias: -2944546.896960, T: 9474738, Avg. loss: 98922306563827317147974949142528.000000\n",
      "Total training time: 20.28 seconds.\n",
      "-- Epoch 2815\n",
      "Norm: 6119883488686.31, NNZs: 843, Bias: 110548613.252796, T: 9478105, Avg. loss: 98904864973447746461194793254912.000000\n",
      "Total training time: 20.29 seconds.\n",
      "-- Epoch 2816\n",
      "Norm: 6131083884736.96, NNZs: 843, Bias: -69676294.118687, T: 9481472, Avg. loss: 98887452746054167716460022988800.000000\n",
      "Total training time: 20.30 seconds.\n",
      "-- Epoch 2817\n",
      "Norm: 6166915952798.88, NNZs: 843, Bias: -249868249.845499, T: 9484839, Avg. loss: 98869920453501406197940100268032.000000\n",
      "Total training time: 20.30 seconds.\n",
      "-- Epoch 2818\n",
      "Norm: 6184369102302.30, NNZs: 843, Bias: 290687264.318917, T: 9488206, Avg. loss: 98852504816867164949442570747904.000000\n",
      "Total training time: 20.31 seconds.\n",
      "-- Epoch 2819\n",
      "Norm: 6330639058202.04, NNZs: 843, Bias: 110496176.927786, T: 9491573, Avg. loss: 98835137348377444082797087031296.000000\n",
      "Total training time: 20.32 seconds.\n",
      "-- Epoch 2820\n",
      "Norm: 6089823509847.28, NNZs: 843, Bias: 16702337.595627, T: 9494940, Avg. loss: 98817542299530375369750669688832.000000\n",
      "Total training time: 20.32 seconds.\n",
      "-- Epoch 2821\n",
      "Norm: 6021733391410.45, NNZs: 843, Bias: 196826983.289377, T: 9498307, Avg. loss: 98800006857640532519749257527296.000000\n",
      "Total training time: 20.33 seconds.\n",
      "-- Epoch 2822\n",
      "Norm: 5998090390904.92, NNZs: 843, Bias: 16691456.385447, T: 9501674, Avg. loss: 98782551883977341844421005541376.000000\n",
      "Total training time: 20.34 seconds.\n",
      "-- Epoch 2823\n",
      "Norm: 6303149915384.76, NNZs: 843, Bias: 196786071.185167, T: 9505041, Avg. loss: 98764835229039816141214748508160.000000\n",
      "Total training time: 20.35 seconds.\n",
      "-- Epoch 2824\n",
      "Norm: 5994160600956.28, NNZs: 843, Bias: 376848555.263247, T: 9508408, Avg. loss: 98747720887785980949962109747200.000000\n",
      "Total training time: 20.35 seconds.\n",
      "-- Epoch 2825\n",
      "Norm: 5869679307424.92, NNZs: 843, Bias: 343774236.901614, T: 9511775, Avg. loss: 98730357948276877916980058783744.000000\n",
      "Total training time: 20.36 seconds.\n",
      "-- Epoch 2826\n",
      "Norm: 5683017202247.53, NNZs: 843, Bias: -196413276.969936, T: 9515142, Avg. loss: 98712911053065891048319365414912.000000\n",
      "Total training time: 20.37 seconds.\n",
      "-- Epoch 2827\n",
      "Norm: 6115342149089.44, NNZs: 843, Bias: -240362594.310344, T: 9518509, Avg. loss: 98695324547232333699165567582208.000000\n",
      "Total training time: 20.37 seconds.\n",
      "-- Epoch 2828\n",
      "Norm: 5777605116196.99, NNZs: 843, Bias: -60324415.567047, T: 9521876, Avg. loss: 98677750151092740352640502726656.000000\n",
      "Total training time: 20.38 seconds.\n",
      "-- Epoch 2829\n",
      "Norm: 5752765954333.30, NNZs: 843, Bias: 119683328.066222, T: 9525243, Avg. loss: 98660253086945526633975848632320.000000\n",
      "Total training time: 20.39 seconds.\n",
      "-- Epoch 2830\n",
      "Norm: 5677672622099.34, NNZs: 843, Bias: 10741057.638346, T: 9528610, Avg. loss: 98642799736655748348929185415168.000000\n",
      "Total training time: 20.40 seconds.\n",
      "-- Epoch 2831\n",
      "Norm: 6237990272207.25, NNZs: 843, Bias: -662578473.477894, T: 9531977, Avg. loss: 98625334907455537357990389612544.000000\n",
      "Total training time: 20.40 seconds.\n",
      "-- Epoch 2832\n",
      "Norm: 5921121378507.84, NNZs: 843, Bias: -122655817.523457, T: 9535344, Avg. loss: 98607808053207968652881207230464.000000\n",
      "Total training time: 20.41 seconds.\n",
      "-- Epoch 2833\n",
      "Norm: 5800290081910.97, NNZs: 843, Bias: 57291714.818995, T: 9538711, Avg. loss: 98590277814303014312379817132032.000000\n",
      "Total training time: 20.42 seconds.\n",
      "-- Epoch 2834\n",
      "Norm: 5754032500435.09, NNZs: 843, Bias: -225218503.304480, T: 9542078, Avg. loss: 98572946638734018634408890728448.000000\n",
      "Total training time: 20.43 seconds.\n",
      "-- Epoch 2835\n",
      "Norm: 6120148949293.95, NNZs: 843, Bias: 176596464.875377, T: 9545445, Avg. loss: 98555437349338220666147653550080.000000\n",
      "Total training time: 20.43 seconds.\n",
      "-- Epoch 2836\n",
      "Norm: 5706768906125.06, NNZs: 843, Bias: -334142830.796737, T: 9548812, Avg. loss: 98538050033746619130475033133056.000000\n",
      "Total training time: 20.44 seconds.\n",
      "-- Epoch 2837\n",
      "Norm: 5594171264091.92, NNZs: 843, Bias: -182524443.717890, T: 9552179, Avg. loss: 98520899835210132608091984756736.000000\n",
      "Total training time: 20.45 seconds.\n",
      "-- Epoch 2838\n",
      "Norm: 5562255842905.95, NNZs: 843, Bias: -2651622.811654, T: 9555546, Avg. loss: 98503717415314760038497262764032.000000\n",
      "Total training time: 20.45 seconds.\n",
      "-- Epoch 2839\n",
      "Norm: 5718362083555.99, NNZs: 843, Bias: 177190664.525972, T: 9558913, Avg. loss: 98486427096367410551693561036800.000000\n",
      "Total training time: 20.46 seconds.\n",
      "-- Epoch 2840\n",
      "Norm: 5837882519002.30, NNZs: 843, Bias: -2658814.223185, T: 9562280, Avg. loss: 98469040508682600782342937116672.000000\n",
      "Total training time: 20.47 seconds.\n",
      "-- Epoch 2841\n",
      "Norm: 6062448634275.51, NNZs: 843, Bias: -69871749.597092, T: 9565647, Avg. loss: 98451818456404559103593835134976.000000\n",
      "Total training time: 20.48 seconds.\n",
      "-- Epoch 2842\n",
      "Norm: 6138089107190.46, NNZs: 843, Bias: 364150293.554670, T: 9569014, Avg. loss: 98434537798441878549734409371648.000000\n",
      "Total training time: 20.48 seconds.\n",
      "-- Epoch 2843\n",
      "Norm: 6193830811567.50, NNZs: 843, Bias: -175229864.293475, T: 9572381, Avg. loss: 98417133696395892056325359665152.000000\n",
      "Total training time: 20.49 seconds.\n",
      "-- Epoch 2844\n",
      "Norm: 6311179924099.43, NNZs: 843, Bias: 296880929.383508, T: 9575748, Avg. loss: 98399874073077766712171601854464.000000\n",
      "Total training time: 20.50 seconds.\n",
      "-- Epoch 2845\n",
      "Norm: 5699814633866.05, NNZs: 843, Bias: -77023032.773641, T: 9579115, Avg. loss: 98382513366307622445539758440448.000000\n",
      "Total training time: 20.50 seconds.\n",
      "-- Epoch 2846\n",
      "Norm: 6106300712614.71, NNZs: 843, Bias: 102714492.644145, T: 9582482, Avg. loss: 98365654811550050813489715347456.000000\n",
      "Total training time: 20.51 seconds.\n",
      "-- Epoch 2847\n",
      "Norm: 5840399380150.83, NNZs: 843, Bias: 282421305.208796, T: 9585849, Avg. loss: 98348217944349178181951787368448.000000\n",
      "Total training time: 20.52 seconds.\n",
      "-- Epoch 2848\n",
      "Norm: 5919149454937.12, NNZs: 843, Bias: 417733989.441836, T: 9589216, Avg. loss: 98330935265889525177561464700928.000000\n",
      "Total training time: 20.53 seconds.\n",
      "-- Epoch 2849\n",
      "Norm: 5771020773140.23, NNZs: 843, Bias: -121301413.923897, T: 9592583, Avg. loss: 98313532918431974334454298574848.000000\n",
      "Total training time: 20.53 seconds.\n",
      "-- Epoch 2850\n",
      "Norm: 5807670950476.12, NNZs: 843, Bias: 58377633.855299, T: 9595950, Avg. loss: 98296134163868095775591727366144.000000\n",
      "Total training time: 20.54 seconds.\n",
      "-- Epoch 2851\n",
      "Norm: 6089378227101.14, NNZs: 843, Bias: -121285063.319583, T: 9599317, Avg. loss: 98278948300680614135663568093184.000000\n",
      "Total training time: 20.55 seconds.\n",
      "-- Epoch 2852\n",
      "Norm: 6171648744956.84, NNZs: 843, Bias: -151313319.007915, T: 9602684, Avg. loss: 98261713276975047917640234303488.000000\n",
      "Total training time: 20.55 seconds.\n",
      "-- Epoch 2853\n",
      "Norm: 6282258923945.57, NNZs: 843, Bias: -444891037.985124, T: 9606051, Avg. loss: 98244421983023797421421027655680.000000\n",
      "Total training time: 20.56 seconds.\n",
      "-- Epoch 2854\n",
      "Norm: 6335915700873.94, NNZs: 843, Bias: -265246220.108385, T: 9609418, Avg. loss: 98227226926373476685618941526016.000000\n",
      "Total training time: 20.57 seconds.\n",
      "-- Epoch 2855\n",
      "Norm: 6349788899361.51, NNZs: 843, Bias: -201689373.460388, T: 9612785, Avg. loss: 98209694359079682748873969762304.000000\n",
      "Total training time: 20.58 seconds.\n",
      "-- Epoch 2856\n",
      "Norm: 6134841800567.99, NNZs: 843, Bias: -446083247.906643, T: 9616152, Avg. loss: 98192624025645282302314439245824.000000\n",
      "Total training time: 20.58 seconds.\n",
      "-- Epoch 2857\n",
      "Norm: 5875199073206.98, NNZs: 843, Bias: 92635201.456511, T: 9619519, Avg. loss: 98175043463106120444377401655296.000000\n",
      "Total training time: 20.59 seconds.\n",
      "-- Epoch 2858\n",
      "Norm: 5858060688844.41, NNZs: 843, Bias: -86921779.671046, T: 9622886, Avg. loss: 98157409266079275263049628385280.000000\n",
      "Total training time: 20.60 seconds.\n",
      "-- Epoch 2859\n",
      "Norm: 5699232010240.06, NNZs: 843, Bias: 92613618.084667, T: 9626253, Avg. loss: 98140424954014745293233164648448.000000\n",
      "Total training time: 20.60 seconds.\n",
      "-- Epoch 2860\n",
      "Norm: 5863533606564.14, NNZs: 843, Bias: -86911013.805881, T: 9629620, Avg. loss: 98123372344661203864847741091840.000000\n",
      "Total training time: 20.61 seconds.\n",
      "-- Epoch 2861\n",
      "Norm: 5663216953552.01, NNZs: 843, Bias: 95091575.124126, T: 9632987, Avg. loss: 98106385050006107357281291075584.000000\n",
      "Total training time: 20.62 seconds.\n",
      "-- Epoch 2862\n",
      "Norm: 5660820614023.00, NNZs: 843, Bias: -139187841.120689, T: 9636354, Avg. loss: 98089341172996288734736550985728.000000\n",
      "Total training time: 20.63 seconds.\n",
      "-- Epoch 2863\n",
      "Norm: 5934548524898.83, NNZs: 843, Bias: -136524481.766189, T: 9639721, Avg. loss: 98072396113033898074626586574848.000000\n",
      "Total training time: 20.63 seconds.\n",
      "-- Epoch 2864\n",
      "Norm: 6049106694950.71, NNZs: 843, Bias: 42934580.258738, T: 9643088, Avg. loss: 98055544867184320617617507221504.000000\n",
      "Total training time: 20.64 seconds.\n",
      "-- Epoch 2865\n",
      "Norm: 5658648944286.88, NNZs: 843, Bias: 78150914.866554, T: 9646455, Avg. loss: 98038363613500539126443075960832.000000\n",
      "Total training time: 20.65 seconds.\n",
      "-- Epoch 2866\n",
      "Norm: 5954300346811.86, NNZs: 843, Bias: 257560782.917071, T: 9649822, Avg. loss: 98021567157223714889457513529344.000000\n",
      "Total training time: 20.66 seconds.\n",
      "-- Epoch 2867\n",
      "Norm: 5868423505262.80, NNZs: 843, Bias: 78133013.883536, T: 9653189, Avg. loss: 98004624194988256875442887196672.000000\n",
      "Total training time: 20.66 seconds.\n",
      "-- Epoch 2868\n",
      "Norm: 5823552990209.08, NNZs: 843, Bias: -101265521.599111, T: 9656556, Avg. loss: 97987403866599233483076850941952.000000\n",
      "Total training time: 20.67 seconds.\n",
      "-- Epoch 2869\n",
      "Norm: 5622380852359.66, NNZs: 843, Bias: 78114237.013710, T: 9659923, Avg. loss: 97970273043579884396017551933440.000000\n",
      "Total training time: 20.68 seconds.\n",
      "-- Epoch 2870\n",
      "Norm: 6005057209023.99, NNZs: 843, Bias: 257462344.444766, T: 9663290, Avg. loss: 97953020664617678183967395676160.000000\n",
      "Total training time: 20.68 seconds.\n",
      "-- Epoch 2871\n",
      "Norm: 5686586179397.90, NNZs: 843, Bias: 30402345.232249, T: 9666657, Avg. loss: 97936100357232499907235695034368.000000\n",
      "Total training time: 20.69 seconds.\n",
      "-- Epoch 2872\n",
      "Norm: 5910502603772.44, NNZs: 843, Bias: -73102846.318297, T: 9670024, Avg. loss: 97919086541140460636401318232064.000000\n",
      "Total training time: 20.70 seconds.\n",
      "-- Epoch 2873\n",
      "Norm: 5790624320587.01, NNZs: 843, Bias: -252409641.553392, T: 9673391, Avg. loss: 97902110972665646145127931969536.000000\n",
      "Total training time: 20.71 seconds.\n",
      "-- Epoch 2874\n",
      "Norm: 5978305485381.25, NNZs: 843, Bias: -170293885.592128, T: 9676758, Avg. loss: 97884892364181288434411774148608.000000\n",
      "Total training time: 20.71 seconds.\n",
      "-- Epoch 2875\n",
      "Norm: 5726559379947.90, NNZs: 843, Bias: 8996150.868042, T: 9680125, Avg. loss: 97867775615353370637131938004992.000000\n",
      "Total training time: 20.72 seconds.\n",
      "-- Epoch 2876\n",
      "Norm: 5733021646474.75, NNZs: 843, Bias: -200158636.890901, T: 9683492, Avg. loss: 97851041559017552298677226176512.000000\n",
      "Total training time: 20.73 seconds.\n",
      "-- Epoch 2877\n",
      "Norm: 5755242028000.48, NNZs: 843, Bias: 46297476.936081, T: 9686859, Avg. loss: 97834044298118688025817588432896.000000\n",
      "Total training time: 20.73 seconds.\n",
      "-- Epoch 2878\n",
      "Norm: 6051745456603.13, NNZs: 843, Bias: 225521992.722961, T: 9690226, Avg. loss: 97816982336344047687423628410880.000000\n",
      "Total training time: 20.74 seconds.\n",
      "-- Epoch 2879\n",
      "Norm: 5679992524800.89, NNZs: 843, Bias: 46282453.484981, T: 9693593, Avg. loss: 97800107914942144888212056702976.000000\n",
      "Total training time: 20.75 seconds.\n",
      "-- Epoch 2880\n",
      "Norm: 6140824836479.19, NNZs: 843, Bias: 225476858.468771, T: 9696960, Avg. loss: 97783311522508727271199344689152.000000\n",
      "Total training time: 20.76 seconds.\n",
      "-- Epoch 2881\n",
      "Norm: 5674006760587.56, NNZs: 843, Bias: 46269447.971050, T: 9700327, Avg. loss: 97766404141730619538202867793920.000000\n",
      "Total training time: 20.76 seconds.\n",
      "-- Epoch 2882\n",
      "Norm: 5640564318232.05, NNZs: 843, Bias: 203009123.937516, T: 9703694, Avg. loss: 97749294815110904261462722609152.000000\n",
      "Total training time: 20.77 seconds.\n",
      "-- Epoch 2883\n",
      "Norm: 5578721472021.82, NNZs: 843, Bias: 23834016.378335, T: 9707061, Avg. loss: 97732298679982207133161795616768.000000\n",
      "Total training time: 20.78 seconds.\n",
      "-- Epoch 2884\n",
      "Norm: 5743815476399.67, NNZs: 843, Bias: -155310401.160902, T: 9710428, Avg. loss: 97715321948976287294143363809280.000000\n",
      "Total training time: 20.78 seconds.\n",
      "-- Epoch 2885\n",
      "Norm: 5653767690529.70, NNZs: 843, Bias: -40512164.019012, T: 9713795, Avg. loss: 97698473773348263881063853457408.000000\n",
      "Total training time: 20.79 seconds.\n",
      "-- Epoch 2886\n",
      "Norm: 5707968263652.06, NNZs: 843, Bias: -219619329.157548, T: 9717162, Avg. loss: 97681312779837025284496717512704.000000\n",
      "Total training time: 20.80 seconds.\n",
      "-- Epoch 2887\n",
      "Norm: 5909859403447.00, NNZs: 843, Bias: 317674225.530528, T: 9720529, Avg. loss: 97664117118809110932720950706176.000000\n",
      "Total training time: 20.81 seconds.\n",
      "-- Epoch 2888\n",
      "Norm: 5715350083603.00, NNZs: 843, Bias: 32485566.974948, T: 9723896, Avg. loss: 97647076180378791777418214375424.000000\n",
      "Total training time: 20.81 seconds.\n",
      "-- Epoch 2889\n",
      "Norm: 5809138559362.25, NNZs: 843, Bias: 211540390.176191, T: 9727263, Avg. loss: 97630397355047699451541819752448.000000\n",
      "Total training time: 20.82 seconds.\n",
      "-- Epoch 2890\n",
      "Norm: 5826809305394.49, NNZs: 843, Bias: 32473808.289433, T: 9730630, Avg. loss: 97613428198923378595209613934592.000000\n",
      "Total training time: 20.83 seconds.\n",
      "-- Epoch 2891\n",
      "Norm: 5829175041685.97, NNZs: 843, Bias: 89724453.732658, T: 9733997, Avg. loss: 97596767703342695535784035876864.000000\n",
      "Total training time: 20.83 seconds.\n",
      "-- Epoch 2892\n",
      "Norm: 5768416267643.98, NNZs: 843, Bias: -89301851.681607, T: 9737364, Avg. loss: 97579953528913694945282963275776.000000\n",
      "Total training time: 20.84 seconds.\n",
      "-- Epoch 2893\n",
      "Norm: 5633001304430.49, NNZs: 843, Bias: 89702213.215961, T: 9740731, Avg. loss: 97563158275387588701056774701056.000000\n",
      "Total training time: 20.85 seconds.\n",
      "-- Epoch 2894\n",
      "Norm: 5620699131042.85, NNZs: 843, Bias: -89293788.840424, T: 9744098, Avg. loss: 97546032081828740791518082629632.000000\n",
      "Total training time: 20.86 seconds.\n",
      "-- Epoch 2895\n",
      "Norm: 5973683060981.25, NNZs: 843, Bias: -247128661.948071, T: 9747465, Avg. loss: 97529197915893324187274701701120.000000\n",
      "Total training time: 20.86 seconds.\n",
      "-- Epoch 2896\n",
      "Norm: 5625556681588.90, NNZs: 843, Bias: 44321098.112514, T: 9750832, Avg. loss: 97512341011243000707938758164480.000000\n",
      "Total training time: 20.87 seconds.\n",
      "-- Epoch 2897\n",
      "Norm: 5848108660325.10, NNZs: 843, Bias: 154857443.989020, T: 9754199, Avg. loss: 97495292578550776104246442983424.000000\n",
      "Total training time: 20.88 seconds.\n",
      "-- Epoch 2898\n",
      "Norm: 5878168090508.22, NNZs: 843, Bias: -24081393.768060, T: 9757566, Avg. loss: 97478663760285713367174184173568.000000\n",
      "Total training time: 20.89 seconds.\n",
      "-- Epoch 2899\n",
      "Norm: 6078644709829.54, NNZs: 843, Bias: 187409274.443694, T: 9760933, Avg. loss: 97461754463698716048115994984448.000000\n",
      "Total training time: 20.89 seconds.\n",
      "-- Epoch 2900\n",
      "Norm: 5641845246900.54, NNZs: 843, Bias: 8498384.743169, T: 9764300, Avg. loss: 97444511721778061566072514936832.000000\n",
      "Total training time: 20.90 seconds.\n",
      "-- Epoch 2901\n",
      "Norm: 5678239402769.70, NNZs: 843, Bias: -170381545.585278, T: 9767667, Avg. loss: 97427409329471587180082194546688.000000\n",
      "Total training time: 20.91 seconds.\n",
      "-- Epoch 2902\n",
      "Norm: 5636558756022.27, NNZs: 843, Bias: 8489553.476242, T: 9771034, Avg. loss: 97410646907730637172171616550912.000000\n",
      "Total training time: 20.91 seconds.\n",
      "-- Epoch 2903\n",
      "Norm: 5790653116600.84, NNZs: 843, Bias: 37108930.043966, T: 9774401, Avg. loss: 97394146544179786609188860329984.000000\n",
      "Total training time: 20.92 seconds.\n",
      "-- Epoch 2904\n",
      "Norm: 6060500092786.84, NNZs: 843, Bias: -141726975.167383, T: 9777768, Avg. loss: 97377250081226648406503901364224.000000\n",
      "Total training time: 20.93 seconds.\n",
      "-- Epoch 2905\n",
      "Norm: 5990852800152.21, NNZs: 843, Bias: -320532162.596168, T: 9781135, Avg. loss: 97360575572026363778363197227008.000000\n",
      "Total training time: 20.94 seconds.\n",
      "-- Epoch 2906\n",
      "Norm: 5919164476765.46, NNZs: 843, Bias: -141707875.546470, T: 9784502, Avg. loss: 97343687251260083078516904558592.000000\n",
      "Total training time: 20.94 seconds.\n",
      "-- Epoch 2907\n",
      "Norm: 5901825692924.76, NNZs: 843, Bias: 37086561.380847, T: 9787869, Avg. loss: 97326901973166276789721341362176.000000\n",
      "Total training time: 20.95 seconds.\n",
      "-- Epoch 2908\n",
      "Norm: 6002515771652.31, NNZs: 843, Bias: -141687203.203174, T: 9791236, Avg. loss: 97310131328328599813484714131456.000000\n",
      "Total training time: 20.96 seconds.\n",
      "-- Epoch 2909\n",
      "Norm: 5916908702060.94, NNZs: 843, Bias: -80788226.898548, T: 9794603, Avg. loss: 97293472355850160621614408925184.000000\n",
      "Total training time: 20.96 seconds.\n",
      "-- Epoch 2910\n",
      "Norm: 5945472495536.08, NNZs: 843, Bias: 97954598.992376, T: 9797970, Avg. loss: 97276959939659756381699842244608.000000\n",
      "Total training time: 20.97 seconds.\n",
      "-- Epoch 2911\n",
      "Norm: 5872052638782.90, NNZs: 843, Bias: -80776875.796779, T: 9801337, Avg. loss: 97260445996105045702074641154048.000000\n",
      "Total training time: 20.98 seconds.\n",
      "-- Epoch 2912\n",
      "Norm: 5980895528017.30, NNZs: 843, Bias: 97935840.348714, T: 9804704, Avg. loss: 97243873351389582205468205383680.000000\n",
      "Total training time: 20.99 seconds.\n",
      "-- Epoch 2913\n",
      "Norm: 6298545686887.10, NNZs: 843, Bias: -523611402.019453, T: 9808071, Avg. loss: 97227258809343070557576565358592.000000\n",
      "Total training time: 20.99 seconds.\n",
      "-- Epoch 2914\n",
      "Norm: 5942042507226.08, NNZs: 843, Bias: 12459480.612320, T: 9811438, Avg. loss: 97210538562418712746494330404864.000000\n",
      "Total training time: 21.00 seconds.\n",
      "-- Epoch 2915\n",
      "Norm: 5845029491669.45, NNZs: 843, Bias: -166205046.056033, T: 9814805, Avg. loss: 97193792473924847145260368789504.000000\n",
      "Total training time: 21.01 seconds.\n",
      "-- Epoch 2916\n",
      "Norm: 5876505911680.30, NNZs: 843, Bias: 12452395.062645, T: 9818172, Avg. loss: 97176934366243483617959002243072.000000\n",
      "Total training time: 21.01 seconds.\n",
      "-- Epoch 2917\n",
      "Norm: 5923339060526.83, NNZs: 843, Bias: -166181160.042619, T: 9821539, Avg. loss: 97160195083440864197706426351616.000000\n",
      "Total training time: 21.02 seconds.\n",
      "-- Epoch 2918\n",
      "Norm: 5847712084662.21, NNZs: 843, Bias: 12445952.153358, T: 9824906, Avg. loss: 97143781686660327733483288920064.000000\n",
      "Total training time: 21.03 seconds.\n",
      "-- Epoch 2919\n",
      "Norm: 5999913231541.72, NNZs: 843, Bias: -166157248.891308, T: 9828273, Avg. loss: 97127578403041909087474731188224.000000\n",
      "Total training time: 21.04 seconds.\n",
      "-- Epoch 2920\n",
      "Norm: 5998822253655.66, NNZs: 843, Bias: -15994343.024298, T: 9831640, Avg. loss: 97110702805407113405409097940992.000000\n",
      "Total training time: 21.04 seconds.\n",
      "-- Epoch 2921\n",
      "Norm: 5980328290416.19, NNZs: 843, Bias: 162574116.660530, T: 9835007, Avg. loss: 97093684236410042479700363706368.000000\n",
      "Total training time: 21.05 seconds.\n",
      "-- Epoch 2922\n",
      "Norm: 6012766965472.64, NNZs: 843, Bias: -6766141.009952, T: 9838374, Avg. loss: 97077151280961818668797834297344.000000\n",
      "Total training time: 21.06 seconds.\n",
      "-- Epoch 2923\n",
      "Norm: 6058140506477.69, NNZs: 843, Bias: 171771448.441623, T: 9841741, Avg. loss: 97060578639276521116674344091648.000000\n",
      "Total training time: 21.06 seconds.\n",
      "-- Epoch 2924\n",
      "Norm: 6113365807814.42, NNZs: 843, Bias: -6769056.922415, T: 9845108, Avg. loss: 97044049997865381536742961577984.000000\n",
      "Total training time: 21.07 seconds.\n",
      "-- Epoch 2925\n",
      "Norm: 6522772158013.22, NNZs: 843, Bias: -185277570.148635, T: 9848475, Avg. loss: 97027340530001495868598239887360.000000\n",
      "Total training time: 21.08 seconds.\n",
      "-- Epoch 2926\n",
      "Norm: 6290011985118.13, NNZs: 843, Bias: 328132736.577349, T: 9851842, Avg. loss: 97010637101046668372477615276032.000000\n",
      "Total training time: 21.09 seconds.\n",
      "-- Epoch 2927\n",
      "Norm: 6198758954934.29, NNZs: 843, Bias: -207328134.486035, T: 9855209, Avg. loss: 96994306128477835584798808080384.000000\n",
      "Total training time: 21.09 seconds.\n",
      "-- Epoch 2928\n",
      "Norm: 6070706051977.06, NNZs: 843, Bias: -28850109.483920, T: 9858576, Avg. loss: 96978147097644803539709619863552.000000\n",
      "Total training time: 21.10 seconds.\n",
      "-- Epoch 2929\n",
      "Norm: 6287363752191.61, NNZs: 843, Bias: -207295021.601009, T: 9861943, Avg. loss: 96961698114580499145807919841280.000000\n",
      "Total training time: 21.11 seconds.\n",
      "-- Epoch 2930\n",
      "Norm: 6157411036279.37, NNZs: 843, Bias: -28844631.824103, T: 9865310, Avg. loss: 96945460370728999928269163724800.000000\n",
      "Total training time: 21.11 seconds.\n",
      "-- Epoch 2931\n",
      "Norm: 6180264141142.62, NNZs: 843, Bias: 149572002.861516, T: 9868677, Avg. loss: 96928955211858324506227775635456.000000\n",
      "Total training time: 21.12 seconds.\n",
      "-- Epoch 2932\n",
      "Norm: 6147219681850.40, NNZs: 843, Bias: -28842675.188728, T: 9872044, Avg. loss: 96912491744603867931592888418304.000000\n",
      "Total training time: 21.13 seconds.\n",
      "-- Epoch 2933\n",
      "Norm: 5951006587752.13, NNZs: 843, Bias: -207227802.685475, T: 9875411, Avg. loss: 96895860023169344186238906662912.000000\n",
      "Total training time: 21.14 seconds.\n",
      "-- Epoch 2934\n",
      "Norm: 5972990430574.78, NNZs: 843, Bias: -28841017.614979, T: 9878778, Avg. loss: 96879289419105344178173302013952.000000\n",
      "Total training time: 21.14 seconds.\n",
      "-- Epoch 2935\n",
      "Norm: 5972119308321.14, NNZs: 843, Bias: -207195017.533116, T: 9882145, Avg. loss: 96862864675375626711935108513792.000000\n",
      "Total training time: 21.15 seconds.\n",
      "-- Epoch 2936\n",
      "Norm: 5937312391762.29, NNZs: 843, Bias: 327843044.109332, T: 9885512, Avg. loss: 96846472095812736948200714272768.000000\n",
      "Total training time: 21.16 seconds.\n",
      "-- Epoch 2937\n",
      "Norm: 5892646397555.13, NNZs: 843, Bias: -207162765.364828, T: 9888879, Avg. loss: 96830025010288380179939762110464.000000\n",
      "Total training time: 21.16 seconds.\n",
      "-- Epoch 2938\n",
      "Norm: 6146989953675.07, NNZs: 843, Bias: -385455886.388025, T: 9892246, Avg. loss: 96813395669456835635412079738880.000000\n",
      "Total training time: 21.17 seconds.\n",
      "-- Epoch 2939\n",
      "Norm: 5982319629280.95, NNZs: 843, Bias: -43369686.786512, T: 9895613, Avg. loss: 96796904693273485866959602974720.000000\n",
      "Total training time: 21.18 seconds.\n",
      "-- Epoch 2940\n",
      "Norm: 5700482453210.81, NNZs: 843, Bias: 129097129.764974, T: 9898980, Avg. loss: 96780776433864701246812012412928.000000\n",
      "Total training time: 21.19 seconds.\n",
      "-- Epoch 2941\n",
      "Norm: 6156963315186.71, NNZs: 843, Bias: 307350212.229916, T: 9902347, Avg. loss: 96764052517293443998075672592384.000000\n",
      "Total training time: 21.19 seconds.\n",
      "-- Epoch 2942\n",
      "Norm: 5767975553040.86, NNZs: 843, Bias: 129073425.209476, T: 9905714, Avg. loss: 96747833286136713418903566417920.000000\n",
      "Total training time: 21.20 seconds.\n",
      "-- Epoch 2943\n",
      "Norm: 5817590031124.52, NNZs: 843, Bias: -49172932.191318, T: 9909081, Avg. loss: 96731439114146806032371399262208.000000\n",
      "Total training time: 21.21 seconds.\n",
      "-- Epoch 2944\n",
      "Norm: 6227633126996.82, NNZs: 843, Bias: 249477950.832653, T: 9912448, Avg. loss: 96715003061469947752615245774848.000000\n",
      "Total training time: 21.21 seconds.\n",
      "-- Epoch 2945\n",
      "Norm: 5975848165325.51, NNZs: 843, Bias: 415205373.100262, T: 9915815, Avg. loss: 96698424960203804229036450775040.000000\n",
      "Total training time: 21.22 seconds.\n",
      "-- Epoch 2946\n",
      "Norm: 5791295394935.96, NNZs: 843, Bias: -119399838.454570, T: 9919182, Avg. loss: 96682080591792505445533125443584.000000\n",
      "Total training time: 21.23 seconds.\n",
      "-- Epoch 2947\n",
      "Norm: 5750581543410.36, NNZs: 843, Bias: -115948121.298084, T: 9922549, Avg. loss: 96665794889617607218264897748992.000000\n",
      "Total training time: 21.24 seconds.\n",
      "-- Epoch 2948\n",
      "Norm: 5805827232302.84, NNZs: 843, Bias: -294099280.853187, T: 9925916, Avg. loss: 96649106555030315162174394204160.000000\n",
      "Total training time: 21.24 seconds.\n",
      "-- Epoch 2949\n",
      "Norm: 5694244488175.63, NNZs: 843, Bias: -115932716.799388, T: 9929283, Avg. loss: 96632574817232212106449126424576.000000\n",
      "Total training time: 21.25 seconds.\n",
      "-- Epoch 2950\n",
      "Norm: 5740835982797.67, NNZs: 843, Bias: 62204182.423038, T: 9932650, Avg. loss: 96615935040029935733507679584256.000000\n",
      "Total training time: 21.26 seconds.\n",
      "-- Epoch 2951\n",
      "Norm: 5861334240076.02, NNZs: 843, Bias: -115917468.593503, T: 9936017, Avg. loss: 96599611109046736282421148254208.000000\n",
      "Total training time: 21.27 seconds.\n",
      "-- Epoch 2952\n",
      "Norm: 5999754123263.05, NNZs: 843, Bias: -273866282.147096, T: 9939384, Avg. loss: 96583020092396384400123348647936.000000\n",
      "Total training time: 21.27 seconds.\n",
      "-- Epoch 2953\n",
      "Norm: 5671360168070.47, NNZs: 843, Bias: -95761296.118575, T: 9942751, Avg. loss: 96566827531959034728534853550080.000000\n",
      "Total training time: 21.28 seconds.\n",
      "-- Epoch 2954\n",
      "Norm: 5908963581882.73, NNZs: 843, Bias: -273821411.185586, T: 9946118, Avg. loss: 96550272636158215344145932746752.000000\n",
      "Total training time: 21.29 seconds.\n",
      "-- Epoch 2955\n",
      "Norm: 5925618278810.96, NNZs: 843, Bias: -95747183.403580, T: 9949485, Avg. loss: 96533925385901189140027256340480.000000\n",
      "Total training time: 21.29 seconds.\n",
      "-- Epoch 2956\n",
      "Norm: 5696892357988.40, NNZs: 843, Bias: 93161995.229861, T: 9952852, Avg. loss: 96517636634638526638611853475840.000000\n",
      "Total training time: 21.30 seconds.\n",
      "-- Epoch 2957\n",
      "Norm: 5625771538749.92, NNZs: 843, Bias: -84869536.377466, T: 9956219, Avg. loss: 96501220000527525681912394809344.000000\n",
      "Total training time: 21.31 seconds.\n",
      "-- Epoch 2958\n",
      "Norm: 5742366059819.49, NNZs: 843, Bias: -262872618.018902, T: 9959586, Avg. loss: 96484679155494753795762401837056.000000\n",
      "Total training time: 21.32 seconds.\n",
      "-- Epoch 2959\n",
      "Norm: 5821754320270.59, NNZs: 843, Bias: 329725840.865383, T: 9962953, Avg. loss: 96468325602158648176345192857600.000000\n",
      "Total training time: 21.32 seconds.\n",
      "-- Epoch 2960\n",
      "Norm: 5732349565692.09, NNZs: 843, Bias: 25947723.563445, T: 9966320, Avg. loss: 96451946772649775851240138211328.000000\n",
      "Total training time: 21.33 seconds.\n",
      "-- Epoch 2961\n",
      "Norm: 5906589258341.22, NNZs: 843, Bias: 203906197.245832, T: 9969687, Avg. loss: 96435519084271454016719932096512.000000\n",
      "Total training time: 21.34 seconds.\n",
      "-- Epoch 2962\n",
      "Norm: 6092043628697.11, NNZs: 843, Bias: -331681511.005718, T: 9973054, Avg. loss: 96419724772071668397405653434368.000000\n",
      "Total training time: 21.34 seconds.\n",
      "-- Epoch 2963\n",
      "Norm: 5979274302846.18, NNZs: 843, Bias: 2225267.407249, T: 9976421, Avg. loss: 96403800377977491549493569519616.000000\n",
      "Total training time: 21.35 seconds.\n",
      "-- Epoch 2964\n",
      "Norm: 6177986746728.93, NNZs: 843, Bias: -175694948.452947, T: 9979788, Avg. loss: 96387368697564703039786177265664.000000\n",
      "Total training time: 21.36 seconds.\n",
      "-- Epoch 2965\n",
      "Norm: 6079382347633.76, NNZs: 843, Bias: 750509.454169, T: 9983155, Avg. loss: 96371052171448379571267128262656.000000\n",
      "Total training time: 21.37 seconds.\n",
      "-- Epoch 2966\n",
      "Norm: 6286045311208.73, NNZs: 843, Bias: -177140452.263219, T: 9986522, Avg. loss: 96355034949585228290599006765056.000000\n",
      "Total training time: 21.37 seconds.\n",
      "-- Epoch 2967\n",
      "Norm: 6144132175520.72, NNZs: 843, Bias: 87846073.011786, T: 9989889, Avg. loss: 96338717644353176888004338253824.000000\n",
      "Total training time: 21.38 seconds.\n",
      "-- Epoch 2968\n",
      "Norm: 6316937753645.41, NNZs: 843, Bias: -90019290.307824, T: 9993256, Avg. loss: 96322378379345009122232594595840.000000\n",
      "Total training time: 21.39 seconds.\n",
      "-- Epoch 2969\n",
      "Norm: 6081752717190.53, NNZs: 843, Bias: 87829792.671000, T: 9996623, Avg. loss: 96306006226810452388195516547072.000000\n",
      "Total training time: 21.39 seconds.\n",
      "-- Epoch 2970\n",
      "Norm: 6007653928289.91, NNZs: 843, Bias: -90007010.499806, T: 9999990, Avg. loss: 96289829153094165635560716107776.000000\n",
      "Total training time: 21.40 seconds.\n",
      "-- Epoch 2971\n",
      "Norm: 6080199357890.44, NNZs: 843, Bias: 123622631.218707, T: 10003357, Avg. loss: 96273747288569793893464344100864.000000\n",
      "Total training time: 21.41 seconds.\n",
      "-- Epoch 2972\n",
      "Norm: 6149741767988.34, NNZs: 843, Bias: 301408389.674027, T: 10006724, Avg. loss: 96257783889970072206411676254208.000000\n",
      "Total training time: 21.42 seconds.\n",
      "-- Epoch 2973\n",
      "Norm: 5960600428744.20, NNZs: 843, Bias: -231966536.129795, T: 10010091, Avg. loss: 96241249553216897961993208594432.000000\n",
      "Total training time: 21.42 seconds.\n",
      "-- Epoch 2974\n",
      "Norm: 6150930867757.72, NNZs: 843, Bias: -54179921.592239, T: 10013458, Avg. loss: 96225090753975405902134081748992.000000\n",
      "Total training time: 21.43 seconds.\n",
      "-- Epoch 2975\n",
      "Norm: 5984534237224.98, NNZs: 843, Bias: 123576995.151262, T: 10016825, Avg. loss: 96209303127730671977218342649856.000000\n",
      "Total training time: 21.44 seconds.\n",
      "-- Epoch 2976\n",
      "Norm: 6153273606791.65, NNZs: 843, Bias: 150706154.560971, T: 10020192, Avg. loss: 96193429174436584739593353428992.000000\n",
      "Total training time: 21.44 seconds.\n",
      "-- Epoch 2977\n",
      "Norm: 5694330719434.14, NNZs: 843, Bias: -27030527.031909, T: 10023559, Avg. loss: 96177674028589230588948213923840.000000\n",
      "Total training time: 21.45 seconds.\n",
      "-- Epoch 2978\n",
      "Norm: 5916783626566.07, NNZs: 843, Bias: -54309731.343728, T: 10026926, Avg. loss: 96161275404779580346033191780352.000000\n",
      "Total training time: 21.46 seconds.\n",
      "-- Epoch 2979\n",
      "Norm: 5886266694297.66, NNZs: 843, Bias: -231999368.266055, T: 10030293, Avg. loss: 96145207297460599977258826858496.000000\n",
      "Total training time: 21.47 seconds.\n",
      "-- Epoch 2980\n",
      "Norm: 6022740771737.64, NNZs: 843, Bias: -54302295.020163, T: 10033660, Avg. loss: 96128811583534935513463257038848.000000\n",
      "Total training time: 21.47 seconds.\n",
      "-- Epoch 2981\n",
      "Norm: 5865252060263.90, NNZs: 843, Bias: 123364570.773619, T: 10037027, Avg. loss: 96113075585862289032538849542144.000000\n",
      "Total training time: 21.48 seconds.\n",
      "-- Epoch 2982\n",
      "Norm: 5655973865705.05, NNZs: 843, Bias: -82410428.060269, T: 10040394, Avg. loss: 96097045740972602401769394274304.000000\n",
      "Total training time: 21.49 seconds.\n",
      "-- Epoch 2983\n",
      "Norm: 5895725591003.37, NNZs: 843, Bias: 450497908.607439, T: 10043761, Avg. loss: 96081018477668143628880420274176.000000\n",
      "Total training time: 21.50 seconds.\n",
      "-- Epoch 2984\n",
      "Norm: 5745270763755.37, NNZs: 843, Bias: 95468678.970479, T: 10047128, Avg. loss: 96065053228918303956734918524928.000000\n",
      "Total training time: 21.50 seconds.\n",
      "-- Epoch 2985\n",
      "Norm: 5755901083093.92, NNZs: 843, Bias: -82146330.327177, T: 10050495, Avg. loss: 96049254113394481889433444417536.000000\n",
      "Total training time: 21.51 seconds.\n",
      "-- Epoch 2986\n",
      "Norm: 5888849197771.91, NNZs: 843, Bias: -259731243.003181, T: 10053862, Avg. loss: 96033104761779752516630911909888.000000\n",
      "Total training time: 21.52 seconds.\n",
      "-- Epoch 2987\n",
      "Norm: 5987056490953.91, NNZs: 843, Bias: -82135286.816795, T: 10057229, Avg. loss: 96017101976175338899358997282816.000000\n",
      "Total training time: 21.52 seconds.\n",
      "-- Epoch 2988\n",
      "Norm: 5864767155466.02, NNZs: 843, Bias: -259690143.024533, T: 10060596, Avg. loss: 96000939757308658214156655132672.000000\n",
      "Total training time: 21.53 seconds.\n",
      "-- Epoch 2989\n",
      "Norm: 5894876515031.36, NNZs: 843, Bias: -82125293.193467, T: 10063963, Avg. loss: 95984999501881767657197843513344.000000\n",
      "Total training time: 21.54 seconds.\n",
      "-- Epoch 2990\n",
      "Norm: 5837887709214.28, NNZs: 843, Bias: -182707910.392844, T: 10067330, Avg. loss: 95968746718658349777271230824448.000000\n",
      "Total training time: 21.55 seconds.\n",
      "-- Epoch 2991\n",
      "Norm: 5957997109312.17, NNZs: 843, Bias: -464074792.365063, T: 10070697, Avg. loss: 95952555311605006379862152183808.000000\n",
      "Total training time: 21.55 seconds.\n",
      "-- Epoch 2992\n",
      "Norm: 6004837757950.37, NNZs: 843, Bias: -286536434.380977, T: 10074064, Avg. loss: 95936255104384290500666235813888.000000\n",
      "Total training time: 21.56 seconds.\n",
      "-- Epoch 2993\n",
      "Norm: 5896490551241.02, NNZs: 843, Bias: -109027867.410887, T: 10077431, Avg. loss: 95920533463939486039783661109248.000000\n",
      "Total training time: 21.57 seconds.\n",
      "-- Epoch 2994\n",
      "Norm: 5732172380207.21, NNZs: 843, Bias: 68449579.914880, T: 10080798, Avg. loss: 95904678098190281477799117062144.000000\n",
      "Total training time: 21.57 seconds.\n",
      "-- Epoch 2995\n",
      "Norm: 5761824318137.16, NNZs: 843, Bias: -109012943.066220, T: 10084165, Avg. loss: 95888432854065075712489435430912.000000\n",
      "Total training time: 21.58 seconds.\n",
      "-- Epoch 2996\n",
      "Norm: 5744774880777.87, NNZs: 843, Bias: 68434644.500185, T: 10087532, Avg. loss: 95872388585543465751176470855680.000000\n",
      "Total training time: 21.59 seconds.\n",
      "-- Epoch 2997\n",
      "Norm: 5759686561761.73, NNZs: 843, Bias: -108997785.099342, T: 10090899, Avg. loss: 95856325272210326017435087405056.000000\n",
      "Total training time: 21.60 seconds.\n",
      "-- Epoch 2998\n",
      "Norm: 5946382263043.27, NNZs: 843, Bias: 68421057.664595, T: 10094266, Avg. loss: 95840201907692693411377579032576.000000\n",
      "Total training time: 21.60 seconds.\n",
      "-- Epoch 2999\n",
      "Norm: 5935936091231.52, NNZs: 843, Bias: -108983685.062928, T: 10097633, Avg. loss: 95824131904710235237702657114112.000000\n",
      "Total training time: 21.61 seconds.\n",
      "-- Epoch 3000\n",
      "Norm: 5669298135221.55, NNZs: 843, Bias: 68405030.911822, T: 10101000, Avg. loss: 95808508108608711872356842209280.000000\n",
      "Total training time: 21.62 seconds.\n",
      "-- Epoch 3001\n",
      "Norm: 5631792711697.58, NNZs: 843, Bias: -108969605.703722, T: 10104367, Avg. loss: 95792203922649992886619411054592.000000\n",
      "Total training time: 21.62 seconds.\n",
      "-- Epoch 3002\n",
      "Norm: 5617325147754.18, NNZs: 843, Bias: -159536593.803795, T: 10107734, Avg. loss: 95776237907715214573699986358272.000000\n",
      "Total training time: 21.63 seconds.\n",
      "-- Epoch 3003\n",
      "Norm: 5630776189011.41, NNZs: 843, Bias: 41098835.878306, T: 10111101, Avg. loss: 95760445762388846853675202641920.000000\n",
      "Total training time: 21.64 seconds.\n",
      "-- Epoch 3004\n",
      "Norm: 5559929158547.12, NNZs: 843, Bias: -10400473.888594, T: 10114468, Avg. loss: 95744425443189931742117289263104.000000\n",
      "Total training time: 21.65 seconds.\n",
      "-- Epoch 3005\n",
      "Norm: 5658748345321.99, NNZs: 843, Bias: -187708437.242950, T: 10117835, Avg. loss: 95728442505157952897338690240512.000000\n",
      "Total training time: 21.65 seconds.\n",
      "-- Epoch 3006\n",
      "Norm: 5610589713889.15, NNZs: 843, Bias: -10401626.093921, T: 10121202, Avg. loss: 95712499946532656899870429282304.000000\n",
      "Total training time: 21.66 seconds.\n",
      "-- Epoch 3007\n",
      "Norm: 5833001918650.86, NNZs: 843, Bias: -177764188.570705, T: 10124569, Avg. loss: 95696594073010638913746805719040.000000\n",
      "Total training time: 21.67 seconds.\n",
      "-- Epoch 3008\n",
      "Norm: 5833400890500.00, NNZs: 843, Bias: -488275.300534, T: 10127936, Avg. loss: 95680466755591019356266220748800.000000\n",
      "Total training time: 21.67 seconds.\n",
      "-- Epoch 3009\n",
      "Norm: 5663902616965.12, NNZs: 843, Bias: -177739234.419904, T: 10131303, Avg. loss: 95664467615674787504601599836160.000000\n",
      "Total training time: 21.68 seconds.\n",
      "-- Epoch 3010\n",
      "Norm: 5624332185773.21, NNZs: 843, Bias: -190807170.300084, T: 10134670, Avg. loss: 95648414954506318912403382730752.000000\n",
      "Total training time: 21.69 seconds.\n",
      "-- Epoch 3011\n",
      "Norm: 5537044016119.43, NNZs: 843, Bias: -13574454.993766, T: 10138037, Avg. loss: 95632413225985098463675888435200.000000\n",
      "Total training time: 21.70 seconds.\n",
      "-- Epoch 3012\n",
      "Norm: 5560327935467.49, NNZs: 843, Bias: -166043405.998414, T: 10141404, Avg. loss: 95616563626198405948613930778624.000000\n",
      "Total training time: 21.70 seconds.\n",
      "-- Epoch 3013\n",
      "Norm: 5565741986163.26, NNZs: 843, Bias: 76439883.554694, T: 10144771, Avg. loss: 95600789558349693473781224308736.000000\n",
      "Total training time: 21.71 seconds.\n",
      "-- Epoch 3014\n",
      "Norm: 5714830050674.93, NNZs: 843, Bias: 253607991.865647, T: 10148138, Avg. loss: 95584788174951436160651719868416.000000\n",
      "Total training time: 21.72 seconds.\n",
      "-- Epoch 3015\n",
      "Norm: 5618325749240.52, NNZs: 843, Bias: 76424375.599334, T: 10151505, Avg. loss: 95568776436810712682755465936896.000000\n",
      "Total training time: 21.72 seconds.\n",
      "-- Epoch 3016\n",
      "Norm: 5688070857981.47, NNZs: 843, Bias: -100730550.656402, T: 10154872, Avg. loss: 95553009590257976038414625013760.000000\n",
      "Total training time: 21.73 seconds.\n",
      "-- Epoch 3017\n",
      "Norm: 5949087929072.54, NNZs: 843, Bias: 76406673.937336, T: 10158239, Avg. loss: 95536775034257531517696913440768.000000\n",
      "Total training time: 21.74 seconds.\n",
      "-- Epoch 3018\n",
      "Norm: 6326540888222.24, NNZs: 843, Bias: -454951301.466979, T: 10161606, Avg. loss: 95521258017609911142320084877312.000000\n",
      "Total training time: 21.75 seconds.\n",
      "-- Epoch 3019\n",
      "Norm: 6333371120947.13, NNZs: 843, Bias: 76390538.311690, T: 10164973, Avg. loss: 95505620139130439185350285852672.000000\n",
      "Total training time: 21.75 seconds.\n",
      "-- Epoch 3020\n",
      "Norm: 5998134182738.12, NNZs: 843, Bias: 253469849.320683, T: 10168340, Avg. loss: 95489832197605520796433977966592.000000\n",
      "Total training time: 21.76 seconds.\n",
      "-- Epoch 3021\n",
      "Norm: 6246324802439.17, NNZs: 843, Bias: 76374381.106666, T: 10171707, Avg. loss: 95473858098514423504007600275456.000000\n",
      "Total training time: 21.77 seconds.\n",
      "-- Epoch 3022\n",
      "Norm: 6224903000330.42, NNZs: 843, Bias: -100689622.833087, T: 10175074, Avg. loss: 95458467250873293778225148723200.000000\n",
      "Total training time: 21.77 seconds.\n",
      "-- Epoch 3023\n",
      "Norm: 5935289211480.80, NNZs: 843, Bias: 76360290.999202, T: 10178441, Avg. loss: 95442726568846598924092594716672.000000\n",
      "Total training time: 21.78 seconds.\n",
      "-- Epoch 3024\n",
      "Norm: 5810876345942.89, NNZs: 843, Bias: -100676181.632040, T: 10181808, Avg. loss: 95426911008763048720963943268352.000000\n",
      "Total training time: 21.79 seconds.\n",
      "-- Epoch 3025\n",
      "Norm: 6219182051348.63, NNZs: 843, Bias: -277683590.664330, T: 10185175, Avg. loss: 95411172402964921346109437116416.000000\n",
      "Total training time: 21.80 seconds.\n",
      "-- Epoch 3026\n",
      "Norm: 5873453828686.58, NNZs: 843, Bias: -178925797.189192, T: 10188542, Avg. loss: 95395576474321389536950599286784.000000\n",
      "Total training time: 21.80 seconds.\n",
      "-- Epoch 3027\n",
      "Norm: 5857789293942.43, NNZs: 843, Bias: 352041864.761938, T: 10191909, Avg. loss: 95379624817589572635391267176448.000000\n",
      "Total training time: 21.81 seconds.\n",
      "-- Epoch 3028\n",
      "Norm: 5895730969401.64, NNZs: 843, Bias: -178901290.018479, T: 10195276, Avg. loss: 95363980868665666979646041751552.000000\n",
      "Total training time: 21.82 seconds.\n",
      "-- Epoch 3029\n",
      "Norm: 5890012430121.24, NNZs: 843, Bias: -1934324.642630, T: 10198643, Avg. loss: 95348195185183848396205924024320.000000\n",
      "Total training time: 21.83 seconds.\n",
      "-- Epoch 3030\n",
      "Norm: 6302254571916.73, NNZs: 843, Bias: -178876925.441512, T: 10202010, Avg. loss: 95332396753017660941240237031424.000000\n",
      "Total training time: 21.83 seconds.\n",
      "-- Epoch 3031\n",
      "Norm: 6067078046217.06, NNZs: 843, Bias: -1938231.314732, T: 10205377, Avg. loss: 95316961332768719638364512321536.000000\n",
      "Total training time: 21.84 seconds.\n",
      "-- Epoch 3032\n",
      "Norm: 6008117398715.66, NNZs: 843, Bias: 174971862.825194, T: 10208744, Avg. loss: 95301198664290403280996075044864.000000\n",
      "Total training time: 21.85 seconds.\n",
      "-- Epoch 3033\n",
      "Norm: 5910006608114.33, NNZs: 843, Bias: -48053211.831319, T: 10212111, Avg. loss: 95285364593691286227192809259008.000000\n",
      "Total training time: 21.85 seconds.\n",
      "-- Epoch 3034\n",
      "Norm: 6074193193315.95, NNZs: 843, Bias: 347787398.989402, T: 10215478, Avg. loss: 95269685050834590948251913945088.000000\n",
      "Total training time: 21.86 seconds.\n",
      "-- Epoch 3035\n",
      "Norm: 5920381850805.88, NNZs: 843, Bias: -54404550.843377, T: 10218845, Avg. loss: 95253761043392852825373602217984.000000\n",
      "Total training time: 21.87 seconds.\n",
      "-- Epoch 3036\n",
      "Norm: 5714695652918.53, NNZs: 843, Bias: 95233788.676229, T: 10222212, Avg. loss: 95238228119184419645880115658752.000000\n",
      "Total training time: 21.88 seconds.\n",
      "-- Epoch 3037\n",
      "Norm: 5713449038033.73, NNZs: 843, Bias: -81614432.556081, T: 10225579, Avg. loss: 95222689722195684462259213434880.000000\n",
      "Total training time: 21.88 seconds.\n",
      "-- Epoch 3038\n",
      "Norm: 5796050839136.78, NNZs: 843, Bias: 95215608.661284, T: 10228946, Avg. loss: 95206843919246353902665304375296.000000\n",
      "Total training time: 21.89 seconds.\n",
      "-- Epoch 3039\n",
      "Norm: 5716229812929.47, NNZs: 843, Bias: -81604194.309449, T: 10232313, Avg. loss: 95191071986191198605421670563840.000000\n",
      "Total training time: 21.90 seconds.\n",
      "-- Epoch 3040\n",
      "Norm: 6215847929423.16, NNZs: 843, Bias: 95196708.900712, T: 10235680, Avg. loss: 95175624441521864791987371114496.000000\n",
      "Total training time: 21.90 seconds.\n",
      "-- Epoch 3041\n",
      "Norm: 5611860913834.36, NNZs: 843, Bias: -81593947.207145, T: 10239047, Avg. loss: 95159889962917948280352672841728.000000\n",
      "Total training time: 21.91 seconds.\n",
      "-- Epoch 3042\n",
      "Norm: 5996309926438.78, NNZs: 843, Bias: 95176866.773763, T: 10242414, Avg. loss: 95144399215487623093790984634368.000000\n",
      "Total training time: 21.92 seconds.\n",
      "-- Epoch 3043\n",
      "Norm: 5587751479384.30, NNZs: 843, Bias: -81583199.858661, T: 10245781, Avg. loss: 95128603176459932200858239893504.000000\n",
      "Total training time: 21.93 seconds.\n",
      "-- Epoch 3044\n",
      "Norm: 5704565467628.14, NNZs: 843, Bias: -258314472.607490, T: 10249148, Avg. loss: 95113240532871398638900303888384.000000\n",
      "Total training time: 21.93 seconds.\n",
      "-- Epoch 3045\n",
      "Norm: 5625304359374.38, NNZs: 843, Bias: -81571667.974995, T: 10252515, Avg. loss: 95097649210662868235600466542592.000000\n",
      "Total training time: 21.94 seconds.\n",
      "-- Epoch 3046\n",
      "Norm: 5759612118226.51, NNZs: 843, Bias: -258276494.937803, T: 10255882, Avg. loss: 95082023232476586883957078884352.000000\n",
      "Total training time: 21.95 seconds.\n",
      "-- Epoch 3047\n",
      "Norm: 5819289492205.99, NNZs: 843, Bias: -81563118.204778, T: 10259249, Avg. loss: 95066395944316014113672562475008.000000\n",
      "Total training time: 21.96 seconds.\n",
      "-- Epoch 3048\n",
      "Norm: 6266057006970.00, NNZs: 843, Bias: -611596107.378108, T: 10262616, Avg. loss: 95050718359095930391952145514496.000000\n",
      "Total training time: 21.96 seconds.\n",
      "-- Epoch 3049\n",
      "Norm: 5678936689381.97, NNZs: 843, Bias: -32666941.475573, T: 10265983, Avg. loss: 95035342370074838356035240984576.000000\n",
      "Total training time: 21.97 seconds.\n",
      "-- Epoch 3050\n",
      "Norm: 5663024908795.09, NNZs: 843, Bias: 17701806.902862, T: 10269350, Avg. loss: 95019672650263493521077709045760.000000\n",
      "Total training time: 21.98 seconds.\n",
      "-- Epoch 3051\n",
      "Norm: 5968682504830.06, NNZs: 843, Bias: -158938048.695540, T: 10272717, Avg. loss: 95003751120571179599492796121088.000000\n",
      "Total training time: 21.98 seconds.\n",
      "-- Epoch 3052\n",
      "Norm: 5976307796501.94, NNZs: 843, Bias: 17693042.214589, T: 10276084, Avg. loss: 94988092992427431772857789579264.000000\n",
      "Total training time: 21.99 seconds.\n",
      "-- Epoch 3053\n",
      "Norm: 6106198348818.06, NNZs: 843, Bias: -43183393.297001, T: 10279451, Avg. loss: 94972661913084824696981498626048.000000\n",
      "Total training time: 22.00 seconds.\n",
      "-- Epoch 3054\n",
      "Norm: 5746614172747.14, NNZs: 843, Bias: 133409876.901202, T: 10282818, Avg. loss: 94956943667112262669006238908416.000000\n",
      "Total training time: 22.01 seconds.\n",
      "-- Epoch 3055\n",
      "Norm: 6223672891174.17, NNZs: 843, Bias: -43180255.148046, T: 10286185, Avg. loss: 94941324539184020748471229218816.000000\n",
      "Total training time: 22.01 seconds.\n",
      "-- Epoch 3056\n",
      "Norm: 5814668872838.84, NNZs: 843, Bias: -211299825.821508, T: 10289552, Avg. loss: 94925859185727369922306583298048.000000\n",
      "Total training time: 22.02 seconds.\n",
      "-- Epoch 3057\n",
      "Norm: 5957476407441.20, NNZs: 843, Bias: -333107713.340393, T: 10292919, Avg. loss: 94910226894117799257648427696128.000000\n",
      "Total training time: 22.03 seconds.\n",
      "-- Epoch 3058\n",
      "Norm: 6240560579287.66, NNZs: 843, Bias: 196520629.524863, T: 10296286, Avg. loss: 94894787918246201567510425239552.000000\n",
      "Total training time: 22.04 seconds.\n",
      "-- Epoch 3059\n",
      "Norm: 5875245994702.79, NNZs: 843, Bias: 19980701.671715, T: 10299653, Avg. loss: 94879773803885361372749113589760.000000\n",
      "Total training time: 22.04 seconds.\n",
      "-- Epoch 3060\n",
      "Norm: 6201709413852.50, NNZs: 843, Bias: 370288642.382546, T: 10303020, Avg. loss: 94864121056164760523106794078208.000000\n",
      "Total training time: 22.05 seconds.\n",
      "-- Epoch 3061\n",
      "Norm: 5964921070287.77, NNZs: 843, Bias: -159218321.399876, T: 10306387, Avg. loss: 94848946919465543781590899884032.000000\n",
      "Total training time: 22.06 seconds.\n",
      "-- Epoch 3062\n",
      "Norm: 5884977555088.60, NNZs: 843, Bias: -47855147.009211, T: 10309754, Avg. loss: 94833722129071121081608821538816.000000\n",
      "Total training time: 22.06 seconds.\n",
      "-- Epoch 3063\n",
      "Norm: 5844080937967.22, NNZs: 843, Bias: 128610258.841979, T: 10313121, Avg. loss: 94818323251896174932033290633216.000000\n",
      "Total training time: 22.07 seconds.\n",
      "-- Epoch 3064\n",
      "Norm: 6295424155369.25, NNZs: 843, Bias: -480249576.997351, T: 10316488, Avg. loss: 94802812013971277557394460114944.000000\n",
      "Total training time: 22.08 seconds.\n",
      "-- Epoch 3065\n",
      "Norm: 5935461748436.88, NNZs: 843, Bias: -175837142.990077, T: 10319855, Avg. loss: 94787267706628789608235797577728.000000\n",
      "Total training time: 22.09 seconds.\n",
      "-- Epoch 3066\n",
      "Norm: 5989095224313.84, NNZs: 843, Bias: 594350.397712, T: 10323222, Avg. loss: 94771930624568445016752202448896.000000\n",
      "Total training time: 22.09 seconds.\n",
      "-- Epoch 3067\n",
      "Norm: 5869581587081.83, NNZs: 843, Bias: 36202408.146635, T: 10326589, Avg. loss: 94756277701405995385633539883008.000000\n",
      "Total training time: 22.10 seconds.\n",
      "-- Epoch 3068\n",
      "Norm: 5783131478226.51, NNZs: 843, Bias: 68437202.485900, T: 10329956, Avg. loss: 94740812207682410307607072866304.000000\n",
      "Total training time: 22.11 seconds.\n",
      "-- Epoch 3069\n",
      "Norm: 5768411071557.62, NNZs: 843, Bias: 244806251.866834, T: 10333323, Avg. loss: 94725451144496607622540856131584.000000\n",
      "Total training time: 22.11 seconds.\n",
      "-- Epoch 3070\n",
      "Norm: 6170464254957.18, NNZs: 843, Bias: -284301524.029849, T: 10336690, Avg. loss: 94710077095052512494103646175232.000000\n",
      "Total training time: 22.12 seconds.\n",
      "-- Epoch 3071\n",
      "Norm: 5837829271443.51, NNZs: 843, Bias: 244763413.574923, T: 10340057, Avg. loss: 94694552637901698817521451270144.000000\n",
      "Total training time: 22.13 seconds.\n",
      "-- Epoch 3072\n",
      "Norm: 5988815987617.77, NNZs: 843, Bias: -284256384.540027, T: 10343424, Avg. loss: 94679193163065012379060080214016.000000\n",
      "Total training time: 22.14 seconds.\n",
      "-- Epoch 3073\n",
      "Norm: 6009550988314.19, NNZs: 843, Bias: 244723099.590369, T: 10346791, Avg. loss: 94663966413258947379124225703936.000000\n",
      "Total training time: 22.14 seconds.\n",
      "-- Epoch 3074\n",
      "Norm: 5811382158773.42, NNZs: 843, Bias: 68397330.755634, T: 10350158, Avg. loss: 94648760551845255461053847830528.000000\n",
      "Total training time: 22.15 seconds.\n",
      "-- Epoch 3075\n",
      "Norm: 6003865155566.18, NNZs: 843, Bias: -121958121.718217, T: 10353525, Avg. loss: 94633383772284766805859908976640.000000\n",
      "Total training time: 22.16 seconds.\n",
      "-- Epoch 3076\n",
      "Norm: 5868809298395.78, NNZs: 843, Bias: -212160698.395748, T: 10356892, Avg. loss: 94618182696387770322762728472576.000000\n",
      "Total training time: 22.16 seconds.\n",
      "-- Epoch 3077\n",
      "Norm: 5794306958042.40, NNZs: 843, Bias: -35883704.555463, T: 10360259, Avg. loss: 94603017282373802583033223577600.000000\n",
      "Total training time: 22.17 seconds.\n",
      "-- Epoch 3078\n",
      "Norm: 5794526085391.44, NNZs: 843, Bias: -46539692.277994, T: 10363626, Avg. loss: 94587864121716897576794363789312.000000\n",
      "Total training time: 22.18 seconds.\n",
      "-- Epoch 3079\n",
      "Norm: 5931628648691.58, NNZs: 843, Bias: -222768762.125527, T: 10366993, Avg. loss: 94572445783146735547079154728960.000000\n",
      "Total training time: 22.19 seconds.\n",
      "-- Epoch 3080\n",
      "Norm: 5983202305870.45, NNZs: 843, Bias: 305904545.038377, T: 10370360, Avg. loss: 94557072282291461304999378681856.000000\n",
      "Total training time: 22.19 seconds.\n",
      "-- Epoch 3081\n",
      "Norm: 5819858804803.35, NNZs: 843, Bias: 129674161.245911, T: 10373727, Avg. loss: 94541759655087895752793156222976.000000\n",
      "Total training time: 22.20 seconds.\n",
      "-- Epoch 3082\n",
      "Norm: 5801294187392.88, NNZs: 843, Bias: -46527227.567799, T: 10377094, Avg. loss: 94526611388232145550031152218112.000000\n",
      "Total training time: 22.21 seconds.\n",
      "-- Epoch 3083\n",
      "Norm: 5926650956994.21, NNZs: 843, Bias: -80304381.008917, T: 10380461, Avg. loss: 94511102738872460980546258862080.000000\n",
      "Total training time: 22.21 seconds.\n",
      "-- Epoch 3084\n",
      "Norm: 5873442728988.24, NNZs: 843, Bias: 95861520.282585, T: 10383828, Avg. loss: 94496041367690859287066991132672.000000\n",
      "Total training time: 22.22 seconds.\n",
      "-- Epoch 3085\n",
      "Norm: 6053229614985.48, NNZs: 843, Bias: -80294715.250480, T: 10387195, Avg. loss: 94480726251791366236085113847808.000000\n",
      "Total training time: 22.23 seconds.\n",
      "-- Epoch 3086\n",
      "Norm: 6035028751443.72, NNZs: 843, Bias: 95842031.617095, T: 10390562, Avg. loss: 94465179985501953425321781886976.000000\n",
      "Total training time: 22.24 seconds.\n",
      "-- Epoch 3087\n",
      "Norm: 6106446207606.56, NNZs: 843, Bias: -80286218.346800, T: 10393929, Avg. loss: 94449813177336665961989517869056.000000\n",
      "Total training time: 22.24 seconds.\n",
      "-- Epoch 3088\n",
      "Norm: 6100544366005.70, NNZs: 843, Bias: -256386007.885814, T: 10397296, Avg. loss: 94434647688665716551406374092800.000000\n",
      "Total training time: 22.25 seconds.\n",
      "-- Epoch 3089\n",
      "Norm: 5932574581439.09, NNZs: 843, Bias: -80276686.818851, T: 10400663, Avg. loss: 94419477438175166268343637245952.000000\n",
      "Total training time: 22.26 seconds.\n",
      "-- Epoch 3090\n",
      "Norm: 6069293544161.58, NNZs: 843, Bias: -256347604.957802, T: 10404030, Avg. loss: 94403941671765796871210067296256.000000\n",
      "Total training time: 22.26 seconds.\n",
      "-- Epoch 3091\n",
      "Norm: 5918574908104.78, NNZs: 843, Bias: -80268195.073377, T: 10407397, Avg. loss: 94388701935566097251901481418752.000000\n",
      "Total training time: 22.27 seconds.\n",
      "-- Epoch 3092\n",
      "Norm: 6009275751846.74, NNZs: 843, Bias: 277478761.929546, T: 10410764, Avg. loss: 94373430322520507187448151801856.000000\n",
      "Total training time: 22.28 seconds.\n",
      "-- Epoch 3093\n",
      "Norm: 5672053289533.71, NNZs: 843, Bias: 101421841.611221, T: 10414131, Avg. loss: 94358213487689700169181005086720.000000\n",
      "Total training time: 22.29 seconds.\n",
      "-- Epoch 3094\n",
      "Norm: 5783926871498.73, NNZs: 843, Bias: -191250738.499857, T: 10417498, Avg. loss: 94343200591225832032372386693120.000000\n",
      "Total training time: 22.29 seconds.\n",
      "-- Epoch 3095\n",
      "Norm: 5771004087319.17, NNZs: 843, Bias: -15233465.693478, T: 10420865, Avg. loss: 94327812085212287338654807883776.000000\n",
      "Total training time: 22.30 seconds.\n",
      "-- Epoch 3096\n",
      "Norm: 5528769217865.88, NNZs: 843, Bias: -34984192.369996, T: 10424232, Avg. loss: 94312658679906718077137193009152.000000\n",
      "Total training time: 22.31 seconds.\n",
      "-- Epoch 3097\n",
      "Norm: 5719135422771.36, NNZs: 843, Bias: 140993980.478676, T: 10427599, Avg. loss: 94297261626003380515792335405056.000000\n",
      "Total training time: 22.31 seconds.\n",
      "-- Epoch 3098\n",
      "Norm: 5743048483344.01, NNZs: 843, Bias: 147853070.722428, T: 10430966, Avg. loss: 94282221829742933143732482998272.000000\n",
      "Total training time: 22.32 seconds.\n",
      "-- Epoch 3099\n",
      "Norm: 5931952643340.62, NNZs: 843, Bias: -380002409.177482, T: 10434333, Avg. loss: 94267055496318723173947520581632.000000\n",
      "Total training time: 22.33 seconds.\n",
      "-- Epoch 3100\n",
      "Norm: 5735246660579.21, NNZs: 843, Bias: 172318360.159532, T: 10437700, Avg. loss: 94251799506139273771753643442176.000000\n",
      "Total training time: 22.34 seconds.\n",
      "-- Epoch 3101\n",
      "Norm: 5668707072460.58, NNZs: 843, Bias: -121875084.582158, T: 10441067, Avg. loss: 94236720621649941350253437386752.000000\n",
      "Total training time: 22.34 seconds.\n",
      "-- Epoch 3102\n",
      "Norm: 5750321532618.71, NNZs: 843, Bias: 54038167.899399, T: 10444434, Avg. loss: 94221586632966800071792581935104.000000\n",
      "Total training time: 22.35 seconds.\n",
      "-- Epoch 3103\n",
      "Norm: 6048155241950.03, NNZs: 843, Bias: 229922500.943908, T: 10447801, Avg. loss: 94206724895506229187771296120832.000000\n",
      "Total training time: 22.36 seconds.\n",
      "-- Epoch 3104\n",
      "Norm: 5818449821937.19, NNZs: 843, Bias: 54024257.914337, T: 10451168, Avg. loss: 94191436408471067230675379683328.000000\n",
      "Total training time: 22.37 seconds.\n",
      "-- Epoch 3105\n",
      "Norm: 5775668619063.88, NNZs: 843, Bias: -121844822.921758, T: 10454535, Avg. loss: 94176353822774762104413239640064.000000\n",
      "Total training time: 22.37 seconds.\n",
      "-- Epoch 3106\n",
      "Norm: 5881491370608.00, NNZs: 843, Bias: 54010779.867903, T: 10457902, Avg. loss: 94161206872236137006683951988736.000000\n",
      "Total training time: 22.38 seconds.\n",
      "-- Epoch 3107\n",
      "Norm: 5582074476718.42, NNZs: 843, Bias: -121829685.582094, T: 10461269, Avg. loss: 94146095937188897975435666653184.000000\n",
      "Total training time: 22.39 seconds.\n",
      "-- Epoch 3108\n",
      "Norm: 5654800508679.33, NNZs: 843, Bias: 53998606.564723, T: 10464636, Avg. loss: 94130740042624473807919037546496.000000\n",
      "Total training time: 22.39 seconds.\n",
      "-- Epoch 3109\n",
      "Norm: 5765898560921.44, NNZs: 843, Bias: -121812745.654880, T: 10468003, Avg. loss: 94115926943910174855986281447424.000000\n",
      "Total training time: 22.40 seconds.\n",
      "-- Epoch 3110\n",
      "Norm: 5800407496875.48, NNZs: 843, Bias: 53986921.957163, T: 10471370, Avg. loss: 94100860236260630606244405575680.000000\n",
      "Total training time: 22.41 seconds.\n",
      "-- Epoch 3111\n",
      "Norm: 5837206409695.81, NNZs: 843, Bias: -121796499.527187, T: 10474737, Avg. loss: 94085909205016557554998001532928.000000\n",
      "Total training time: 22.42 seconds.\n",
      "-- Epoch 3112\n",
      "Norm: 5998982650449.83, NNZs: 843, Bias: 309430769.947262, T: 10478104, Avg. loss: 94070663406934109402116982308864.000000\n",
      "Total training time: 22.42 seconds.\n",
      "-- Epoch 3113\n",
      "Norm: 5881978854990.19, NNZs: 843, Bias: 133654503.030290, T: 10481471, Avg. loss: 94055537904861037466390464823296.000000\n",
      "Total training time: 22.43 seconds.\n",
      "-- Epoch 3114\n",
      "Norm: 5983109366331.19, NNZs: 843, Bias: 268010298.667978, T: 10484838, Avg. loss: 94040288444139867356796135931904.000000\n",
      "Total training time: 22.44 seconds.\n",
      "-- Epoch 3115\n",
      "Norm: 5987245393777.12, NNZs: 843, Bias: -120835036.264526, T: 10488205, Avg. loss: 94025153112339670382738763939840.000000\n",
      "Total training time: 22.44 seconds.\n",
      "-- Epoch 3116\n",
      "Norm: 5892649464931.12, NNZs: 843, Bias: 29226701.625891, T: 10491572, Avg. loss: 94010031053700480833615306948608.000000\n",
      "Total training time: 22.45 seconds.\n",
      "-- Epoch 3117\n",
      "Norm: 5930861922481.04, NNZs: 843, Bias: -146472264.544713, T: 10494939, Avg. loss: 93995221182100807080221205331968.000000\n",
      "Total training time: 22.46 seconds.\n",
      "-- Epoch 3118\n",
      "Norm: 5809856830821.52, NNZs: 843, Bias: -16393743.694085, T: 10498306, Avg. loss: 93980319792380948146044364914688.000000\n",
      "Total training time: 22.47 seconds.\n",
      "-- Epoch 3119\n",
      "Norm: 6143957549626.70, NNZs: 843, Bias: -550858181.352258, T: 10501673, Avg. loss: 93965528192517466180818957238272.000000\n",
      "Total training time: 22.47 seconds.\n",
      "-- Epoch 3120\n",
      "Norm: 5948051783272.62, NNZs: 843, Bias: -23862504.736325, T: 10505040, Avg. loss: 93950738501575320733133958021120.000000\n",
      "Total training time: 22.48 seconds.\n",
      "-- Epoch 3121\n",
      "Norm: 5907243337564.51, NNZs: 843, Bias: 75326582.971116, T: 10508407, Avg. loss: 93935525542704878933268215365632.000000\n",
      "Total training time: 22.49 seconds.\n",
      "-- Epoch 3122\n",
      "Norm: 5827790285297.93, NNZs: 843, Bias: -107696743.870338, T: 10511774, Avg. loss: 93920424541348272667971584786432.000000\n",
      "Total training time: 22.49 seconds.\n",
      "-- Epoch 3123\n",
      "Norm: 6015251227746.55, NNZs: 843, Bias: 67919665.359918, T: 10515141, Avg. loss: 93905483326280731966637101547520.000000\n",
      "Total training time: 22.50 seconds.\n",
      "-- Epoch 3124\n",
      "Norm: 6067662895782.28, NNZs: 843, Bias: -107683771.147600, T: 10518508, Avg. loss: 93890019472649938708324566433792.000000\n",
      "Total training time: 22.51 seconds.\n",
      "-- Epoch 3125\n",
      "Norm: 5959773771477.10, NNZs: 843, Bias: -283257040.303224, T: 10521875, Avg. loss: 93874832320272863116023518199808.000000\n",
      "Total training time: 22.52 seconds.\n",
      "-- Epoch 3126\n",
      "Norm: 6159080500087.49, NNZs: 843, Bias: 243464122.961750, T: 10525242, Avg. loss: 93859943322203631896876449529856.000000\n",
      "Total training time: 22.52 seconds.\n",
      "-- Epoch 3127\n",
      "Norm: 6073990466162.35, NNZs: 843, Bias: 41130813.163149, T: 10528609, Avg. loss: 93844911276789284032934766968832.000000\n",
      "Total training time: 22.53 seconds.\n",
      "-- Epoch 3128\n",
      "Norm: 6236529691444.94, NNZs: 843, Bias: -485489609.671968, T: 10531976, Avg. loss: 93829958065396478417603328475136.000000\n",
      "Total training time: 22.54 seconds.\n",
      "-- Epoch 3129\n",
      "Norm: 6130927766236.12, NNZs: 843, Bias: -309927210.675596, T: 10535343, Avg. loss: 93814694557720566700655727607808.000000\n",
      "Total training time: 22.55 seconds.\n",
      "-- Epoch 3130\n",
      "Norm: 5909152270584.09, NNZs: 843, Bias: -138333231.894909, T: 10538710, Avg. loss: 93799859199474613427430316048384.000000\n",
      "Total training time: 22.55 seconds.\n",
      "-- Epoch 3131\n",
      "Norm: 6058985053284.45, NNZs: 843, Bias: -313819926.546881, T: 10542077, Avg. loss: 93784898603678315758783417024512.000000\n",
      "Total training time: 22.56 seconds.\n",
      "-- Epoch 3132\n",
      "Norm: 5906083463733.16, NNZs: 843, Bias: 212649568.966086, T: 10545444, Avg. loss: 93770375946540860225429075656704.000000\n",
      "Total training time: 22.57 seconds.\n",
      "-- Epoch 3133\n",
      "Norm: 6295726524019.88, NNZs: 843, Bias: -313773354.125905, T: 10548811, Avg. loss: 93755125421749450355801153601536.000000\n",
      "Total training time: 22.57 seconds.\n",
      "-- Epoch 3134\n",
      "Norm: 5965726825405.73, NNZs: 843, Bias: -138295308.158687, T: 10552178, Avg. loss: 93740236591967847708473262342144.000000\n",
      "Total training time: 22.58 seconds.\n",
      "-- Epoch 3135\n",
      "Norm: 6160089988843.74, NNZs: 843, Bias: 388035404.394233, T: 10555545, Avg. loss: 93724957523269819887468672974848.000000\n",
      "Total training time: 22.59 seconds.\n",
      "-- Epoch 3136\n",
      "Norm: 6140219329837.06, NNZs: 843, Bias: 212576260.241785, T: 10558912, Avg. loss: 93710009248814114680929947484160.000000\n",
      "Total training time: 22.60 seconds.\n",
      "-- Epoch 3137\n",
      "Norm: 5903482918016.09, NNZs: 843, Bias: 176640770.648278, T: 10562279, Avg. loss: 93695351540885534884136948334592.000000\n",
      "Total training time: 22.60 seconds.\n",
      "-- Epoch 3138\n",
      "Norm: 5942614789088.94, NNZs: 843, Bias: -413121956.441437, T: 10565646, Avg. loss: 93680376622341224158574376124416.000000\n",
      "Total training time: 22.61 seconds.\n",
      "-- Epoch 3139\n",
      "Norm: 5868212282929.58, NNZs: 843, Bias: -83405646.709003, T: 10569013, Avg. loss: 93665418793987261814721933737984.000000\n",
      "Total training time: 22.62 seconds.\n",
      "-- Epoch 3140\n",
      "Norm: 5951913047915.14, NNZs: 843, Bias: -167464242.795175, T: 10572380, Avg. loss: 93650503462060948257479694745600.000000\n",
      "Total training time: 22.62 seconds.\n",
      "-- Epoch 3141\n",
      "Norm: 6453549478668.18, NNZs: 843, Bias: 245158509.945821, T: 10575747, Avg. loss: 93635589720052012423483765030912.000000\n",
      "Total training time: 22.63 seconds.\n",
      "-- Epoch 3142\n",
      "Norm: 6119454635110.55, NNZs: 843, Bias: 420479475.024611, T: 10579114, Avg. loss: 93620482579455822020347978317824.000000\n",
      "Total training time: 22.64 seconds.\n",
      "-- Epoch 3143\n",
      "Norm: 6518718517204.38, NNZs: 843, Bias: 19253917.189693, T: 10582481, Avg. loss: 93605673803402171923469432782848.000000\n",
      "Total training time: 22.65 seconds.\n",
      "-- Epoch 3144\n",
      "Norm: 6229835158853.85, NNZs: 843, Bias: -156064848.191752, T: 10585848, Avg. loss: 93590839994594648447533499547648.000000\n",
      "Total training time: 22.65 seconds.\n",
      "-- Epoch 3145\n",
      "Norm: 5987879876856.35, NNZs: 843, Bias: -355527789.233009, T: 10589215, Avg. loss: 93576121669421040137683102334976.000000\n",
      "Total training time: 22.66 seconds.\n",
      "-- Epoch 3146\n",
      "Norm: 5931217100171.31, NNZs: 843, Bias: 170360661.864508, T: 10592582, Avg. loss: 93561459793482954717142926229504.000000\n",
      "Total training time: 22.67 seconds.\n",
      "-- Epoch 3147\n",
      "Norm: 5976610565110.99, NNZs: 843, Bias: -4926882.844324, T: 10595949, Avg. loss: 93546804537957090058839437344768.000000\n",
      "Total training time: 22.67 seconds.\n",
      "-- Epoch 3148\n",
      "Norm: 5734168049375.03, NNZs: 843, Bias: -100208787.717845, T: 10599316, Avg. loss: 93531863591272597980191941197824.000000\n",
      "Total training time: 22.68 seconds.\n",
      "-- Epoch 3149\n",
      "Norm: 5834870012814.80, NNZs: 843, Bias: -275446406.415466, T: 10602683, Avg. loss: 93516848485211067402313671901184.000000\n",
      "Total training time: 22.69 seconds.\n",
      "-- Epoch 3150\n",
      "Norm: 5919051525266.29, NNZs: 843, Bias: 250267569.661865, T: 10606050, Avg. loss: 93502048091595372185808862707712.000000\n",
      "Total training time: 22.70 seconds.\n",
      "-- Epoch 3151\n",
      "Norm: 5865560177886.47, NNZs: 843, Bias: -350899956.825875, T: 10609417, Avg. loss: 93487064945843340665665395621888.000000\n",
      "Total training time: 22.70 seconds.\n",
      "-- Epoch 3152\n",
      "Norm: 5762021859730.35, NNZs: 843, Bias: 174736413.509348, T: 10612784, Avg. loss: 93472055605981517719884061999104.000000\n",
      "Total training time: 22.71 seconds.\n",
      "-- Epoch 3153\n",
      "Norm: 5859761166601.69, NNZs: 843, Bias: -322527679.634825, T: 10616151, Avg. loss: 93457443034841428613265635672064.000000\n",
      "Total training time: 22.72 seconds.\n",
      "-- Epoch 3154\n",
      "Norm: 6041570568463.66, NNZs: 843, Bias: -147326724.497431, T: 10619518, Avg. loss: 93442810948352135399480533975040.000000\n",
      "Total training time: 22.72 seconds.\n",
      "-- Epoch 3155\n",
      "Norm: 5582790165600.76, NNZs: 843, Bias: 27843804.557363, T: 10622885, Avg. loss: 93428079057758619603106040643584.000000\n",
      "Total training time: 22.73 seconds.\n",
      "-- Epoch 3156\n",
      "Norm: 5586805693908.97, NNZs: 843, Bias: 70232826.061844, T: 10626252, Avg. loss: 93413497490308398684485579702272.000000\n",
      "Total training time: 22.74 seconds.\n",
      "-- Epoch 3157\n",
      "Norm: 5642003107836.35, NNZs: 843, Bias: -291228380.304620, T: 10629619, Avg. loss: 93398766651809132504642547089408.000000\n",
      "Total training time: 22.75 seconds.\n",
      "-- Epoch 3158\n",
      "Norm: 6085445234930.73, NNZs: 843, Bias: 116082164.245057, T: 10632986, Avg. loss: 93383966688547203515707459371008.000000\n",
      "Total training time: 22.75 seconds.\n",
      "-- Epoch 3159\n",
      "Norm: 6047419267824.94, NNZs: 843, Bias: -59034942.550588, T: 10636353, Avg. loss: 93369508546682257470370575548416.000000\n",
      "Total training time: 22.76 seconds.\n",
      "-- Epoch 3160\n",
      "Norm: 5702900837533.35, NNZs: 843, Bias: 211818245.863568, T: 10639720, Avg. loss: 93354797732082412315260598878208.000000\n",
      "Total training time: 22.77 seconds.\n",
      "-- Epoch 3161\n",
      "Norm: 5456128952245.58, NNZs: 843, Bias: 36720987.390617, T: 10643087, Avg. loss: 93340145099997131573309902684160.000000\n",
      "Total training time: 22.78 seconds.\n",
      "-- Epoch 3162\n",
      "Norm: 5663084948499.18, NNZs: 843, Bias: -138347288.687243, T: 10646454, Avg. loss: 93325480080554072295537501011968.000000\n",
      "Total training time: 22.78 seconds.\n",
      "-- Epoch 3163\n",
      "Norm: 5834280140645.70, NNZs: 843, Bias: -122870302.677376, T: 10649821, Avg. loss: 93310551691636164734623881887744.000000\n",
      "Total training time: 22.79 seconds.\n",
      "-- Epoch 3164\n",
      "Norm: 5899244029009.57, NNZs: 843, Bias: 7570021.397111, T: 10653188, Avg. loss: 93295822527276885543017221455872.000000\n",
      "Total training time: 22.80 seconds.\n",
      "-- Epoch 3165\n",
      "Norm: 6223314989057.84, NNZs: 843, Bias: 182590535.304857, T: 10656555, Avg. loss: 93280945779080588270438906331136.000000\n",
      "Total training time: 22.80 seconds.\n",
      "-- Epoch 3166\n",
      "Norm: 6012584902055.06, NNZs: 843, Bias: 7566075.376269, T: 10659922, Avg. loss: 93266088318066924622013957931008.000000\n",
      "Total training time: 22.81 seconds.\n",
      "-- Epoch 3167\n",
      "Norm: 6197133371945.39, NNZs: 843, Bias: -56024454.879384, T: 10663289, Avg. loss: 93251490460906709528041882648576.000000\n",
      "Total training time: 22.82 seconds.\n",
      "-- Epoch 3168\n",
      "Norm: 6350099662891.94, NNZs: 843, Bias: -580965528.000033, T: 10666656, Avg. loss: 93236727588565673576151407132672.000000\n",
      "Total training time: 22.83 seconds.\n",
      "-- Epoch 3169\n",
      "Norm: 5626951306035.63, NNZs: 843, Bias: -56015895.928054, T: 10670023, Avg. loss: 93222088782097029799950877196288.000000\n",
      "Total training time: 22.83 seconds.\n",
      "-- Epoch 3170\n",
      "Norm: 5766027004106.79, NNZs: 843, Bias: 118943460.994032, T: 10673390, Avg. loss: 93207302479251452850298819706880.000000\n",
      "Total training time: 22.84 seconds.\n",
      "-- Epoch 3171\n",
      "Norm: 5679230882558.30, NNZs: 843, Bias: 293873124.370438, T: 10676757, Avg. loss: 93192820785878764630250111369216.000000\n",
      "Total training time: 22.85 seconds.\n",
      "-- Epoch 3172\n",
      "Norm: 5710281010099.08, NNZs: 843, Bias: -230931871.256910, T: 10680124, Avg. loss: 93178056351138749801430796730368.000000\n",
      "Total training time: 22.85 seconds.\n",
      "-- Epoch 3173\n",
      "Norm: 5900599717062.15, NNZs: 843, Bias: 293823418.990356, T: 10683491, Avg. loss: 93163448568288744837914357137408.000000\n",
      "Total training time: 22.86 seconds.\n",
      "-- Epoch 3174\n",
      "Norm: 5636081203179.09, NNZs: 843, Bias: -230897862.095146, T: 10686858, Avg. loss: 93148814229281277769518642364416.000000\n",
      "Total training time: 22.87 seconds.\n",
      "-- Epoch 3175\n",
      "Norm: 5522648684573.14, NNZs: 843, Bias: 15721706.735621, T: 10690225, Avg. loss: 93134154260226397833554592530432.000000\n",
      "Total training time: 22.88 seconds.\n",
      "-- Epoch 3176\n",
      "Norm: 5668605842966.98, NNZs: 843, Bias: -87530383.830678, T: 10693592, Avg. loss: 93119712731076708911594032070656.000000\n",
      "Total training time: 22.88 seconds.\n",
      "-- Epoch 3177\n",
      "Norm: 5586233357824.33, NNZs: 843, Bias: 87333162.497234, T: 10696959, Avg. loss: 93104914349856028430379106959360.000000\n",
      "Total training time: 22.89 seconds.\n",
      "-- Epoch 3178\n",
      "Norm: 5773818927713.55, NNZs: 843, Bias: -87518798.392380, T: 10700326, Avg. loss: 93090033049730408933321674326016.000000\n",
      "Total training time: 22.90 seconds.\n",
      "-- Epoch 3179\n",
      "Norm: 5995030588923.89, NNZs: 843, Bias: -127125203.007058, T: 10703693, Avg. loss: 93075357915810212243140083974144.000000\n",
      "Total training time: 22.90 seconds.\n",
      "-- Epoch 3180\n",
      "Norm: 5751340820247.06, NNZs: 843, Bias: 47699653.874312, T: 10707060, Avg. loss: 93060785434046070845082622230528.000000\n",
      "Total training time: 22.91 seconds.\n",
      "-- Epoch 3181\n",
      "Norm: 6012772554815.77, NNZs: 843, Bias: 222495854.853110, T: 10710427, Avg. loss: 93046318648584003185378347450368.000000\n",
      "Total training time: 22.92 seconds.\n",
      "-- Epoch 3182\n",
      "Norm: 5582155431880.85, NNZs: 843, Bias: 47688273.895956, T: 10713794, Avg. loss: 93031484329752721022597426315264.000000\n",
      "Total training time: 22.93 seconds.\n",
      "-- Epoch 3183\n",
      "Norm: 5987661713189.24, NNZs: 843, Bias: 222458727.503138, T: 10717161, Avg. loss: 93016549140282426657111290150912.000000\n",
      "Total training time: 22.93 seconds.\n",
      "-- Epoch 3184\n",
      "Norm: 5702605331870.13, NNZs: 843, Bias: -224601266.437787, T: 10720528, Avg. loss: 93001947612909407810708242956288.000000\n",
      "Total training time: 22.94 seconds.\n",
      "-- Epoch 3185\n",
      "Norm: 5697029232597.83, NNZs: 843, Bias: -49837032.211075, T: 10723895, Avg. loss: 92987430041941032500237239844864.000000\n",
      "Total training time: 22.95 seconds.\n",
      "-- Epoch 3186\n",
      "Norm: 5627916303941.08, NNZs: 843, Bias: 124900920.516831, T: 10727262, Avg. loss: 92973336434805256428166779502592.000000\n",
      "Total training time: 22.96 seconds.\n",
      "-- Epoch 3187\n",
      "Norm: 6081997364471.94, NNZs: 843, Bias: -159444080.350387, T: 10730629, Avg. loss: 92958749864663615130090434723840.000000\n",
      "Total training time: 22.96 seconds.\n",
      "-- Epoch 3188\n",
      "Norm: 5761351682546.37, NNZs: 843, Bias: -240738557.979622, T: 10733996, Avg. loss: 92944208611672351781379196846080.000000\n",
      "Total training time: 22.97 seconds.\n",
      "-- Epoch 3189\n",
      "Norm: 5545124733839.00, NNZs: 843, Bias: -66026584.994695, T: 10737363, Avg. loss: 92929747910808162354989096763392.000000\n",
      "Total training time: 22.98 seconds.\n",
      "-- Epoch 3190\n",
      "Norm: 5530017389309.21, NNZs: 843, Bias: 108656292.089173, T: 10740730, Avg. loss: 92915246944546797353699729473536.000000\n",
      "Total training time: 22.98 seconds.\n",
      "-- Epoch 3191\n",
      "Norm: 5650445758114.85, NNZs: 843, Bias: 283312871.013137, T: 10744097, Avg. loss: 92900538809358884773452392169472.000000\n",
      "Total training time: 22.99 seconds.\n",
      "-- Epoch 3192\n",
      "Norm: 5443010513169.05, NNZs: 843, Bias: 108637300.162889, T: 10747464, Avg. loss: 92885757900858684530462568742912.000000\n",
      "Total training time: 23.00 seconds.\n",
      "-- Epoch 3193\n",
      "Norm: 5452345017588.99, NNZs: 843, Bias: -66010466.533140, T: 10750831, Avg. loss: 92871242395617617275347714179072.000000\n",
      "Total training time: 23.01 seconds.\n",
      "-- Epoch 3194\n",
      "Norm: 5440045791226.45, NNZs: 843, Bias: 108617916.865749, T: 10754198, Avg. loss: 92856389600670343079658596073472.000000\n",
      "Total training time: 23.01 seconds.\n",
      "-- Epoch 3195\n",
      "Norm: 5423190009316.49, NNZs: 843, Bias: -66003423.126173, T: 10757565, Avg. loss: 92842167490769919185209505349632.000000\n",
      "Total training time: 23.02 seconds.\n",
      "-- Epoch 3196\n",
      "Norm: 5579798598335.11, NNZs: 843, Bias: -240596420.421015, T: 10760932, Avg. loss: 92827454972069854491983912894464.000000\n",
      "Total training time: 23.03 seconds.\n",
      "-- Epoch 3197\n",
      "Norm: 5466050706513.74, NNZs: 843, Bias: -65993943.614856, T: 10764299, Avg. loss: 92812952044004336910297957662720.000000\n",
      "Total training time: 23.03 seconds.\n",
      "-- Epoch 3198\n",
      "Norm: 5501793669746.14, NNZs: 843, Bias: 108540251.130483, T: 10767666, Avg. loss: 92798300663646690324738421555200.000000\n",
      "Total training time: 23.04 seconds.\n",
      "-- Epoch 3199\n",
      "Norm: 5620531824117.26, NNZs: 843, Bias: -66023797.751988, T: 10771033, Avg. loss: 92783944611905445567091150159872.000000\n",
      "Total training time: 23.05 seconds.\n",
      "-- Epoch 3200\n",
      "Norm: 5366601634262.11, NNZs: 843, Bias: 108523182.413458, T: 10774400, Avg. loss: 92769387532929843682195508559872.000000\n",
      "Total training time: 23.06 seconds.\n",
      "-- Epoch 3201\n",
      "Norm: 5334305943257.08, NNZs: 843, Bias: -66014802.870221, T: 10777767, Avg. loss: 92755143280055121177654908157952.000000\n",
      "Total training time: 23.06 seconds.\n",
      "-- Epoch 3202\n",
      "Norm: 5496267871194.68, NNZs: 843, Bias: -240524390.782483, T: 10781134, Avg. loss: 92740603746817328663903466422272.000000\n",
      "Total training time: 23.07 seconds.\n",
      "-- Epoch 3203\n",
      "Norm: 5505210939487.81, NNZs: 843, Bias: -66005045.478393, T: 10784501, Avg. loss: 92726127856223448940583317405696.000000\n",
      "Total training time: 23.08 seconds.\n",
      "-- Epoch 3204\n",
      "Norm: 5432432087954.55, NNZs: 843, Bias: -240489735.457176, T: 10787868, Avg. loss: 92711946049281266348416980484096.000000\n",
      "Total training time: 23.08 seconds.\n",
      "-- Epoch 3205\n",
      "Norm: 5337551441303.32, NNZs: 843, Bias: -65996663.316987, T: 10791235, Avg. loss: 92697461500616758657872725803008.000000\n",
      "Total training time: 23.09 seconds.\n",
      "-- Epoch 3206\n",
      "Norm: 5487358888794.85, NNZs: 843, Bias: 108467712.754911, T: 10794602, Avg. loss: 92683232400536102624729392742400.000000\n",
      "Total training time: 23.10 seconds.\n",
      "-- Epoch 3207\n",
      "Norm: 5433394452687.29, NNZs: 843, Bias: 219668272.530274, T: 10797969, Avg. loss: 92668846165720595545844478902272.000000\n",
      "Total training time: 23.11 seconds.\n",
      "-- Epoch 3208\n",
      "Norm: 5495253266597.32, NNZs: 843, Bias: -329819465.819248, T: 10801336, Avg. loss: 92654602898267176449579644092416.000000\n",
      "Total training time: 23.11 seconds.\n",
      "-- Epoch 3209\n",
      "Norm: 5423583875240.78, NNZs: 843, Bias: -155376273.275698, T: 10804703, Avg. loss: 92639879574293690183885699153920.000000\n",
      "Total training time: 23.12 seconds.\n",
      "-- Epoch 3210\n",
      "Norm: 5307764179290.56, NNZs: 843, Bias: -187731416.628069, T: 10808070, Avg. loss: 92625663180841582737182108418048.000000\n",
      "Total training time: 23.13 seconds.\n",
      "-- Epoch 3211\n",
      "Norm: 5488976804422.08, NNZs: 843, Bias: -13325693.173014, T: 10811437, Avg. loss: 92611168343357129291417308364800.000000\n",
      "Total training time: 23.13 seconds.\n",
      "-- Epoch 3212\n",
      "Norm: 5409979355434.57, NNZs: 843, Bias: 161054629.095751, T: 10814804, Avg. loss: 92596758672823696271728216899584.000000\n",
      "Total training time: 23.14 seconds.\n",
      "-- Epoch 3213\n",
      "Norm: 5384630553337.89, NNZs: 843, Bias: 261980065.461339, T: 10818171, Avg. loss: 92582491763468282878405514362880.000000\n",
      "Total training time: 23.15 seconds.\n",
      "-- Epoch 3214\n",
      "Norm: 5298751117552.66, NNZs: 843, Bias: 87606001.855720, T: 10821538, Avg. loss: 92567950701310964872822839574528.000000\n",
      "Total training time: 23.16 seconds.\n",
      "-- Epoch 3215\n",
      "Norm: 5524460187756.04, NNZs: 843, Bias: 162199829.898110, T: 10824905, Avg. loss: 92553863927442317773694992121856.000000\n",
      "Total training time: 23.16 seconds.\n",
      "-- Epoch 3216\n",
      "Norm: 5382112719932.31, NNZs: 843, Bias: -12139036.290227, T: 10828272, Avg. loss: 92539466901633006504040885911552.000000\n",
      "Total training time: 23.17 seconds.\n",
      "-- Epoch 3217\n",
      "Norm: 5460980651442.98, NNZs: 843, Bias: 162172642.494182, T: 10831639, Avg. loss: 92524938985701821691630712782848.000000\n",
      "Total training time: 23.18 seconds.\n",
      "-- Epoch 3218\n",
      "Norm: 5567907722780.29, NNZs: 843, Bias: 336456884.313743, T: 10835006, Avg. loss: 92510647683459052045257605644288.000000\n",
      "Total training time: 23.18 seconds.\n",
      "-- Epoch 3219\n",
      "Norm: 5599250440300.64, NNZs: 843, Bias: -186423838.643419, T: 10838373, Avg. loss: 92496156949496286539785942597632.000000\n",
      "Total training time: 23.19 seconds.\n",
      "-- Epoch 3220\n",
      "Norm: 5371974249444.16, NNZs: 843, Bias: -12140081.657615, T: 10841740, Avg. loss: 92481766719277296005267540934656.000000\n",
      "Total training time: 23.20 seconds.\n",
      "-- Epoch 3221\n",
      "Norm: 5604067457065.22, NNZs: 843, Bias: 426717054.873769, T: 10845107, Avg. loss: 92467200453154829242711922442240.000000\n",
      "Total training time: 23.21 seconds.\n",
      "-- Epoch 3222\n",
      "Norm: 5331766102223.32, NNZs: 843, Bias: 147455862.212993, T: 10848474, Avg. loss: 92452722088892236940458781048832.000000\n",
      "Total training time: 23.21 seconds.\n",
      "-- Epoch 3223\n",
      "Norm: 5304065960052.76, NNZs: 843, Bias: -26786268.807341, T: 10851841, Avg. loss: 92438382281142406510535382138880.000000\n",
      "Total training time: 23.22 seconds.\n",
      "-- Epoch 3224\n",
      "Norm: 5296114148114.95, NNZs: 843, Bias: 55773559.299934, T: 10855208, Avg. loss: 92423919265287045947373641531392.000000\n",
      "Total training time: 23.23 seconds.\n",
      "-- Epoch 3225\n",
      "Norm: 5594047834076.89, NNZs: 843, Bias: 174477778.678716, T: 10858575, Avg. loss: 92409663248413297742876860481536.000000\n",
      "Total training time: 23.24 seconds.\n",
      "-- Epoch 3226\n",
      "Norm: 5434492847126.01, NNZs: 843, Bias: 90684960.352018, T: 10861942, Avg. loss: 92395538208309130673472673939456.000000\n",
      "Total training time: 23.24 seconds.\n",
      "-- Epoch 3227\n",
      "Norm: 5662019145673.03, NNZs: 843, Bias: 264853696.133318, T: 10865309, Avg. loss: 92381239305965000447905771814912.000000\n",
      "Total training time: 23.25 seconds.\n",
      "-- Epoch 3228\n",
      "Norm: 5352889633597.67, NNZs: 843, Bias: 90670320.271858, T: 10868676, Avg. loss: 92367127305420452390901043429376.000000\n",
      "Total training time: 23.26 seconds.\n",
      "-- Epoch 3229\n",
      "Norm: 5454909452096.91, NNZs: 843, Bias: 284303244.081950, T: 10872043, Avg. loss: 92352784141909016081937899454464.000000\n",
      "Total training time: 23.26 seconds.\n",
      "-- Epoch 3230\n",
      "Norm: 5562136308038.77, NNZs: 843, Bias: -62319388.093749, T: 10875410, Avg. loss: 92338539581472107938806644080640.000000\n",
      "Total training time: 23.27 seconds.\n",
      "-- Epoch 3231\n",
      "Norm: 5458658225466.03, NNZs: 843, Bias: 111807049.054162, T: 10878777, Avg. loss: 92324509502788647844622561705984.000000\n",
      "Total training time: 23.28 seconds.\n",
      "-- Epoch 3232\n",
      "Norm: 5539041140742.78, NNZs: 843, Bias: 285906388.209193, T: 10882144, Avg. loss: 92310100697327329794145736720384.000000\n",
      "Total training time: 23.29 seconds.\n",
      "-- Epoch 3233\n",
      "Norm: 5441877866430.22, NNZs: 843, Bias: 111788413.444023, T: 10885511, Avg. loss: 92295815035780403322505872277504.000000\n",
      "Total training time: 23.29 seconds.\n",
      "-- Epoch 3234\n",
      "Norm: 5415403020307.90, NNZs: 843, Bias: -62302725.215470, T: 10888878, Avg. loss: 92281511394855048143307532140544.000000\n",
      "Total training time: 23.30 seconds.\n",
      "-- Epoch 3235\n",
      "Norm: 5480112882512.38, NNZs: 843, Bias: -236367546.575643, T: 10892245, Avg. loss: 92267272439732047177624030543872.000000\n",
      "Total training time: 23.31 seconds.\n",
      "-- Epoch 3236\n",
      "Norm: 5761515038055.71, NNZs: 843, Bias: -406343543.566672, T: 10895612, Avg. loss: 92253058743067552301712869949440.000000\n",
      "Total training time: 23.31 seconds.\n",
      "-- Epoch 3237\n",
      "Norm: 5548664496887.29, NNZs: 843, Bias: 115812869.178265, T: 10898979, Avg. loss: 92238662344616784812180101922816.000000\n",
      "Total training time: 23.32 seconds.\n",
      "-- Epoch 3238\n",
      "Norm: 5479931152990.00, NNZs: 843, Bias: 108585125.957601, T: 10902346, Avg. loss: 92224447430156360639555932520448.000000\n",
      "Total training time: 23.33 seconds.\n",
      "-- Epoch 3239\n",
      "Norm: 5453277917821.03, NNZs: 843, Bias: -65437944.471182, T: 10905713, Avg. loss: 92210199243747201237113978748928.000000\n",
      "Total training time: 23.34 seconds.\n",
      "-- Epoch 3240\n",
      "Norm: 5525359819506.44, NNZs: 843, Bias: -239435442.539047, T: 10909080, Avg. loss: 92195934081133565834528525647872.000000\n",
      "Total training time: 23.34 seconds.\n",
      "-- Epoch 3241\n",
      "Norm: 5502339982621.23, NNZs: 843, Bias: 282546583.298286, T: 10912447, Avg. loss: 92181660246527036706252400885760.000000\n",
      "Total training time: 23.35 seconds.\n",
      "-- Epoch 3242\n",
      "Norm: 5706934654799.99, NNZs: 843, Bias: 108550944.257149, T: 10915814, Avg. loss: 92167511525529034025189762924544.000000\n",
      "Total training time: 23.36 seconds.\n",
      "-- Epoch 3243\n",
      "Norm: 5575153599858.28, NNZs: 843, Bias: 282501547.433163, T: 10919181, Avg. loss: 92153342035164707145202205196288.000000\n",
      "Total training time: 23.36 seconds.\n",
      "-- Epoch 3244\n",
      "Norm: 5473891169586.49, NNZs: 843, Bias: -239365736.992963, T: 10922548, Avg. loss: 92139301228613804361950359977984.000000\n",
      "Total training time: 23.37 seconds.\n",
      "-- Epoch 3245\n",
      "Norm: 5315958351203.69, NNZs: 843, Bias: -65414669.220437, T: 10925915, Avg. loss: 92125119013781107921996465831936.000000\n",
      "Total training time: 23.38 seconds.\n",
      "-- Epoch 3246\n",
      "Norm: 5466097441808.69, NNZs: 843, Bias: -239331697.558658, T: 10929282, Avg. loss: 92111048577922480137858233401344.000000\n",
      "Total training time: 23.39 seconds.\n",
      "-- Epoch 3247\n",
      "Norm: 5336209052031.12, NNZs: 843, Bias: -180624245.728671, T: 10932649, Avg. loss: 92096942257901142387455615303680.000000\n",
      "Total training time: 23.39 seconds.\n",
      "-- Epoch 3248\n",
      "Norm: 5347214560070.95, NNZs: 843, Bias: -47620790.290555, T: 10936016, Avg. loss: 92083067138284846375364703813632.000000\n",
      "Total training time: 23.40 seconds.\n",
      "-- Epoch 3249\n",
      "Norm: 5432490856802.59, NNZs: 843, Bias: -78930637.412739, T: 10939383, Avg. loss: 92069068677517211663895011262464.000000\n",
      "Total training time: 23.41 seconds.\n",
      "-- Epoch 3250\n",
      "Norm: 5359415934778.22, NNZs: 843, Bias: -252792726.419918, T: 10942750, Avg. loss: 92054807934879771275639375527936.000000\n",
      "Total training time: 23.42 seconds.\n",
      "-- Epoch 3251\n",
      "Norm: 5355731238922.59, NNZs: 843, Bias: -78920686.715790, T: 10946117, Avg. loss: 92040562964345279222534358695936.000000\n",
      "Total training time: 23.42 seconds.\n",
      "-- Epoch 3252\n",
      "Norm: 5269190172768.95, NNZs: 843, Bias: 94925180.362038, T: 10949484, Avg. loss: 92026785805064148047968713310208.000000\n",
      "Total training time: 23.43 seconds.\n",
      "-- Epoch 3253\n",
      "Norm: 5322010259598.78, NNZs: 843, Bias: 97743897.666696, T: 10952851, Avg. loss: 92012298080858850982847494225920.000000\n",
      "Total training time: 23.44 seconds.\n",
      "-- Epoch 3254\n",
      "Norm: 5836342855393.10, NNZs: 843, Bias: -423706905.145827, T: 10956218, Avg. loss: 91998175778421236114697106751488.000000\n",
      "Total training time: 23.44 seconds.\n",
      "-- Epoch 3255\n",
      "Norm: 5520711879519.22, NNZs: 843, Bias: 97725840.122744, T: 10959585, Avg. loss: 91983949192932475653687422746624.000000\n",
      "Total training time: 23.45 seconds.\n",
      "-- Epoch 3256\n",
      "Norm: 5872153891071.21, NNZs: 843, Bias: -76068712.279506, T: 10962952, Avg. loss: 91969472876272256548054264446976.000000\n",
      "Total training time: 23.46 seconds.\n",
      "-- Epoch 3257\n",
      "Norm: 5402819906317.26, NNZs: 843, Bias: -249839032.410282, T: 10966319, Avg. loss: 91955266434792514050665025110016.000000\n",
      "Total training time: 23.47 seconds.\n",
      "-- Epoch 3258\n",
      "Norm: 5467227324259.90, NNZs: 843, Bias: 39496587.228588, T: 10969686, Avg. loss: 91941134886070060633152542474240.000000\n",
      "Total training time: 23.47 seconds.\n",
      "-- Epoch 3259\n",
      "Norm: 5704055314763.00, NNZs: 843, Bias: -481750503.788633, T: 10973053, Avg. loss: 91926938109485624754739966115840.000000\n",
      "Total training time: 23.48 seconds.\n",
      "-- Epoch 3260\n",
      "Norm: 5409814054344.35, NNZs: 843, Bias: -247148720.178338, T: 10976420, Avg. loss: 91912702490814469804992904560640.000000\n",
      "Total training time: 23.49 seconds.\n",
      "-- Epoch 3261\n",
      "Norm: 5348758638539.05, NNZs: 843, Bias: -73409963.963964, T: 10979787, Avg. loss: 91898552286447861472799026577408.000000\n",
      "Total training time: 23.49 seconds.\n",
      "-- Epoch 3262\n",
      "Norm: 5390031028055.51, NNZs: 843, Bias: 100301612.475040, T: 10983154, Avg. loss: 91884362169098224382828628410368.000000\n",
      "Total training time: 23.50 seconds.\n",
      "-- Epoch 3263\n",
      "Norm: 5415279411435.38, NNZs: 843, Bias: -73400889.893840, T: 10986521, Avg. loss: 91870393473675107866110584684544.000000\n",
      "Total training time: 23.51 seconds.\n",
      "-- Epoch 3264\n",
      "Norm: 5472240231960.25, NNZs: 843, Bias: 100284470.687712, T: 10989888, Avg. loss: 91856431528973183184216617123840.000000\n",
      "Total training time: 23.52 seconds.\n",
      "-- Epoch 3265\n",
      "Norm: 5447787284063.63, NNZs: 843, Bias: -73392035.805887, T: 10993255, Avg. loss: 91842297434391981292001494564864.000000\n",
      "Total training time: 23.52 seconds.\n",
      "-- Epoch 3266\n",
      "Norm: 5579447383264.24, NNZs: 843, Bias: 100266449.361146, T: 10996622, Avg. loss: 91828049370621496263990124216320.000000\n",
      "Total training time: 23.53 seconds.\n",
      "-- Epoch 3267\n",
      "Norm: 5728480601688.20, NNZs: 843, Bias: -449777710.769643, T: 10999989, Avg. loss: 91814160221605616092825700532224.000000\n",
      "Total training time: 23.54 seconds.\n",
      "-- Epoch 3268\n",
      "Norm: 5565196195133.02, NNZs: 843, Bias: 71136949.488814, T: 11003356, Avg. loss: 91800210522286338370628497702912.000000\n",
      "Total training time: 23.54 seconds.\n",
      "-- Epoch 3269\n",
      "Norm: 5447587616432.75, NNZs: 843, Bias: -102484247.549910, T: 11006723, Avg. loss: 91786074943077882817823138381824.000000\n",
      "Total training time: 23.55 seconds.\n",
      "-- Epoch 3270\n",
      "Norm: 5602211164410.15, NNZs: 843, Bias: 127223845.978284, T: 11010090, Avg. loss: 91772271945782907816508810854400.000000\n",
      "Total training time: 23.56 seconds.\n",
      "-- Epoch 3271\n",
      "Norm: 5515972562221.37, NNZs: 843, Bias: -393550613.741977, T: 11013457, Avg. loss: 91758226893118030238302544592896.000000\n",
      "Total training time: 23.57 seconds.\n",
      "-- Epoch 3272\n",
      "Norm: 5424113411993.51, NNZs: 843, Bias: -219946644.020126, T: 11016824, Avg. loss: 91744408536549015899420508553216.000000\n",
      "Total training time: 23.57 seconds.\n",
      "-- Epoch 3273\n",
      "Norm: 5380964738988.43, NNZs: 843, Bias: 12021573.181884, T: 11020191, Avg. loss: 91730351016160903457260208914432.000000\n",
      "Total training time: 23.58 seconds.\n",
      "-- Epoch 3274\n",
      "Norm: 5540675177354.96, NNZs: 843, Bias: -161529382.478796, T: 11023558, Avg. loss: 91716286135675280193898544103424.000000\n",
      "Total training time: 23.59 seconds.\n",
      "-- Epoch 3275\n",
      "Norm: 5519368873529.47, NNZs: 843, Bias: 12016271.408304, T: 11026925, Avg. loss: 91702147189500803321110930128896.000000\n",
      "Total training time: 23.60 seconds.\n",
      "-- Epoch 3276\n",
      "Norm: 6182321476456.24, NNZs: 843, Bias: 532577502.889306, T: 11030292, Avg. loss: 91688326059578335031322585595904.000000\n",
      "Total training time: 23.60 seconds.\n",
      "-- Epoch 3277\n",
      "Norm: 5602702939886.68, NNZs: 843, Bias: 12009683.960589, T: 11033659, Avg. loss: 91674357602670556538156984827904.000000\n",
      "Total training time: 23.61 seconds.\n",
      "-- Epoch 3278\n",
      "Norm: 5572551813211.88, NNZs: 843, Bias: -161487529.252454, T: 11037026, Avg. loss: 91660522804493130985654714368000.000000\n",
      "Total training time: 23.62 seconds.\n",
      "-- Epoch 3279\n",
      "Norm: 5738944617356.51, NNZs: 843, Bias: 12003479.132762, T: 11040393, Avg. loss: 91646690823050154852299093049344.000000\n",
      "Total training time: 23.62 seconds.\n",
      "-- Epoch 3280\n",
      "Norm: 5721544596469.33, NNZs: 843, Bias: 185470568.744503, T: 11043760, Avg. loss: 91632729747441807405280112148480.000000\n",
      "Total training time: 23.63 seconds.\n",
      "-- Epoch 3281\n",
      "Norm: 5753394221369.01, NNZs: 843, Bias: -298239825.245760, T: 11047127, Avg. loss: 91618664604866952027278747369472.000000\n",
      "Total training time: 23.64 seconds.\n",
      "-- Epoch 3282\n",
      "Norm: 5679208052605.68, NNZs: 843, Bias: -124777001.619008, T: 11050494, Avg. loss: 91604703307190538496653941276672.000000\n",
      "Total training time: 23.65 seconds.\n",
      "-- Epoch 3283\n",
      "Norm: 5815768339459.49, NNZs: 843, Bias: 282102656.360324, T: 11053861, Avg. loss: 91590863323171212445034460741632.000000\n",
      "Total training time: 23.65 seconds.\n",
      "-- Epoch 3284\n",
      "Norm: 6022232383141.62, NNZs: 843, Bias: 6209215.614257, T: 11057228, Avg. loss: 91576841720423750144761356353536.000000\n",
      "Total training time: 23.66 seconds.\n",
      "-- Epoch 3285\n",
      "Norm: 5833061031334.49, NNZs: 843, Bias: -167194543.890496, T: 11060595, Avg. loss: 91562681914623768910726875316224.000000\n",
      "Total training time: 23.67 seconds.\n",
      "-- Epoch 3286\n",
      "Norm: 5787891534736.87, NNZs: 843, Bias: -128263309.018183, T: 11063962, Avg. loss: 91548772777421760558298296221696.000000\n",
      "Total training time: 23.67 seconds.\n",
      "-- Epoch 3287\n",
      "Norm: 5807297998628.27, NNZs: 843, Bias: 45120905.915646, T: 11067329, Avg. loss: 91534687324528563860371601883136.000000\n",
      "Total training time: 23.68 seconds.\n",
      "-- Epoch 3288\n",
      "Norm: 6023190135274.56, NNZs: 843, Bias: -128247709.822574, T: 11070696, Avg. loss: 91520681721493308931574580903936.000000\n",
      "Total training time: 23.69 seconds.\n",
      "-- Epoch 3289\n",
      "Norm: 6017697108807.91, NNZs: 843, Bias: 45110417.929201, T: 11074063, Avg. loss: 91506864472916304049822618353664.000000\n",
      "Total training time: 23.70 seconds.\n",
      "-- Epoch 3290\n",
      "Norm: 5912948930961.71, NNZs: 843, Bias: -128232224.177637, T: 11077430, Avg. loss: 91492995892860239596891976761344.000000\n",
      "Total training time: 23.70 seconds.\n",
      "-- Epoch 3291\n",
      "Norm: 5884377097328.96, NNZs: 843, Bias: 116990516.455442, T: 11080797, Avg. loss: 91478739564566180039427293708288.000000\n",
      "Total training time: 23.71 seconds.\n",
      "-- Epoch 3292\n",
      "Norm: 5740725174444.30, NNZs: 843, Bias: -56328868.919650, T: 11084164, Avg. loss: 91464841147717484498588371255296.000000\n",
      "Total training time: 23.72 seconds.\n",
      "-- Epoch 3293\n",
      "Norm: 5852251984194.63, NNZs: 843, Bias: 294170285.238845, T: 11087531, Avg. loss: 91450833054853682793529034670080.000000\n",
      "Total training time: 23.73 seconds.\n",
      "-- Epoch 3294\n",
      "Norm: 5734843997028.00, NNZs: 843, Bias: 120863819.681202, T: 11090898, Avg. loss: 91437099823613709354940734898176.000000\n",
      "Total training time: 23.73 seconds.\n",
      "-- Epoch 3295\n",
      "Norm: 5950547124279.48, NNZs: 843, Bias: -52416936.838827, T: 11094265, Avg. loss: 91423108601598290860027102101504.000000\n",
      "Total training time: 23.74 seconds.\n",
      "-- Epoch 3296\n",
      "Norm: 5793604653177.16, NNZs: 843, Bias: -225670429.611759, T: 11097632, Avg. loss: 91409044292885514961567919112192.000000\n",
      "Total training time: 23.75 seconds.\n",
      "-- Epoch 3297\n",
      "Norm: 5792178088769.29, NNZs: 843, Bias: 294080114.255847, T: 11100999, Avg. loss: 91395388717627148559851154571264.000000\n",
      "Total training time: 23.75 seconds.\n",
      "-- Epoch 3298\n",
      "Norm: 5734383496125.64, NNZs: 843, Bias: 120824672.866645, T: 11104366, Avg. loss: 91381332938284823429316486365184.000000\n",
      "Total training time: 23.76 seconds.\n",
      "-- Epoch 3299\n",
      "Norm: 5942184423624.03, NNZs: 843, Bias: -398840229.358839, T: 11107733, Avg. loss: 91367132567191778483479783669760.000000\n",
      "Total training time: 23.77 seconds.\n",
      "-- Epoch 3300\n",
      "Norm: 5870477890649.03, NNZs: 843, Bias: -225605641.345256, T: 11111100, Avg. loss: 91353345324217270674245162106880.000000\n",
      "Total training time: 23.78 seconds.\n",
      "-- Epoch 3301\n",
      "Norm: 6311436232527.70, NNZs: 843, Bias: -313653737.042458, T: 11114467, Avg. loss: 91339687956770077462896828219392.000000\n",
      "Total training time: 23.78 seconds.\n",
      "-- Epoch 3302\n",
      "Norm: 6091834381475.21, NNZs: 843, Bias: -26085248.762151, T: 11117834, Avg. loss: 91325937862087980473366152740864.000000\n",
      "Total training time: 23.79 seconds.\n",
      "-- Epoch 3303\n",
      "Norm: 6174033023788.16, NNZs: 843, Bias: 13727964.482215, T: 11121201, Avg. loss: 91312147829572136097416636006400.000000\n",
      "Total training time: 23.80 seconds.\n",
      "-- Epoch 3304\n",
      "Norm: 6242075584569.94, NNZs: 843, Bias: -159427090.768489, T: 11124568, Avg. loss: 91297924961325775102391726637056.000000\n",
      "Total training time: 23.81 seconds.\n",
      "-- Epoch 3305\n",
      "Norm: 6109030909298.35, NNZs: 843, Bias: -98433885.685966, T: 11127935, Avg. loss: 91284433553087023103766251438080.000000\n",
      "Total training time: 23.81 seconds.\n",
      "-- Epoch 3306\n",
      "Norm: 5922444038300.49, NNZs: 843, Bias: 74699641.139146, T: 11131302, Avg. loss: 91270441098339325018378774511616.000000\n",
      "Total training time: 23.82 seconds.\n",
      "-- Epoch 3307\n",
      "Norm: 6119159960156.23, NNZs: 843, Bias: 8110548.706819, T: 11134669, Avg. loss: 91256908814250195729978419576832.000000\n",
      "Total training time: 23.83 seconds.\n",
      "-- Epoch 3308\n",
      "Norm: 6216185981151.58, NNZs: 843, Bias: -164991101.967795, T: 11138036, Avg. loss: 91243037979271395509391509684224.000000\n",
      "Total training time: 23.83 seconds.\n",
      "-- Epoch 3309\n",
      "Norm: 6310209043411.10, NNZs: 843, Bias: 58820155.572344, T: 11141403, Avg. loss: 91229696184220152463325187276800.000000\n",
      "Total training time: 23.84 seconds.\n",
      "-- Epoch 3310\n",
      "Norm: 6115020003901.85, NNZs: 843, Bias: -114259627.120105, T: 11144770, Avg. loss: 91215896060773448693138702991360.000000\n",
      "Total training time: 23.85 seconds.\n",
      "-- Epoch 3311\n",
      "Norm: 6085920475759.19, NNZs: 843, Bias: 58807239.151941, T: 11148137, Avg. loss: 91202047241226753433931723309056.000000\n",
      "Total training time: 23.86 seconds.\n",
      "-- Epoch 3312\n",
      "Norm: 6028767193444.32, NNZs: 843, Bias: -114245830.614701, T: 11151504, Avg. loss: 91188328082708254381147665989632.000000\n",
      "Total training time: 23.86 seconds.\n",
      "-- Epoch 3313\n",
      "Norm: 6146550837126.73, NNZs: 843, Bias: -287273628.578778, T: 11154871, Avg. loss: 91174521767463697813812673511424.000000\n",
      "Total training time: 23.87 seconds.\n",
      "-- Epoch 3314\n",
      "Norm: 6056561026037.56, NNZs: 843, Bias: -93969288.266510, T: 11158238, Avg. loss: 91160687770385511128462172094464.000000\n",
      "Total training time: 23.88 seconds.\n",
      "-- Epoch 3315\n",
      "Norm: 6104767117225.53, NNZs: 843, Bias: 79045487.266529, T: 11161605, Avg. loss: 91146758702357471248773759893504.000000\n",
      "Total training time: 23.88 seconds.\n",
      "-- Epoch 3316\n",
      "Norm: 5764272968584.80, NNZs: 843, Bias: -93957969.145512, T: 11164972, Avg. loss: 91132704930633841247123432013824.000000\n",
      "Total training time: 23.89 seconds.\n",
      "-- Epoch 3317\n",
      "Norm: 5941752529829.96, NNZs: 843, Bias: -266935439.558794, T: 11168339, Avg. loss: 91119223418322462678195699187712.000000\n",
      "Total training time: 23.90 seconds.\n",
      "-- Epoch 3318\n",
      "Norm: 5796425838078.18, NNZs: 843, Bias: -93947092.410753, T: 11171706, Avg. loss: 91105427526524964333384020000768.000000\n",
      "Total training time: 23.91 seconds.\n",
      "-- Epoch 3319\n",
      "Norm: 5805134027887.42, NNZs: 843, Bias: 79014966.678962, T: 11175073, Avg. loss: 91091752278016975519639771545600.000000\n",
      "Total training time: 23.91 seconds.\n",
      "-- Epoch 3320\n",
      "Norm: 5795307019208.75, NNZs: 843, Bias: -93935589.233263, T: 11178440, Avg. loss: 91078146842317117008250333560832.000000\n",
      "Total training time: 23.92 seconds.\n",
      "-- Epoch 3321\n",
      "Norm: 5973799763475.84, NNZs: 843, Bias: -266860306.783715, T: 11181807, Avg. loss: 91064545874194285801480189378560.000000\n",
      "Total training time: 23.93 seconds.\n",
      "-- Epoch 3322\n",
      "Norm: 6154787331648.49, NNZs: 843, Bias: 251911312.914479, T: 11185174, Avg. loss: 91050685800608462486445903314944.000000\n",
      "Total training time: 23.94 seconds.\n",
      "-- Epoch 3323\n",
      "Norm: 5953617878838.44, NNZs: 843, Bias: 157223654.800626, T: 11188541, Avg. loss: 91037048139091370469530114457600.000000\n",
      "Total training time: 23.94 seconds.\n",
      "-- Epoch 3324\n",
      "Norm: 6068840908451.73, NNZs: 843, Bias: -210167062.708543, T: 11191908, Avg. loss: 91023477269991257047253590736896.000000\n",
      "Total training time: 23.95 seconds.\n",
      "-- Epoch 3325\n",
      "Norm: 6191723141939.12, NNZs: 843, Bias: 308482804.444337, T: 11195275, Avg. loss: 91009619938491818607774508515328.000000\n",
      "Total training time: 23.96 seconds.\n",
      "-- Epoch 3326\n",
      "Norm: 5927375108788.50, NNZs: 843, Bias: -210138737.901617, T: 11198642, Avg. loss: 90996191270918820247187847380992.000000\n",
      "Total training time: 23.96 seconds.\n",
      "-- Epoch 3327\n",
      "Norm: 6195640368096.66, NNZs: 843, Bias: 294036347.858056, T: 11202009, Avg. loss: 90982619333383920579601925931008.000000\n",
      "Total training time: 23.97 seconds.\n",
      "-- Epoch 3328\n",
      "Norm: 6228775776660.01, NNZs: 843, Bias: 266893391.642073, T: 11205376, Avg. loss: 90969099283825186286417689444352.000000\n",
      "Total training time: 23.98 seconds.\n",
      "-- Epoch 3329\n",
      "Norm: 6515317519671.26, NNZs: 843, Bias: 94045061.649429, T: 11208743, Avg. loss: 90955552928391927493451065589760.000000\n",
      "Total training time: 23.99 seconds.\n",
      "-- Epoch 3330\n",
      "Norm: 6156637642656.26, NNZs: 843, Bias: 217287455.858708, T: 11212110, Avg. loss: 90942000760348808461980750839808.000000\n",
      "Total training time: 23.99 seconds.\n",
      "-- Epoch 3331\n",
      "Norm: 6255520707568.41, NNZs: 843, Bias: 44467856.162467, T: 11215477, Avg. loss: 90928205423364051645357002588160.000000\n",
      "Total training time: 24.00 seconds.\n",
      "-- Epoch 3332\n",
      "Norm: 6229074380403.07, NNZs: 843, Bias: -128325021.265233, T: 11218844, Avg. loss: 90914598834599257886188093046784.000000\n",
      "Total training time: 24.01 seconds.\n",
      "-- Epoch 3333\n",
      "Norm: 6097090400079.47, NNZs: 843, Bias: 44457079.020189, T: 11222211, Avg. loss: 90900904477794765167185738334208.000000\n",
      "Total training time: 24.01 seconds.\n",
      "-- Epoch 3334\n",
      "Norm: 6011623804651.06, NNZs: 843, Bias: -160795000.536576, T: 11225578, Avg. loss: 90887194936210002112936490827776.000000\n",
      "Total training time: 24.02 seconds.\n",
      "-- Epoch 3335\n",
      "Norm: 6493746893213.25, NNZs: 843, Bias: 357461300.587116, T: 11228945, Avg. loss: 90873682945427118753648549560320.000000\n",
      "Total training time: 24.03 seconds.\n",
      "-- Epoch 3336\n",
      "Norm: 6222245317341.23, NNZs: 843, Bias: 154114193.492230, T: 11232312, Avg. loss: 90860160870700206418830699790336.000000\n",
      "Total training time: 24.04 seconds.\n",
      "-- Epoch 3337\n",
      "Norm: 6188315172542.90, NNZs: 843, Bias: 72282815.889723, T: 11235679, Avg. loss: 90846701128256902924541834035200.000000\n",
      "Total training time: 24.04 seconds.\n",
      "-- Epoch 3338\n",
      "Norm: 5915502826336.55, NNZs: 843, Bias: -100434603.521823, T: 11239046, Avg. loss: 90833273939375882290032342466560.000000\n",
      "Total training time: 24.05 seconds.\n",
      "-- Epoch 3339\n",
      "Norm: 6117234548552.94, NNZs: 843, Bias: -163963825.735190, T: 11242413, Avg. loss: 90819428897912443245215078154240.000000\n",
      "Total training time: 24.06 seconds.\n",
      "-- Epoch 3340\n",
      "Norm: 6177818835589.02, NNZs: 843, Bias: -336639659.805592, T: 11245780, Avg. loss: 90805778564160108448944664608768.000000\n",
      "Total training time: 24.06 seconds.\n",
      "-- Epoch 3341\n",
      "Norm: 5898782591872.74, NNZs: 843, Bias: -163945917.753831, T: 11249147, Avg. loss: 90792070339779733669515649613824.000000\n",
      "Total training time: 24.07 seconds.\n",
      "-- Epoch 3342\n",
      "Norm: 6125427801491.99, NNZs: 843, Bias: 8721484.532268, T: 11252514, Avg. loss: 90778478061888365872970154901504.000000\n",
      "Total training time: 24.08 seconds.\n",
      "-- Epoch 3343\n",
      "Norm: 6273552634285.03, NNZs: 843, Bias: -163926550.085917, T: 11255881, Avg. loss: 90764797338283423406779107639296.000000\n",
      "Total training time: 24.09 seconds.\n",
      "-- Epoch 3344\n",
      "Norm: 6148335272183.07, NNZs: 843, Bias: 8714379.492069, T: 11259248, Avg. loss: 90751355682113449345124973674496.000000\n",
      "Total training time: 24.09 seconds.\n",
      "-- Epoch 3345\n",
      "Norm: 6203952080142.05, NNZs: 843, Bias: -163907879.288487, T: 11262615, Avg. loss: 90737828604522826923506554372096.000000\n",
      "Total training time: 24.10 seconds.\n",
      "-- Epoch 3346\n",
      "Norm: 6265113931265.63, NNZs: 843, Bias: -196041268.201005, T: 11265982, Avg. loss: 90724582018899937868415611437056.000000\n",
      "Total training time: 24.11 seconds.\n",
      "-- Epoch 3347\n",
      "Norm: 6278889281865.41, NNZs: 843, Bias: -23435743.667847, T: 11269349, Avg. loss: 90711105869401316682370734096384.000000\n",
      "Total training time: 24.12 seconds.\n",
      "-- Epoch 3348\n",
      "Norm: 6036821825713.83, NNZs: 843, Bias: 149143481.165598, T: 11272716, Avg. loss: 90697763405575851222131688865792.000000\n",
      "Total training time: 24.12 seconds.\n",
      "-- Epoch 3349\n",
      "Norm: 5952976940110.21, NNZs: 843, Bias: -86357484.828147, T: 11276083, Avg. loss: 90684477156406788759566273216512.000000\n",
      "Total training time: 24.13 seconds.\n",
      "-- Epoch 3350\n",
      "Norm: 6281851846756.02, NNZs: 843, Bias: 86201179.515422, T: 11279450, Avg. loss: 90670830086212315432907624677376.000000\n",
      "Total training time: 24.14 seconds.\n",
      "-- Epoch 3351\n",
      "Norm: 6021049325903.29, NNZs: 843, Bias: 258733691.437564, T: 11282817, Avg. loss: 90657426821126560139502172504064.000000\n",
      "Total training time: 24.14 seconds.\n",
      "-- Epoch 3352\n",
      "Norm: 5994861625225.84, NNZs: 843, Bias: -12947230.589199, T: 11286184, Avg. loss: 90643788331364584343075345661952.000000\n",
      "Total training time: 24.15 seconds.\n",
      "-- Epoch 3353\n",
      "Norm: 6076201465412.17, NNZs: 843, Bias: -185464422.840461, T: 11289551, Avg. loss: 90630460431115788947899545550848.000000\n",
      "Total training time: 24.16 seconds.\n",
      "-- Epoch 3354\n",
      "Norm: 6191527912810.97, NNZs: 843, Bias: -12950229.159211, T: 11292918, Avg. loss: 90616873363750857452863610683392.000000\n",
      "Total training time: 24.17 seconds.\n",
      "-- Epoch 3355\n",
      "Norm: 6142031067849.76, NNZs: 843, Bias: 24326981.751936, T: 11296285, Avg. loss: 90603413801971938997536500482048.000000\n",
      "Total training time: 24.17 seconds.\n",
      "-- Epoch 3356\n",
      "Norm: 5995927796993.93, NNZs: 843, Bias: 196800468.910957, T: 11299652, Avg. loss: 90590040485038313971713052770304.000000\n",
      "Total training time: 24.18 seconds.\n",
      "-- Epoch 3357\n",
      "Norm: 6054358545905.68, NNZs: 843, Bias: 44388570.780591, T: 11303019, Avg. loss: 90576595076859628501451692572672.000000\n",
      "Total training time: 24.19 seconds.\n",
      "-- Epoch 3358\n",
      "Norm: 6515548463745.80, NNZs: 843, Bias: -154077224.016728, T: 11306386, Avg. loss: 90562552962168804470982250070016.000000\n",
      "Total training time: 24.19 seconds.\n",
      "-- Epoch 3359\n",
      "Norm: 6083558460522.72, NNZs: 843, Bias: 18371461.533330, T: 11309753, Avg. loss: 90549026979392442824025719177216.000000\n",
      "Total training time: 24.20 seconds.\n",
      "-- Epoch 3360\n",
      "Norm: 6170747685013.90, NNZs: 843, Bias: -412623396.980432, T: 11313120, Avg. loss: 90535592851440434537224134983680.000000\n",
      "Total training time: 24.21 seconds.\n",
      "-- Epoch 3361\n",
      "Norm: 6174844189373.31, NNZs: 843, Bias: -151503637.418377, T: 11316487, Avg. loss: 90522255782324200604741840928768.000000\n",
      "Total training time: 24.22 seconds.\n",
      "-- Epoch 3362\n",
      "Norm: 6619283464975.87, NNZs: 843, Bias: -280454683.685561, T: 11319854, Avg. loss: 90508952514661915684219496955904.000000\n",
      "Total training time: 24.22 seconds.\n",
      "-- Epoch 3363\n",
      "Norm: 6589575082772.73, NNZs: 843, Bias: 236728300.542121, T: 11323221, Avg. loss: 90495489575589681531394300313600.000000\n",
      "Total training time: 24.23 seconds.\n",
      "-- Epoch 3364\n",
      "Norm: 6101642225688.00, NNZs: 843, Bias: 64333011.954236, T: 11326588, Avg. loss: 90482239616938133424345997901824.000000\n",
      "Total training time: 24.24 seconds.\n",
      "-- Epoch 3365\n",
      "Norm: 6349185862525.08, NNZs: 843, Bias: -89348920.868979, T: 11329955, Avg. loss: 90468703112753909655379411730432.000000\n",
      "Total training time: 24.24 seconds.\n",
      "-- Epoch 3366\n",
      "Norm: 6108757048388.33, NNZs: 843, Bias: -261694878.530311, T: 11333322, Avg. loss: 90455269266131033508901061394432.000000\n",
      "Total training time: 24.25 seconds.\n",
      "-- Epoch 3367\n",
      "Norm: 6100769118035.74, NNZs: 843, Bias: -89341108.125557, T: 11336689, Avg. loss: 90441630565022981477726991417344.000000\n",
      "Total training time: 24.26 seconds.\n",
      "-- Epoch 3368\n",
      "Norm: 6198804439496.92, NNZs: 843, Bias: 427636867.778010, T: 11340056, Avg. loss: 90428197993276097457136074752000.000000\n",
      "Total training time: 24.27 seconds.\n",
      "-- Epoch 3369\n",
      "Norm: 6051148931435.62, NNZs: 843, Bias: -89330398.672514, T: 11343423, Avg. loss: 90414770822963994547027441614848.000000\n",
      "Total training time: 24.27 seconds.\n",
      "-- Epoch 3370\n",
      "Norm: 5867211395107.89, NNZs: 843, Bias: 19466270.306047, T: 11346790, Avg. loss: 90401299110948213048406106439680.000000\n",
      "Total training time: 24.28 seconds.\n",
      "-- Epoch 3371\n",
      "Norm: 5877128897912.78, NNZs: 843, Bias: 191747660.420258, T: 11350157, Avg. loss: 90387718262503360544815622127616.000000\n",
      "Total training time: 24.29 seconds.\n",
      "-- Epoch 3372\n",
      "Norm: 5835690268702.94, NNZs: 843, Bias: -95334895.437975, T: 11353524, Avg. loss: 90374478263373957759210042163200.000000\n",
      "Total training time: 24.30 seconds.\n",
      "-- Epoch 3373\n",
      "Norm: 6184187714709.91, NNZs: 843, Bias: 39662852.304089, T: 11356891, Avg. loss: 90361155677796331242169990905856.000000\n",
      "Total training time: 24.30 seconds.\n",
      "-- Epoch 3374\n",
      "Norm: 5805979879224.55, NNZs: 843, Bias: -132589877.634983, T: 11360258, Avg. loss: 90347920176817110205949563371520.000000\n",
      "Total training time: 24.31 seconds.\n",
      "-- Epoch 3375\n",
      "Norm: 5766587471851.67, NNZs: 843, Bias: -304817513.065873, T: 11363625, Avg. loss: 90334804942361351949759054610432.000000\n",
      "Total training time: 24.32 seconds.\n",
      "-- Epoch 3376\n",
      "Norm: 5645953611240.51, NNZs: 843, Bias: -132576490.318735, T: 11366992, Avg. loss: 90321663407956707425096652816384.000000\n",
      "Total training time: 24.32 seconds.\n",
      "-- Epoch 3377\n",
      "Norm: 5791280498431.62, NNZs: 843, Bias: -304777109.415734, T: 11370359, Avg. loss: 90308195989457232354543233138688.000000\n",
      "Total training time: 24.33 seconds.\n",
      "-- Epoch 3378\n",
      "Norm: 5703356513371.21, NNZs: 843, Bias: -132560924.346046, T: 11373726, Avg. loss: 90294947348155899078249419898880.000000\n",
      "Total training time: 24.34 seconds.\n",
      "-- Epoch 3379\n",
      "Norm: 5525836019978.52, NNZs: 843, Bias: -2187033.363661, T: 11377093, Avg. loss: 90281632222349360263382003875840.000000\n",
      "Total training time: 24.35 seconds.\n",
      "-- Epoch 3380\n",
      "Norm: 5624384584128.49, NNZs: 843, Bias: 169981960.175454, T: 11380460, Avg. loss: 90268404785403708538832302374912.000000\n",
      "Total training time: 24.35 seconds.\n",
      "-- Epoch 3381\n",
      "Norm: 5465537792019.33, NNZs: 843, Bias: -2191696.661527, T: 11383827, Avg. loss: 90255108110958964275444566720512.000000\n",
      "Total training time: 24.36 seconds.\n",
      "-- Epoch 3382\n",
      "Norm: 5708975795261.90, NNZs: 843, Bias: -82810316.197554, T: 11387194, Avg. loss: 90241715941413804673693791551488.000000\n",
      "Total training time: 24.37 seconds.\n",
      "-- Epoch 3383\n",
      "Norm: 5567897716255.77, NNZs: 843, Bias: -144247651.727391, T: 11390561, Avg. loss: 90228440190304052353006284308480.000000\n",
      "Total training time: 24.37 seconds.\n",
      "-- Epoch 3384\n",
      "Norm: 5650131165279.94, NNZs: 843, Bias: 27881262.415023, T: 11393928, Avg. loss: 90215181265490481655130169016320.000000\n",
      "Total training time: 24.38 seconds.\n",
      "-- Epoch 3385\n",
      "Norm: 5620735709845.52, NNZs: 843, Bias: 34737814.271577, T: 11397295, Avg. loss: 90201875108149142309324856492032.000000\n",
      "Total training time: 24.39 seconds.\n",
      "-- Epoch 3386\n",
      "Norm: 5842001418652.50, NNZs: 843, Bias: -481550976.978926, T: 11400662, Avg. loss: 90188548778226561855175442563072.000000\n",
      "Total training time: 24.40 seconds.\n",
      "-- Epoch 3387\n",
      "Norm: 5776923463313.77, NNZs: 843, Bias: -68508560.191466, T: 11404029, Avg. loss: 90175421735011167735338244243456.000000\n",
      "Total training time: 24.40 seconds.\n",
      "-- Epoch 3388\n",
      "Norm: 5868333266621.20, NNZs: 843, Bias: 112390210.059858, T: 11407396, Avg. loss: 90162044059253997652992034078720.000000\n",
      "Total training time: 24.41 seconds.\n",
      "-- Epoch 3389\n",
      "Norm: 5853419174755.17, NNZs: 843, Bias: 20093929.165276, T: 11410763, Avg. loss: 90148904825025554988497327095808.000000\n",
      "Total training time: 24.42 seconds.\n",
      "-- Epoch 3390\n",
      "Norm: 5891789456222.71, NNZs: 843, Bias: -151953170.067660, T: 11414130, Avg. loss: 90135553818358435526146120482816.000000\n",
      "Total training time: 24.42 seconds.\n",
      "-- Epoch 3391\n",
      "Norm: 5949564334919.17, NNZs: 843, Bias: 20086923.969048, T: 11417497, Avg. loss: 90122263600876950892822755540992.000000\n",
      "Total training time: 24.43 seconds.\n",
      "-- Epoch 3392\n",
      "Norm: 6035372561598.17, NNZs: 843, Bias: -151935537.776721, T: 11420864, Avg. loss: 90108860484871043818095353266176.000000\n",
      "Total training time: 24.44 seconds.\n",
      "-- Epoch 3393\n",
      "Norm: 6230565400417.54, NNZs: 843, Bias: 323944811.065496, T: 11424231, Avg. loss: 90095570390861597648649717809152.000000\n",
      "Total training time: 24.45 seconds.\n",
      "-- Epoch 3394\n",
      "Norm: 5941147592520.80, NNZs: 843, Bias: -364378536.282698, T: 11427598, Avg. loss: 90082149932135324545839534178304.000000\n",
      "Total training time: 24.45 seconds.\n",
      "-- Epoch 3395\n",
      "Norm: 5926639397326.38, NNZs: 843, Bias: 151587479.761707, T: 11430965, Avg. loss: 90068943049019105159417148997632.000000\n",
      "Total training time: 24.46 seconds.\n",
      "-- Epoch 3396\n",
      "Norm: 5857913011259.43, NNZs: 843, Bias: -20393055.072422, T: 11434332, Avg. loss: 90055614111122292701223247151104.000000\n",
      "Total training time: 24.47 seconds.\n",
      "-- Epoch 3397\n",
      "Norm: 5693816024405.09, NNZs: 843, Bias: 151562660.604744, T: 11437699, Avg. loss: 90042365885151600001118145347584.000000\n",
      "Total training time: 24.48 seconds.\n",
      "-- Epoch 3398\n",
      "Norm: 5821901866456.29, NNZs: 843, Bias: -219659012.291030, T: 11441066, Avg. loss: 90029157564732222054492337602560.000000\n",
      "Total training time: 24.48 seconds.\n",
      "-- Epoch 3399\n",
      "Norm: 5674480699240.17, NNZs: 843, Bias: -47713459.079927, T: 11444433, Avg. loss: 90016257167308366041777893801984.000000\n",
      "Total training time: 24.49 seconds.\n",
      "-- Epoch 3400\n",
      "Norm: 5742732523662.21, NNZs: 843, Bias: 124207143.519130, T: 11447800, Avg. loss: 90003011163207482637740459163648.000000\n",
      "Total training time: 24.50 seconds.\n",
      "-- Epoch 3401\n",
      "Norm: 5673690925983.10, NNZs: 843, Bias: -47707478.190150, T: 11451167, Avg. loss: 89989872325415404773553851072512.000000\n",
      "Total training time: 24.50 seconds.\n",
      "-- Epoch 3402\n",
      "Norm: 5792135735839.82, NNZs: 843, Bias: 18271904.779751, T: 11454534, Avg. loss: 89976765730373410466663298498560.000000\n",
      "Total training time: 24.51 seconds.\n",
      "-- Epoch 3403\n",
      "Norm: 5834900104214.86, NNZs: 843, Bias: 139348394.742135, T: 11457901, Avg. loss: 89963733930732518597336504467456.000000\n",
      "Total training time: 24.52 seconds.\n",
      "-- Epoch 3404\n",
      "Norm: 5907512223388.32, NNZs: 843, Bias: 311202792.380729, T: 11461268, Avg. loss: 89950517660583938286254242660352.000000\n",
      "Total training time: 24.53 seconds.\n",
      "-- Epoch 3405\n",
      "Norm: 5872332739122.15, NNZs: 843, Bias: -96321999.611450, T: 11464635, Avg. loss: 89937318193789662664202781720576.000000\n",
      "Total training time: 24.53 seconds.\n",
      "-- Epoch 3406\n",
      "Norm: 5966749700917.23, NNZs: 843, Bias: -268158192.521847, T: 11468002, Avg. loss: 89923939867664085527979857281024.000000\n",
      "Total training time: 24.54 seconds.\n",
      "-- Epoch 3407\n",
      "Norm: 5934035770379.89, NNZs: 843, Bias: 247346769.796404, T: 11471369, Avg. loss: 89910785903728316246534062604288.000000\n",
      "Total training time: 24.55 seconds.\n",
      "-- Epoch 3408\n",
      "Norm: 5941066401312.98, NNZs: 843, Bias: 75511034.526510, T: 11474736, Avg. loss: 89897630140605630737681213292544.000000\n",
      "Total training time: 24.55 seconds.\n",
      "-- Epoch 3409\n",
      "Norm: 5888977804137.40, NNZs: 843, Bias: 247307902.396709, T: 11478103, Avg. loss: 89884540558211907148361064513536.000000\n",
      "Total training time: 24.56 seconds.\n",
      "-- Epoch 3410\n",
      "Norm: 6051305000186.94, NNZs: 843, Bias: -268084089.572053, T: 11481470, Avg. loss: 89871186752918663850173631823872.000000\n",
      "Total training time: 24.57 seconds.\n",
      "-- Epoch 3411\n",
      "Norm: 6131399469331.38, NNZs: 843, Bias: -64941364.299224, T: 11484837, Avg. loss: 89857995566485609437562721533952.000000\n",
      "Total training time: 24.58 seconds.\n",
      "-- Epoch 3412\n",
      "Norm: 6052644559174.49, NNZs: 843, Bias: -236703745.212874, T: 11488204, Avg. loss: 89844699760077483605303136092160.000000\n",
      "Total training time: 24.58 seconds.\n",
      "-- Epoch 3413\n",
      "Norm: 6318112386167.94, NNZs: 843, Bias: -64935462.132073, T: 11491571, Avg. loss: 89831516898442001392652609650688.000000\n",
      "Total training time: 24.59 seconds.\n",
      "-- Epoch 3414\n",
      "Norm: 5855158721834.10, NNZs: 843, Bias: -205101368.016528, T: 11494938, Avg. loss: 89818449026009993052245765128192.000000\n",
      "Total training time: 24.60 seconds.\n",
      "-- Epoch 3415\n",
      "Norm: 5930867672875.37, NNZs: 843, Bias: -33360100.521970, T: 11498305, Avg. loss: 89805235129986144022002439028736.000000\n",
      "Total training time: 24.60 seconds.\n",
      "-- Epoch 3416\n",
      "Norm: 5947701644413.34, NNZs: 843, Bias: -205074593.353294, T: 11501672, Avg. loss: 89792149400095354646360238325760.000000\n",
      "Total training time: 24.61 seconds.\n",
      "-- Epoch 3417\n",
      "Norm: 6005265898258.41, NNZs: 843, Bias: -94769322.228531, T: 11505039, Avg. loss: 89779355083734120951168446234624.000000\n",
      "Total training time: 24.62 seconds.\n",
      "-- Epoch 3418\n",
      "Norm: 5872302218977.02, NNZs: 843, Bias: 76927559.343698, T: 11508406, Avg. loss: 89766163788165913229240609800192.000000\n",
      "Total training time: 24.63 seconds.\n",
      "-- Epoch 3419\n",
      "Norm: 5781197457865.09, NNZs: 843, Bias: -94757554.937097, T: 11511773, Avg. loss: 89753116801584824643380033093632.000000\n",
      "Total training time: 24.63 seconds.\n",
      "-- Epoch 3420\n",
      "Norm: 5665833147222.48, NNZs: 843, Bias: -1279365.855055, T: 11515140, Avg. loss: 89739904299927121570764481888256.000000\n",
      "Total training time: 24.64 seconds.\n",
      "-- Epoch 3421\n",
      "Norm: 5774627434460.75, NNZs: 843, Bias: 170370339.087359, T: 11518507, Avg. loss: 89726634883431445889758137417728.000000\n",
      "Total training time: 24.65 seconds.\n",
      "-- Epoch 3422\n",
      "Norm: 5683905297474.46, NNZs: 843, Bias: -1284980.471878, T: 11521874, Avg. loss: 89713743172850743086018386198528.000000\n",
      "Total training time: 24.66 seconds.\n",
      "-- Epoch 3423\n",
      "Norm: 5565134981897.26, NNZs: 843, Bias: -166635402.874536, T: 11525241, Avg. loss: 89700696301998221779133217636352.000000\n",
      "Total training time: 24.66 seconds.\n",
      "-- Epoch 3424\n",
      "Norm: 5694831196468.76, NNZs: 843, Bias: -338240978.491853, T: 11528608, Avg. loss: 89687560908414267135088696229888.000000\n",
      "Total training time: 24.67 seconds.\n",
      "-- Epoch 3425\n",
      "Norm: 5660642789863.72, NNZs: 843, Bias: 176589328.565128, T: 11531975, Avg. loss: 89674491815653991456295431962624.000000\n",
      "Total training time: 24.68 seconds.\n",
      "-- Epoch 3426\n",
      "Norm: 5827351574162.93, NNZs: 843, Bias: -338196325.560032, T: 11535342, Avg. loss: 89661301950881150761028103962624.000000\n",
      "Total training time: 24.68 seconds.\n",
      "-- Epoch 3427\n",
      "Norm: 5815618547867.89, NNZs: 843, Bias: -166595463.958791, T: 11538709, Avg. loss: 89648203933605270803386906705920.000000\n",
      "Total training time: 24.69 seconds.\n",
      "-- Epoch 3428\n",
      "Norm: 5610111189594.55, NNZs: 843, Bias: 4980111.352732, T: 11542076, Avg. loss: 89635143064822500079243794317312.000000\n",
      "Total training time: 24.70 seconds.\n",
      "-- Epoch 3429\n",
      "Norm: 5676143344971.17, NNZs: 843, Bias: 176529656.535344, T: 11545443, Avg. loss: 89622090424601520428045299613696.000000\n",
      "Total training time: 24.71 seconds.\n",
      "-- Epoch 3430\n",
      "Norm: 5633067034424.52, NNZs: 843, Bias: 4974022.915332, T: 11548810, Avg. loss: 89608938547304010862175755173888.000000\n",
      "Total training time: 24.71 seconds.\n",
      "-- Epoch 3431\n",
      "Norm: 5815692187733.29, NNZs: 843, Bias: 87346057.733259, T: 11552177, Avg. loss: 89595974190554735549375355813888.000000\n",
      "Total training time: 24.72 seconds.\n",
      "-- Epoch 3432\n",
      "Norm: 6033771242946.98, NNZs: 843, Bias: -84177212.011492, T: 11555544, Avg. loss: 89582847309887348449055751536640.000000\n",
      "Total training time: 24.73 seconds.\n",
      "-- Epoch 3433\n",
      "Norm: 5794211220377.41, NNZs: 843, Bias: 201910098.976959, T: 11558911, Avg. loss: 89569633922893295653498235387904.000000\n",
      "Total training time: 24.73 seconds.\n",
      "-- Epoch 3434\n",
      "Norm: 5768036192842.40, NNZs: 843, Bias: 30403447.125976, T: 11562278, Avg. loss: 89556469792291073196206600486912.000000\n",
      "Total training time: 24.74 seconds.\n",
      "-- Epoch 3435\n",
      "Norm: 6002780154993.85, NNZs: 843, Bias: -141077955.755631, T: 11565645, Avg. loss: 89543510928827943754072487624704.000000\n",
      "Total training time: 24.75 seconds.\n",
      "-- Epoch 3436\n",
      "Norm: 6064837867722.19, NNZs: 843, Bias: 299994464.469092, T: 11569012, Avg. loss: 89530247951038457231327298060288.000000\n",
      "Total training time: 24.76 seconds.\n",
      "-- Epoch 3437\n",
      "Norm: 5666586686002.75, NNZs: 843, Bias: 128519714.720457, T: 11572379, Avg. loss: 89517316991528112824987410235392.000000\n",
      "Total training time: 24.76 seconds.\n",
      "-- Epoch 3438\n",
      "Norm: 5773171955279.10, NNZs: 843, Bias: 392033000.077690, T: 11575746, Avg. loss: 89504223037739622280092062842880.000000\n",
      "Total training time: 24.77 seconds.\n",
      "-- Epoch 3439\n",
      "Norm: 5703015265173.64, NNZs: 843, Bias: 37885880.402324, T: 11579113, Avg. loss: 89491005163682025038636910641152.000000\n",
      "Total training time: 24.78 seconds.\n",
      "-- Epoch 3440\n",
      "Norm: 5945342277316.78, NNZs: 843, Bias: -24836969.710669, T: 11582480, Avg. loss: 89478001499627521810854043451392.000000\n",
      "Total training time: 24.78 seconds.\n",
      "-- Epoch 3441\n",
      "Norm: 5628829365592.04, NNZs: 843, Bias: -74958036.535388, T: 11585847, Avg. loss: 89465250543269649830497581268992.000000\n",
      "Total training time: 24.79 seconds.\n",
      "-- Epoch 3442\n",
      "Norm: 5732003300997.76, NNZs: 843, Bias: -246344059.424721, T: 11589214, Avg. loss: 89452224756573879734260614561792.000000\n",
      "Total training time: 24.80 seconds.\n",
      "-- Epoch 3443\n",
      "Norm: 5503849839173.86, NNZs: 843, Bias: 24392669.669642, T: 11592581, Avg. loss: 89439457864057193200796768927744.000000\n",
      "Total training time: 24.81 seconds.\n",
      "-- Epoch 3444\n",
      "Norm: 5612295468699.57, NNZs: 843, Bias: -146976730.962328, T: 11595948, Avg. loss: 89426403982909039961972999716864.000000\n",
      "Total training time: 24.81 seconds.\n",
      "-- Epoch 3445\n",
      "Norm: 5617607625889.37, NNZs: 843, Bias: 24385941.583720, T: 11599315, Avg. loss: 89413263296730489755568862920704.000000\n",
      "Total training time: 24.82 seconds.\n",
      "-- Epoch 3446\n",
      "Norm: 5644602473422.88, NNZs: 843, Bias: 195723040.122823, T: 11602682, Avg. loss: 89400342696398621118158644183040.000000\n",
      "Total training time: 24.83 seconds.\n",
      "-- Epoch 3447\n",
      "Norm: 5720711958104.67, NNZs: 843, Bias: 24378605.841058, T: 11606049, Avg. loss: 89387363647148507509928509833216.000000\n",
      "Total training time: 24.84 seconds.\n",
      "-- Epoch 3448\n",
      "Norm: 5580177700182.12, NNZs: 843, Bias: 195691334.847255, T: 11609416, Avg. loss: 89374270270567231899494055411712.000000\n",
      "Total training time: 24.84 seconds.\n",
      "-- Epoch 3449\n",
      "Norm: 5520086151391.83, NNZs: 843, Bias: 24370840.485205, T: 11612783, Avg. loss: 89361053492747483274912605405184.000000\n",
      "Total training time: 24.85 seconds.\n",
      "-- Epoch 3450\n",
      "Norm: 5707833107440.63, NNZs: 843, Bias: 195657277.776224, T: 11616150, Avg. loss: 89348089749855518505730591162368.000000\n",
      "Total training time: 24.86 seconds.\n",
      "-- Epoch 3451\n",
      "Norm: 6020368615927.19, NNZs: 843, Bias: 148622295.010079, T: 11619517, Avg. loss: 89335199445453327021459122421760.000000\n",
      "Total training time: 24.86 seconds.\n",
      "-- Epoch 3452\n",
      "Norm: 5844544429567.58, NNZs: 843, Bias: -22656212.045367, T: 11622884, Avg. loss: 89322158259663338200790862921728.000000\n",
      "Total training time: 24.87 seconds.\n",
      "-- Epoch 3453\n",
      "Norm: 5752093805618.08, NNZs: 843, Bias: 148597374.147942, T: 11626251, Avg. loss: 89309266411619570215596123488256.000000\n",
      "Total training time: 24.88 seconds.\n",
      "-- Epoch 3454\n",
      "Norm: 5760971674125.42, NNZs: 843, Bias: 133636092.672591, T: 11629618, Avg. loss: 89296162096278774552582628048896.000000\n",
      "Total training time: 24.89 seconds.\n",
      "-- Epoch 3455\n",
      "Norm: 5806620270489.45, NNZs: 843, Bias: 304854739.392273, T: 11632985, Avg. loss: 89283312892928522976025198460928.000000\n",
      "Total training time: 24.89 seconds.\n",
      "-- Epoch 3456\n",
      "Norm: 5735959959995.82, NNZs: 843, Bias: -208817712.666492, T: 11636352, Avg. loss: 89270482759474400736387113418752.000000\n",
      "Total training time: 24.90 seconds.\n",
      "-- Epoch 3457\n",
      "Norm: 5808126745823.67, NNZs: 843, Bias: 304808872.481691, T: 11639719, Avg. loss: 89257591257318162988329798205440.000000\n",
      "Total training time: 24.91 seconds.\n",
      "-- Epoch 3458\n",
      "Norm: 6136448830418.04, NNZs: 843, Bias: 133592753.741543, T: 11643086, Avg. loss: 89244510162788725209169651564544.000000\n",
      "Total training time: 24.91 seconds.\n",
      "-- Epoch 3459\n",
      "Norm: 5982017999533.42, NNZs: 843, Bias: -379956137.143274, T: 11646453, Avg. loss: 89231806156252631243940935237632.000000\n",
      "Total training time: 24.92 seconds.\n",
      "-- Epoch 3460\n",
      "Norm: 5824180667016.78, NNZs: 843, Bias: 133571731.848826, T: 11649820, Avg. loss: 89218919545122844540140494258176.000000\n",
      "Total training time: 24.93 seconds.\n",
      "-- Epoch 3461\n",
      "Norm: 5823803139347.26, NNZs: 843, Bias: -124741337.673836, T: 11653187, Avg. loss: 89206107811140077974216253636608.000000\n",
      "Total training time: 24.94 seconds.\n",
      "-- Epoch 3462\n",
      "Norm: 5909258067435.51, NNZs: 843, Bias: 46407602.517113, T: 11656554, Avg. loss: 89193408027064206884262167707648.000000\n",
      "Total training time: 24.94 seconds.\n",
      "-- Epoch 3463\n",
      "Norm: 5506365606336.64, NNZs: 843, Bias: 8766603.074194, T: 11659921, Avg. loss: 89180493409210680598374408781824.000000\n",
      "Total training time: 24.95 seconds.\n",
      "-- Epoch 3464\n",
      "Norm: 5672061909022.33, NNZs: 843, Bias: -162355393.768498, T: 11663288, Avg. loss: 89167636799457537335940006019072.000000\n",
      "Total training time: 24.96 seconds.\n",
      "-- Epoch 3465\n",
      "Norm: 5683362858758.68, NNZs: 843, Bias: 8759211.835597, T: 11666655, Avg. loss: 89154695097973683574073544671232.000000\n",
      "Total training time: 24.96 seconds.\n",
      "-- Epoch 3466\n",
      "Norm: 5504785986933.03, NNZs: 843, Bias: -162337372.596238, T: 11670022, Avg. loss: 89141814691384036294720088965120.000000\n",
      "Total training time: 24.97 seconds.\n",
      "-- Epoch 3467\n",
      "Norm: 5467987086299.23, NNZs: 843, Bias: 8752508.103816, T: 11673389, Avg. loss: 89129192525771454681917052420096.000000\n",
      "Total training time: 24.98 seconds.\n",
      "-- Epoch 3468\n",
      "Norm: 5531058994846.10, NNZs: 843, Bias: -162318280.041247, T: 11676756, Avg. loss: 89116274012484304633652525924352.000000\n",
      "Total training time: 24.99 seconds.\n",
      "-- Epoch 3469\n",
      "Norm: 5624304113171.53, NNZs: 843, Bias: -359655158.682652, T: 11680123, Avg. loss: 89103399204426161507325366501376.000000\n",
      "Total training time: 24.99 seconds.\n",
      "-- Epoch 3470\n",
      "Norm: 5564678107987.13, NNZs: 843, Bias: 153499810.784494, T: 11683490, Avg. loss: 89090573999089001935861463384064.000000\n",
      "Total training time: 25.00 seconds.\n",
      "-- Epoch 3471\n",
      "Norm: 5757553750390.11, NNZs: 843, Bias: -219446357.202896, T: 11686857, Avg. loss: 89077845870295842511131678605312.000000\n",
      "Total training time: 25.01 seconds.\n",
      "-- Epoch 3472\n",
      "Norm: 5447432291040.93, NNZs: 843, Bias: -48414095.647059, T: 11690224, Avg. loss: 89064760429323687396654490583040.000000\n",
      "Total training time: 25.01 seconds.\n",
      "-- Epoch 3473\n",
      "Norm: 5642460351632.01, NNZs: 843, Bias: -219419172.425156, T: 11693591, Avg. loss: 89052031061842309931847418642432.000000\n",
      "Total training time: 25.02 seconds.\n",
      "-- Epoch 3474\n",
      "Norm: 5703551467922.99, NNZs: 843, Bias: -48410509.083001, T: 11696958, Avg. loss: 89039201555039396114855861682176.000000\n",
      "Total training time: 25.03 seconds.\n",
      "-- Epoch 3475\n",
      "Norm: 5579914739074.58, NNZs: 843, Bias: -238967285.481126, T: 11700325, Avg. loss: 89026332036708262552953446989824.000000\n",
      "Total training time: 25.04 seconds.\n",
      "-- Epoch 3476\n",
      "Norm: 5502272752207.00, NNZs: 843, Bias: -67981144.933374, T: 11703692, Avg. loss: 89013396520019146153275863072768.000000\n",
      "Total training time: 25.04 seconds.\n",
      "-- Epoch 3477\n",
      "Norm: 5595565316543.70, NNZs: 843, Bias: -274237078.285488, T: 11707059, Avg. loss: 89000870906188722176596480360448.000000\n",
      "Total training time: 25.05 seconds.\n",
      "-- Epoch 3478\n",
      "Norm: 5544984481349.67, NNZs: 843, Bias: 221860452.844282, T: 11710426, Avg. loss: 88987815275547253704298673995776.000000\n",
      "Total training time: 25.06 seconds.\n",
      "-- Epoch 3479\n",
      "Norm: 5709468072544.90, NNZs: 843, Bias: 50909615.775756, T: 11713793, Avg. loss: 88975122735991412916869570494464.000000\n",
      "Total training time: 25.07 seconds.\n",
      "-- Epoch 3480\n",
      "Norm: 5576308213918.49, NNZs: 843, Bias: -120017145.200240, T: 11717160, Avg. loss: 88962316054230619403424909754368.000000\n",
      "Total training time: 25.07 seconds.\n",
      "-- Epoch 3481\n",
      "Norm: 5461041592584.85, NNZs: 843, Bias: 55081902.182255, T: 11720527, Avg. loss: 88949262455778747036605981130752.000000\n",
      "Total training time: 25.08 seconds.\n",
      "-- Epoch 3482\n",
      "Norm: 5392314938868.06, NNZs: 843, Bias: 97342849.250126, T: 11723894, Avg. loss: 88936442245332683217999715368960.000000\n",
      "Total training time: 25.09 seconds.\n",
      "-- Epoch 3483\n",
      "Norm: 5830384833358.14, NNZs: 843, Bias: -415316755.243920, T: 11727261, Avg. loss: 88923586181141833805938716311552.000000\n",
      "Total training time: 25.09 seconds.\n",
      "-- Epoch 3484\n",
      "Norm: 5545559427806.15, NNZs: 843, Bias: 97325593.065495, T: 11730628, Avg. loss: 88910824646709089017821686398976.000000\n",
      "Total training time: 25.10 seconds.\n",
      "-- Epoch 3485\n",
      "Norm: 5733285346220.26, NNZs: 843, Bias: -73542687.135054, T: 11733995, Avg. loss: 88897810856759184011981060308992.000000\n",
      "Total training time: 25.11 seconds.\n",
      "-- Epoch 3486\n",
      "Norm: 5467127849427.30, NNZs: 843, Bias: -244386174.709253, T: 11737362, Avg. loss: 88885208956967595975230414127104.000000\n",
      "Total training time: 25.12 seconds.\n",
      "-- Epoch 3487\n",
      "Norm: 5547766017616.33, NNZs: 843, Bias: 268134042.636034, T: 11740729, Avg. loss: 88872495702286586742543040380928.000000\n",
      "Total training time: 25.12 seconds.\n",
      "-- Epoch 3488\n",
      "Norm: 5408260856819.17, NNZs: 843, Bias: -89419131.285722, T: 11744096, Avg. loss: 88859788861676806490422566191104.000000\n",
      "Total training time: 25.13 seconds.\n",
      "-- Epoch 3489\n",
      "Norm: 5464264728359.28, NNZs: 843, Bias: 81395832.231364, T: 11747463, Avg. loss: 88847202709289345568412169928704.000000\n",
      "Total training time: 25.14 seconds.\n",
      "-- Epoch 3490\n",
      "Norm: 5516630991334.19, NNZs: 843, Bias: -89409320.374505, T: 11750830, Avg. loss: 88834595346973881261313637744640.000000\n",
      "Total training time: 25.14 seconds.\n",
      "-- Epoch 3491\n",
      "Norm: 5820681289854.76, NNZs: 843, Bias: 210533016.759311, T: 11754197, Avg. loss: 88822152699719502759351728209920.000000\n",
      "Total training time: 25.15 seconds.\n",
      "-- Epoch 3492\n",
      "Norm: 5665320411135.18, NNZs: 843, Bias: -46851164.248604, T: 11757564, Avg. loss: 88810008323073348947007183519744.000000\n",
      "Total training time: 25.16 seconds.\n",
      "-- Epoch 3493\n",
      "Norm: 5620247153858.83, NNZs: 843, Bias: -77198739.098956, T: 11760931, Avg. loss: 88797252199417147933231528542208.000000\n",
      "Total training time: 25.17 seconds.\n",
      "-- Epoch 3494\n",
      "Norm: 5651762167882.93, NNZs: 843, Bias: -247944452.598204, T: 11764298, Avg. loss: 88784419252577669130141608968192.000000\n",
      "Total training time: 25.17 seconds.\n",
      "-- Epoch 3495\n",
      "Norm: 5487107307340.77, NNZs: 843, Bias: -77192015.487119, T: 11767665, Avg. loss: 88771750701390047414626396667904.000000\n",
      "Total training time: 25.18 seconds.\n",
      "-- Epoch 3496\n",
      "Norm: 5633953652181.86, NNZs: 843, Bias: 34109690.200983, T: 11771032, Avg. loss: 88758905098197612191675586707456.000000\n",
      "Total training time: 25.19 seconds.\n",
      "-- Epoch 3497\n",
      "Norm: 5585410059776.80, NNZs: 843, Bias: -136607624.099801, T: 11774399, Avg. loss: 88746380957502858908004278861824.000000\n",
      "Total training time: 25.19 seconds.\n",
      "-- Epoch 3498\n",
      "Norm: 5741905224751.68, NNZs: 843, Bias: 34100610.242169, T: 11777766, Avg. loss: 88733899864200602403107244081152.000000\n",
      "Total training time: 25.20 seconds.\n",
      "-- Epoch 3499\n",
      "Norm: 5692189349938.11, NNZs: 843, Bias: 81813782.879853, T: 11781133, Avg. loss: 88721186161852062434597447663616.000000\n",
      "Total training time: 25.21 seconds.\n",
      "-- Epoch 3500\n",
      "Norm: 5832006716186.34, NNZs: 843, Bias: -88870366.364465, T: 11784500, Avg. loss: 88708692811635638080925964697600.000000\n",
      "Total training time: 25.22 seconds.\n",
      "-- Epoch 3501\n",
      "Norm: 5710531811870.56, NNZs: 843, Bias: -259530167.290172, T: 11787867, Avg. loss: 88696193954237895753479890141184.000000\n",
      "Total training time: 25.22 seconds.\n",
      "-- Epoch 3502\n",
      "Norm: 5764369776009.31, NNZs: 843, Bias: -88861994.777381, T: 11791234, Avg. loss: 88683646133211085875668326547456.000000\n",
      "Total training time: 25.23 seconds.\n",
      "-- Epoch 3503\n",
      "Norm: 5925560300603.60, NNZs: 843, Bias: -259496804.077522, T: 11794601, Avg. loss: 88671127084547550291444927299584.000000\n",
      "Total training time: 25.24 seconds.\n",
      "-- Epoch 3504\n",
      "Norm: 5670431935238.31, NNZs: 843, Bias: -88852428.077017, T: 11797968, Avg. loss: 88658502352165466702102753443840.000000\n",
      "Total training time: 25.24 seconds.\n",
      "-- Epoch 3505\n",
      "Norm: 5993072781389.21, NNZs: 843, Bias: 81767544.271812, T: 11801335, Avg. loss: 88645861495794434095988206469120.000000\n",
      "Total training time: 25.25 seconds.\n",
      "-- Epoch 3506\n",
      "Norm: 5981135268282.20, NNZs: 843, Bias: 109874855.048798, T: 11804702, Avg. loss: 88633428219597734309237306163200.000000\n",
      "Total training time: 25.26 seconds.\n",
      "-- Epoch 3507\n",
      "Norm: 5696025127080.85, NNZs: 843, Bias: -60724165.014203, T: 11808069, Avg. loss: 88620790875740812385202101813248.000000\n",
      "Total training time: 25.27 seconds.\n",
      "-- Epoch 3508\n",
      "Norm: 5795992290521.68, NNZs: 843, Bias: 109857074.585910, T: 11811436, Avg. loss: 88608328958154072308089064783872.000000\n",
      "Total training time: 25.27 seconds.\n",
      "-- Epoch 3509\n",
      "Norm: 6130756619378.59, NNZs: 843, Bias: 280414376.658487, T: 11814803, Avg. loss: 88595952671724938693226587488256.000000\n",
      "Total training time: 25.28 seconds.\n",
      "-- Epoch 3510\n",
      "Norm: 5798458910814.77, NNZs: 843, Bias: 109837552.856558, T: 11818170, Avg. loss: 88583584204371078653175104798720.000000\n",
      "Total training time: 25.29 seconds.\n",
      "-- Epoch 3511\n",
      "Norm: 5893714819349.63, NNZs: 843, Bias: 280369627.925093, T: 11821537, Avg. loss: 88570932541969659449223772372992.000000\n",
      "Total training time: 25.29 seconds.\n",
      "-- Epoch 3512\n",
      "Norm: 5862894984164.09, NNZs: 843, Bias: -231240667.660545, T: 11824904, Avg. loss: 88558459405159534768415899123712.000000\n",
      "Total training time: 25.30 seconds.\n",
      "-- Epoch 3513\n",
      "Norm: 5724805555214.42, NNZs: 843, Bias: -60708979.119764, T: 11828271, Avg. loss: 88545781139297509104665004867584.000000\n",
      "Total training time: 25.31 seconds.\n",
      "-- Epoch 3514\n",
      "Norm: 5727714144249.26, NNZs: 843, Bias: -231211457.284089, T: 11831638, Avg. loss: 88533055027828878466555282718720.000000\n",
      "Total training time: 25.32 seconds.\n",
      "-- Epoch 3515\n",
      "Norm: 5772886181947.60, NNZs: 843, Bias: 230713209.981053, T: 11835005, Avg. loss: 88520435950452969983104178454528.000000\n",
      "Total training time: 25.32 seconds.\n",
      "-- Epoch 3516\n",
      "Norm: 5662074742616.21, NNZs: 843, Bias: -83463007.908526, T: 11838372, Avg. loss: 88507739807857981858070849912832.000000\n",
      "Total training time: 25.33 seconds.\n",
      "-- Epoch 3517\n",
      "Norm: 5879937866895.36, NNZs: 843, Bias: -253928037.622414, T: 11841739, Avg. loss: 88495163507405796456880698032128.000000\n",
      "Total training time: 25.34 seconds.\n",
      "-- Epoch 3518\n",
      "Norm: 5851524109351.63, NNZs: 843, Bias: -99726099.808522, T: 11845106, Avg. loss: 88482485306392777166698957504512.000000\n",
      "Total training time: 25.35 seconds.\n",
      "-- Epoch 3519\n",
      "Norm: 5852071124294.25, NNZs: 843, Bias: -321328266.114383, T: 11848473, Avg. loss: 88469908651892286801996909903872.000000\n",
      "Total training time: 25.35 seconds.\n",
      "-- Epoch 3520\n",
      "Norm: 5750663823135.27, NNZs: 843, Bias: -150875423.471323, T: 11851840, Avg. loss: 88457270630192586167724695093248.000000\n",
      "Total training time: 25.36 seconds.\n",
      "-- Epoch 3521\n",
      "Norm: 5807631766467.40, NNZs: 843, Bias: -321289370.902150, T: 11855207, Avg. loss: 88444903341503472737314794373120.000000\n",
      "Total training time: 25.37 seconds.\n",
      "-- Epoch 3522\n",
      "Norm: 6208423598608.15, NNZs: 843, Bias: -443356411.048856, T: 11858574, Avg. loss: 88432345078102038252653782237184.000000\n",
      "Total training time: 25.37 seconds.\n",
      "-- Epoch 3523\n",
      "Norm: 5904366479757.94, NNZs: 843, Bias: 67862258.081346, T: 11861941, Avg. loss: 88419663762177453182610623692800.000000\n",
      "Total training time: 25.38 seconds.\n",
      "-- Epoch 3524\n",
      "Norm: 6264300235234.16, NNZs: 843, Bias: -102530567.309231, T: 11865308, Avg. loss: 88407107210841052335453553819648.000000\n",
      "Total training time: 25.39 seconds.\n",
      "-- Epoch 3525\n",
      "Norm: 5944105059408.07, NNZs: 843, Bias: -22990628.337441, T: 11868675, Avg. loss: 88394684894342440334416832102400.000000\n",
      "Total training time: 25.40 seconds.\n",
      "-- Epoch 3526\n",
      "Norm: 6086548411866.58, NNZs: 843, Bias: 166695655.194347, T: 11872042, Avg. loss: 88381987303681429130103484317696.000000\n",
      "Total training time: 25.40 seconds.\n",
      "-- Epoch 3527\n",
      "Norm: 5911863129733.58, NNZs: 843, Bias: -38474362.433870, T: 11875409, Avg. loss: 88369505353851827300996772528128.000000\n",
      "Total training time: 25.41 seconds.\n",
      "-- Epoch 3528\n",
      "Norm: 5866293931986.15, NNZs: 843, Bias: -28527760.691343, T: 11878776, Avg. loss: 88356911885323969504757095071744.000000\n",
      "Total training time: 25.42 seconds.\n",
      "-- Epoch 3529\n",
      "Norm: 5981562278765.16, NNZs: 843, Bias: -198853943.990110, T: 11882143, Avg. loss: 88344556294596429502266675822592.000000\n",
      "Total training time: 25.42 seconds.\n",
      "-- Epoch 3530\n",
      "Norm: 5960737703246.19, NNZs: 843, Bias: 21068641.847611, T: 11885510, Avg. loss: 88332104067054555529646419476480.000000\n",
      "Total training time: 25.43 seconds.\n",
      "-- Epoch 3531\n",
      "Norm: 5983552951887.84, NNZs: 843, Bias: -149236991.165978, T: 11888877, Avg. loss: 88319646110529262079239738359808.000000\n",
      "Total training time: 25.44 seconds.\n",
      "-- Epoch 3532\n",
      "Norm: 6224423549201.37, NNZs: 843, Bias: 393818288.804958, T: 11892244, Avg. loss: 88306995448563859210841253478400.000000\n",
      "Total training time: 25.45 seconds.\n",
      "-- Epoch 3533\n",
      "Norm: 6120142127921.23, NNZs: 843, Bias: 223511447.136954, T: 11895611, Avg. loss: 88294581436356561212060326690816.000000\n",
      "Total training time: 25.45 seconds.\n",
      "-- Epoch 3534\n",
      "Norm: 6166765401883.54, NNZs: 843, Bias: -287299228.385272, T: 11898978, Avg. loss: 88282352401787994258024257552384.000000\n",
      "Total training time: 25.46 seconds.\n",
      "-- Epoch 3535\n",
      "Norm: 6394596007757.10, NNZs: 843, Bias: -117030668.520259, T: 11902345, Avg. loss: 88269914099658021394056988327936.000000\n",
      "Total training time: 25.47 seconds.\n",
      "-- Epoch 3536\n",
      "Norm: 5978563612219.94, NNZs: 843, Bias: 53214026.842234, T: 11905712, Avg. loss: 88257614294864646205877241511936.000000\n",
      "Total training time: 25.47 seconds.\n",
      "-- Epoch 3537\n",
      "Norm: 6051239027120.99, NNZs: 843, Bias: 223434684.441262, T: 11909079, Avg. loss: 88245107653934506937189373313024.000000\n",
      "Total training time: 25.48 seconds.\n",
      "-- Epoch 3538\n",
      "Norm: 6012760062772.48, NNZs: 843, Bias: 328760490.685848, T: 11912446, Avg. loss: 88232725997518651949020482633728.000000\n",
      "Total training time: 25.49 seconds.\n",
      "-- Epoch 3539\n",
      "Norm: 5808802054814.56, NNZs: 843, Bias: -181877619.400368, T: 11915813, Avg. loss: 88220560124709825724350510137344.000000\n",
      "Total training time: 25.50 seconds.\n",
      "-- Epoch 3540\n",
      "Norm: 5847977522128.84, NNZs: 843, Bias: -352060501.411064, T: 11919180, Avg. loss: 88208117619101978155302080479232.000000\n",
      "Total training time: 25.50 seconds.\n",
      "-- Epoch 3541\n",
      "Norm: 5912310114063.39, NNZs: 843, Bias: -181859967.265724, T: 11922547, Avg. loss: 88195556450349755175643190919168.000000\n",
      "Total training time: 25.51 seconds.\n",
      "-- Epoch 3542\n",
      "Norm: 5841773243717.65, NNZs: 843, Bias: 56494423.388271, T: 11925914, Avg. loss: 88183257843130030064575337988096.000000\n",
      "Total training time: 25.52 seconds.\n",
      "-- Epoch 3543\n",
      "Norm: 5816782581281.30, NNZs: 843, Bias: -113668193.542467, T: 11929281, Avg. loss: 88170888225736680948817845026816.000000\n",
      "Total training time: 25.52 seconds.\n",
      "-- Epoch 3544\n",
      "Norm: 6036919452495.16, NNZs: 843, Bias: 138624597.614547, T: 11932648, Avg. loss: 88158266613717121969922733244416.000000\n",
      "Total training time: 25.53 seconds.\n",
      "-- Epoch 3545\n",
      "Norm: 5854403721013.63, NNZs: 843, Bias: 308742441.119743, T: 11936015, Avg. loss: 88145858460230995959178455941120.000000\n",
      "Total training time: 25.54 seconds.\n",
      "-- Epoch 3546\n",
      "Norm: 5743054354875.04, NNZs: 843, Bias: 138598352.906417, T: 11939382, Avg. loss: 88133597024807607835090978078720.000000\n",
      "Total training time: 25.55 seconds.\n",
      "-- Epoch 3547\n",
      "Norm: 5681553901448.64, NNZs: 843, Bias: -7101799.071210, T: 11942749, Avg. loss: 88121112293650496569254267060224.000000\n",
      "Total training time: 25.55 seconds.\n",
      "-- Epoch 3548\n",
      "Norm: 5659239838897.02, NNZs: 843, Bias: -76521913.974657, T: 11946116, Avg. loss: 88108518395493483711090271453184.000000\n",
      "Total training time: 25.56 seconds.\n",
      "-- Epoch 3549\n",
      "Norm: 5845440618876.26, NNZs: 843, Bias: -246602901.702033, T: 11949483, Avg. loss: 88096378971371334574672197976064.000000\n",
      "Total training time: 25.57 seconds.\n",
      "-- Epoch 3550\n",
      "Norm: 6080162957639.77, NNZs: 843, Bias: -76516402.217090, T: 11952850, Avg. loss: 88084143803131434062077853958144.000000\n",
      "Total training time: 25.58 seconds.\n",
      "-- Epoch 3551\n",
      "Norm: 5614916342443.93, NNZs: 843, Bias: -246573900.274969, T: 11956217, Avg. loss: 88071619192765625112763939422208.000000\n",
      "Total training time: 25.58 seconds.\n",
      "-- Epoch 3552\n",
      "Norm: 5786009461731.53, NNZs: 843, Bias: 263583810.042063, T: 11959584, Avg. loss: 88059152167253779704069153095680.000000\n",
      "Total training time: 25.59 seconds.\n",
      "-- Epoch 3553\n",
      "Norm: 5789988709096.83, NNZs: 843, Bias: -339932196.809587, T: 11962951, Avg. loss: 88046750875220560934165737897984.000000\n",
      "Total training time: 25.60 seconds.\n",
      "-- Epoch 3554\n",
      "Norm: 5498802913305.25, NNZs: 843, Bias: -169888383.589450, T: 11966318, Avg. loss: 88034351010792485315683138142208.000000\n",
      "Total training time: 25.60 seconds.\n",
      "-- Epoch 3555\n",
      "Norm: 5541762556376.26, NNZs: 843, Bias: -174955485.684353, T: 11969685, Avg. loss: 88022060518532007281876539539456.000000\n",
      "Total training time: 25.61 seconds.\n",
      "-- Epoch 3556\n",
      "Norm: 5673520954752.09, NNZs: 843, Bias: -4947406.233522, T: 11973052, Avg. loss: 88009950045289930959700491239424.000000\n",
      "Total training time: 25.62 seconds.\n",
      "-- Epoch 3557\n",
      "Norm: 5595113340564.85, NNZs: 843, Bias: -174939267.387843, T: 11976419, Avg. loss: 87997831837181625485625871302656.000000\n",
      "Total training time: 25.63 seconds.\n",
      "-- Epoch 3558\n",
      "Norm: 5846368394506.19, NNZs: 843, Bias: -4954336.468277, T: 11979786, Avg. loss: 87985382040110026538193357111296.000000\n",
      "Total training time: 25.63 seconds.\n",
      "-- Epoch 3559\n",
      "Norm: 5632937123857.63, NNZs: 843, Bias: 165006968.075554, T: 11983153, Avg. loss: 87973062743693761159722919002112.000000\n",
      "Total training time: 25.64 seconds.\n",
      "-- Epoch 3560\n",
      "Norm: 5870460282995.73, NNZs: 843, Bias: 53927524.159595, T: 11986520, Avg. loss: 87960821101897106204416121241600.000000\n",
      "Total training time: 25.65 seconds.\n",
      "-- Epoch 3561\n",
      "Norm: 5778784483138.82, NNZs: 843, Bias: 223861275.115991, T: 11989887, Avg. loss: 87948547957615901526768360620032.000000\n",
      "Total training time: 25.65 seconds.\n",
      "-- Epoch 3562\n",
      "Norm: 5619327845663.39, NNZs: 843, Bias: 53914069.786861, T: 11993254, Avg. loss: 87936596850079125486211235315712.000000\n",
      "Total training time: 25.66 seconds.\n",
      "-- Epoch 3563\n",
      "Norm: 5723929733062.50, NNZs: 843, Bias: -150867933.481089, T: 11996621, Avg. loss: 87924394312323782102418754371584.000000\n",
      "Total training time: 25.67 seconds.\n",
      "-- Epoch 3564\n",
      "Norm: 5925022701004.14, NNZs: 843, Bias: 358852013.391705, T: 11999988, Avg. loss: 87911902986822869984634621394944.000000\n",
      "Total training time: 25.68 seconds.\n",
      "-- Epoch 3565\n",
      "Norm: 5963798303909.23, NNZs: 843, Bias: -71477660.834590, T: 12003355, Avg. loss: 87899682362557722158973438656512.000000\n",
      "Total training time: 25.68 seconds.\n",
      "-- Epoch 3566\n",
      "Norm: 5709187768805.20, NNZs: 843, Bias: 98405345.025364, T: 12006722, Avg. loss: 87887220037132205586592076660736.000000\n",
      "Total training time: 25.69 seconds.\n",
      "-- Epoch 3567\n",
      "Norm: 5727384528543.86, NNZs: 843, Bias: -50135935.282083, T: 12010089, Avg. loss: 87875051489414001579316410843136.000000\n",
      "Total training time: 25.70 seconds.\n",
      "-- Epoch 3568\n",
      "Norm: 5749096645712.73, NNZs: 843, Bias: -393514429.904524, T: 12013456, Avg. loss: 87862884744933611592275013402624.000000\n",
      "Total training time: 25.70 seconds.\n",
      "-- Epoch 3569\n",
      "Norm: 5634309718605.02, NNZs: 843, Bias: 116044306.730307, T: 12016823, Avg. loss: 87850592124925457997900483133440.000000\n",
      "Total training time: 25.71 seconds.\n",
      "-- Epoch 3570\n",
      "Norm: 5707311293890.33, NNZs: 843, Bias: -53800833.887701, T: 12020190, Avg. loss: 87838466336505491412824992251904.000000\n",
      "Total training time: 25.72 seconds.\n",
      "-- Epoch 3571\n",
      "Norm: 6086516489274.18, NNZs: 843, Bias: -282649079.707340, T: 12023557, Avg. loss: 87826094469458293834261540634624.000000\n",
      "Total training time: 25.73 seconds.\n",
      "-- Epoch 3572\n",
      "Norm: 5769357536344.85, NNZs: 843, Bias: 94790478.270337, T: 12026924, Avg. loss: 87813980767686615353822556454912.000000\n",
      "Total training time: 25.73 seconds.\n",
      "-- Epoch 3573\n",
      "Norm: 5828008750305.98, NNZs: 843, Bias: -400107545.484476, T: 12030291, Avg. loss: 87801680185548886051403063099392.000000\n",
      "Total training time: 25.74 seconds.\n",
      "-- Epoch 3574\n",
      "Norm: 5640618680449.86, NNZs: 843, Bias: -103703169.221115, T: 12033658, Avg. loss: 87789522202674033189868976209920.000000\n",
      "Total training time: 25.75 seconds.\n",
      "-- Epoch 3575\n",
      "Norm: 5624114938745.52, NNZs: 843, Bias: -273472116.468946, T: 12037025, Avg. loss: 87777231249600989885464989138944.000000\n",
      "Total training time: 25.75 seconds.\n",
      "-- Epoch 3576\n",
      "Norm: 5837921544825.77, NNZs: 843, Bias: -103693948.496272, T: 12040392, Avg. loss: 87765009309707263294600510439424.000000\n",
      "Total training time: 25.76 seconds.\n",
      "-- Epoch 3577\n",
      "Norm: 5671767580403.08, NNZs: 843, Bias: 66061006.664589, T: 12043759, Avg. loss: 87752861739797214422095341027328.000000\n",
      "Total training time: 25.77 seconds.\n",
      "-- Epoch 3578\n",
      "Norm: 5600551622829.22, NNZs: 843, Bias: -103685156.914737, T: 12047126, Avg. loss: 87740520494927548676402105548800.000000\n",
      "Total training time: 25.78 seconds.\n",
      "-- Epoch 3579\n",
      "Norm: 5684394925382.96, NNZs: 843, Bias: 66044490.373117, T: 12050493, Avg. loss: 87728374525666901395971608936448.000000\n",
      "Total training time: 25.78 seconds.\n",
      "-- Epoch 3580\n",
      "Norm: 5782376920774.92, NNZs: 843, Bias: -103677766.103573, T: 12053860, Avg. loss: 87716092938567387158643059195904.000000\n",
      "Total training time: 25.79 seconds.\n",
      "-- Epoch 3581\n",
      "Norm: 5871745966881.07, NNZs: 843, Bias: -273376964.809086, T: 12057227, Avg. loss: 87703872117298686759528904523776.000000\n",
      "Total training time: 25.80 seconds.\n",
      "-- Epoch 3582\n",
      "Norm: 5902434033433.68, NNZs: 843, Bias: 235710900.212451, T: 12060594, Avg. loss: 87691832301608291538112874545152.000000\n",
      "Total training time: 25.80 seconds.\n",
      "-- Epoch 3583\n",
      "Norm: 5768377872655.37, NNZs: 843, Bias: -18339423.983763, T: 12063961, Avg. loss: 87679785263327207650929999347712.000000\n",
      "Total training time: 25.81 seconds.\n",
      "-- Epoch 3584\n",
      "Norm: 5917118194496.83, NNZs: 843, Bias: 151325407.599152, T: 12067328, Avg. loss: 87667416846720790917616230203392.000000\n",
      "Total training time: 25.82 seconds.\n",
      "-- Epoch 3585\n",
      "Norm: 5892783850450.71, NNZs: 843, Bias: -18343554.259092, T: 12070695, Avg. loss: 87655144059404134091789346799616.000000\n",
      "Total training time: 25.83 seconds.\n",
      "-- Epoch 3586\n",
      "Norm: 6042448097555.54, NNZs: 843, Bias: -187988484.470447, T: 12074062, Avg. loss: 87642939558915473466879706660864.000000\n",
      "Total training time: 25.83 seconds.\n",
      "-- Epoch 3587\n",
      "Norm: 6286459405240.38, NNZs: 843, Bias: -119251587.804685, T: 12077429, Avg. loss: 87630643999109596706059453464576.000000\n",
      "Total training time: 25.84 seconds.\n",
      "-- Epoch 3588\n",
      "Norm: 5893086735851.11, NNZs: 843, Bias: 50373915.969850, T: 12080796, Avg. loss: 87618416368346522370236114534400.000000\n",
      "Total training time: 25.85 seconds.\n",
      "-- Epoch 3589\n",
      "Norm: 6040449586982.37, NNZs: 843, Bias: 265850059.573849, T: 12084163, Avg. loss: 87606249458371203954706493734912.000000\n",
      "Total training time: 25.85 seconds.\n",
      "-- Epoch 3590\n",
      "Norm: 6134874023902.09, NNZs: 843, Bias: 96232724.213223, T: 12087530, Avg. loss: 87594005026985582363025730437120.000000\n",
      "Total training time: 25.86 seconds.\n",
      "-- Epoch 3591\n",
      "Norm: 5852914974411.85, NNZs: 843, Bias: -73360415.566244, T: 12090897, Avg. loss: 87581930484616974881448548892672.000000\n",
      "Total training time: 25.87 seconds.\n",
      "-- Epoch 3592\n",
      "Norm: 5887128528341.63, NNZs: 843, Bias: -25652461.190799, T: 12094264, Avg. loss: 87569951853384813116858904346624.000000\n",
      "Total training time: 25.88 seconds.\n",
      "-- Epoch 3593\n",
      "Norm: 5812107126506.76, NNZs: 843, Bias: -195215464.934640, T: 12097631, Avg. loss: 87557817041538798074202345701376.000000\n",
      "Total training time: 25.88 seconds.\n",
      "-- Epoch 3594\n",
      "Norm: 5794794481754.63, NNZs: 843, Bias: 109150352.508010, T: 12100998, Avg. loss: 87545487210691809689638193332224.000000\n",
      "Total training time: 25.89 seconds.\n",
      "-- Epoch 3595\n",
      "Norm: 5783294497380.25, NNZs: 843, Bias: 278675576.008409, T: 12104365, Avg. loss: 87533384117434257437604655923200.000000\n",
      "Total training time: 25.90 seconds.\n",
      "-- Epoch 3596\n",
      "Norm: 5678642988444.98, NNZs: 843, Bias: -229921550.026861, T: 12107732, Avg. loss: 87521221637211205833929045049344.000000\n",
      "Total training time: 25.90 seconds.\n",
      "-- Epoch 3597\n",
      "Norm: 5784557203982.75, NNZs: 843, Bias: -60394138.666110, T: 12111099, Avg. loss: 87509319707164869999168092897280.000000\n",
      "Total training time: 25.91 seconds.\n",
      "-- Epoch 3598\n",
      "Norm: 6023991165213.40, NNZs: 843, Bias: -229893665.407912, T: 12114466, Avg. loss: 87496995934279976456572714876928.000000\n",
      "Total training time: 25.92 seconds.\n",
      "-- Epoch 3599\n",
      "Norm: 5899651414266.74, NNZs: 843, Bias: -140320282.498657, T: 12117833, Avg. loss: 87484947031102296050826110566400.000000\n",
      "Total training time: 25.93 seconds.\n",
      "-- Epoch 3600\n",
      "Norm: 6108876313484.74, NNZs: 843, Bias: 29164404.195312, T: 12121200, Avg. loss: 87472649800151055837993222799360.000000\n",
      "Total training time: 25.93 seconds.\n",
      "-- Epoch 3601\n",
      "Norm: 6099091425886.22, NNZs: 843, Bias: -140308394.071259, T: 12124567, Avg. loss: 87460466372700011145333162639360.000000\n",
      "Total training time: 25.94 seconds.\n",
      "-- Epoch 3602\n",
      "Norm: 6170489665984.36, NNZs: 843, Bias: 29152876.288046, T: 12127934, Avg. loss: 87448180000371329287641257476096.000000\n",
      "Total training time: 25.95 seconds.\n",
      "-- Epoch 3603\n",
      "Norm: 5917879613162.02, NNZs: 843, Bias: -140295256.748227, T: 12131301, Avg. loss: 87435980359363187742710605807616.000000\n",
      "Total training time: 25.95 seconds.\n",
      "-- Epoch 3604\n",
      "Norm: 6063205948283.16, NNZs: 843, Bias: -309719476.554581, T: 12134668, Avg. loss: 87423816532435837239448311955456.000000\n",
      "Total training time: 25.96 seconds.\n",
      "-- Epoch 3605\n",
      "Norm: 6110950140083.56, NNZs: 843, Bias: -479120744.621608, T: 12138035, Avg. loss: 87411700500282314198067716292608.000000\n",
      "Total training time: 25.97 seconds.\n",
      "-- Epoch 3606\n",
      "Norm: 6228946646453.77, NNZs: 843, Bias: 29131876.587523, T: 12141402, Avg. loss: 87399438820713602127454919458816.000000\n",
      "Total training time: 25.98 seconds.\n",
      "-- Epoch 3607\n",
      "Norm: 5989225033740.22, NNZs: 843, Bias: 198522439.217014, T: 12144769, Avg. loss: 87387446068786812098377582379008.000000\n",
      "Total training time: 25.98 seconds.\n",
      "-- Epoch 3608\n",
      "Norm: 5966544556737.52, NNZs: 843, Bias: 29120910.159254, T: 12148136, Avg. loss: 87375198543623630669399908679680.000000\n",
      "Total training time: 25.99 seconds.\n",
      "-- Epoch 3609\n",
      "Norm: 6323233856540.99, NNZs: 843, Bias: 89566683.004253, T: 12151503, Avg. loss: 87363119339579823515758225260544.000000\n",
      "Total training time: 26.00 seconds.\n",
      "-- Epoch 3610\n",
      "Norm: 5865797233772.20, NNZs: 843, Bias: -79802143.218584, T: 12154870, Avg. loss: 87350970724211272832877359792128.000000\n",
      "Total training time: 26.00 seconds.\n",
      "-- Epoch 3611\n",
      "Norm: 6105522227630.90, NNZs: 843, Bias: -349438901.799168, T: 12158237, Avg. loss: 87338700626304521740277107392512.000000\n",
      "Total training time: 26.01 seconds.\n",
      "-- Epoch 3612\n",
      "Norm: 5846763371263.95, NNZs: 843, Bias: -180080495.357436, T: 12161604, Avg. loss: 87326767936587342716093530636288.000000\n",
      "Total training time: 26.02 seconds.\n",
      "-- Epoch 3613\n",
      "Norm: 6085755378692.80, NNZs: 843, Bias: -349397753.058765, T: 12164971, Avg. loss: 87314552032361938048592868540416.000000\n",
      "Total training time: 26.03 seconds.\n",
      "-- Epoch 3614\n",
      "Norm: 5809402933467.07, NNZs: 843, Bias: -85854510.125757, T: 12168338, Avg. loss: 87302483099250265319015876395008.000000\n",
      "Total training time: 26.03 seconds.\n",
      "-- Epoch 3615\n",
      "Norm: 5979742662041.71, NNZs: 843, Bias: 83450652.703699, T: 12171705, Avg. loss: 87290330545579695115105212563456.000000\n",
      "Total training time: 26.04 seconds.\n",
      "-- Epoch 3616\n",
      "Norm: 5744780300980.36, NNZs: 843, Bias: -85849088.758081, T: 12175072, Avg. loss: 87278390814043109904871812235264.000000\n",
      "Total training time: 26.05 seconds.\n",
      "-- Epoch 3617\n",
      "Norm: 5875666543301.08, NNZs: 843, Bias: 83432628.623118, T: 12178439, Avg. loss: 87266537806303171325929419964416.000000\n",
      "Total training time: 26.06 seconds.\n",
      "-- Epoch 3618\n",
      "Norm: 5883208638123.04, NNZs: 843, Bias: -85842524.623238, T: 12181806, Avg. loss: 87254275053264972885780181549056.000000\n",
      "Total training time: 26.06 seconds.\n",
      "-- Epoch 3619\n",
      "Norm: 5995744832032.23, NNZs: 843, Bias: 36745442.410705, T: 12185173, Avg. loss: 87241938835561233449713129226240.000000\n",
      "Total training time: 26.07 seconds.\n",
      "-- Epoch 3620\n",
      "Norm: 5786694331723.80, NNZs: 843, Bias: -132504439.022241, T: 12188540, Avg. loss: 87229883727951301852192827244544.000000\n",
      "Total training time: 26.08 seconds.\n",
      "-- Epoch 3621\n",
      "Norm: 5747268680350.79, NNZs: 843, Bias: 36733157.187934, T: 12191907, Avg. loss: 87217670063939246065408561643520.000000\n",
      "Total training time: 26.08 seconds.\n",
      "-- Epoch 3622\n",
      "Norm: 5844525609353.87, NNZs: 843, Bias: 205948390.133206, T: 12195274, Avg. loss: 87205587465189027312634806927360.000000\n",
      "Total training time: 26.09 seconds.\n",
      "-- Epoch 3623\n",
      "Norm: 5922398446537.48, NNZs: 843, Bias: 36723135.391307, T: 12198641, Avg. loss: 87193382695945590951795018956800.000000\n",
      "Total training time: 26.10 seconds.\n",
      "-- Epoch 3624\n",
      "Norm: 6132503132882.90, NNZs: 843, Bias: 205914846.369162, T: 12202008, Avg. loss: 87181312893358435738942022090752.000000\n",
      "Total training time: 26.11 seconds.\n",
      "-- Epoch 3625\n",
      "Norm: 6135263924875.80, NNZs: 843, Bias: 36713217.571731, T: 12205375, Avg. loss: 87169546529723552068202502553600.000000\n",
      "Total training time: 26.11 seconds.\n",
      "-- Epoch 3626\n",
      "Norm: 6494018013508.40, NNZs: 843, Bias: -470812138.530691, T: 12208742, Avg. loss: 87157687540749039254027815092224.000000\n",
      "Total training time: 26.12 seconds.\n",
      "-- Epoch 3627\n",
      "Norm: 6433595940381.28, NNZs: 843, Bias: -49164422.650907, T: 12212109, Avg. loss: 87145490553303033113981610885120.000000\n",
      "Total training time: 26.13 seconds.\n",
      "-- Epoch 3628\n",
      "Norm: 6399812159935.32, NNZs: 843, Bias: -218313902.958645, T: 12215476, Avg. loss: 87133719259946824443813443928064.000000\n",
      "Total training time: 26.13 seconds.\n",
      "-- Epoch 3629\n",
      "Norm: 6519447487131.39, NNZs: 843, Bias: 289114155.655665, T: 12218843, Avg. loss: 87121805021525921603730561564672.000000\n",
      "Total training time: 26.14 seconds.\n",
      "-- Epoch 3630\n",
      "Norm: 6291295733124.25, NNZs: 843, Bias: -127587124.532241, T: 12222210, Avg. loss: 87109839569741740518725432377344.000000\n",
      "Total training time: 26.15 seconds.\n",
      "-- Epoch 3631\n",
      "Norm: 6253121945952.96, NNZs: 843, Bias: -169085333.135454, T: 12225577, Avg. loss: 87097929926112088492709794283520.000000\n",
      "Total training time: 26.16 seconds.\n",
      "-- Epoch 3632\n",
      "Norm: 6054993991194.54, NNZs: 843, Bias: 26850.706270, T: 12228944, Avg. loss: 87085972582316879298489074319360.000000\n",
      "Total training time: 26.16 seconds.\n",
      "-- Epoch 3633\n",
      "Norm: 5947901419266.71, NNZs: 843, Bias: -108823819.635203, T: 12232311, Avg. loss: 87074070850889429137127834124288.000000\n",
      "Total training time: 26.17 seconds.\n",
      "-- Epoch 3634\n",
      "Norm: 5942533188006.25, NNZs: 843, Bias: -277898780.480927, T: 12235678, Avg. loss: 87062199517955715308317036249088.000000\n",
      "Total training time: 26.18 seconds.\n",
      "-- Epoch 3635\n",
      "Norm: 6114455178211.79, NNZs: 843, Bias: 229322083.903970, T: 12239045, Avg. loss: 87050182377240042845480050229248.000000\n",
      "Total training time: 26.18 seconds.\n",
      "-- Epoch 3636\n",
      "Norm: 5943779817488.98, NNZs: 843, Bias: -97630751.140422, T: 12242412, Avg. loss: 87038297158650151777012922974208.000000\n",
      "Total training time: 26.19 seconds.\n",
      "-- Epoch 3637\n",
      "Norm: 5911773295843.63, NNZs: 843, Bias: -30458454.448073, T: 12245779, Avg. loss: 87026547731940104761228834897920.000000\n",
      "Total training time: 26.20 seconds.\n",
      "-- Epoch 3638\n",
      "Norm: 5887624252453.86, NNZs: 843, Bias: 9424346.626522, T: 12249146, Avg. loss: 87014554347183095563158611296256.000000\n",
      "Total training time: 26.21 seconds.\n",
      "-- Epoch 3639\n",
      "Norm: 5986922682588.11, NNZs: 843, Bias: -129056200.468401, T: 12252513, Avg. loss: 87002585546911054342881315651584.000000\n",
      "Total training time: 26.21 seconds.\n",
      "-- Epoch 3640\n",
      "Norm: 6110775302431.26, NNZs: 843, Bias: 233867422.318020, T: 12255880, Avg. loss: 86990499053655475768525890519040.000000\n",
      "Total training time: 26.22 seconds.\n",
      "-- Epoch 3641\n",
      "Norm: 6053838929028.28, NNZs: 843, Bias: 132443799.261636, T: 12259247, Avg. loss: 86978893637775661697620441563136.000000\n",
      "Total training time: 26.23 seconds.\n",
      "-- Epoch 3642\n",
      "Norm: 5940676943717.52, NNZs: 843, Bias: 301418949.575819, T: 12262614, Avg. loss: 86967103112806591090235382169600.000000\n",
      "Total training time: 26.23 seconds.\n",
      "-- Epoch 3643\n",
      "Norm: 5999465720973.12, NNZs: 843, Bias: -205532861.575630, T: 12265981, Avg. loss: 86955116274951365391082697261056.000000\n",
      "Total training time: 26.24 seconds.\n",
      "-- Epoch 3644\n",
      "Norm: 5922934770704.92, NNZs: 843, Bias: -142239748.549259, T: 12269348, Avg. loss: 86943123997517148759913800925184.000000\n",
      "Total training time: 26.25 seconds.\n",
      "-- Epoch 3645\n",
      "Norm: 5817408500104.84, NNZs: 843, Bias: 26720129.195505, T: 12272715, Avg. loss: 86931226052431980068787462340608.000000\n",
      "Total training time: 26.26 seconds.\n",
      "-- Epoch 3646\n",
      "Norm: 5871947532733.70, NNZs: 843, Bias: -26418983.630753, T: 12276082, Avg. loss: 86919352882944587587509395914752.000000\n",
      "Total training time: 26.26 seconds.\n",
      "-- Epoch 3647\n",
      "Norm: 5912830161145.89, NNZs: 843, Bias: -60277424.382754, T: 12279449, Avg. loss: 86907676422366879027665255268352.000000\n",
      "Total training time: 26.27 seconds.\n",
      "-- Epoch 3648\n",
      "Norm: 5862644762608.86, NNZs: 843, Bias: 108642149.902668, T: 12282816, Avg. loss: 86896042529955738582726517719040.000000\n",
      "Total training time: 26.28 seconds.\n",
      "-- Epoch 3649\n",
      "Norm: 5651760247845.34, NNZs: 843, Bias: -60274539.668005, T: 12286183, Avg. loss: 86884047694162762495110527582208.000000\n",
      "Total training time: 26.28 seconds.\n",
      "-- Epoch 3650\n",
      "Norm: 5653011902798.33, NNZs: 843, Bias: -229167813.908642, T: 12289550, Avg. loss: 86871999953403020985672402993152.000000\n",
      "Total training time: 26.29 seconds.\n",
      "-- Epoch 3651\n",
      "Norm: 5691967532711.95, NNZs: 843, Bias: -60271835.590121, T: 12292917, Avg. loss: 86860202101555945171691759468544.000000\n",
      "Total training time: 26.30 seconds.\n",
      "-- Epoch 3652\n",
      "Norm: 5672753593316.88, NNZs: 843, Bias: 108600873.175938, T: 12296284, Avg. loss: 86848368693405528118640926261248.000000\n",
      "Total training time: 26.31 seconds.\n",
      "-- Epoch 3653\n",
      "Norm: 5642118001360.04, NNZs: 843, Bias: -60270595.892018, T: 12299651, Avg. loss: 86836379968856680021218520203264.000000\n",
      "Total training time: 26.31 seconds.\n",
      "-- Epoch 3654\n",
      "Norm: 5969788603069.83, NNZs: 843, Bias: -338887541.635585, T: 12303018, Avg. loss: 86824431083462072967793920180224.000000\n",
      "Total training time: 26.32 seconds.\n",
      "-- Epoch 3655\n",
      "Norm: 5769308387001.75, NNZs: 843, Bias: -170030670.688434, T: 12306385, Avg. loss: 86812493976261405294233076629504.000000\n",
      "Total training time: 26.33 seconds.\n",
      "-- Epoch 3656\n",
      "Norm: 5701456607623.26, NNZs: 843, Bias: -116416904.736802, T: 12309752, Avg. loss: 86800731602089342839459642605568.000000\n",
      "Total training time: 26.33 seconds.\n",
      "-- Epoch 3657\n",
      "Norm: 5709069220809.94, NNZs: 843, Bias: 52403048.560574, T: 12313119, Avg. loss: 86788774142048697244229457412096.000000\n",
      "Total training time: 26.34 seconds.\n",
      "-- Epoch 3658\n",
      "Norm: 5785614641322.59, NNZs: 843, Bias: -116405597.157737, T: 12316486, Avg. loss: 86776871258416409291923962986496.000000\n",
      "Total training time: 26.35 seconds.\n",
      "-- Epoch 3659\n",
      "Norm: 5819381566054.09, NNZs: 843, Bias: 40642800.682357, T: 12319853, Avg. loss: 86764983287113709883135851757568.000000\n",
      "Total training time: 26.36 seconds.\n",
      "-- Epoch 3660\n",
      "Norm: 5875585545366.57, NNZs: 843, Bias: 19773908.341793, T: 12323220, Avg. loss: 86752920065319024243472454385664.000000\n",
      "Total training time: 26.36 seconds.\n",
      "-- Epoch 3661\n",
      "Norm: 5921104417169.26, NNZs: 843, Bias: -148997776.897365, T: 12326587, Avg. loss: 86741076422465679968738884976640.000000\n",
      "Total training time: 26.37 seconds.\n",
      "-- Epoch 3662\n",
      "Norm: 5925093920233.97, NNZs: 843, Bias: -19075181.530273, T: 12329954, Avg. loss: 86729058841532097086495127502848.000000\n",
      "Total training time: 26.38 seconds.\n",
      "-- Epoch 3663\n",
      "Norm: 6028297942798.25, NNZs: 843, Bias: -99214604.766119, T: 12333321, Avg. loss: 86717096713320151630290180112384.000000\n",
      "Total training time: 26.38 seconds.\n",
      "-- Epoch 3664\n",
      "Norm: 5936426158454.76, NNZs: 843, Bias: 115388224.662354, T: 12336688, Avg. loss: 86705328766668305585361425268736.000000\n",
      "Total training time: 26.39 seconds.\n",
      "-- Epoch 3665\n",
      "Norm: 6350010176870.27, NNZs: 843, Bias: -387844931.020554, T: 12340055, Avg. loss: 86693407296510101394705824612352.000000\n",
      "Total training time: 26.40 seconds.\n",
      "-- Epoch 3666\n",
      "Norm: 6036408484837.66, NNZs: 843, Bias: 118308570.301357, T: 12343422, Avg. loss: 86681664521126171933731134111744.000000\n",
      "Total training time: 26.41 seconds.\n",
      "-- Epoch 3667\n",
      "Norm: 5905564531779.98, NNZs: 843, Bias: -9329379.207279, T: 12346789, Avg. loss: 86669915882842864937626071203840.000000\n",
      "Total training time: 26.41 seconds.\n",
      "-- Epoch 3668\n",
      "Norm: 6032695576398.79, NNZs: 843, Bias: -282638595.867523, T: 12350156, Avg. loss: 86658081518677794559982334640128.000000\n",
      "Total training time: 26.42 seconds.\n",
      "-- Epoch 3669\n",
      "Norm: 6103303420681.99, NNZs: 843, Bias: -451297720.565746, T: 12353523, Avg. loss: 86646409630713900881174715170816.000000\n",
      "Total training time: 26.43 seconds.\n",
      "-- Epoch 3670\n",
      "Norm: 6123085129599.12, NNZs: 843, Bias: -282605297.191273, T: 12356890, Avg. loss: 86634879865407329386572881592320.000000\n",
      "Total training time: 26.44 seconds.\n",
      "-- Epoch 3671\n",
      "Norm: 5987156117290.23, NNZs: 843, Bias: -113937164.922401, T: 12360257, Avg. loss: 86623330283469980100153554501632.000000\n",
      "Total training time: 26.44 seconds.\n",
      "-- Epoch 3672\n",
      "Norm: 6118678096159.60, NNZs: 843, Bias: 54708141.761276, T: 12363624, Avg. loss: 86611379370965585119344863477760.000000\n",
      "Total training time: 26.45 seconds.\n",
      "-- Epoch 3673\n",
      "Norm: 6081755416827.87, NNZs: 843, Bias: -113928322.750489, T: 12366991, Avg. loss: 86599562929361080190390688022528.000000\n",
      "Total training time: 26.46 seconds.\n",
      "-- Epoch 3674\n",
      "Norm: 5862702075492.89, NNZs: 843, Bias: 54694223.439294, T: 12370358, Avg. loss: 86587628485777662752150360424448.000000\n",
      "Total training time: 26.46 seconds.\n",
      "-- Epoch 3675\n",
      "Norm: 6008055360144.02, NNZs: 843, Bias: 223293494.161615, T: 12373725, Avg. loss: 86575733223509201997374284627968.000000\n",
      "Total training time: 26.47 seconds.\n",
      "-- Epoch 3676\n",
      "Norm: 5910834261674.92, NNZs: 843, Bias: -9402261.767849, T: 12377092, Avg. loss: 86563843679399326670435418898432.000000\n",
      "Total training time: 26.48 seconds.\n",
      "-- Epoch 3677\n",
      "Norm: 5864297611664.50, NNZs: 843, Bias: 159177874.976373, T: 12380459, Avg. loss: 86552046258106769541014164078592.000000\n",
      "Total training time: 26.49 seconds.\n",
      "-- Epoch 3678\n",
      "Norm: 5737062268820.76, NNZs: 843, Bias: 71118410.306044, T: 12383826, Avg. loss: 86540234434555057861458581782528.000000\n",
      "Total training time: 26.49 seconds.\n",
      "-- Epoch 3679\n",
      "Norm: 5748531980410.75, NNZs: 843, Bias: -97450598.930301, T: 12387193, Avg. loss: 86528470255872446117357467729920.000000\n",
      "Total training time: 26.50 seconds.\n",
      "-- Epoch 3680\n",
      "Norm: 5812359140295.71, NNZs: 843, Bias: -2847323.242864, T: 12390560, Avg. loss: 86516525310049855665482636460032.000000\n",
      "Total training time: 26.51 seconds.\n",
      "-- Epoch 3681\n",
      "Norm: 5761900360922.88, NNZs: 843, Bias: -171388937.876804, T: 12393927, Avg. loss: 86504781346650229587767436771328.000000\n",
      "Total training time: 26.51 seconds.\n",
      "-- Epoch 3682\n",
      "Norm: 5855332906021.26, NNZs: 843, Bias: 149747342.061413, T: 12397294, Avg. loss: 86492942122729920552875111481344.000000\n",
      "Total training time: 26.52 seconds.\n",
      "-- Epoch 3683\n",
      "Norm: 5744133223076.46, NNZs: 843, Bias: -124395403.314978, T: 12400661, Avg. loss: 86481251612153631512726056468480.000000\n",
      "Total training time: 26.53 seconds.\n",
      "-- Epoch 3684\n",
      "Norm: 5708269750402.25, NNZs: 843, Bias: -235962219.141395, T: 12404028, Avg. loss: 86469446252797365989641048555520.000000\n",
      "Total training time: 26.54 seconds.\n",
      "-- Epoch 3685\n",
      "Norm: 5736028638025.45, NNZs: 843, Bias: -67456646.080376, T: 12407395, Avg. loss: 86457778029600433321855449825280.000000\n",
      "Total training time: 26.54 seconds.\n",
      "-- Epoch 3686\n",
      "Norm: 5682134377164.35, NNZs: 843, Bias: 101024506.273998, T: 12410762, Avg. loss: 86446225945797244117543907491840.000000\n",
      "Total training time: 26.55 seconds.\n",
      "-- Epoch 3687\n",
      "Norm: 5719181333075.65, NNZs: 843, Bias: -246912039.756350, T: 12414129, Avg. loss: 86434605847423500051233418248192.000000\n",
      "Total training time: 26.56 seconds.\n",
      "-- Epoch 3688\n",
      "Norm: 5739220599631.14, NNZs: 843, Bias: -78440423.335788, T: 12417496, Avg. loss: 86422747493595912097566528897024.000000\n",
      "Total training time: 26.56 seconds.\n",
      "-- Epoch 3689\n",
      "Norm: 5791220528903.12, NNZs: 843, Bias: -246884198.719321, T: 12420863, Avg. loss: 86410961715066433462408704950272.000000\n",
      "Total training time: 26.57 seconds.\n",
      "-- Epoch 3690\n",
      "Norm: 5812027528858.27, NNZs: 843, Bias: -287105742.437608, T: 12424230, Avg. loss: 86399228952365211381952109608960.000000\n",
      "Total training time: 26.58 seconds.\n",
      "-- Epoch 3691\n",
      "Norm: 5808196997317.26, NNZs: 843, Bias: -118665994.870984, T: 12427597, Avg. loss: 86387437880934406021013720530944.000000\n",
      "Total training time: 26.59 seconds.\n",
      "-- Epoch 3692\n",
      "Norm: 5963899992662.36, NNZs: 843, Bias: 49751989.161234, T: 12430964, Avg. loss: 86375700527732188232312581783552.000000\n",
      "Total training time: 26.59 seconds.\n",
      "-- Epoch 3693\n",
      "Norm: 5900256478657.39, NNZs: 843, Bias: -118654794.217621, T: 12434331, Avg. loss: 86363834840405474957484043010048.000000\n",
      "Total training time: 26.60 seconds.\n",
      "-- Epoch 3694\n",
      "Norm: 6313827717438.11, NNZs: 843, Bias: 49740149.062872, T: 12437698, Avg. loss: 86352197776524294797953003945984.000000\n",
      "Total training time: 26.61 seconds.\n",
      "-- Epoch 3695\n",
      "Norm: 5862949473440.19, NNZs: 843, Bias: -125430446.065339, T: 12441065, Avg. loss: 86340521273830759991587951869952.000000\n",
      "Total training time: 26.61 seconds.\n",
      "-- Epoch 3696\n",
      "Norm: 6130044000231.37, NNZs: 843, Bias: -521567739.180560, T: 12444432, Avg. loss: 86328767814636301556260112695296.000000\n",
      "Total training time: 26.62 seconds.\n",
      "-- Epoch 3697\n",
      "Norm: 5938862554666.21, NNZs: 843, Bias: -16470025.050068, T: 12447799, Avg. loss: 86317253121239453470256099491840.000000\n",
      "Total training time: 26.63 seconds.\n",
      "-- Epoch 3698\n",
      "Norm: 5857034757060.77, NNZs: 843, Bias: 4819668.742281, T: 12451166, Avg. loss: 86305578578232784944272689332224.000000\n",
      "Total training time: 26.64 seconds.\n",
      "-- Epoch 3699\n",
      "Norm: 5895803007678.96, NNZs: 843, Bias: 106293625.301637, T: 12454533, Avg. loss: 86293704090092322288838868205568.000000\n",
      "Total training time: 26.64 seconds.\n",
      "-- Epoch 3700\n",
      "Norm: 5814542826702.74, NNZs: 843, Bias: -62037012.812606, T: 12457900, Avg. loss: 86282053931591126562226009276416.000000\n",
      "Total training time: 26.65 seconds.\n",
      "-- Epoch 3701\n",
      "Norm: 5857351279505.48, NNZs: 843, Bias: 875630.493945, T: 12461267, Avg. loss: 86270498932845994866092229525504.000000\n",
      "Total training time: 26.66 seconds.\n",
      "-- Epoch 3702\n",
      "Norm: 6101875368839.17, NNZs: 843, Bias: 169169870.916066, T: 12464634, Avg. loss: 86258783784991075043517493411840.000000\n",
      "Total training time: 26.66 seconds.\n",
      "-- Epoch 3703\n",
      "Norm: 5839178116020.56, NNZs: 843, Bias: 868133.323770, T: 12468001, Avg. loss: 86246965406268646690018971090944.000000\n",
      "Total training time: 26.67 seconds.\n",
      "-- Epoch 3704\n",
      "Norm: 5866757856824.03, NNZs: 843, Bias: -167411537.987656, T: 12471368, Avg. loss: 86235173977990369053172828209152.000000\n",
      "Total training time: 26.68 seconds.\n",
      "-- Epoch 3705\n",
      "Norm: 6116395350044.12, NNZs: 843, Bias: 337388419.851588, T: 12474735, Avg. loss: 86223493753265703253844424654848.000000\n",
      "Total training time: 26.69 seconds.\n",
      "-- Epoch 3706\n",
      "Norm: 6012944415737.32, NNZs: 843, Bias: 169108707.808383, T: 12478102, Avg. loss: 86211966799932393666731661328384.000000\n",
      "Total training time: 26.69 seconds.\n",
      "-- Epoch 3707\n",
      "Norm: 6279668877916.08, NNZs: 843, Bias: -327329585.459686, T: 12481469, Avg. loss: 86200347254003524545327262597120.000000\n",
      "Total training time: 26.70 seconds.\n",
      "-- Epoch 3708\n",
      "Norm: 5965937380732.79, NNZs: 843, Bias: 35649154.790918, T: 12484836, Avg. loss: 86188636961238420410819942547456.000000\n",
      "Total training time: 26.71 seconds.\n",
      "-- Epoch 3709\n",
      "Norm: 6211048959318.48, NNZs: 843, Bias: -132575837.250019, T: 12488203, Avg. loss: 86176959558547463611375484403712.000000\n",
      "Total training time: 26.71 seconds.\n",
      "-- Epoch 3710\n",
      "Norm: 5945500709197.92, NNZs: 843, Bias: -31139694.328426, T: 12491570, Avg. loss: 86165385861813243633795806003200.000000\n",
      "Total training time: 26.72 seconds.\n",
      "-- Epoch 3711\n",
      "Norm: 5957716379689.91, NNZs: 843, Bias: -27814798.760732, T: 12494937, Avg. loss: 86153857381670146821891282698240.000000\n",
      "Total training time: 26.73 seconds.\n",
      "-- Epoch 3712\n",
      "Norm: 6043734338748.72, NNZs: 843, Bias: -196003659.803250, T: 12498304, Avg. loss: 86142178769933376499345990352896.000000\n",
      "Total training time: 26.74 seconds.\n",
      "-- Epoch 3713\n",
      "Norm: 6232313562554.44, NNZs: 843, Bias: 249511260.389251, T: 12501671, Avg. loss: 86130669658588316694706078613504.000000\n",
      "Total training time: 26.74 seconds.\n",
      "-- Epoch 3714\n",
      "Norm: 6233797057232.24, NNZs: 843, Bias: -180364267.472641, T: 12505038, Avg. loss: 86119356655164540102303333482496.000000\n",
      "Total training time: 26.75 seconds.\n",
      "-- Epoch 3715\n",
      "Norm: 6114654377692.91, NNZs: 843, Bias: -12205892.207432, T: 12508405, Avg. loss: 86107724251839433121833737519104.000000\n",
      "Total training time: 26.76 seconds.\n",
      "-- Epoch 3716\n",
      "Norm: 6193956683097.68, NNZs: 843, Bias: -180348582.890316, T: 12511772, Avg. loss: 86096002349874795090402854567936.000000\n",
      "Total training time: 26.76 seconds.\n",
      "-- Epoch 3717\n",
      "Norm: 6462462575491.47, NNZs: 843, Bias: -12212298.787807, T: 12515139, Avg. loss: 86084226252448267485195182014464.000000\n",
      "Total training time: 26.77 seconds.\n",
      "-- Epoch 3718\n",
      "Norm: 6370737254668.57, NNZs: 843, Bias: 86837823.684794, T: 12518506, Avg. loss: 86072692484505305432613277663232.000000\n",
      "Total training time: 26.78 seconds.\n",
      "-- Epoch 3719\n",
      "Norm: 6198995136258.10, NNZs: 843, Bias: -131407740.918916, T: 12521873, Avg. loss: 86061257216984956144920825954304.000000\n",
      "Total training time: 26.79 seconds.\n",
      "-- Epoch 3720\n",
      "Norm: 6071652747818.48, NNZs: 843, Bias: 36691366.273527, T: 12525240, Avg. loss: 86049665870112335717540586061824.000000\n",
      "Total training time: 26.79 seconds.\n",
      "-- Epoch 3721\n",
      "Norm: 6151037985725.66, NNZs: 843, Bias: -310023232.924923, T: 12528607, Avg. loss: 86038143849306652144062083104768.000000\n",
      "Total training time: 26.80 seconds.\n",
      "-- Epoch 3722\n",
      "Norm: 6208625259078.95, NNZs: 843, Bias: -141934876.911608, T: 12531974, Avg. loss: 86026680737117948552159613157376.000000\n",
      "Total training time: 26.81 seconds.\n",
      "-- Epoch 3723\n",
      "Norm: 6032238558884.48, NNZs: 843, Bias: 26131928.219540, T: 12535341, Avg. loss: 86015182536140419712462452948992.000000\n",
      "Total training time: 26.82 seconds.\n",
      "-- Epoch 3724\n",
      "Norm: 6217729201448.13, NNZs: 843, Bias: -141923819.395483, T: 12538708, Avg. loss: 86003614725481749470808060723200.000000\n",
      "Total training time: 26.82 seconds.\n",
      "-- Epoch 3725\n",
      "Norm: 6260455424128.52, NNZs: 843, Bias: -51799478.121415, T: 12542075, Avg. loss: 85992005325681964461319037911040.000000\n",
      "Total training time: 26.83 seconds.\n",
      "-- Epoch 3726\n",
      "Norm: 6376660905739.65, NNZs: 843, Bias: 116226455.906282, T: 12545442, Avg. loss: 85980429848289717987245094338560.000000\n",
      "Total training time: 26.84 seconds.\n",
      "-- Epoch 3727\n",
      "Norm: 6071100487235.17, NNZs: 843, Bias: -51802421.177028, T: 12548809, Avg. loss: 85968952188784032338546761138176.000000\n",
      "Total training time: 26.84 seconds.\n",
      "-- Epoch 3728\n",
      "Norm: 5999793800688.56, NNZs: 843, Bias: -1115968.789768, T: 12552176, Avg. loss: 85957287881126143249608035270656.000000\n",
      "Total training time: 26.85 seconds.\n",
      "-- Epoch 3729\n",
      "Norm: 6153109851337.63, NNZs: 843, Bias: 40351038.749597, T: 12555543, Avg. loss: 85945868138701542916933518622720.000000\n",
      "Total training time: 26.86 seconds.\n",
      "-- Epoch 3730\n",
      "Norm: 6269018218910.84, NNZs: 843, Bias: 76904303.114372, T: 12558910, Avg. loss: 85934290646671759309116826189824.000000\n",
      "Total training time: 26.87 seconds.\n",
      "-- Epoch 3731\n",
      "Norm: 6133590620740.91, NNZs: 843, Bias: -36317187.660712, T: 12562277, Avg. loss: 85922890723815760905983143968768.000000\n",
      "Total training time: 26.87 seconds.\n",
      "-- Epoch 3732\n",
      "Norm: 5975479370435.16, NNZs: 843, Bias: 223105063.683040, T: 12565644, Avg. loss: 85911232225880582420831940902912.000000\n",
      "Total training time: 26.88 seconds.\n",
      "-- Epoch 3733\n",
      "Norm: 5856697292689.09, NNZs: 843, Bias: -280757471.829986, T: 12569011, Avg. loss: 85899828893650272691393652588544.000000\n",
      "Total training time: 26.89 seconds.\n",
      "-- Epoch 3734\n",
      "Norm: 5854603845114.29, NNZs: 843, Bias: 72522300.777549, T: 12572378, Avg. loss: 85888309591893710819582055284736.000000\n",
      "Total training time: 26.89 seconds.\n",
      "-- Epoch 3735\n",
      "Norm: 5855419145074.63, NNZs: 843, Bias: -95411832.245110, T: 12575745, Avg. loss: 85876722305512379088046150123520.000000\n",
      "Total training time: 26.90 seconds.\n",
      "-- Epoch 3736\n",
      "Norm: 5882198237669.56, NNZs: 843, Bias: 72505566.002838, T: 12579112, Avg. loss: 85865349786775143464359132397568.000000\n",
      "Total training time: 26.91 seconds.\n",
      "-- Epoch 3737\n",
      "Norm: 6250235665232.20, NNZs: 843, Bias: -352594862.375143, T: 12582479, Avg. loss: 85853765800951086013089849540608.000000\n",
      "Total training time: 26.92 seconds.\n",
      "-- Epoch 3738\n",
      "Norm: 6015675960262.88, NNZs: 843, Bias: -184684508.007297, T: 12585846, Avg. loss: 85842274587833552798464161611776.000000\n",
      "Total training time: 26.92 seconds.\n",
      "-- Epoch 3739\n",
      "Norm: 5861407304356.01, NNZs: 843, Bias: -16796136.700315, T: 12589213, Avg. loss: 85830685192810202234351198928896.000000\n",
      "Total training time: 26.93 seconds.\n",
      "-- Epoch 3740\n",
      "Norm: 6299869659964.07, NNZs: 843, Bias: -184668582.373099, T: 12592580, Avg. loss: 85819158714832589920931686645760.000000\n",
      "Total training time: 26.94 seconds.\n",
      "-- Epoch 3741\n",
      "Norm: 6332955949561.80, NNZs: 843, Bias: -282280342.394621, T: 12595947, Avg. loss: 85807597842788494530933151498240.000000\n",
      "Total training time: 26.95 seconds.\n",
      "-- Epoch 3742\n",
      "Norm: 6313173689697.78, NNZs: 843, Bias: -114418668.407884, T: 12599314, Avg. loss: 85795991453102592836304673701888.000000\n",
      "Total training time: 26.95 seconds.\n",
      "-- Epoch 3743\n",
      "Norm: 6011301976285.57, NNZs: 843, Bias: -181022426.678421, T: 12602681, Avg. loss: 85784444177183731019460162617344.000000\n",
      "Total training time: 26.96 seconds.\n",
      "-- Epoch 3744\n",
      "Norm: 6062067186227.79, NNZs: 843, Bias: -36571432.587028, T: 12606048, Avg. loss: 85773037045849577786083349889024.000000\n",
      "Total training time: 26.97 seconds.\n",
      "-- Epoch 3745\n",
      "Norm: 6114058896765.52, NNZs: 843, Bias: 131239844.067447, T: 12609415, Avg. loss: 85761491375885916368709749833728.000000\n",
      "Total training time: 26.97 seconds.\n",
      "-- Epoch 3746\n",
      "Norm: 6078941808899.74, NNZs: 843, Bias: -123361297.289732, T: 12612782, Avg. loss: 85750181974459514448107886608384.000000\n",
      "Total training time: 26.98 seconds.\n",
      "-- Epoch 3747\n",
      "Norm: 6093195474352.07, NNZs: 843, Bias: 26284353.596425, T: 12616149, Avg. loss: 85738602466174915051562681237504.000000\n",
      "Total training time: 26.99 seconds.\n",
      "-- Epoch 3748\n",
      "Norm: 6272521119197.98, NNZs: 843, Bias: -96835045.549419, T: 12619516, Avg. loss: 85727293814646165704950832693248.000000\n",
      "Total training time: 27.00 seconds.\n",
      "-- Epoch 3749\n",
      "Norm: 6078790329551.16, NNZs: 843, Bias: 70936451.684980, T: 12622883, Avg. loss: 85715939690798899954843220180992.000000\n",
      "Total training time: 27.00 seconds.\n",
      "-- Epoch 3750\n",
      "Norm: 6198513709538.41, NNZs: 843, Bias: 30694383.435086, T: 12626250, Avg. loss: 85704440215437694486124259442688.000000\n",
      "Total training time: 27.01 seconds.\n",
      "-- Epoch 3751\n",
      "Norm: 6268718039949.09, NNZs: 843, Bias: -137057261.563952, T: 12629617, Avg. loss: 85692889205132274257009045405696.000000\n",
      "Total training time: 27.02 seconds.\n",
      "-- Epoch 3752\n",
      "Norm: 5992928628018.85, NNZs: 843, Bias: 30682937.770990, T: 12632984, Avg. loss: 85681616075140374062859840126976.000000\n",
      "Total training time: 27.03 seconds.\n",
      "-- Epoch 3753\n",
      "Norm: 6276502683450.30, NNZs: 843, Bias: -472494520.263737, T: 12636351, Avg. loss: 85670213455181096783852074958848.000000\n",
      "Total training time: 27.03 seconds.\n",
      "-- Epoch 3754\n",
      "Norm: 6028818532474.93, NNZs: 843, Bias: 42175995.806086, T: 12639718, Avg. loss: 85658909129808762380653453901824.000000\n",
      "Total training time: 27.04 seconds.\n",
      "-- Epoch 3755\n",
      "Norm: 5979477059437.12, NNZs: 843, Bias: -125531773.687389, T: 12643085, Avg. loss: 85647211158616474319440039641088.000000\n",
      "Total training time: 27.05 seconds.\n",
      "-- Epoch 3756\n",
      "Norm: 6170872610069.93, NNZs: 843, Bias: 135963022.185578, T: 12646452, Avg. loss: 85635827991454572857560429232128.000000\n",
      "Total training time: 27.05 seconds.\n",
      "-- Epoch 3757\n",
      "Norm: 6307216339283.05, NNZs: 843, Bias: 264359307.501723, T: 12649819, Avg. loss: 85624271475147489526665541844992.000000\n",
      "Total training time: 27.06 seconds.\n",
      "-- Epoch 3758\n",
      "Norm: 6068704980850.36, NNZs: 843, Bias: 96669278.324274, T: 12653186, Avg. loss: 85612815461853931494345044656128.000000\n",
      "Total training time: 27.07 seconds.\n",
      "-- Epoch 3759\n",
      "Norm: 6023560521132.09, NNZs: 843, Bias: -70996899.006995, T: 12656553, Avg. loss: 85601540547609195552337425334272.000000\n",
      "Total training time: 27.08 seconds.\n",
      "-- Epoch 3760\n",
      "Norm: 6045486972735.54, NNZs: 843, Bias: -127878462.385659, T: 12659920, Avg. loss: 85590566530761496380866323021824.000000\n",
      "Total training time: 27.08 seconds.\n",
      "-- Epoch 3761\n",
      "Norm: 5879134233109.64, NNZs: 843, Bias: 39761430.094295, T: 12663287, Avg. loss: 85579179191202754595849236381696.000000\n",
      "Total training time: 27.09 seconds.\n",
      "-- Epoch 3762\n",
      "Norm: 5882785828237.19, NNZs: 843, Bias: -127868894.523829, T: 12666654, Avg. loss: 85567661205148558465535773245440.000000\n",
      "Total training time: 27.10 seconds.\n",
      "-- Epoch 3763\n",
      "Norm: 5988687478988.17, NNZs: 843, Bias: -295476072.156077, T: 12670021, Avg. loss: 85556197371489787838734314504192.000000\n",
      "Total training time: 27.10 seconds.\n",
      "-- Epoch 3764\n",
      "Norm: 5720178901186.39, NNZs: 843, Bias: 27777928.592188, T: 12673388, Avg. loss: 85544892503869340536545728266240.000000\n",
      "Total training time: 27.11 seconds.\n",
      "-- Epoch 3765\n",
      "Norm: 5961848007224.36, NNZs: 843, Bias: 195362753.300550, T: 12676755, Avg. loss: 85533516604274658484658123046912.000000\n",
      "Total training time: 27.12 seconds.\n",
      "-- Epoch 3766\n",
      "Norm: 6032839103539.15, NNZs: 843, Bias: -307390513.113477, T: 12680122, Avg. loss: 85522202985550942287200656556032.000000\n",
      "Total training time: 27.13 seconds.\n",
      "-- Epoch 3767\n",
      "Norm: 5793397680959.46, NNZs: 843, Bias: 69649085.090143, T: 12683489, Avg. loss: 85510806760065750426234719830016.000000\n",
      "Total training time: 27.13 seconds.\n",
      "-- Epoch 3768\n",
      "Norm: 6102863407843.80, NNZs: 843, Bias: 23483068.441826, T: 12686856, Avg. loss: 85499384422931964547160850038784.000000\n",
      "Total training time: 27.14 seconds.\n",
      "-- Epoch 3769\n",
      "Norm: 6107766208411.49, NNZs: 843, Bias: -144067493.855169, T: 12690223, Avg. loss: 85487954687060287447205024890880.000000\n",
      "Total training time: 27.15 seconds.\n",
      "-- Epoch 3770\n",
      "Norm: 6306143244048.81, NNZs: 843, Bias: 23473248.964494, T: 12693590, Avg. loss: 85476632327207568564404191494144.000000\n",
      "Total training time: 27.15 seconds.\n",
      "-- Epoch 3771\n",
      "Norm: 6135764414089.33, NNZs: 843, Bias: -144054965.305691, T: 12696957, Avg. loss: 85465366208171672122754177433600.000000\n",
      "Total training time: 27.16 seconds.\n",
      "-- Epoch 3772\n",
      "Norm: 6088864641606.17, NNZs: 843, Bias: 23462585.126916, T: 12700324, Avg. loss: 85454138419269754273198329823232.000000\n",
      "Total training time: 27.17 seconds.\n",
      "-- Epoch 3773\n",
      "Norm: 6236621254212.84, NNZs: 843, Bias: 190958898.678602, T: 12703691, Avg. loss: 85442905059040181704559591161856.000000\n",
      "Total training time: 27.18 seconds.\n",
      "-- Epoch 3774\n",
      "Norm: 6121841367908.54, NNZs: 843, Bias: 23452455.674942, T: 12707058, Avg. loss: 85431524030551021966503884881920.000000\n",
      "Total training time: 27.18 seconds.\n",
      "-- Epoch 3775\n",
      "Norm: 6152109719328.05, NNZs: 843, Bias: -144032292.977936, T: 12710425, Avg. loss: 85420159799058741568294711459840.000000\n",
      "Total training time: 27.19 seconds.\n",
      "-- Epoch 3776\n",
      "Norm: 6108948082426.58, NNZs: 843, Bias: -68431276.217080, T: 12713792, Avg. loss: 85408962611665805637732567875584.000000\n",
      "Total training time: 27.20 seconds.\n",
      "-- Epoch 3777\n",
      "Norm: 6059408715373.86, NNZs: 843, Bias: -235887043.492358, T: 12717159, Avg. loss: 85397652019971565013133267304448.000000\n",
      "Total training time: 27.20 seconds.\n",
      "-- Epoch 3778\n",
      "Norm: 6056224153375.72, NNZs: 843, Bias: -68428600.026827, T: 12720526, Avg. loss: 85386455445511818997051592540160.000000\n",
      "Total training time: 27.21 seconds.\n",
      "-- Epoch 3779\n",
      "Norm: 6303253102289.04, NNZs: 843, Bias: -75335945.617282, T: 12723893, Avg. loss: 85375228745947208314522806779904.000000\n",
      "Total training time: 27.22 seconds.\n",
      "-- Epoch 3780\n",
      "Norm: 6067556062542.97, NNZs: 843, Bias: 92090113.073323, T: 12727260, Avg. loss: 85363933514063655959088683024384.000000\n",
      "Total training time: 27.23 seconds.\n",
      "-- Epoch 3781\n",
      "Norm: 5982472761995.05, NNZs: 843, Bias: -165399391.848848, T: 12730627, Avg. loss: 85352841241830391957291855249408.000000\n",
      "Total training time: 27.23 seconds.\n",
      "-- Epoch 3782\n",
      "Norm: 5867381930224.33, NNZs: 843, Bias: 2008911.507613, T: 12733994, Avg. loss: 85341667212563805172363939545088.000000\n",
      "Total training time: 27.24 seconds.\n",
      "-- Epoch 3783\n",
      "Norm: 6032583846407.37, NNZs: 843, Bias: 192249066.859767, T: 12737361, Avg. loss: 85330333540513949826550991945728.000000\n",
      "Total training time: 27.25 seconds.\n",
      "-- Epoch 3784\n",
      "Norm: 5735934215985.44, NNZs: 843, Bias: 24852965.335937, T: 12740728, Avg. loss: 85319127733034123488376205082624.000000\n",
      "Total training time: 27.26 seconds.\n",
      "-- Epoch 3785\n",
      "Norm: 5681594916153.52, NNZs: 843, Bias: 192215662.010144, T: 12744095, Avg. loss: 85307925598951794357529672155136.000000\n",
      "Total training time: 27.26 seconds.\n",
      "-- Epoch 3786\n",
      "Norm: 6063231231208.98, NNZs: 843, Bias: -252483595.248986, T: 12747462, Avg. loss: 85296583454116137099557370593280.000000\n",
      "Total training time: 27.27 seconds.\n",
      "-- Epoch 3787\n",
      "Norm: 5811814566704.14, NNZs: 843, Bias: -92883577.053260, T: 12750829, Avg. loss: 85285517531082782617584499949568.000000\n",
      "Total training time: 27.28 seconds.\n",
      "-- Epoch 3788\n",
      "Norm: 5769057722135.12, NNZs: 843, Bias: 329405227.688676, T: 12754196, Avg. loss: 85274471263887143596306754174976.000000\n",
      "Total training time: 27.28 seconds.\n",
      "-- Epoch 3789\n",
      "Norm: 5912584268626.12, NNZs: 843, Bias: -207464567.612240, T: 12757563, Avg. loss: 85263161921914723926060342181888.000000\n",
      "Total training time: 27.29 seconds.\n",
      "-- Epoch 3790\n",
      "Norm: 5619904202537.32, NNZs: 843, Bias: 34998737.532775, T: 12760930, Avg. loss: 85252143583536297526484858306560.000000\n",
      "Total training time: 27.30 seconds.\n",
      "-- Epoch 3791\n",
      "Norm: 5591168131898.72, NNZs: 843, Bias: 90396639.835413, T: 12764297, Avg. loss: 85240714549080698752702487724032.000000\n",
      "Total training time: 27.31 seconds.\n",
      "-- Epoch 3792\n",
      "Norm: 6121206973748.89, NNZs: 843, Bias: -76902778.224226, T: 12767664, Avg. loss: 85229605118649471389770824286208.000000\n",
      "Total training time: 27.31 seconds.\n",
      "-- Epoch 3793\n",
      "Norm: 5675822042491.01, NNZs: 843, Bias: -139119497.134824, T: 12771031, Avg. loss: 85218417894927540872573179920384.000000\n",
      "Total training time: 27.32 seconds.\n",
      "-- Epoch 3794\n",
      "Norm: 5672778320720.96, NNZs: 843, Bias: -318144773.641525, T: 12774398, Avg. loss: 85207134228740893727086256062464.000000\n",
      "Total training time: 27.33 seconds.\n",
      "-- Epoch 3795\n",
      "Norm: 5800837314948.81, NNZs: 843, Bias: -44714716.118104, T: 12777765, Avg. loss: 85195948219726341896549443829760.000000\n",
      "Total training time: 27.33 seconds.\n",
      "-- Epoch 3796\n",
      "Norm: 5777940802314.20, NNZs: 843, Bias: 122531140.537648, T: 12781132, Avg. loss: 85184641825585724854406039470080.000000\n",
      "Total training time: 27.34 seconds.\n",
      "-- Epoch 3797\n",
      "Norm: 5769951693413.62, NNZs: 843, Bias: -104000638.860953, T: 12784499, Avg. loss: 85173234501551154448333235290112.000000\n",
      "Total training time: 27.35 seconds.\n",
      "-- Epoch 3798\n",
      "Norm: 5626015339561.05, NNZs: 843, Bias: -271222322.482410, T: 12787866, Avg. loss: 85161939974263113669605079384064.000000\n",
      "Total training time: 27.36 seconds.\n",
      "-- Epoch 3799\n",
      "Norm: 5959224130847.58, NNZs: 843, Bias: -586109940.511637, T: 12791233, Avg. loss: 85150909744745603339289001721856.000000\n",
      "Total training time: 27.36 seconds.\n",
      "-- Epoch 3800\n",
      "Norm: 5836054273078.61, NNZs: 843, Bias: 249939032.354823, T: 12794600, Avg. loss: 85139757000684993955721723772928.000000\n",
      "Total training time: 27.37 seconds.\n",
      "-- Epoch 3801\n",
      "Norm: 5774722867639.44, NNZs: 843, Bias: 82727727.675217, T: 12797967, Avg. loss: 85128623650256494903838617108480.000000\n",
      "Total training time: 27.38 seconds.\n",
      "-- Epoch 3802\n",
      "Norm: 5773787571325.01, NNZs: 843, Bias: -84461741.001422, T: 12801334, Avg. loss: 85117501211796358326044869001216.000000\n",
      "Total training time: 27.38 seconds.\n",
      "-- Epoch 3803\n",
      "Norm: 5675197492690.35, NNZs: 843, Bias: 40995191.940852, T: 12804701, Avg. loss: 85106248827296750346166847143936.000000\n",
      "Total training time: 27.39 seconds.\n",
      "-- Epoch 3804\n",
      "Norm: 5792762149117.00, NNZs: 843, Bias: -126168728.780399, T: 12808068, Avg. loss: 85095018470588259612279493885952.000000\n",
      "Total training time: 27.40 seconds.\n",
      "-- Epoch 3805\n",
      "Norm: 5744700328098.75, NNZs: 843, Bias: -54612968.996208, T: 12811435, Avg. loss: 85083897391552514822407224033280.000000\n",
      "Total training time: 27.41 seconds.\n",
      "-- Epoch 3806\n",
      "Norm: 5869045854552.32, NNZs: 843, Bias: 112524059.747255, T: 12814802, Avg. loss: 85072724140981344025531694186496.000000\n",
      "Total training time: 27.41 seconds.\n",
      "-- Epoch 3807\n",
      "Norm: 5904417417562.49, NNZs: 843, Bias: -398978431.892068, T: 12818169, Avg. loss: 85061617971330500155621353979904.000000\n",
      "Total training time: 27.42 seconds.\n",
      "-- Epoch 3808\n",
      "Norm: 5856155784354.23, NNZs: 843, Bias: -80765177.647249, T: 12821536, Avg. loss: 85050504084749787268951350181888.000000\n",
      "Total training time: 27.43 seconds.\n",
      "-- Epoch 3809\n",
      "Norm: 6007311503602.43, NNZs: 843, Bias: 86340638.766794, T: 12824903, Avg. loss: 85039292315317944816898255355904.000000\n",
      "Total training time: 27.43 seconds.\n",
      "-- Epoch 3810\n",
      "Norm: 6086618770704.86, NNZs: 843, Bias: 253425047.060551, T: 12828270, Avg. loss: 85028244632335742973525973008384.000000\n",
      "Total training time: 27.44 seconds.\n",
      "-- Epoch 3811\n",
      "Norm: 5902699329487.28, NNZs: 843, Bias: -297159211.783283, T: 12831637, Avg. loss: 85017271556845533980986890518528.000000\n",
      "Total training time: 27.45 seconds.\n",
      "-- Epoch 3812\n",
      "Norm: 5849417686525.83, NNZs: 843, Bias: 43721098.304588, T: 12835004, Avg. loss: 85006292449408111264398118486016.000000\n",
      "Total training time: 27.46 seconds.\n",
      "-- Epoch 3813\n",
      "Norm: 5845778420345.73, NNZs: 843, Bias: -123344849.486669, T: 12838371, Avg. loss: 84995026584168482554750754095104.000000\n",
      "Total training time: 27.46 seconds.\n",
      "-- Epoch 3814\n",
      "Norm: 5912092183460.34, NNZs: 843, Bias: -290390287.680924, T: 12841738, Avg. loss: 84983817161922675881254255067136.000000\n",
      "Total training time: 27.47 seconds.\n",
      "-- Epoch 3815\n",
      "Norm: 6079467559020.84, NNZs: 843, Bias: -332291529.234928, T: 12845105, Avg. loss: 84972584974730169842293436055552.000000\n",
      "Total training time: 27.48 seconds.\n",
      "-- Epoch 3816\n",
      "Norm: 5907420115604.25, NNZs: 843, Bias: -165245721.645286, T: 12848472, Avg. loss: 84961384904971034489379987914752.000000\n",
      "Total training time: 27.48 seconds.\n",
      "-- Epoch 3817\n",
      "Norm: 5977316278066.75, NNZs: 843, Bias: -332256946.039638, T: 12851839, Avg. loss: 84950240160702757820827185446912.000000\n",
      "Total training time: 27.49 seconds.\n",
      "-- Epoch 3818\n",
      "Norm: 5965756219195.45, NNZs: 843, Bias: 168775890.892678, T: 12855206, Avg. loss: 84939205882704914885675810029568.000000\n",
      "Total training time: 27.50 seconds.\n",
      "-- Epoch 3819\n",
      "Norm: 6076263861755.97, NNZs: 843, Bias: 1765673.386723, T: 12858573, Avg. loss: 84928054799734752282068702461952.000000\n",
      "Total training time: 27.51 seconds.\n",
      "-- Epoch 3820\n",
      "Norm: 5828264065945.00, NNZs: 843, Bias: 168744426.617631, T: 12861940, Avg. loss: 84916842725245428145784818237440.000000\n",
      "Total training time: 27.51 seconds.\n",
      "-- Epoch 3821\n",
      "Norm: 5990536740532.67, NNZs: 843, Bias: -332188082.891512, T: 12865307, Avg. loss: 84905623704406416653851155234816.000000\n",
      "Total training time: 27.52 seconds.\n",
      "-- Epoch 3822\n",
      "Norm: 5805866817558.32, NNZs: 843, Bias: 29212356.196583, T: 12868674, Avg. loss: 84894431215201434230274715025408.000000\n",
      "Total training time: 27.53 seconds.\n",
      "-- Epoch 3823\n",
      "Norm: 5954968004229.96, NNZs: 843, Bias: 196158033.980442, T: 12872041, Avg. loss: 84883212313230323216766866554880.000000\n",
      "Total training time: 27.53 seconds.\n",
      "-- Epoch 3824\n",
      "Norm: 5941561616782.41, NNZs: 843, Bias: -77948315.573393, T: 12875408, Avg. loss: 84872068904533825471309580075008.000000\n",
      "Total training time: 27.54 seconds.\n",
      "-- Epoch 3825\n",
      "Norm: 5862483110069.57, NNZs: 843, Bias: 57867620.574467, T: 12878775, Avg. loss: 84861036876610142944987396964352.000000\n",
      "Total training time: 27.55 seconds.\n",
      "-- Epoch 3826\n",
      "Norm: 5980430049556.14, NNZs: 843, Bias: 164030817.204914, T: 12882142, Avg. loss: 84849925540454936641564772925440.000000\n",
      "Total training time: 27.56 seconds.\n",
      "-- Epoch 3827\n",
      "Norm: 5817286543954.83, NNZs: 843, Bias: -2891519.924517, T: 12885509, Avg. loss: 84838934776124139398258506072064.000000\n",
      "Total training time: 27.56 seconds.\n",
      "-- Epoch 3828\n",
      "Norm: 5886928663349.72, NNZs: 843, Bias: -169791139.253224, T: 12888876, Avg. loss: 84827780454414552151552962854912.000000\n",
      "Total training time: 27.57 seconds.\n",
      "-- Epoch 3829\n",
      "Norm: 6082572730447.39, NNZs: 843, Bias: -37098802.496946, T: 12892243, Avg. loss: 84816775922303857566663235862528.000000\n",
      "Total training time: 27.58 seconds.\n",
      "-- Epoch 3830\n",
      "Norm: 6068645910163.57, NNZs: 843, Bias: 129772835.847837, T: 12895610, Avg. loss: 84805610787924713180248340955136.000000\n",
      "Total training time: 27.58 seconds.\n",
      "-- Epoch 3831\n",
      "Norm: 6042569103333.83, NNZs: 843, Bias: -37105178.550590, T: 12898977, Avg. loss: 84794590215645291224070004670464.000000\n",
      "Total training time: 27.59 seconds.\n",
      "-- Epoch 3832\n",
      "Norm: 6297869684188.72, NNZs: 843, Bias: 129745588.801576, T: 12902344, Avg. loss: 84783403029811045493631264751616.000000\n",
      "Total training time: 27.60 seconds.\n",
      "-- Epoch 3833\n",
      "Norm: 6098665732559.00, NNZs: 843, Bias: -37109507.337537, T: 12905711, Avg. loss: 84772537366152094695924187529216.000000\n",
      "Total training time: 27.61 seconds.\n",
      "-- Epoch 3834\n",
      "Norm: 6112770493404.70, NNZs: 843, Bias: 129718262.946109, T: 12909078, Avg. loss: 84761510434519872986955816894464.000000\n",
      "Total training time: 27.61 seconds.\n",
      "-- Epoch 3835\n",
      "Norm: 6097691709699.21, NNZs: 843, Bias: -37115222.753242, T: 12912445, Avg. loss: 84750274375263606199187094372352.000000\n",
      "Total training time: 27.62 seconds.\n",
      "-- Epoch 3836\n",
      "Norm: 6332533395842.16, NNZs: 843, Bias: 129690948.306213, T: 12915812, Avg. loss: 84739063037753809465743694102528.000000\n",
      "Total training time: 27.63 seconds.\n",
      "-- Epoch 3837\n",
      "Norm: 6230976593391.96, NNZs: 843, Bias: -37121489.789591, T: 12919179, Avg. loss: 84728036735409419574066653691904.000000\n",
      "Total training time: 27.63 seconds.\n",
      "-- Epoch 3838\n",
      "Norm: 6071829267640.90, NNZs: 843, Bias: 167424175.978004, T: 12922546, Avg. loss: 84717093437566374007953789288448.000000\n",
      "Total training time: 27.64 seconds.\n",
      "-- Epoch 3839\n",
      "Norm: 5975599774514.31, NNZs: 843, Bias: 632204.619187, T: 12925913, Avg. loss: 84706289559986777095118392393728.000000\n",
      "Total training time: 27.65 seconds.\n",
      "-- Epoch 3840\n",
      "Norm: 5989069698984.56, NNZs: 843, Bias: -259875174.787976, T: 12929280, Avg. loss: 84695094810330367943731625066496.000000\n",
      "Total training time: 27.66 seconds.\n",
      "-- Epoch 3841\n",
      "Norm: 6180890374478.75, NNZs: 843, Bias: -93108493.559683, T: 12932647, Avg. loss: 84684018696013380319547517566976.000000\n",
      "Total training time: 27.66 seconds.\n",
      "-- Epoch 3842\n",
      "Norm: 6064377412635.04, NNZs: 843, Bias: -286789575.675164, T: 12936014, Avg. loss: 84673051083214203201125804408832.000000\n",
      "Total training time: 27.67 seconds.\n",
      "-- Epoch 3843\n",
      "Norm: 5935100905871.45, NNZs: 843, Bias: -120041908.790673, T: 12939381, Avg. loss: 84662077724986604479183243444224.000000\n",
      "Total training time: 27.68 seconds.\n",
      "-- Epoch 3844\n",
      "Norm: 6155302527813.57, NNZs: 843, Bias: -286760843.680783, T: 12942748, Avg. loss: 84651120628857394588386766880768.000000\n",
      "Total training time: 27.68 seconds.\n",
      "-- Epoch 3845\n",
      "Norm: 5856000660784.41, NNZs: 843, Bias: -120034685.580220, T: 12946115, Avg. loss: 84640029393474722285187311337472.000000\n",
      "Total training time: 27.69 seconds.\n",
      "-- Epoch 3846\n",
      "Norm: 6244547336730.60, NNZs: 843, Bias: -107406807.927166, T: 12949482, Avg. loss: 84629170554627680390410892476416.000000\n",
      "Total training time: 27.70 seconds.\n",
      "-- Epoch 3847\n",
      "Norm: 5828081361645.88, NNZs: 843, Bias: -141700955.519184, T: 12952849, Avg. loss: 84618300106502255576434908069888.000000\n",
      "Total training time: 27.71 seconds.\n",
      "-- Epoch 3848\n",
      "Norm: 5811389920582.25, NNZs: 843, Bias: 24982713.780704, T: 12956216, Avg. loss: 84607303785204359688248012832768.000000\n",
      "Total training time: 27.71 seconds.\n",
      "-- Epoch 3849\n",
      "Norm: 6201191875592.81, NNZs: 843, Bias: 191645707.864306, T: 12959583, Avg. loss: 84596192020724565034160736960512.000000\n",
      "Total training time: 27.72 seconds.\n",
      "-- Epoch 3850\n",
      "Norm: 5850216294617.16, NNZs: 843, Bias: 24972087.685121, T: 12962950, Avg. loss: 84585206192164670083338847911936.000000\n",
      "Total training time: 27.73 seconds.\n",
      "-- Epoch 3851\n",
      "Norm: 5948883201246.01, NNZs: 843, Bias: 191612490.118306, T: 12966317, Avg. loss: 84574266797183125217017457541120.000000\n",
      "Total training time: 27.73 seconds.\n",
      "-- Epoch 3852\n",
      "Norm: 5925047095643.51, NNZs: 843, Bias: 154950885.136510, T: 12969684, Avg. loss: 84563212591817330721283429629952.000000\n",
      "Total training time: 27.74 seconds.\n",
      "-- Epoch 3853\n",
      "Norm: 6033725495210.95, NNZs: 843, Bias: 275170693.410180, T: 12973051, Avg. loss: 84552384455209734372624783376384.000000\n",
      "Total training time: 27.75 seconds.\n",
      "-- Epoch 3854\n",
      "Norm: 5882087179822.08, NNZs: 843, Bias: -224693780.421778, T: 12976418, Avg. loss: 84541343662444588316537040928768.000000\n",
      "Total training time: 27.76 seconds.\n",
      "-- Epoch 3855\n",
      "Norm: 5828202760035.05, NNZs: 843, Bias: -58081582.906806, T: 12979785, Avg. loss: 84530484363746895988374043099136.000000\n",
      "Total training time: 27.76 seconds.\n",
      "-- Epoch 3856\n",
      "Norm: 5937840202477.55, NNZs: 843, Bias: -224676083.269671, T: 12983152, Avg. loss: 84519567975006235581946578075648.000000\n",
      "Total training time: 27.77 seconds.\n",
      "-- Epoch 3857\n",
      "Norm: 5964919677014.83, NNZs: 843, Bias: -58085807.637277, T: 12986519, Avg. loss: 84508980106391973331980261523456.000000\n",
      "Total training time: 27.78 seconds.\n",
      "-- Epoch 3858\n",
      "Norm: 6008102017795.02, NNZs: 843, Bias: -224657474.284425, T: 12989886, Avg. loss: 84497957873467780814966339141632.000000\n",
      "Total training time: 27.78 seconds.\n",
      "-- Epoch 3859\n",
      "Norm: 6039581354613.10, NNZs: 843, Bias: -391208027.543935, T: 12993253, Avg. loss: 84487036768900836975355132641280.000000\n",
      "Total training time: 27.79 seconds.\n",
      "-- Epoch 3860\n",
      "Norm: 6021061175710.00, NNZs: 843, Bias: -224638428.539654, T: 12996620, Avg. loss: 84476178268319865653918538137600.000000\n",
      "Total training time: 27.80 seconds.\n",
      "-- Epoch 3861\n",
      "Norm: 6037504546824.03, NNZs: 843, Bias: 274986063.320771, T: 12999987, Avg. loss: 84465234327599664625300023541760.000000\n",
      "Total training time: 27.81 seconds.\n",
      "-- Epoch 3862\n",
      "Norm: 5954885163713.18, NNZs: 843, Bias: -224620176.443580, T: 13003354, Avg. loss: 84454373929622914909494065496064.000000\n",
      "Total training time: 27.81 seconds.\n",
      "-- Epoch 3863\n",
      "Norm: 6009284660208.84, NNZs: 843, Bias: -391128056.567870, T: 13006721, Avg. loss: 84443446787910486483140029186048.000000\n",
      "Total training time: 27.82 seconds.\n",
      "-- Epoch 3864\n",
      "Norm: 6115234756118.44, NNZs: 843, Bias: 108408908.052859, T: 13010088, Avg. loss: 84432370757322189345613007552512.000000\n",
      "Total training time: 27.83 seconds.\n",
      "-- Epoch 3865\n",
      "Norm: 6022176614620.89, NNZs: 843, Bias: -391090029.815510, T: 13013455, Avg. loss: 84421616354626864937697766539264.000000\n",
      "Total training time: 27.84 seconds.\n",
      "-- Epoch 3866\n",
      "Norm: 5853425893199.25, NNZs: 843, Bias: -224585167.219581, T: 13016822, Avg. loss: 84410793867248784539293111549952.000000\n",
      "Total training time: 27.84 seconds.\n",
      "-- Epoch 3867\n",
      "Norm: 5913121006539.81, NNZs: 843, Bias: -58102558.859469, T: 13020189, Avg. loss: 84399784195395287442629704286208.000000\n",
      "Total training time: 27.85 seconds.\n",
      "-- Epoch 3868\n",
      "Norm: 6230387358042.61, NNZs: 843, Bias: 108357531.140779, T: 13023556, Avg. loss: 84388811770223063202539295997952.000000\n",
      "Total training time: 27.86 seconds.\n",
      "-- Epoch 3869\n",
      "Norm: 6045439131829.93, NNZs: 843, Bias: -58106377.763586, T: 13026923, Avg. loss: 84378036451652203029925654953984.000000\n",
      "Total training time: 27.86 seconds.\n",
      "-- Epoch 3870\n",
      "Norm: 6072147107896.73, NNZs: 843, Bias: -224549564.207278, T: 13030290, Avg. loss: 84367116577472064094853738266624.000000\n",
      "Total training time: 27.87 seconds.\n",
      "-- Epoch 3871\n",
      "Norm: 6110637323546.79, NNZs: 843, Bias: -58109274.167749, T: 13033657, Avg. loss: 84356350839220368705478113361920.000000\n",
      "Total training time: 27.88 seconds.\n",
      "-- Epoch 3872\n",
      "Norm: 6127463960687.39, NNZs: 843, Bias: -224530292.182746, T: 13037024, Avg. loss: 84345467118864416178379977392128.000000\n",
      "Total training time: 27.89 seconds.\n",
      "-- Epoch 3873\n",
      "Norm: 5883518032621.94, NNZs: 843, Bias: -268145096.272999, T: 13040391, Avg. loss: 84334762061829508511414660104192.000000\n",
      "Total training time: 27.89 seconds.\n",
      "-- Epoch 3874\n",
      "Norm: 5979827973477.89, NNZs: 843, Bias: 7013683.152147, T: 13043758, Avg. loss: 84323925195860166785385411641344.000000\n",
      "Total training time: 27.90 seconds.\n",
      "-- Epoch 3875\n",
      "Norm: 6291308014018.05, NNZs: 843, Bias: 173396747.007051, T: 13047125, Avg. loss: 84313055627360599424439641178112.000000\n",
      "Total training time: 27.91 seconds.\n",
      "-- Epoch 3876\n",
      "Norm: 6162304503582.00, NNZs: 843, Bias: 238778697.091167, T: 13050492, Avg. loss: 84302485196243624295078492635136.000000\n",
      "Total training time: 27.91 seconds.\n",
      "-- Epoch 3877\n",
      "Norm: 6156918735752.66, NNZs: 843, Bias: -128649335.221857, T: 13053859, Avg. loss: 84291270839881663932983102406656.000000\n",
      "Total training time: 27.92 seconds.\n",
      "-- Epoch 3878\n",
      "Norm: 5987735633339.75, NNZs: 843, Bias: 37709559.723536, T: 13057226, Avg. loss: 84280334102330630378675301777408.000000\n",
      "Total training time: 27.93 seconds.\n",
      "-- Epoch 3879\n",
      "Norm: 6025201328976.16, NNZs: 843, Bias: -70596982.622265, T: 13060593, Avg. loss: 84269507033412960925755992178688.000000\n",
      "Total training time: 27.94 seconds.\n",
      "-- Epoch 3880\n",
      "Norm: 6174450947971.14, NNZs: 843, Bias: 87683991.124163, T: 13063960, Avg. loss: 84258540138556649628475204829184.000000\n",
      "Total training time: 27.94 seconds.\n",
      "-- Epoch 3881\n",
      "Norm: 6112098645539.02, NNZs: 843, Bias: -78649439.276656, T: 13067327, Avg. loss: 84247773377755921459585916862464.000000\n",
      "Total training time: 27.95 seconds.\n",
      "-- Epoch 3882\n",
      "Norm: 6042959144924.02, NNZs: 843, Bias: 87664074.775236, T: 13070694, Avg. loss: 84237041788996678544535343595520.000000\n",
      "Total training time: 27.96 seconds.\n",
      "-- Epoch 3883\n",
      "Norm: 6100706347804.63, NNZs: 843, Bias: -78647552.942202, T: 13074061, Avg. loss: 84226201996875520522992975085568.000000\n",
      "Total training time: 27.96 seconds.\n",
      "-- Epoch 3884\n",
      "Norm: 5853236722681.11, NNZs: 843, Bias: -41818732.424963, T: 13077428, Avg. loss: 84215571648148061266152662237184.000000\n",
      "Total training time: 27.97 seconds.\n",
      "-- Epoch 3885\n",
      "Norm: 5907585161895.58, NNZs: 843, Bias: -192860709.623219, T: 13080795, Avg. loss: 84204577909268324979729166761984.000000\n",
      "Total training time: 27.98 seconds.\n",
      "-- Epoch 3886\n",
      "Norm: 5837605245037.74, NNZs: 843, Bias: 66970066.863183, T: 13084162, Avg. loss: 84193684545558089096142111375360.000000\n",
      "Total training time: 27.99 seconds.\n",
      "-- Epoch 3887\n",
      "Norm: 5892372557829.64, NNZs: 843, Bias: 233220489.985408, T: 13087529, Avg. loss: 84182932095853751100228096753664.000000\n",
      "Total training time: 27.99 seconds.\n",
      "-- Epoch 3888\n",
      "Norm: 5770211343096.24, NNZs: 843, Bias: 62499422.605809, T: 13090896, Avg. loss: 84172061614721804905223936802816.000000\n",
      "Total training time: 28.00 seconds.\n",
      "-- Epoch 3889\n",
      "Norm: 5746935954242.26, NNZs: 843, Bias: -103746761.677751, T: 13094263, Avg. loss: 84161325696997342031161705103360.000000\n",
      "Total training time: 28.01 seconds.\n",
      "-- Epoch 3890\n",
      "Norm: 5738922890168.55, NNZs: 843, Bias: -269971228.051028, T: 13097630, Avg. loss: 84150466104907806104748694700032.000000\n",
      "Total training time: 28.02 seconds.\n",
      "-- Epoch 3891\n",
      "Norm: 5954294723437.10, NNZs: 843, Bias: -103741656.540825, T: 13100997, Avg. loss: 84139573174600147295671620206592.000000\n",
      "Total training time: 28.02 seconds.\n",
      "-- Epoch 3892\n",
      "Norm: 6013805328767.02, NNZs: 843, Bias: -269944245.423035, T: 13104364, Avg. loss: 84128739509053244295102437785600.000000\n",
      "Total training time: 28.03 seconds.\n",
      "-- Epoch 3893\n",
      "Norm: 5866167472561.68, NNZs: 843, Bias: -103736858.788617, T: 13107731, Avg. loss: 84118026594398393152926413488128.000000\n",
      "Total training time: 28.04 seconds.\n",
      "-- Epoch 3894\n",
      "Norm: 5829333231695.61, NNZs: 843, Bias: -269918839.540380, T: 13111098, Avg. loss: 84107284643291455382368534134784.000000\n",
      "Total training time: 28.04 seconds.\n",
      "-- Epoch 3895\n",
      "Norm: 5882637627344.06, NNZs: 843, Bias: -122692842.158327, T: 13114465, Avg. loss: 84096413269502576284530177474560.000000\n",
      "Total training time: 28.05 seconds.\n",
      "-- Epoch 3896\n",
      "Norm: 5826494748575.32, NNZs: 843, Bias: -54948987.613259, T: 13117832, Avg. loss: 84085626023031549862801531994112.000000\n",
      "Total training time: 28.06 seconds.\n",
      "-- Epoch 3897\n",
      "Norm: 6019582291622.96, NNZs: 843, Bias: 113954427.052138, T: 13121199, Avg. loss: 84074880036201608137462249422848.000000\n",
      "Total training time: 28.07 seconds.\n",
      "-- Epoch 3898\n",
      "Norm: 6175694795937.79, NNZs: 843, Bias: 280084430.190924, T: 13124566, Avg. loss: 84064034923377520184854279356416.000000\n",
      "Total training time: 28.07 seconds.\n",
      "-- Epoch 3899\n",
      "Norm: 6038170387543.48, NNZs: 843, Bias: 194919993.371120, T: 13127933, Avg. loss: 84053435287571900675066479771648.000000\n",
      "Total training time: 28.08 seconds.\n",
      "-- Epoch 3900\n",
      "Norm: 6107203165143.15, NNZs: 843, Bias: -303457274.395258, T: 13131300, Avg. loss: 84042708409431627102454747758592.000000\n",
      "Total training time: 28.09 seconds.\n",
      "-- Epoch 3901\n",
      "Norm: 6052544396931.44, NNZs: 843, Bias: 194887553.927867, T: 13134667, Avg. loss: 84031841909314812071981682262016.000000\n",
      "Total training time: 28.09 seconds.\n",
      "-- Epoch 3902\n",
      "Norm: 5991828381237.42, NNZs: 843, Bias: 28771086.014557, T: 13138034, Avg. loss: 84020806573449266927596455591936.000000\n",
      "Total training time: 28.10 seconds.\n",
      "-- Epoch 3903\n",
      "Norm: 5860864975497.76, NNZs: 843, Bias: -137322784.450385, T: 13141401, Avg. loss: 84009908009671574373119985451008.000000\n",
      "Total training time: 28.11 seconds.\n",
      "-- Epoch 3904\n",
      "Norm: 5975886129841.56, NNZs: 843, Bias: 28759588.280614, T: 13144768, Avg. loss: 83999079346589254445975289724928.000000\n",
      "Total training time: 28.12 seconds.\n",
      "-- Epoch 3905\n",
      "Norm: 5842148805752.77, NNZs: 843, Bias: -137313911.789745, T: 13148135, Avg. loss: 83988353998568535521903933128704.000000\n",
      "Total training time: 28.12 seconds.\n",
      "-- Epoch 3906\n",
      "Norm: 5946194302594.69, NNZs: 843, Bias: 28748097.557268, T: 13151502, Avg. loss: 83977537766913247708274552209408.000000\n",
      "Total training time: 28.13 seconds.\n",
      "-- Epoch 3907\n",
      "Norm: 6061400878743.43, NNZs: 843, Bias: 194788722.690088, T: 13154869, Avg. loss: 83966917865181430535138987474944.000000\n",
      "Total training time: 28.14 seconds.\n",
      "-- Epoch 3908\n",
      "Norm: 5877472029900.07, NNZs: 843, Bias: -303333804.288088, T: 13158236, Avg. loss: 83956248867517002521682190008320.000000\n",
      "Total training time: 28.14 seconds.\n",
      "-- Epoch 3909\n",
      "Norm: 5889686508369.68, NNZs: 843, Bias: 194755779.284659, T: 13161603, Avg. loss: 83945574637482545106862829731840.000000\n",
      "Total training time: 28.15 seconds.\n",
      "-- Epoch 3910\n",
      "Norm: 5983713077226.04, NNZs: 843, Bias: 28723896.469737, T: 13164970, Avg. loss: 83934587785782384817446078906368.000000\n",
      "Total training time: 28.16 seconds.\n",
      "-- Epoch 3911\n",
      "Norm: 5871829669018.83, NNZs: 843, Bias: 194721198.541330, T: 13168337, Avg. loss: 83923860801502175946891971264512.000000\n",
      "Total training time: 28.17 seconds.\n",
      "-- Epoch 3912\n",
      "Norm: 5786452898418.57, NNZs: 843, Bias: 28710751.447569, T: 13171704, Avg. loss: 83912932998766724468228101242880.000000\n",
      "Total training time: 28.17 seconds.\n",
      "-- Epoch 3913\n",
      "Norm: 5697600285849.46, NNZs: 843, Bias: -137277825.575981, T: 13175071, Avg. loss: 83902108455669595394380203556864.000000\n",
      "Total training time: 28.18 seconds.\n",
      "-- Epoch 3914\n",
      "Norm: 6074704550500.63, NNZs: 843, Bias: -564645631.941188, T: 13178438, Avg. loss: 83891311236544575656142810644480.000000\n",
      "Total training time: 28.19 seconds.\n",
      "-- Epoch 3915\n",
      "Norm: 5842937795081.19, NNZs: 843, Bias: 90651558.013962, T: 13181805, Avg. loss: 83880678914566849193161680486400.000000\n",
      "Total training time: 28.20 seconds.\n",
      "-- Epoch 3916\n",
      "Norm: 5831529487241.77, NNZs: 843, Bias: -186464112.182648, T: 13185172, Avg. loss: 83870081039735067043745258012672.000000\n",
      "Total training time: 28.20 seconds.\n",
      "-- Epoch 3917\n",
      "Norm: 5902253598333.51, NNZs: 843, Bias: -20516587.750220, T: 13188539, Avg. loss: 83859264835581478729015057448960.000000\n",
      "Total training time: 28.21 seconds.\n",
      "-- Epoch 3918\n",
      "Norm: 6142319130994.45, NNZs: 843, Bias: -186449639.259655, T: 13191906, Avg. loss: 83848526623629924987353480298496.000000\n",
      "Total training time: 28.22 seconds.\n",
      "-- Epoch 3919\n",
      "Norm: 5912566659468.37, NNZs: 843, Bias: 41096434.147885, T: 13195273, Avg. loss: 83837755579085053063443911802880.000000\n",
      "Total training time: 28.22 seconds.\n",
      "-- Epoch 3920\n",
      "Norm: 5769797033572.28, NNZs: 843, Bias: -124816798.102280, T: 13198640, Avg. loss: 83826844269528461915442183667712.000000\n",
      "Total training time: 28.23 seconds.\n",
      "-- Epoch 3921\n",
      "Norm: 6198322814204.17, NNZs: 843, Bias: 119353615.016896, T: 13202007, Avg. loss: 83816257762601037776601697222656.000000\n",
      "Total training time: 28.24 seconds.\n",
      "-- Epoch 3922\n",
      "Norm: 5687787482310.82, NNZs: 843, Bias: -46545639.355933, T: 13205374, Avg. loss: 83805478725517024452745912909824.000000\n",
      "Total training time: 28.25 seconds.\n",
      "-- Epoch 3923\n",
      "Norm: 5732434157719.77, NNZs: 843, Bias: 35869377.993841, T: 13208741, Avg. loss: 83794798330672967353359969288192.000000\n",
      "Total training time: 28.25 seconds.\n",
      "-- Epoch 3924\n",
      "Norm: 5883717434313.24, NNZs: 843, Bias: 201728404.014588, T: 13212108, Avg. loss: 83784122210990472141520314040320.000000\n",
      "Total training time: 28.26 seconds.\n",
      "-- Epoch 3925\n",
      "Norm: 5925962860304.70, NNZs: 843, Bias: -295854144.065091, T: 13215475, Avg. loss: 83773413164277255023618487746560.000000\n",
      "Total training time: 28.27 seconds.\n",
      "-- Epoch 3926\n",
      "Norm: 5811394243169.08, NNZs: 843, Bias: 49367140.525838, T: 13218842, Avg. loss: 83762718165175458381936080191488.000000\n",
      "Total training time: 28.27 seconds.\n",
      "-- Epoch 3927\n",
      "Norm: 5722137920742.27, NNZs: 843, Bias: 67356442.775923, T: 13222209, Avg. loss: 83752004296332361722449492967424.000000\n",
      "Total training time: 28.28 seconds.\n",
      "-- Epoch 3928\n",
      "Norm: 5771138084746.66, NNZs: 843, Bias: -98476469.596334, T: 13225576, Avg. loss: 83741363637863178396664334385152.000000\n",
      "Total training time: 28.29 seconds.\n",
      "-- Epoch 3929\n",
      "Norm: 5756217166861.10, NNZs: 843, Bias: -264286873.549286, T: 13228943, Avg. loss: 83730563408662519911602026708992.000000\n",
      "Total training time: 28.30 seconds.\n",
      "-- Epoch 3930\n",
      "Norm: 5888093764529.75, NNZs: 843, Bias: 175763353.288648, T: 13232310, Avg. loss: 83720003962703526096594020597760.000000\n",
      "Total training time: 28.30 seconds.\n",
      "-- Epoch 3931\n",
      "Norm: 5866935204427.73, NNZs: 843, Bias: -268980595.650337, T: 13235677, Avg. loss: 83708995714307014620971521802240.000000\n",
      "Total training time: 28.31 seconds.\n",
      "-- Epoch 3932\n",
      "Norm: 5993876800855.24, NNZs: 843, Bias: -205104931.642781, T: 13239044, Avg. loss: 83698490059618995539446553640960.000000\n",
      "Total training time: 28.32 seconds.\n",
      "-- Epoch 3933\n",
      "Norm: 5954115302153.02, NNZs: 843, Bias: -39325926.193644, T: 13242411, Avg. loss: 83688040086945966979343292825600.000000\n",
      "Total training time: 28.32 seconds.\n",
      "-- Epoch 3934\n",
      "Norm: 6147488435928.09, NNZs: 843, Bias: 126432102.650353, T: 13245778, Avg. loss: 83677364787561249601361169874944.000000\n",
      "Total training time: 28.33 seconds.\n",
      "-- Epoch 3935\n",
      "Norm: 6121937654247.99, NNZs: 843, Bias: -370829608.289796, T: 13249145, Avg. loss: 83666659496516531078854420201472.000000\n",
      "Total training time: 28.34 seconds.\n",
      "-- Epoch 3936\n",
      "Norm: 6187924730296.99, NNZs: 843, Bias: -62570748.398444, T: 13252512, Avg. loss: 83656026039386342410809153421312.000000\n",
      "Total training time: 28.35 seconds.\n",
      "-- Epoch 3937\n",
      "Norm: 6318948927190.90, NNZs: 843, Bias: 103156859.047934, T: 13255879, Avg. loss: 83645262876956222119400777449472.000000\n",
      "Total training time: 28.35 seconds.\n",
      "-- Epoch 3938\n",
      "Norm: 5900341390005.51, NNZs: 843, Bias: 236038341.098130, T: 13259246, Avg. loss: 83634709298558516000994967420928.000000\n",
      "Total training time: 28.36 seconds.\n",
      "-- Epoch 3939\n",
      "Norm: 5754902162102.09, NNZs: 843, Bias: 70311898.369721, T: 13262613, Avg. loss: 83624286118690969063026050727936.000000\n",
      "Total training time: 28.37 seconds.\n",
      "-- Epoch 3940\n",
      "Norm: 5689387503480.91, NNZs: 843, Bias: -95393610.595028, T: 13265980, Avg. loss: 83613825630398392852385447804928.000000\n",
      "Total training time: 28.37 seconds.\n",
      "-- Epoch 3941\n",
      "Norm: 5697337620582.85, NNZs: 843, Bias: 70295170.708584, T: 13269347, Avg. loss: 83603212699608583131321973342208.000000\n",
      "Total training time: 28.38 seconds.\n",
      "-- Epoch 3942\n",
      "Norm: 5843698395347.14, NNZs: 843, Bias: 235961532.278315, T: 13272714, Avg. loss: 83592655153889625062548422262784.000000\n",
      "Total training time: 28.39 seconds.\n",
      "-- Epoch 3943\n",
      "Norm: 5752888917049.47, NNZs: 843, Bias: 70276712.064375, T: 13276081, Avg. loss: 83581981327507992404544586776576.000000\n",
      "Total training time: 28.40 seconds.\n",
      "-- Epoch 3944\n",
      "Norm: 5800427487951.37, NNZs: 843, Bias: -95386820.237179, T: 13279448, Avg. loss: 83571231578531466293409449246720.000000\n",
      "Total training time: 28.40 seconds.\n",
      "-- Epoch 3945\n",
      "Norm: 5748238833457.23, NNZs: 843, Bias: -229958583.170449, T: 13282815, Avg. loss: 83560597562069173871109958795264.000000\n",
      "Total training time: 28.41 seconds.\n",
      "-- Epoch 3946\n",
      "Norm: 6080264765516.57, NNZs: 843, Bias: -395582949.138312, T: 13286182, Avg. loss: 83550032408831985083217199759360.000000\n",
      "Total training time: 28.42 seconds.\n",
      "-- Epoch 3947\n",
      "Norm: 5690082658583.92, NNZs: 843, Bias: -184122824.015296, T: 13289549, Avg. loss: 83539505523483443573620413038592.000000\n",
      "Total training time: 28.42 seconds.\n",
      "-- Epoch 3948\n",
      "Norm: 5775294282138.59, NNZs: 843, Bias: -349728775.911003, T: 13292916, Avg. loss: 83528870617627778740058520027136.000000\n",
      "Total training time: 28.43 seconds.\n",
      "-- Epoch 3949\n",
      "Norm: 5848940311631.38, NNZs: 843, Bias: 147097311.379447, T: 13296283, Avg. loss: 83518476283994501545164198117376.000000\n",
      "Total training time: 28.44 seconds.\n",
      "-- Epoch 3950\n",
      "Norm: 5852216602147.05, NNZs: 843, Bias: -18508984.190743, T: 13299650, Avg. loss: 83507957630210282965681429282816.000000\n",
      "Total training time: 28.45 seconds.\n",
      "-- Epoch 3951\n",
      "Norm: 6031752199951.86, NNZs: 843, Bias: 200970706.935410, T: 13303017, Avg. loss: 83497374494721747210336517226496.000000\n",
      "Total training time: 28.45 seconds.\n",
      "-- Epoch 3952\n",
      "Norm: 6142209516597.87, NNZs: 843, Bias: 35382499.835189, T: 13306384, Avg. loss: 83486677514825829727494540886016.000000\n",
      "Total training time: 28.46 seconds.\n",
      "-- Epoch 3953\n",
      "Norm: 6090033593571.81, NNZs: 843, Bias: 200937953.735797, T: 13309751, Avg. loss: 83476155220010223454699609128960.000000\n",
      "Total training time: 28.47 seconds.\n",
      "-- Epoch 3954\n",
      "Norm: 5695056112572.29, NNZs: 843, Bias: 122162374.411401, T: 13313118, Avg. loss: 83465562887838414581940675936256.000000\n",
      "Total training time: 28.47 seconds.\n",
      "-- Epoch 3955\n",
      "Norm: 5896332292678.72, NNZs: 843, Bias: 287690742.770629, T: 13316485, Avg. loss: 83455010389963570439872392986624.000000\n",
      "Total training time: 28.48 seconds.\n",
      "-- Epoch 3956\n",
      "Norm: 5739390390219.17, NNZs: 843, Bias: -4433942.618357, T: 13319852, Avg. loss: 83444573652973710098433367343104.000000\n",
      "Total training time: 28.49 seconds.\n",
      "-- Epoch 3957\n",
      "Norm: 5807855586099.38, NNZs: 843, Bias: -169956810.477248, T: 13323219, Avg. loss: 83434185352028831039862861725696.000000\n",
      "Total training time: 28.50 seconds.\n",
      "-- Epoch 3958\n",
      "Norm: 5991284413545.30, NNZs: 843, Bias: -4441946.313852, T: 13326586, Avg. loss: 83423654490435479177436866805760.000000\n",
      "Total training time: 28.50 seconds.\n",
      "-- Epoch 3959\n",
      "Norm: 5781831221852.28, NNZs: 843, Bias: -169944369.160104, T: 13329953, Avg. loss: 83413015908939542536514722332672.000000\n",
      "Total training time: 28.51 seconds.\n",
      "-- Epoch 3960\n",
      "Norm: 6040862161532.13, NNZs: 843, Bias: -4450261.636631, T: 13333320, Avg. loss: 83402417176747753534983346585600.000000\n",
      "Total training time: 28.52 seconds.\n",
      "-- Epoch 3961\n",
      "Norm: 5646231271068.89, NNZs: 843, Bias: -169931532.555164, T: 13336687, Avg. loss: 83391825325494974584405457108992.000000\n",
      "Total training time: 28.52 seconds.\n",
      "-- Epoch 3962\n",
      "Norm: 5744575064773.85, NNZs: 843, Bias: -335392379.183306, T: 13340054, Avg. loss: 83381522870786829968358667976704.000000\n",
      "Total training time: 28.53 seconds.\n",
      "-- Epoch 3963\n",
      "Norm: 5788808393894.77, NNZs: 843, Bias: 160992606.365981, T: 13343421, Avg. loss: 83370985268991722223126563520512.000000\n",
      "Total training time: 28.54 seconds.\n",
      "-- Epoch 3964\n",
      "Norm: 5898999528943.20, NNZs: 843, Bias: -335359535.259992, T: 13346788, Avg. loss: 83360451330205298136797000237056.000000\n",
      "Total training time: 28.55 seconds.\n",
      "-- Epoch 3965\n",
      "Norm: 5813446587907.81, NNZs: 843, Bias: -50792517.118458, T: 13350155, Avg. loss: 83349758102150305954844934930432.000000\n",
      "Total training time: 28.55 seconds.\n",
      "-- Epoch 3966\n",
      "Norm: 5846617417065.37, NNZs: 843, Bias: 51200058.520713, T: 13353522, Avg. loss: 83339137390676391869670882279424.000000\n",
      "Total training time: 28.56 seconds.\n",
      "-- Epoch 3967\n",
      "Norm: 5932183111831.87, NNZs: 843, Bias: -177181804.193931, T: 13356889, Avg. loss: 83328618322197639139767720345600.000000\n",
      "Total training time: 28.57 seconds.\n",
      "-- Epoch 3968\n",
      "Norm: 5863997879649.66, NNZs: 843, Bias: -11770661.028632, T: 13360256, Avg. loss: 83318221596036913780282085081088.000000\n",
      "Total training time: 28.57 seconds.\n",
      "-- Epoch 3969\n",
      "Norm: 6100322696822.27, NNZs: 843, Bias: -177167867.100801, T: 13363623, Avg. loss: 83307632007855611705530601439232.000000\n",
      "Total training time: 28.58 seconds.\n",
      "-- Epoch 3970\n",
      "Norm: 5795025040746.08, NNZs: 843, Bias: -233467292.489733, T: 13366990, Avg. loss: 83297107196427020448627708395520.000000\n",
      "Total training time: 28.59 seconds.\n",
      "-- Epoch 3971\n",
      "Norm: 5877640488892.70, NNZs: 843, Bias: -177687752.026972, T: 13370357, Avg. loss: 83286712877203298491881440149504.000000\n",
      "Total training time: 28.60 seconds.\n",
      "-- Epoch 3972\n",
      "Norm: 5598861472013.96, NNZs: 843, Bias: -12317942.022602, T: 13373724, Avg. loss: 83276257140499946240185611583488.000000\n",
      "Total training time: 28.60 seconds.\n",
      "-- Epoch 3973\n",
      "Norm: 5597118276022.69, NNZs: 843, Bias: 153030221.574829, T: 13377091, Avg. loss: 83265920912447988420735806734336.000000\n",
      "Total training time: 28.61 seconds.\n",
      "-- Epoch 3974\n",
      "Norm: 5815910784424.75, NNZs: 843, Bias: -12325479.013967, T: 13380458, Avg. loss: 83255682719162853013108306214912.000000\n",
      "Total training time: 28.62 seconds.\n",
      "-- Epoch 3975\n",
      "Norm: 5866409132257.60, NNZs: 843, Bias: -177660360.015917, T: 13383825, Avg. loss: 83245235075030471385488031744000.000000\n",
      "Total training time: 28.62 seconds.\n",
      "-- Epoch 3976\n",
      "Norm: 5683489085131.15, NNZs: 843, Bias: -127265264.365101, T: 13387192, Avg. loss: 83234650083854587860002732507136.000000\n",
      "Total training time: 28.63 seconds.\n",
      "-- Epoch 3977\n",
      "Norm: 5767872746289.21, NNZs: 843, Bias: 167961203.308630, T: 13390559, Avg. loss: 83224208142344958212860291317760.000000\n",
      "Total training time: 28.64 seconds.\n",
      "-- Epoch 3978\n",
      "Norm: 5686704714682.90, NNZs: 843, Bias: 2646489.115263, T: 13393926, Avg. loss: 83213752068146875903203637985280.000000\n",
      "Total training time: 28.65 seconds.\n",
      "-- Epoch 3979\n",
      "Norm: 6121749453733.54, NNZs: 843, Bias: 167932327.442315, T: 13397293, Avg. loss: 83203494576297754008320129105920.000000\n",
      "Total training time: 28.65 seconds.\n",
      "-- Epoch 3980\n",
      "Norm: 5749466937789.21, NNZs: 843, Bias: -254859134.132885, T: 13400660, Avg. loss: 83193175930483749315979088232448.000000\n",
      "Total training time: 28.66 seconds.\n",
      "-- Epoch 3981\n",
      "Norm: 6163417710475.49, NNZs: 843, Bias: 8904543.596864, T: 13404027, Avg. loss: 83182941403572171036043249713152.000000\n",
      "Total training time: 28.67 seconds.\n",
      "-- Epoch 3982\n",
      "Norm: 5925689497564.22, NNZs: 843, Bias: -335183522.344608, T: 13407394, Avg. loss: 83172386838276647364329002237952.000000\n",
      "Total training time: 28.68 seconds.\n",
      "-- Epoch 3983\n",
      "Norm: 5675310684350.41, NNZs: 843, Bias: -45004785.477400, T: 13410761, Avg. loss: 83162106869367001143372245106688.000000\n",
      "Total training time: 28.68 seconds.\n",
      "-- Epoch 3984\n",
      "Norm: 5808095379385.69, NNZs: 843, Bias: -16554828.337042, T: 13414128, Avg. loss: 83151770929512633270346927046656.000000\n",
      "Total training time: 28.69 seconds.\n",
      "-- Epoch 3985\n",
      "Norm: 5922288225720.56, NNZs: 843, Bias: 148669774.960001, T: 13417495, Avg. loss: 83141328466102748315825234509824.000000\n",
      "Total training time: 28.70 seconds.\n",
      "-- Epoch 3986\n",
      "Norm: 5797956354947.71, NNZs: 843, Bias: -346996173.224548, T: 13420862, Avg. loss: 83130759699080192975474415108096.000000\n",
      "Total training time: 28.70 seconds.\n",
      "-- Epoch 3987\n",
      "Norm: 5868409051552.17, NNZs: 843, Bias: -181771645.222205, T: 13424229, Avg. loss: 83120007033417696409618560843776.000000\n",
      "Total training time: 28.71 seconds.\n",
      "-- Epoch 3988\n",
      "Norm: 6126333398814.74, NNZs: 843, Bias: 225959275.762404, T: 13427596, Avg. loss: 83109554786613629311161129238528.000000\n",
      "Total training time: 28.72 seconds.\n",
      "-- Epoch 3989\n",
      "Norm: 5949464048292.68, NNZs: 843, Bias: -269615883.138968, T: 13430963, Avg. loss: 83099069772599971794981813747712.000000\n",
      "Total training time: 28.73 seconds.\n",
      "-- Epoch 3990\n",
      "Norm: 6058731395173.49, NNZs: 843, Bias: -104426898.386495, T: 13434330, Avg. loss: 83088624652928637934117874302976.000000\n",
      "Total training time: 28.73 seconds.\n",
      "-- Epoch 3991\n",
      "Norm: 5768718422902.84, NNZs: 843, Bias: -1274974.452209, T: 13437697, Avg. loss: 83078350152123008065806142537728.000000\n",
      "Total training time: 28.74 seconds.\n",
      "-- Epoch 3992\n",
      "Norm: 6056163480066.97, NNZs: 843, Bias: -166434895.855879, T: 13441064, Avg. loss: 83067929223549124258164310540288.000000\n",
      "Total training time: 28.75 seconds.\n",
      "-- Epoch 3993\n",
      "Norm: 5991090295234.92, NNZs: 843, Bias: -1284383.617143, T: 13444431, Avg. loss: 83057575106808822305824786350080.000000\n",
      "Total training time: 28.75 seconds.\n",
      "-- Epoch 3994\n",
      "Norm: 6123497240662.66, NNZs: 843, Bias: 146224978.996674, T: 13447798, Avg. loss: 83047239253783296802148837228544.000000\n",
      "Total training time: 28.76 seconds.\n",
      "-- Epoch 3995\n",
      "Norm: 5876022589538.51, NNZs: 843, Bias: -18912739.063572, T: 13451165, Avg. loss: 83037147931030678996971996315648.000000\n",
      "Total training time: 28.77 seconds.\n",
      "-- Epoch 3996\n",
      "Norm: 5782500279618.18, NNZs: 843, Bias: -184030197.841572, T: 13454532, Avg. loss: 83026669998328114496885684699136.000000\n",
      "Total training time: 28.78 seconds.\n",
      "-- Epoch 3997\n",
      "Norm: 5902456404385.08, NNZs: 843, Bias: -18919988.845026, T: 13457899, Avg. loss: 83016337276756975082712063803392.000000\n",
      "Total training time: 28.78 seconds.\n",
      "-- Epoch 3998\n",
      "Norm: 5753973324541.24, NNZs: 843, Bias: 146168865.695828, T: 13461266, Avg. loss: 83006131431885755992115895074816.000000\n",
      "Total training time: 28.79 seconds.\n",
      "-- Epoch 3999\n",
      "Norm: 5715041714781.51, NNZs: 843, Bias: -18928165.994523, T: 13464633, Avg. loss: 82995882299993484328901061640192.000000\n",
      "Total training time: 28.80 seconds.\n",
      "-- Epoch 4000\n",
      "Norm: 5728833054062.32, NNZs: 843, Bias: -149027564.483705, T: 13468000, Avg. loss: 82985358355372947539178156982272.000000\n",
      "Total training time: 28.80 seconds.\n",
      "-- Epoch 4001\n",
      "Norm: 5938192535419.49, NNZs: 843, Bias: 174895534.634477, T: 13471367, Avg. loss: 82974939372652511266900166049792.000000\n",
      "Total training time: 28.81 seconds.\n",
      "-- Epoch 4002\n",
      "Norm: 5777339874021.82, NNZs: 843, Bias: 9828452.045794, T: 13474734, Avg. loss: 82964729531146560611344434855936.000000\n",
      "Total training time: 28.82 seconds.\n",
      "-- Epoch 4003\n",
      "Norm: 6178774829028.46, NNZs: 843, Bias: -485301267.863466, T: 13478101, Avg. loss: 82954234434004962400911763701760.000000\n",
      "Total training time: 28.83 seconds.\n",
      "-- Epoch 4004\n",
      "Norm: 5838821332685.49, NNZs: 843, Bias: -320244243.681606, T: 13481468, Avg. loss: 82944009291020513102563844816896.000000\n",
      "Total training time: 28.83 seconds.\n",
      "-- Epoch 4005\n",
      "Norm: 5769842317512.28, NNZs: 843, Bias: 174834186.015269, T: 13484835, Avg. loss: 82933585778196099119969634091008.000000\n",
      "Total training time: 28.84 seconds.\n",
      "-- Epoch 4006\n",
      "Norm: 5774951160017.55, NNZs: 843, Bias: 9808834.675404, T: 13488202, Avg. loss: 82923133920743748219206755155968.000000\n",
      "Total training time: 28.85 seconds.\n",
      "-- Epoch 4007\n",
      "Norm: 6089735938077.24, NNZs: 843, Bias: 174804155.478018, T: 13491569, Avg. loss: 82912769936932202902402581397504.000000\n",
      "Total training time: 28.85 seconds.\n",
      "-- Epoch 4008\n",
      "Norm: 5885054676935.43, NNZs: 843, Bias: 13589514.127073, T: 13494936, Avg. loss: 82902267620768748514916540874752.000000\n",
      "Total training time: 28.86 seconds.\n",
      "-- Epoch 4009\n",
      "Norm: 6034366907401.18, NNZs: 843, Bias: -151394890.226127, T: 13498303, Avg. loss: 82891840946487201912458380836864.000000\n",
      "Total training time: 28.87 seconds.\n",
      "-- Epoch 4010\n",
      "Norm: 6239425942361.68, NNZs: 843, Bias: 389202180.558498, T: 13501670, Avg. loss: 82881475182488958821762028535808.000000\n",
      "Total training time: 28.88 seconds.\n",
      "-- Epoch 4011\n",
      "Norm: 6139316864396.56, NNZs: 843, Bias: -105703778.696422, T: 13505037, Avg. loss: 82871040741939564449586871271424.000000\n",
      "Total training time: 28.88 seconds.\n",
      "-- Epoch 4012\n",
      "Norm: 6065250741133.84, NNZs: 843, Bias: 59246510.643753, T: 13508404, Avg. loss: 82860858754236925467211396546560.000000\n",
      "Total training time: 28.89 seconds.\n",
      "-- Epoch 4013\n",
      "Norm: 6058854871515.21, NNZs: 843, Bias: -435576864.208994, T: 13511771, Avg. loss: 82850604239839672863255590600704.000000\n",
      "Total training time: 28.90 seconds.\n",
      "-- Epoch 4014\n",
      "Norm: 5913413363751.65, NNZs: 843, Bias: -38802066.335048, T: 13515138, Avg. loss: 82840249212122983674240317784064.000000\n",
      "Total training time: 28.90 seconds.\n",
      "-- Epoch 4015\n",
      "Norm: 6019009154002.71, NNZs: 843, Bias: 126112928.697400, T: 13518505, Avg. loss: 82829863272703691679766614638592.000000\n",
      "Total training time: 28.91 seconds.\n",
      "-- Epoch 4016\n",
      "Norm: 6165648820569.22, NNZs: 843, Bias: 439771217.238375, T: 13521872, Avg. loss: 82819596634480395615275573051392.000000\n",
      "Total training time: 28.92 seconds.\n",
      "-- Epoch 4017\n",
      "Norm: 5983629211727.78, NNZs: 843, Bias: 63780834.588267, T: 13525239, Avg. loss: 82809405817524838454962193170432.000000\n",
      "Total training time: 28.93 seconds.\n",
      "-- Epoch 4018\n",
      "Norm: 6044347993164.14, NNZs: 843, Bias: -430888811.538132, T: 13528606, Avg. loss: 82799004693674701413130019274752.000000\n",
      "Total training time: 28.93 seconds.\n",
      "-- Epoch 4019\n",
      "Norm: 6110301539010.25, NNZs: 843, Bias: -19040164.055618, T: 13531973, Avg. loss: 82788657290861686743155884425216.000000\n",
      "Total training time: 28.94 seconds.\n",
      "-- Epoch 4020\n",
      "Norm: 5968788140432.29, NNZs: 843, Bias: -183911004.252943, T: 13535340, Avg. loss: 82778436885997419897386231136256.000000\n",
      "Total training time: 28.95 seconds.\n",
      "-- Epoch 4021\n",
      "Norm: 5875434124276.53, NNZs: 843, Bias: -19048480.306328, T: 13538707, Avg. loss: 82767986359716272637424099655680.000000\n",
      "Total training time: 28.95 seconds.\n",
      "-- Epoch 4022\n",
      "Norm: 5852361801186.88, NNZs: 843, Bias: -19549080.436699, T: 13542074, Avg. loss: 82757853560202753078032110452736.000000\n",
      "Total training time: 28.96 seconds.\n",
      "-- Epoch 4023\n",
      "Norm: 6199250643143.62, NNZs: 843, Bias: -514061311.082716, T: 13545441, Avg. loss: 82747441365590771451142200098816.000000\n",
      "Total training time: 28.97 seconds.\n",
      "-- Epoch 4024\n",
      "Norm: 6118988978904.39, NNZs: 843, Bias: -349208667.074780, T: 13548808, Avg. loss: 82737103359630919254851178201088.000000\n",
      "Total training time: 28.98 seconds.\n",
      "-- Epoch 4025\n",
      "Norm: 6125453089323.55, NNZs: 843, Bias: 54076082.308764, T: 13552175, Avg. loss: 82726765524005617689816243109888.000000\n",
      "Total training time: 28.98 seconds.\n",
      "-- Epoch 4026\n",
      "Norm: 6211945114412.17, NNZs: 843, Bias: -110737928.791164, T: 13555542, Avg. loss: 82716486373262033469495977181184.000000\n",
      "Total training time: 28.99 seconds.\n",
      "-- Epoch 4027\n",
      "Norm: 6099775050451.52, NNZs: 843, Bias: -248743343.321615, T: 13558909, Avg. loss: 82706160700847816955863324688384.000000\n",
      "Total training time: 29.00 seconds.\n",
      "-- Epoch 4028\n",
      "Norm: 6030927802251.86, NNZs: 843, Bias: -83948990.880905, T: 13562276, Avg. loss: 82695854914407004258213562417152.000000\n",
      "Total training time: 29.01 seconds.\n",
      "-- Epoch 4029\n",
      "Norm: 6291197665764.05, NNZs: 843, Bias: -248723195.325912, T: 13565643, Avg. loss: 82685750293844655849947306917888.000000\n",
      "Total training time: 29.01 seconds.\n",
      "-- Epoch 4030\n",
      "Norm: 6245358942025.26, NNZs: 843, Bias: -317841488.155839, T: 13569010, Avg. loss: 82675521823386628666397711925248.000000\n",
      "Total training time: 29.02 seconds.\n",
      "-- Epoch 4031\n",
      "Norm: 6070673653081.78, NNZs: 843, Bias: -153073363.157284, T: 13572377, Avg. loss: 82665184620537178815654046728192.000000\n",
      "Total training time: 29.03 seconds.\n",
      "-- Epoch 4032\n",
      "Norm: 6165318948764.74, NNZs: 843, Bias: -31442945.244686, T: 13575744, Avg. loss: 82655015668084322088675522379776.000000\n",
      "Total training time: 29.03 seconds.\n",
      "-- Epoch 4033\n",
      "Norm: 6134300374872.46, NNZs: 843, Bias: -196180045.666092, T: 13579111, Avg. loss: 82644863878615259931650383937536.000000\n",
      "Total training time: 29.04 seconds.\n",
      "-- Epoch 4034\n",
      "Norm: 5889559110213.03, NNZs: 843, Bias: -66529909.688478, T: 13582478, Avg. loss: 82635012077434922419520786137088.000000\n",
      "Total training time: 29.05 seconds.\n",
      "-- Epoch 4035\n",
      "Norm: 6213850384734.42, NNZs: 843, Bias: -231242698.604940, T: 13585845, Avg. loss: 82624715473252371247215855796224.000000\n",
      "Total training time: 29.06 seconds.\n",
      "-- Epoch 4036\n",
      "Norm: 5861727031931.64, NNZs: 843, Bias: -66529694.258418, T: 13589212, Avg. loss: 82614645950418421289480687190016.000000\n",
      "Total training time: 29.06 seconds.\n",
      "-- Epoch 4037\n",
      "Norm: 5993996735662.94, NNZs: 843, Bias: 98161636.484688, T: 13592579, Avg. loss: 82604274367553773242749475094528.000000\n",
      "Total training time: 29.07 seconds.\n",
      "-- Epoch 4038\n",
      "Norm: 5860406321782.42, NNZs: 843, Bias: -66532677.935153, T: 13595946, Avg. loss: 82594117705582736244745428795392.000000\n",
      "Total training time: 29.08 seconds.\n",
      "-- Epoch 4039\n",
      "Norm: 5914657396976.42, NNZs: 843, Bias: -231206700.198396, T: 13599313, Avg. loss: 82583881369526945090872954322944.000000\n",
      "Total training time: 29.08 seconds.\n",
      "-- Epoch 4040\n",
      "Norm: 5788218378164.34, NNZs: 843, Bias: -66534721.530089, T: 13602680, Avg. loss: 82573520472714177481044639350784.000000\n",
      "Total training time: 29.09 seconds.\n",
      "-- Epoch 4041\n",
      "Norm: 5886762161266.11, NNZs: 843, Bias: 98116300.342684, T: 13606047, Avg. loss: 82563518170985487160121319489536.000000\n",
      "Total training time: 29.10 seconds.\n",
      "-- Epoch 4042\n",
      "Norm: 5828638979494.80, NNZs: 843, Bias: -66536485.326101, T: 13609414, Avg. loss: 82553081842946504191534979612672.000000\n",
      "Total training time: 29.11 seconds.\n",
      "-- Epoch 4043\n",
      "Norm: 5896218720509.87, NNZs: 843, Bias: -67282711.526479, T: 13612781, Avg. loss: 82542958877683741308986884882432.000000\n",
      "Total training time: 29.11 seconds.\n",
      "-- Epoch 4044\n",
      "Norm: 5770395778464.98, NNZs: 843, Bias: 97337887.195130, T: 13616148, Avg. loss: 82532817295234537910551837671424.000000\n",
      "Total training time: 29.12 seconds.\n",
      "-- Epoch 4045\n",
      "Norm: 5874370702733.64, NNZs: 843, Bias: -59527933.390053, T: 13619515, Avg. loss: 82522404799992022961143788077056.000000\n",
      "Total training time: 29.13 seconds.\n",
      "-- Epoch 4046\n",
      "Norm: 5725591692546.91, NNZs: 843, Bias: 105072487.726199, T: 13622882, Avg. loss: 82512312893070147615148042354688.000000\n",
      "Total training time: 29.13 seconds.\n",
      "-- Epoch 4047\n",
      "Norm: 5689569089237.31, NNZs: 843, Bias: -59529488.880588, T: 13626249, Avg. loss: 82502196910439836097400371412992.000000\n",
      "Total training time: 29.14 seconds.\n",
      "-- Epoch 4048\n",
      "Norm: 5737783291703.52, NNZs: 843, Bias: -224112063.345618, T: 13629616, Avg. loss: 82492124531338785148264811331584.000000\n",
      "Total training time: 29.15 seconds.\n",
      "-- Epoch 4049\n",
      "Norm: 5773559582914.26, NNZs: 843, Bias: -314288491.253134, T: 13632983, Avg. loss: 82482043389977950148554574004224.000000\n",
      "Total training time: 29.16 seconds.\n",
      "-- Epoch 4050\n",
      "Norm: 5851691106419.40, NNZs: 843, Bias: -149712541.269166, T: 13636350, Avg. loss: 82471799337904917420811302731776.000000\n",
      "Total training time: 29.16 seconds.\n",
      "-- Epoch 4051\n",
      "Norm: 5878621418076.08, NNZs: 843, Bias: 14842858.697561, T: 13639717, Avg. loss: 82461534254077865824998314213376.000000\n",
      "Total training time: 29.17 seconds.\n",
      "-- Epoch 4052\n",
      "Norm: 5880888493177.22, NNZs: 843, Bias: -149701674.622489, T: 13643084, Avg. loss: 82451227031616717899403850940416.000000\n",
      "Total training time: 29.18 seconds.\n",
      "-- Epoch 4053\n",
      "Norm: 5922166632281.08, NNZs: 843, Bias: 14833756.269963, T: 13646451, Avg. loss: 82441098274837946626638937587712.000000\n",
      "Total training time: 29.19 seconds.\n",
      "-- Epoch 4054\n",
      "Norm: 5823321510209.60, NNZs: 843, Bias: -149691881.423776, T: 13649818, Avg. loss: 82430985378584465707327259934720.000000\n",
      "Total training time: 29.19 seconds.\n",
      "-- Epoch 4055\n",
      "Norm: 5912440544706.26, NNZs: 843, Bias: -314197310.807783, T: 13653185, Avg. loss: 82420871398223635810659839508480.000000\n",
      "Total training time: 29.20 seconds.\n",
      "-- Epoch 4056\n",
      "Norm: 5888571042397.26, NNZs: 843, Bias: -303016479.103471, T: 13656552, Avg. loss: 82410910858702188178015518720000.000000\n",
      "Total training time: 29.21 seconds.\n",
      "-- Epoch 4057\n",
      "Norm: 5838480523654.72, NNZs: 843, Bias: 249028404.134576, T: 13659919, Avg. loss: 82400648723144292777318741442560.000000\n",
      "Total training time: 29.21 seconds.\n",
      "-- Epoch 4058\n",
      "Norm: 5760963711337.41, NNZs: 843, Bias: 84529358.430461, T: 13663286, Avg. loss: 82390494240950873719705891241984.000000\n",
      "Total training time: 29.22 seconds.\n",
      "-- Epoch 4059\n",
      "Norm: 5710685736303.72, NNZs: 843, Bias: 62964928.362118, T: 13666653, Avg. loss: 82380404574316260331510946594816.000000\n",
      "Total training time: 29.23 seconds.\n",
      "-- Epoch 4060\n",
      "Norm: 5778393727557.82, NNZs: 843, Bias: -179717775.088305, T: 13670020, Avg. loss: 82370051944739999637866912153600.000000\n",
      "Total training time: 29.24 seconds.\n",
      "-- Epoch 4061\n",
      "Norm: 5942794654024.58, NNZs: 843, Bias: -344161313.488230, T: 13673387, Avg. loss: 82359988517533063256891168129024.000000\n",
      "Total training time: 29.24 seconds.\n",
      "-- Epoch 4062\n",
      "Norm: 5930718646456.17, NNZs: 843, Bias: -181233590.424025, T: 13676754, Avg. loss: 82350084859139010114663380680704.000000\n",
      "Total training time: 29.25 seconds.\n",
      "-- Epoch 4063\n",
      "Norm: 5960438500066.50, NNZs: 843, Bias: -16798166.265850, T: 13680121, Avg. loss: 82340055852940458875483687223296.000000\n",
      "Total training time: 29.26 seconds.\n",
      "-- Epoch 4064\n",
      "Norm: 6253833568201.25, NNZs: 843, Bias: 147616984.965836, T: 13683488, Avg. loss: 82329837511422212774898777980928.000000\n",
      "Total training time: 29.26 seconds.\n",
      "-- Epoch 4065\n",
      "Norm: 6049390443159.61, NNZs: 843, Bias: -217669797.634780, T: 13686855, Avg. loss: 82319726453666524395197664591872.000000\n",
      "Total training time: 29.27 seconds.\n",
      "-- Epoch 4066\n",
      "Norm: 6058533927426.39, NNZs: 843, Bias: -53263438.947158, T: 13690222, Avg. loss: 82309665227050843834084996677632.000000\n",
      "Total training time: 29.28 seconds.\n",
      "-- Epoch 4067\n",
      "Norm: 6157779647094.53, NNZs: 843, Bias: -217653391.270652, T: 13693589, Avg. loss: 82299711312146141546993439211520.000000\n",
      "Total training time: 29.29 seconds.\n",
      "-- Epoch 4068\n",
      "Norm: 6246179209169.41, NNZs: 843, Bias: 275489094.148237, T: 13696956, Avg. loss: 82289680584686584533587949780992.000000\n",
      "Total training time: 29.29 seconds.\n",
      "-- Epoch 4069\n",
      "Norm: 6309189844848.49, NNZs: 843, Bias: -288687074.979432, T: 13700323, Avg. loss: 82279662216750957077387156652032.000000\n",
      "Total training time: 29.30 seconds.\n",
      "-- Epoch 4070\n",
      "Norm: 6565336829452.50, NNZs: 843, Bias: 187525704.859897, T: 13703690, Avg. loss: 82269468226945943665414911295488.000000\n",
      "Total training time: 29.31 seconds.\n",
      "-- Epoch 4071\n",
      "Norm: 6185642162802.00, NNZs: 843, Bias: 22913174.150440, T: 13707057, Avg. loss: 82259445570894059921544050638848.000000\n",
      "Total training time: 29.32 seconds.\n",
      "-- Epoch 4072\n",
      "Norm: 6019083239255.49, NNZs: 843, Bias: -141430568.954672, T: 13710424, Avg. loss: 82249404377094297143410188877824.000000\n",
      "Total training time: 29.32 seconds.\n",
      "-- Epoch 4073\n",
      "Norm: 6183613552997.25, NNZs: 843, Bias: 351556998.324490, T: 13713791, Avg. loss: 82239284770027858156052367278080.000000\n",
      "Total training time: 29.33 seconds.\n",
      "-- Epoch 4074\n",
      "Norm: 6176028380199.03, NNZs: 843, Bias: 187213489.686859, T: 13717158, Avg. loss: 82229148711438027130643503120384.000000\n",
      "Total training time: 29.34 seconds.\n",
      "-- Epoch 4075\n",
      "Norm: 6218517341263.92, NNZs: 843, Bias: -192716137.020338, T: 13720525, Avg. loss: 82218862397461071232509962878976.000000\n",
      "Total training time: 29.34 seconds.\n",
      "-- Epoch 4076\n",
      "Norm: 5952846577536.81, NNZs: 843, Bias: -28412129.888186, T: 13723892, Avg. loss: 82208820282963739292434551537664.000000\n",
      "Total training time: 29.35 seconds.\n",
      "-- Epoch 4077\n",
      "Norm: 5850360873173.40, NNZs: 843, Bias: 135872654.773093, T: 13727259, Avg. loss: 82198815666732608466301837377536.000000\n",
      "Total training time: 29.36 seconds.\n",
      "-- Epoch 4078\n",
      "Norm: 5982049294804.57, NNZs: 843, Bias: -28417787.663077, T: 13730626, Avg. loss: 82188565687811083093001702998016.000000\n",
      "Total training time: 29.37 seconds.\n",
      "-- Epoch 4079\n",
      "Norm: 6002456512465.32, NNZs: 843, Bias: 59894473.759063, T: 13733993, Avg. loss: 82178657036140564736321148420096.000000\n",
      "Total training time: 29.37 seconds.\n",
      "-- Epoch 4080\n",
      "Norm: 6070201109463.18, NNZs: 843, Bias: 333382447.215428, T: 13737360, Avg. loss: 82168735251146924277268167524352.000000\n",
      "Total training time: 29.38 seconds.\n",
      "-- Epoch 4081\n",
      "Norm: 5808140039215.43, NNZs: 843, Bias: -159383404.446183, T: 13740727, Avg. loss: 82158595047142489232201585000448.000000\n",
      "Total training time: 29.39 seconds.\n",
      "-- Epoch 4082\n",
      "Norm: 6173023850116.72, NNZs: 843, Bias: 53608255.132815, T: 13744094, Avg. loss: 82148503985315229282763832033280.000000\n",
      "Total training time: 29.39 seconds.\n",
      "-- Epoch 4083\n",
      "Norm: 6096961798033.86, NNZs: 843, Bias: -30568167.340351, T: 13747461, Avg. loss: 82138476852704933612596773257216.000000\n",
      "Total training time: 29.40 seconds.\n",
      "-- Epoch 4084\n",
      "Norm: 5902870671615.65, NNZs: 843, Bias: -523222640.130277, T: 13750828, Avg. loss: 82128616301939544682780417851392.000000\n",
      "Total training time: 29.41 seconds.\n",
      "-- Epoch 4085\n",
      "Norm: 5721833568998.61, NNZs: 843, Bias: -175792005.155120, T: 13754195, Avg. loss: 82118508157817791333526564503552.000000\n",
      "Total training time: 29.42 seconds.\n",
      "-- Epoch 4086\n",
      "Norm: 6052451200685.61, NNZs: 843, Bias: -11589476.742469, T: 13757562, Avg. loss: 82108298229594507435395157852160.000000\n",
      "Total training time: 29.42 seconds.\n",
      "-- Epoch 4087\n",
      "Norm: 5569455388216.27, NNZs: 843, Bias: 68837254.020571, T: 13760929, Avg. loss: 82098152657368671998958848966656.000000\n",
      "Total training time: 29.43 seconds.\n",
      "-- Epoch 4088\n",
      "Norm: 5798513678310.04, NNZs: 843, Bias: 233005544.004217, T: 13764296, Avg. loss: 82087960813899581321556067876864.000000\n",
      "Total training time: 29.44 seconds.\n",
      "-- Epoch 4089\n",
      "Norm: 5550975929290.99, NNZs: 843, Bias: -259512312.068642, T: 13767663, Avg. loss: 82077955178045064120240621748224.000000\n",
      "Total training time: 29.44 seconds.\n",
      "-- Epoch 4090\n",
      "Norm: 5790192077544.34, NNZs: 843, Bias: 7206576.263992, T: 13771030, Avg. loss: 82067932058341635972670559354880.000000\n",
      "Total training time: 29.45 seconds.\n",
      "-- Epoch 4091\n",
      "Norm: 5716612656927.70, NNZs: 843, Bias: -81378634.347875, T: 13774397, Avg. loss: 82057885703413097348246385197056.000000\n",
      "Total training time: 29.46 seconds.\n",
      "-- Epoch 4092\n",
      "Norm: 5722133476613.64, NNZs: 843, Bias: 82758751.995277, T: 13777764, Avg. loss: 82047914715810063660948723335168.000000\n",
      "Total training time: 29.47 seconds.\n",
      "-- Epoch 4093\n",
      "Norm: 5823209239872.67, NNZs: 843, Bias: -81377440.006782, T: 13781131, Avg. loss: 82037813602642782396713371959296.000000\n",
      "Total training time: 29.47 seconds.\n",
      "-- Epoch 4094\n",
      "Norm: 5794593580659.05, NNZs: 843, Bias: -245492561.224149, T: 13784498, Avg. loss: 82027952596651471476458690445312.000000\n",
      "Total training time: 29.48 seconds.\n",
      "-- Epoch 4095\n",
      "Norm: 5758573293271.19, NNZs: 843, Bias: -81374400.999009, T: 13787865, Avg. loss: 82018092589468179669657950617600.000000\n",
      "Total training time: 29.49 seconds.\n",
      "-- Epoch 4096\n",
      "Norm: 5830865205517.42, NNZs: 843, Bias: 82723432.730632, T: 13791232, Avg. loss: 82008230728090483369190143033344.000000\n",
      "Total training time: 29.49 seconds.\n",
      "-- Epoch 4097\n",
      "Norm: 5724759156746.66, NNZs: 843, Bias: -81371322.293100, T: 13794599, Avg. loss: 81998264229684619487253163409408.000000\n",
      "Total training time: 29.50 seconds.\n",
      "-- Epoch 4098\n",
      "Norm: 5772940246233.96, NNZs: 843, Bias: -177913703.199435, T: 13797966, Avg. loss: 81988364816070917203924336246784.000000\n",
      "Total training time: 29.51 seconds.\n",
      "-- Epoch 4099\n",
      "Norm: 5810476145865.80, NNZs: 843, Bias: -13841629.644383, T: 13801333, Avg. loss: 81978257541173273421145176014848.000000\n",
      "Total training time: 29.52 seconds.\n",
      "-- Epoch 4100\n",
      "Norm: 6039855360057.83, NNZs: 843, Bias: -177901325.283856, T: 13804700, Avg. loss: 81968121345165629383337142386688.000000\n",
      "Total training time: 29.52 seconds.\n",
      "-- Epoch 4101\n",
      "Norm: 6397290429767.83, NNZs: 843, Bias: 320158216.968581, T: 13808067, Avg. loss: 81958044376676386165083269496832.000000\n",
      "Total training time: 29.53 seconds.\n",
      "-- Epoch 4102\n",
      "Norm: 6190082193008.56, NNZs: 843, Bias: -171974872.544139, T: 13811434, Avg. loss: 81948131557649834490586996932608.000000\n",
      "Total training time: 29.54 seconds.\n",
      "-- Epoch 4103\n",
      "Norm: 6061010988612.64, NNZs: 843, Bias: -7943614.920914, T: 13814801, Avg. loss: 81938196209974475118451181486080.000000\n",
      "Total training time: 29.54 seconds.\n",
      "-- Epoch 4104\n",
      "Norm: 5767168417656.30, NNZs: 843, Bias: 156068928.942382, T: 13818168, Avg. loss: 81928279314101938802104184340480.000000\n",
      "Total training time: 29.55 seconds.\n",
      "-- Epoch 4105\n",
      "Norm: 5649395853996.09, NNZs: 843, Bias: -7952051.082313, T: 13821535, Avg. loss: 81918483797701500444004128391168.000000\n",
      "Total training time: 29.56 seconds.\n",
      "-- Epoch 4106\n",
      "Norm: 5721209866299.04, NNZs: 843, Bias: 156040631.406428, T: 13824902, Avg. loss: 81908505824552654807358578360320.000000\n",
      "Total training time: 29.57 seconds.\n",
      "-- Epoch 4107\n",
      "Norm: 5647098060805.49, NNZs: 843, Bias: -7960333.813197, T: 13828269, Avg. loss: 81898660041962665172430705655808.000000\n",
      "Total training time: 29.57 seconds.\n",
      "-- Epoch 4108\n",
      "Norm: 5634892088523.49, NNZs: 843, Bias: -171941520.199027, T: 13831636, Avg. loss: 81888910040370278161426528862208.000000\n",
      "Total training time: 29.58 seconds.\n",
      "-- Epoch 4109\n",
      "Norm: 5805711101431.18, NNZs: 843, Bias: -7968185.169942, T: 13835003, Avg. loss: 81878777923361861804909091880960.000000\n",
      "Total training time: 29.59 seconds.\n",
      "-- Epoch 4110\n",
      "Norm: 5749859188620.58, NNZs: 843, Bias: -53101949.259757, T: 13838370, Avg. loss: 81868795431052767538059117854720.000000\n",
      "Total training time: 29.59 seconds.\n",
      "-- Epoch 4111\n",
      "Norm: 5777209775891.81, NNZs: 843, Bias: -217048925.344108, T: 13841737, Avg. loss: 81858686635061395249539378577408.000000\n",
      "Total training time: 29.60 seconds.\n",
      "-- Epoch 4112\n",
      "Norm: 6089130343414.49, NNZs: 843, Bias: 274770564.612922, T: 13845104, Avg. loss: 81848718701766914697218884632576.000000\n",
      "Total training time: 29.61 seconds.\n",
      "-- Epoch 4113\n",
      "Norm: 5737176181222.04, NNZs: 843, Bias: -217031143.209844, T: 13848471, Avg. loss: 81838990603011417787764488273920.000000\n",
      "Total training time: 29.62 seconds.\n",
      "-- Epoch 4114\n",
      "Norm: 5708776820960.08, NNZs: 843, Bias: -53105861.952329, T: 13851838, Avg. loss: 81828937856461914181230242299904.000000\n",
      "Total training time: 29.62 seconds.\n",
      "-- Epoch 4115\n",
      "Norm: 5866369672494.34, NNZs: 843, Bias: -267298138.563253, T: 13855205, Avg. loss: 81819004949918183217757834706944.000000\n",
      "Total training time: 29.63 seconds.\n",
      "-- Epoch 4116\n",
      "Norm: 6008497539388.84, NNZs: 843, Bias: 130738366.637023, T: 13858572, Avg. loss: 81809160703873394107710013702144.000000\n",
      "Total training time: 29.64 seconds.\n",
      "-- Epoch 4117\n",
      "Norm: 5803602037427.89, NNZs: 843, Bias: -317634422.217059, T: 13861939, Avg. loss: 81799236267169642525735409156096.000000\n",
      "Total training time: 29.65 seconds.\n",
      "-- Epoch 4118\n",
      "Norm: 5763867906132.95, NNZs: 843, Bias: 174011435.008352, T: 13865306, Avg. loss: 81789308595833262772064505298944.000000\n",
      "Total training time: 29.65 seconds.\n",
      "-- Epoch 4119\n",
      "Norm: 5659746340358.91, NNZs: 843, Bias: 10129758.915088, T: 13868673, Avg. loss: 81779368243794908968379463237632.000000\n",
      "Total training time: 29.66 seconds.\n",
      "-- Epoch 4120\n",
      "Norm: 5739198347580.34, NNZs: 843, Bias: 173982590.002323, T: 13872040, Avg. loss: 81769501494326333961564437086208.000000\n",
      "Total training time: 29.67 seconds.\n",
      "-- Epoch 4121\n",
      "Norm: 5826229060267.39, NNZs: 843, Bias: -345840520.246319, T: 13875407, Avg. loss: 81759523966180131074835130351616.000000\n",
      "Total training time: 29.67 seconds.\n",
      "-- Epoch 4122\n",
      "Norm: 5777372688984.57, NNZs: 843, Bias: 145687534.088563, T: 13878774, Avg. loss: 81749597426728311564615783809024.000000\n",
      "Total training time: 29.68 seconds.\n",
      "-- Epoch 4123\n",
      "Norm: 5890297981065.64, NNZs: 843, Bias: -18152866.075051, T: 13882141, Avg. loss: 81739532079481768078829464584192.000000\n",
      "Total training time: 29.69 seconds.\n",
      "-- Epoch 4124\n",
      "Norm: 5743598976860.19, NNZs: 843, Bias: -143243448.790574, T: 13885508, Avg. loss: 81729638148832141464218841907200.000000\n",
      "Total training time: 29.70 seconds.\n",
      "-- Epoch 4125\n",
      "Norm: 5752203529503.96, NNZs: 843, Bias: -228725149.160773, T: 13888875, Avg. loss: 81719738698389028248969138929664.000000\n",
      "Total training time: 29.70 seconds.\n",
      "-- Epoch 4126\n",
      "Norm: 5942125832614.33, NNZs: 843, Bias: 262677195.007386, T: 13892242, Avg. loss: 81710030109897162661063596965888.000000\n",
      "Total training time: 29.71 seconds.\n",
      "-- Epoch 4127\n",
      "Norm: 5678073905717.50, NNZs: 843, Bias: -40045927.708558, T: 13895609, Avg. loss: 81700184909138161640423867547648.000000\n",
      "Total training time: 29.72 seconds.\n",
      "-- Epoch 4128\n",
      "Norm: 5660770022162.89, NNZs: 843, Bias: -65886175.796154, T: 13898976, Avg. loss: 81690238956703741380387105931264.000000\n",
      "Total training time: 29.72 seconds.\n",
      "-- Epoch 4129\n",
      "Norm: 5676967994488.57, NNZs: 843, Bias: -229653607.923286, T: 13902343, Avg. loss: 81680319029409765091374706720768.000000\n",
      "Total training time: 29.73 seconds.\n",
      "-- Epoch 4130\n",
      "Norm: 6009358261203.78, NNZs: 843, Bias: -65885218.877261, T: 13905710, Avg. loss: 81670462786916744783104206438400.000000\n",
      "Total training time: 29.74 seconds.\n",
      "-- Epoch 4131\n",
      "Norm: 5761330853119.59, NNZs: 843, Bias: -169444744.816533, T: 13909077, Avg. loss: 81660612727250983275012489740288.000000\n",
      "Total training time: 29.75 seconds.\n",
      "-- Epoch 4132\n",
      "Norm: 5494857768876.91, NNZs: 843, Bias: -47221007.795415, T: 13912444, Avg. loss: 81650792764277524902633766125568.000000\n",
      "Total training time: 29.75 seconds.\n",
      "-- Epoch 4133\n",
      "Norm: 5612563762104.82, NNZs: 843, Bias: -142141793.097931, T: 13915811, Avg. loss: 81640775472285805566434840936448.000000\n",
      "Total training time: 29.76 seconds.\n",
      "-- Epoch 4134\n",
      "Norm: 5572142167647.23, NNZs: 843, Bias: -305856037.738256, T: 13919178, Avg. loss: 81630960412390088567584758693888.000000\n",
      "Total training time: 29.77 seconds.\n",
      "-- Epoch 4135\n",
      "Norm: 5602762659972.55, NNZs: 843, Bias: 185282533.514871, T: 13922545, Avg. loss: 81620951228910893586051317628928.000000\n",
      "Total training time: 29.77 seconds.\n",
      "-- Epoch 4136\n",
      "Norm: 5549341254312.87, NNZs: 843, Bias: -163504552.314339, T: 13925912, Avg. loss: 81611047433952477575209579184128.000000\n",
      "Total training time: 29.78 seconds.\n",
      "-- Epoch 4137\n",
      "Norm: 5809340845605.26, NNZs: 843, Bias: 359909978.075807, T: 13929279, Avg. loss: 81601234511600161643225466011648.000000\n",
      "Total training time: 29.79 seconds.\n",
      "-- Epoch 4138\n",
      "Norm: 5586635430487.02, NNZs: 843, Bias: -131150292.292382, T: 13932646, Avg. loss: 81591380624856087257813199355904.000000\n",
      "Total training time: 29.80 seconds.\n",
      "-- Epoch 4139\n",
      "Norm: 5694674046191.01, NNZs: 843, Bias: -494629849.168232, T: 13936013, Avg. loss: 81581548664220155170078672814080.000000\n",
      "Total training time: 29.80 seconds.\n",
      "-- Epoch 4140\n",
      "Norm: 5733429470747.01, NNZs: 843, Bias: -3628626.556542, T: 13939380, Avg. loss: 81571734830322683397278936334336.000000\n",
      "Total training time: 29.81 seconds.\n",
      "-- Epoch 4141\n",
      "Norm: 5540329787680.69, NNZs: 843, Bias: -167282082.049271, T: 13942747, Avg. loss: 81562034084667317205371749138432.000000\n",
      "Total training time: 29.82 seconds.\n",
      "-- Epoch 4142\n",
      "Norm: 5554332028121.66, NNZs: 843, Bias: 24159520.327280, T: 13946114, Avg. loss: 81552256727128704656307304529920.000000\n",
      "Total training time: 29.82 seconds.\n",
      "-- Epoch 4143\n",
      "Norm: 5656593833644.93, NNZs: 843, Bias: -139473809.214712, T: 13949481, Avg. loss: 81542519876047427725535052038144.000000\n",
      "Total training time: 29.83 seconds.\n",
      "-- Epoch 4144\n",
      "Norm: 5672811472143.48, NNZs: 843, Bias: 24150637.742279, T: 13952848, Avg. loss: 81532610291252827971087180496896.000000\n",
      "Total training time: 29.84 seconds.\n",
      "-- Epoch 4145\n",
      "Norm: 5606495602949.02, NNZs: 843, Bias: -225221819.883896, T: 13956215, Avg. loss: 81522694826689285211703042113536.000000\n",
      "Total training time: 29.85 seconds.\n",
      "-- Epoch 4146\n",
      "Norm: 5809101429089.23, NNZs: 843, Bias: -235431461.137594, T: 13959582, Avg. loss: 81512736084864098358603160748032.000000\n",
      "Total training time: 29.85 seconds.\n",
      "-- Epoch 4147\n",
      "Norm: 5523883831204.71, NNZs: 843, Bias: 79626222.817850, T: 13962949, Avg. loss: 81503035875812195422326344384512.000000\n",
      "Total training time: 29.86 seconds.\n",
      "-- Epoch 4148\n",
      "Norm: 5539587139914.48, NNZs: 843, Bias: -83961587.153268, T: 13966316, Avg. loss: 81493220559167524674979948920832.000000\n",
      "Total training time: 29.87 seconds.\n",
      "-- Epoch 4149\n",
      "Norm: 5922967180196.58, NNZs: 843, Bias: -247530320.974363, T: 13969683, Avg. loss: 81483378046787663519252800864256.000000\n",
      "Total training time: 29.87 seconds.\n",
      "-- Epoch 4150\n",
      "Norm: 5700647905703.23, NNZs: 843, Bias: -17336493.966574, T: 13973050, Avg. loss: 81473390768225625350347100782592.000000\n",
      "Total training time: 29.88 seconds.\n",
      "-- Epoch 4151\n",
      "Norm: 5715629194601.13, NNZs: 843, Bias: 54530435.145861, T: 13976417, Avg. loss: 81463569328993970519319242604544.000000\n",
      "Total training time: 29.89 seconds.\n",
      "-- Epoch 4152\n",
      "Norm: 5722290715719.01, NNZs: 843, Bias: -241531172.211035, T: 13979784, Avg. loss: 81453811990029362507614789828608.000000\n",
      "Total training time: 29.90 seconds.\n",
      "-- Epoch 4153\n",
      "Norm: 5728272671836.53, NNZs: 843, Bias: -77989819.005275, T: 13983151, Avg. loss: 81443967860163716812061540352000.000000\n",
      "Total training time: 29.90 seconds.\n",
      "-- Epoch 4154\n",
      "Norm: 5801089663427.73, NNZs: 843, Bias: 85532211.076290, T: 13986518, Avg. loss: 81434031631776682116736484376576.000000\n",
      "Total training time: 29.91 seconds.\n",
      "-- Epoch 4155\n",
      "Norm: 5798627612808.01, NNZs: 843, Bias: -251320164.870315, T: 13989885, Avg. loss: 81424346686090070781552746299392.000000\n",
      "Total training time: 29.92 seconds.\n",
      "-- Epoch 4156\n",
      "Norm: 6114361489094.72, NNZs: 843, Bias: -87806797.897843, T: 13993252, Avg. loss: 81414495956598640461485371817984.000000\n",
      "Total training time: 29.92 seconds.\n",
      "-- Epoch 4157\n",
      "Norm: 5812269896582.05, NNZs: 843, Bias: 125021041.236136, T: 13996619, Avg. loss: 81404618279015325538938246922240.000000\n",
      "Total training time: 29.93 seconds.\n",
      "-- Epoch 4158\n",
      "Norm: 5477050056315.35, NNZs: 843, Bias: -38471684.327839, T: 13999986, Avg. loss: 81394979522516708493203714605056.000000\n",
      "Total training time: 29.94 seconds.\n",
      "-- Epoch 4159\n",
      "Norm: 5432275501901.84, NNZs: 843, Bias: -201945115.602794, T: 14003353, Avg. loss: 81385036126848628904571367849984.000000\n",
      "Total training time: 29.95 seconds.\n",
      "-- Epoch 4160\n",
      "Norm: 5650377862129.19, NNZs: 843, Bias: -38474378.161824, T: 14006720, Avg. loss: 81375230196638205022573028179968.000000\n",
      "Total training time: 29.95 seconds.\n",
      "-- Epoch 4161\n",
      "Norm: 5499038094710.29, NNZs: 843, Bias: -12128977.498286, T: 14010087, Avg. loss: 81365435037018531622700114247680.000000\n",
      "Total training time: 29.96 seconds.\n",
      "-- Epoch 4162\n",
      "Norm: 5698349734325.92, NNZs: 843, Bias: -175573670.283205, T: 14013454, Avg. loss: 81355589687821373272688592683008.000000\n",
      "Total training time: 29.97 seconds.\n",
      "-- Epoch 4163\n",
      "Norm: 5520160260952.58, NNZs: 843, Bias: -12135453.359838, T: 14016821, Avg. loss: 81345701571070987469897833906176.000000\n",
      "Total training time: 29.98 seconds.\n",
      "-- Epoch 4164\n",
      "Norm: 5485323455505.36, NNZs: 843, Bias: 151283459.096745, T: 14020188, Avg. loss: 81335792887129745839308895420416.000000\n",
      "Total training time: 29.98 seconds.\n",
      "-- Epoch 4165\n",
      "Norm: 5605706799044.49, NNZs: 843, Bias: -47463114.628160, T: 14023555, Avg. loss: 81325994244004547677175477698560.000000\n",
      "Total training time: 29.99 seconds.\n",
      "-- Epoch 4166\n",
      "Norm: 5584316386743.49, NNZs: 843, Bias: 231331760.018755, T: 14026922, Avg. loss: 81316128594191278785272452481024.000000\n",
      "Total training time: 30.00 seconds.\n",
      "-- Epoch 4167\n",
      "Norm: 5591951193405.08, NNZs: 843, Bias: -258864192.396523, T: 14030289, Avg. loss: 81306733476682936983092702740480.000000\n",
      "Total training time: 30.00 seconds.\n",
      "-- Epoch 4168\n",
      "Norm: 5779221765781.04, NNZs: 843, Bias: 231296761.927848, T: 14033656, Avg. loss: 81297106240293051530487768547328.000000\n",
      "Total training time: 30.01 seconds.\n",
      "-- Epoch 4169\n",
      "Norm: 5830895330914.11, NNZs: 843, Bias: -258840840.963712, T: 14037023, Avg. loss: 81287545375439231886764969820160.000000\n",
      "Total training time: 30.02 seconds.\n",
      "-- Epoch 4170\n",
      "Norm: 5828496400288.75, NNZs: 843, Bias: -95464394.848298, T: 14040390, Avg. loss: 81277656406216649950639985524736.000000\n",
      "Total training time: 30.03 seconds.\n",
      "-- Epoch 4171\n",
      "Norm: 5878493667281.79, NNZs: 843, Bias: 67891680.037537, T: 14043757, Avg. loss: 81267926182117534990505047228416.000000\n",
      "Total training time: 30.03 seconds.\n",
      "-- Epoch 4172\n",
      "Norm: 5705789886901.17, NNZs: 843, Bias: -108104900.752529, T: 14047124, Avg. loss: 81258387433995232683411216793600.000000\n",
      "Total training time: 30.04 seconds.\n",
      "-- Epoch 4173\n",
      "Norm: 5838689730518.33, NNZs: 843, Bias: -102353893.671566, T: 14050491, Avg. loss: 81248837174885447194892310675456.000000\n",
      "Total training time: 30.05 seconds.\n",
      "-- Epoch 4174\n",
      "Norm: 5987408646444.54, NNZs: 843, Bias: 171638694.463213, T: 14053858, Avg. loss: 81239147403850684024404855226368.000000\n",
      "Total training time: 30.05 seconds.\n",
      "-- Epoch 4175\n",
      "Norm: 6062787942754.10, NNZs: 843, Bias: -171193298.042136, T: 14057225, Avg. loss: 81229579749925341976578559574016.000000\n",
      "Total training time: 30.06 seconds.\n",
      "-- Epoch 4176\n",
      "Norm: 5651434928940.49, NNZs: 843, Bias: -7882637.163274, T: 14060592, Avg. loss: 81220000773383713670224469295104.000000\n",
      "Total training time: 30.07 seconds.\n",
      "-- Epoch 4177\n",
      "Norm: 5843914757978.71, NNZs: 843, Bias: -497771946.856405, T: 14063959, Avg. loss: 81210331028517194944746778263552.000000\n",
      "Total training time: 30.08 seconds.\n",
      "-- Epoch 4178\n",
      "Norm: 5529488165418.36, NNZs: 843, Bias: -156541842.880210, T: 14067326, Avg. loss: 81200676033866224603362297380864.000000\n",
      "Total training time: 30.08 seconds.\n",
      "-- Epoch 4179\n",
      "Norm: 5505667798172.88, NNZs: 843, Bias: 6738910.691287, T: 14070693, Avg. loss: 81190967022673937937046416916480.000000\n",
      "Total training time: 30.09 seconds.\n",
      "-- Epoch 4180\n",
      "Norm: 5734724451724.14, NNZs: 843, Bias: 170000347.974511, T: 14074060, Avg. loss: 81181169804358516652023457251328.000000\n",
      "Total training time: 30.10 seconds.\n",
      "-- Epoch 4181\n",
      "Norm: 5619150368854.80, NNZs: 843, Bias: -319781546.552998, T: 14077427, Avg. loss: 81171481111815474011530925703168.000000\n",
      "Total training time: 30.10 seconds.\n",
      "-- Epoch 4182\n",
      "Norm: 5634862514517.41, NNZs: 843, Bias: -424345826.774672, T: 14080794, Avg. loss: 81161693336424444913040440164352.000000\n",
      "Total training time: 30.11 seconds.\n",
      "-- Epoch 4183\n",
      "Norm: 5454526107363.24, NNZs: 843, Bias: 65386281.183605, T: 14084161, Avg. loss: 81151997507684562295321970868224.000000\n",
      "Total training time: 30.12 seconds.\n",
      "-- Epoch 4184\n",
      "Norm: 5634656241395.66, NNZs: 843, Bias: -97847314.364650, T: 14087528, Avg. loss: 81142203404395854825433038061568.000000\n",
      "Total training time: 30.13 seconds.\n",
      "-- Epoch 4185\n",
      "Norm: 5520560421012.89, NNZs: 843, Bias: 65372252.741422, T: 14090895, Avg. loss: 81132368950034861563835242774528.000000\n",
      "Total training time: 30.13 seconds.\n",
      "-- Epoch 4186\n",
      "Norm: 5739275103999.25, NNZs: 843, Bias: -136667129.737248, T: 14094262, Avg. loss: 81122789777379618991994354794496.000000\n",
      "Total training time: 30.14 seconds.\n",
      "-- Epoch 4187\n",
      "Norm: 5605403598041.11, NNZs: 843, Bias: 26534545.017098, T: 14097629, Avg. loss: 81113104688575700164789734998016.000000\n",
      "Total training time: 30.15 seconds.\n",
      "-- Epoch 4188\n",
      "Norm: 5502963633387.32, NNZs: 843, Bias: 189717111.858693, T: 14100996, Avg. loss: 81103353835596781768223028674560.000000\n",
      "Total training time: 30.15 seconds.\n",
      "-- Epoch 4189\n",
      "Norm: 5584371783312.93, NNZs: 843, Bias: -481043205.743827, T: 14104363, Avg. loss: 81093576736680152038279777091584.000000\n",
      "Total training time: 30.16 seconds.\n",
      "-- Epoch 4190\n",
      "Norm: 5454274226825.59, NNZs: 843, Bias: -170949329.291766, T: 14107730, Avg. loss: 81083851447399581171827687292928.000000\n",
      "Total training time: 30.17 seconds.\n",
      "-- Epoch 4191\n",
      "Norm: 5471090702846.19, NNZs: 843, Bias: -106883511.338582, T: 14111097, Avg. loss: 81074092944758998743905080442880.000000\n",
      "Total training time: 30.18 seconds.\n",
      "-- Epoch 4192\n",
      "Norm: 5523442483296.96, NNZs: 843, Bias: 56268194.555111, T: 14114464, Avg. loss: 81064367750661510605969412849664.000000\n",
      "Total training time: 30.18 seconds.\n",
      "-- Epoch 4193\n",
      "Norm: 5522516513932.51, NNZs: 843, Bias: -106877931.148529, T: 14117831, Avg. loss: 81054878497110750402063747776512.000000\n",
      "Total training time: 30.19 seconds.\n",
      "-- Epoch 4194\n",
      "Norm: 5456407951207.88, NNZs: 843, Bias: -270004918.742773, T: 14121198, Avg. loss: 81045147527080960585309586718720.000000\n",
      "Total training time: 30.20 seconds.\n",
      "-- Epoch 4195\n",
      "Norm: 5469296517519.51, NNZs: 843, Bias: -106872302.499605, T: 14124565, Avg. loss: 81035628276931993238268291317760.000000\n",
      "Total training time: 30.20 seconds.\n",
      "-- Epoch 4196\n",
      "Norm: 5478750316085.02, NNZs: 843, Bias: 119767513.120584, T: 14127932, Avg. loss: 81026016377228433264146890358784.000000\n",
      "Total training time: 30.21 seconds.\n",
      "-- Epoch 4197\n",
      "Norm: 5773782287811.31, NNZs: 843, Bias: -240709866.046429, T: 14131299, Avg. loss: 81016378557119890051550249222144.000000\n",
      "Total training time: 30.22 seconds.\n",
      "-- Epoch 4198\n",
      "Norm: 5386417980912.09, NNZs: 843, Bias: -73273677.022212, T: 14134666, Avg. loss: 81006809010859419474634542678016.000000\n",
      "Total training time: 30.23 seconds.\n",
      "-- Epoch 4199\n",
      "Norm: 5689710468175.96, NNZs: 843, Bias: -90414893.393837, T: 14138033, Avg. loss: 80997108758662788913768496627712.000000\n",
      "Total training time: 30.23 seconds.\n",
      "-- Epoch 4200\n",
      "Norm: 5425627136414.49, NNZs: 843, Bias: -253484602.654122, T: 14141400, Avg. loss: 80987677903264374622672069328896.000000\n",
      "Total training time: 30.24 seconds.\n",
      "-- Epoch 4201\n",
      "Norm: 5391815594591.41, NNZs: 843, Bias: -90410929.620223, T: 14144767, Avg. loss: 80977992400563581196648409202688.000000\n",
      "Total training time: 30.25 seconds.\n",
      "-- Epoch 4202\n",
      "Norm: 5434347256549.13, NNZs: 843, Bias: -253461295.633782, T: 14148134, Avg. loss: 80968451820672098380416158793728.000000\n",
      "Total training time: 30.26 seconds.\n",
      "-- Epoch 4203\n",
      "Norm: 5346779836677.08, NNZs: 843, Bias: -90408103.265914, T: 14151501, Avg. loss: 80958739802856740528008642691072.000000\n",
      "Total training time: 30.26 seconds.\n",
      "-- Epoch 4204\n",
      "Norm: 5414815758532.51, NNZs: 843, Bias: 72625540.135722, T: 14154868, Avg. loss: 80949282241077578989563285602304.000000\n",
      "Total training time: 30.27 seconds.\n",
      "-- Epoch 4205\n",
      "Norm: 5714602692049.15, NNZs: 843, Bias: 133522347.584228, T: 14158235, Avg. loss: 80939732797161569819608890736640.000000\n",
      "Total training time: 30.28 seconds.\n",
      "-- Epoch 4206\n",
      "Norm: 5432238141281.34, NNZs: 843, Bias: -355529159.362472, T: 14161602, Avg. loss: 80930193226549650790411090788352.000000\n",
      "Total training time: 30.28 seconds.\n",
      "-- Epoch 4207\n",
      "Norm: 5381425728984.12, NNZs: 843, Bias: -126079057.111925, T: 14164969, Avg. loss: 80920482965317219740880520347648.000000\n",
      "Total training time: 30.29 seconds.\n",
      "-- Epoch 4208\n",
      "Norm: 5647147546871.32, NNZs: 843, Bias: 233520359.484606, T: 14168336, Avg. loss: 80910852449924215767010119778304.000000\n",
      "Total training time: 30.30 seconds.\n",
      "-- Epoch 4209\n",
      "Norm: 5498449900965.69, NNZs: 843, Bias: 9881265.065668, T: 14171703, Avg. loss: 80901188457698435289903521595392.000000\n",
      "Total training time: 30.31 seconds.\n",
      "-- Epoch 4210\n",
      "Norm: 5852697441509.09, NNZs: 843, Bias: 54063738.303717, T: 14175070, Avg. loss: 80891685916964665185249628520448.000000\n",
      "Total training time: 30.31 seconds.\n",
      "-- Epoch 4211\n",
      "Norm: 5511043961603.76, NNZs: 843, Bias: -108907165.291497, T: 14178437, Avg. loss: 80882039279211889359808131760128.000000\n",
      "Total training time: 30.32 seconds.\n",
      "-- Epoch 4212\n",
      "Norm: 5551573134091.59, NNZs: 843, Bias: 54050644.068573, T: 14181804, Avg. loss: 80872477015425435865890460532736.000000\n",
      "Total training time: 30.33 seconds.\n",
      "-- Epoch 4213\n",
      "Norm: 5469571095511.81, NNZs: 843, Bias: 228156769.202350, T: 14185171, Avg. loss: 80862903164601689262937201442816.000000\n",
      "Total training time: 30.33 seconds.\n",
      "-- Epoch 4214\n",
      "Norm: 5476277284253.86, NNZs: 843, Bias: -212394379.083400, T: 14188538, Avg. loss: 80853312691657881485164595380224.000000\n",
      "Total training time: 30.34 seconds.\n",
      "-- Epoch 4215\n",
      "Norm: 5404059318150.21, NNZs: 843, Bias: -49460362.859850, T: 14191905, Avg. loss: 80843665060751761202399278006272.000000\n",
      "Total training time: 30.35 seconds.\n",
      "-- Epoch 4216\n",
      "Norm: 5347221631715.17, NNZs: 843, Bias: -130376079.976585, T: 14195272, Avg. loss: 80834016644251701094367996411904.000000\n",
      "Total training time: 30.36 seconds.\n",
      "-- Epoch 4217\n",
      "Norm: 5732244587887.59, NNZs: 843, Bias: -293278606.448269, T: 14198639, Avg. loss: 80824418447607132476061828251648.000000\n",
      "Total training time: 30.36 seconds.\n",
      "-- Epoch 4218\n",
      "Norm: 5593995715936.84, NNZs: 843, Bias: -130368659.431147, T: 14202006, Avg. loss: 80814841536853430676048293396480.000000\n",
      "Total training time: 30.37 seconds.\n",
      "-- Epoch 4219\n",
      "Norm: 5405886631896.77, NNZs: 843, Bias: 32522472.877609, T: 14205373, Avg. loss: 80805085520699255726517847588864.000000\n",
      "Total training time: 30.38 seconds.\n",
      "-- Epoch 4220\n",
      "Norm: 5398414960707.85, NNZs: 843, Bias: -130360308.882703, T: 14208740, Avg. loss: 80795542068900327938573069189120.000000\n",
      "Total training time: 30.38 seconds.\n",
      "-- Epoch 4221\n",
      "Norm: 5767963559141.14, NNZs: 843, Bias: 32511041.353185, T: 14212107, Avg. loss: 80785790568683067279546926497792.000000\n",
      "Total training time: 30.39 seconds.\n",
      "-- Epoch 4222\n",
      "Norm: 5428802139186.02, NNZs: 843, Bias: 24009647.624514, T: 14215474, Avg. loss: 80776102881983846313563422457856.000000\n",
      "Total training time: 30.40 seconds.\n",
      "-- Epoch 4223\n",
      "Norm: 5366513824907.41, NNZs: 843, Bias: -138843891.304772, T: 14218841, Avg. loss: 80766425523202129915053942505472.000000\n",
      "Total training time: 30.41 seconds.\n",
      "-- Epoch 4224\n",
      "Norm: 5444067332403.74, NNZs: 843, Bias: -301678863.274563, T: 14222208, Avg. loss: 80756958848564339439512405934080.000000\n",
      "Total training time: 30.41 seconds.\n",
      "-- Epoch 4225\n",
      "Norm: 5450052046532.42, NNZs: 843, Bias: 186823031.150615, T: 14225575, Avg. loss: 80747209357034135827373942112256.000000\n",
      "Total training time: 30.42 seconds.\n",
      "-- Epoch 4226\n",
      "Norm: 5464629211530.53, NNZs: 843, Bias: 23988855.063960, T: 14228942, Avg. loss: 80737694118248652410629774114816.000000\n",
      "Total training time: 30.43 seconds.\n",
      "-- Epoch 4227\n",
      "Norm: 5986365536432.24, NNZs: 843, Bias: -293681543.693129, T: 14232309, Avg. loss: 80728045478473021086902089940992.000000\n",
      "Total training time: 30.43 seconds.\n",
      "-- Epoch 4228\n",
      "Norm: 5540946638251.67, NNZs: 843, Bias: -130867652.372441, T: 14235676, Avg. loss: 80718473543209538348981927542784.000000\n",
      "Total training time: 30.44 seconds.\n",
      "-- Epoch 4229\n",
      "Norm: 5505476274602.93, NNZs: 843, Bias: 31927136.620880, T: 14239043, Avg. loss: 80709037599849602176109449838592.000000\n",
      "Total training time: 30.45 seconds.\n",
      "-- Epoch 4230\n",
      "Norm: 5612961126849.00, NNZs: 843, Bias: -130859537.968603, T: 14242410, Avg. loss: 80699399273418016176869214257152.000000\n",
      "Total training time: 30.46 seconds.\n",
      "-- Epoch 4231\n",
      "Norm: 5513826647834.09, NNZs: 843, Bias: 31917132.992741, T: 14245777, Avg. loss: 80689847027448067393897342435328.000000\n",
      "Total training time: 30.46 seconds.\n",
      "-- Epoch 4232\n",
      "Norm: 5716352433020.16, NNZs: 843, Bias: -130849320.295872, T: 14249144, Avg. loss: 80680143095296599489059798122496.000000\n",
      "Total training time: 30.47 seconds.\n",
      "-- Epoch 4233\n",
      "Norm: 5637526869216.83, NNZs: 843, Bias: 31907723.743436, T: 14252511, Avg. loss: 80670579392352116878011382366208.000000\n",
      "Total training time: 30.48 seconds.\n",
      "-- Epoch 4234\n",
      "Norm: 5729844881754.27, NNZs: 843, Bias: -130840221.944455, T: 14255878, Avg. loss: 80661093446282600129480561262592.000000\n",
      "Total training time: 30.48 seconds.\n",
      "-- Epoch 4235\n",
      "Norm: 5705335160444.48, NNZs: 843, Bias: -293569980.446515, T: 14259245, Avg. loss: 80651633336098886311773315530752.000000\n",
      "Total training time: 30.49 seconds.\n",
      "-- Epoch 4236\n",
      "Norm: 5821479503410.73, NNZs: 843, Bias: -130832572.876258, T: 14262612, Avg. loss: 80642120497354215325161914105856.000000\n",
      "Total training time: 30.50 seconds.\n",
      "-- Epoch 4237\n",
      "Norm: 5978566831765.64, NNZs: 843, Bias: 31884516.851753, T: 14265979, Avg. loss: 80632550284597190824018348867584.000000\n",
      "Total training time: 30.51 seconds.\n",
      "-- Epoch 4238\n",
      "Norm: 6004833634665.28, NNZs: 843, Bias: -456234741.193417, T: 14269346, Avg. loss: 80622930767441522202310485213184.000000\n",
      "Total training time: 30.51 seconds.\n",
      "-- Epoch 4239\n",
      "Norm: 5970848038427.80, NNZs: 843, Bias: -293516994.914575, T: 14272713, Avg. loss: 80613518128896894409173326888960.000000\n",
      "Total training time: 30.52 seconds.\n",
      "-- Epoch 4240\n",
      "Norm: 5922361845870.04, NNZs: 843, Bias: 137242585.012792, T: 14276080, Avg. loss: 80604026050779663825345332641792.000000\n",
      "Total training time: 30.53 seconds.\n",
      "-- Epoch 4241\n",
      "Norm: 5686978446575.76, NNZs: 843, Bias: -25444876.785234, T: 14279447, Avg. loss: 80594470888109116896063044714496.000000\n",
      "Total training time: 30.53 seconds.\n",
      "-- Epoch 4242\n",
      "Norm: 5576246647297.44, NNZs: 843, Bias: -51423382.244139, T: 14282814, Avg. loss: 80585067880061851207201240121344.000000\n",
      "Total training time: 30.54 seconds.\n",
      "-- Epoch 4243\n",
      "Norm: 5563950660601.96, NNZs: 843, Bias: -214081477.809557, T: 14286181, Avg. loss: 80575756829213423535191540891648.000000\n",
      "Total training time: 30.55 seconds.\n",
      "-- Epoch 4244\n",
      "Norm: 5602540868947.45, NNZs: 843, Bias: -51425992.408773, T: 14289548, Avg. loss: 80566340686277152167919098003456.000000\n",
      "Total training time: 30.56 seconds.\n",
      "-- Epoch 4245\n",
      "Norm: 5645725835726.74, NNZs: 843, Bias: -214064048.728129, T: 14292915, Avg. loss: 80556764606460793170241542684672.000000\n",
      "Total training time: 30.56 seconds.\n",
      "-- Epoch 4246\n",
      "Norm: 5520591650555.11, NNZs: 843, Bias: -51428282.353536, T: 14296282, Avg. loss: 80547062552327420507090055069696.000000\n",
      "Total training time: 30.57 seconds.\n",
      "-- Epoch 4247\n",
      "Norm: 5832245675405.58, NNZs: 843, Bias: 46012943.049694, T: 14299649, Avg. loss: 80537579305786857422532085022720.000000\n",
      "Total training time: 30.58 seconds.\n",
      "-- Epoch 4248\n",
      "Norm: 5478922836741.30, NNZs: 843, Bias: 139904696.956260, T: 14303016, Avg. loss: 80528251252752372248104579104768.000000\n",
      "Total training time: 30.58 seconds.\n",
      "-- Epoch 4249\n",
      "Norm: 5526966411673.62, NNZs: 843, Bias: -22706759.267664, T: 14306383, Avg. loss: 80518694970158294260910741520384.000000\n",
      "Total training time: 30.59 seconds.\n",
      "-- Epoch 4250\n",
      "Norm: 5668211724476.15, NNZs: 843, Bias: -185298742.817697, T: 14309750, Avg. loss: 80509265831262958838512747544576.000000\n",
      "Total training time: 30.60 seconds.\n",
      "-- Epoch 4251\n",
      "Norm: 5675537817116.39, NNZs: 843, Bias: -347871892.672829, T: 14313117, Avg. loss: 80499764289657134557641941450752.000000\n",
      "Total training time: 30.61 seconds.\n",
      "-- Epoch 4252\n",
      "Norm: 5993635322034.34, NNZs: 843, Bias: 139854533.268214, T: 14316484, Avg. loss: 80490356955098786447512228069376.000000\n",
      "Total training time: 30.61 seconds.\n",
      "-- Epoch 4253\n",
      "Norm: 5677873850655.44, NNZs: 843, Bias: -347840219.462632, T: 14319851, Avg. loss: 80480632055417332520684661768192.000000\n",
      "Total training time: 30.62 seconds.\n",
      "-- Epoch 4254\n",
      "Norm: 5687555719124.15, NNZs: 843, Bias: 265516444.667401, T: 14323218, Avg. loss: 80471138805438267744339588284416.000000\n",
      "Total training time: 30.63 seconds.\n",
      "-- Epoch 4255\n",
      "Norm: 5529778690653.90, NNZs: 843, Bias: -222128852.742166, T: 14326585, Avg. loss: 80461731777279257834614958325760.000000\n",
      "Total training time: 30.63 seconds.\n",
      "-- Epoch 4256\n",
      "Norm: 5604482314170.15, NNZs: 843, Bias: -59588195.683306, T: 14329952, Avg. loss: 80452448690733273342646379610112.000000\n",
      "Total training time: 30.64 seconds.\n",
      "-- Epoch 4257\n",
      "Norm: 5594323276793.96, NNZs: 843, Bias: 272515670.835425, T: 14333319, Avg. loss: 80443117746394384637417893134336.000000\n",
      "Total training time: 30.65 seconds.\n",
      "-- Epoch 4258\n",
      "Norm: 5671220500386.87, NNZs: 843, Bias: -273418420.065023, T: 14336686, Avg. loss: 80433599464714298002902106505216.000000\n",
      "Total training time: 30.66 seconds.\n",
      "-- Epoch 4259\n",
      "Norm: 5508646429234.94, NNZs: 843, Bias: -110902665.182240, T: 14340053, Avg. loss: 80424281662004123876038243516416.000000\n",
      "Total training time: 30.66 seconds.\n",
      "-- Epoch 4260\n",
      "Norm: 5531543542124.43, NNZs: 843, Bias: -168028292.110776, T: 14343420, Avg. loss: 80414989523789179993717525708800.000000\n",
      "Total training time: 30.67 seconds.\n",
      "-- Epoch 4261\n",
      "Norm: 5501688212591.14, NNZs: 843, Bias: -330506923.484650, T: 14346787, Avg. loss: 80405473896552466117572280975360.000000\n",
      "Total training time: 30.68 seconds.\n",
      "-- Epoch 4262\n",
      "Norm: 5685784695770.57, NNZs: 843, Bias: 66317442.710634, T: 14350154, Avg. loss: 80396115876360218895295795167232.000000\n",
      "Total training time: 30.68 seconds.\n",
      "-- Epoch 4263\n",
      "Norm: 5949565138114.39, NNZs: 843, Bias: 238942222.241701, T: 14353521, Avg. loss: 80386838047256712657386059333632.000000\n",
      "Total training time: 30.69 seconds.\n",
      "-- Epoch 4264\n",
      "Norm: 5649256904078.45, NNZs: 843, Bias: 139828641.109209, T: 14356888, Avg. loss: 80377391615015135850471893237760.000000\n",
      "Total training time: 30.70 seconds.\n",
      "-- Epoch 4265\n",
      "Norm: 5682267802341.78, NNZs: 843, Bias: -347523098.331700, T: 14360255, Avg. loss: 80368046091703499336457346088960.000000\n",
      "Total training time: 30.71 seconds.\n",
      "-- Epoch 4266\n",
      "Norm: 5685264432419.52, NNZs: 843, Bias: 139802162.162321, T: 14363622, Avg. loss: 80358672513816692652400600678400.000000\n",
      "Total training time: 30.71 seconds.\n",
      "-- Epoch 4267\n",
      "Norm: 5520820893118.77, NNZs: 843, Bias: -22637998.401818, T: 14366989, Avg. loss: 80349313350887308963018740596736.000000\n",
      "Total training time: 30.72 seconds.\n",
      "-- Epoch 4268\n",
      "Norm: 5512032959769.33, NNZs: 843, Bias: -185058545.689426, T: 14370356, Avg. loss: 80339982373966669386416409018368.000000\n",
      "Total training time: 30.73 seconds.\n",
      "-- Epoch 4269\n",
      "Norm: 5500881171310.73, NNZs: 843, Bias: -22644121.484417, T: 14373723, Avg. loss: 80330502364139343458204378988544.000000\n",
      "Total training time: 30.74 seconds.\n",
      "-- Epoch 4270\n",
      "Norm: 5528184548491.61, NNZs: 843, Bias: -185045159.408631, T: 14377090, Avg. loss: 80320862234408658455000617320448.000000\n",
      "Total training time: 30.74 seconds.\n",
      "-- Epoch 4271\n",
      "Norm: 5743678099945.04, NNZs: 843, Bias: -184876638.741707, T: 14380457, Avg. loss: 80311565808938420432973504446464.000000\n",
      "Total training time: 30.75 seconds.\n",
      "-- Epoch 4272\n",
      "Norm: 5681114064108.83, NNZs: 843, Bias: -101862718.450416, T: 14383824, Avg. loss: 80302078689659043044261812502528.000000\n",
      "Total training time: 30.76 seconds.\n",
      "-- Epoch 4273\n",
      "Norm: 5709605375600.50, NNZs: 843, Bias: -107642987.725231, T: 14387191, Avg. loss: 80292739950149122895176469053440.000000\n",
      "Total training time: 30.76 seconds.\n",
      "-- Epoch 4274\n",
      "Norm: 5568398331541.86, NNZs: 843, Bias: -86423974.861227, T: 14390558, Avg. loss: 80283332882715355542259840843776.000000\n",
      "Total training time: 30.77 seconds.\n",
      "-- Epoch 4275\n",
      "Norm: 5586487385568.92, NNZs: 843, Bias: -138641935.063746, T: 14393925, Avg. loss: 80273821918397456332081932009472.000000\n",
      "Total training time: 30.78 seconds.\n",
      "-- Epoch 4276\n",
      "Norm: 5600189175787.26, NNZs: 843, Bias: -171040355.303524, T: 14397292, Avg. loss: 80264473069296498121509726846976.000000\n",
      "Total training time: 30.79 seconds.\n",
      "-- Epoch 4277\n",
      "Norm: 5649187477765.63, NNZs: 843, Bias: -333366804.634628, T: 14400659, Avg. loss: 80255125598261672629234439290880.000000\n",
      "Total training time: 30.79 seconds.\n",
      "-- Epoch 4278\n",
      "Norm: 5621181245068.12, NNZs: 843, Bias: -171028274.073782, T: 14404026, Avg. loss: 80245759435891142757809800609792.000000\n",
      "Total training time: 30.80 seconds.\n",
      "-- Epoch 4279\n",
      "Norm: 5648705733049.71, NNZs: 843, Bias: -8709063.595619, T: 14407393, Avg. loss: 80236557303927297602480936845312.000000\n",
      "Total training time: 30.81 seconds.\n",
      "-- Epoch 4280\n",
      "Norm: 5851968874063.15, NNZs: 843, Bias: -372747629.328550, T: 14410760, Avg. loss: 80227255180971131349353287909376.000000\n",
      "Total training time: 30.81 seconds.\n",
      "-- Epoch 4281\n",
      "Norm: 5736187379859.25, NNZs: 843, Bias: 29391156.209402, T: 14414127, Avg. loss: 80217859130538631944064452263936.000000\n",
      "Total training time: 30.82 seconds.\n",
      "-- Epoch 4282\n",
      "Norm: 5739329551562.87, NNZs: 843, Bias: -132899229.277425, T: 14417494, Avg. loss: 80208313033666259573677798457344.000000\n",
      "Total training time: 30.83 seconds.\n",
      "-- Epoch 4283\n",
      "Norm: 5602523461699.09, NNZs: 843, Bias: 29379036.332008, T: 14420861, Avg. loss: 80199159433241641135188159234048.000000\n",
      "Total training time: 30.84 seconds.\n",
      "-- Epoch 4284\n",
      "Norm: 5654833282453.14, NNZs: 843, Bias: -132893194.869585, T: 14424228, Avg. loss: 80189860028323826697758891311104.000000\n",
      "Total training time: 30.84 seconds.\n",
      "-- Epoch 4285\n",
      "Norm: 5913599490543.26, NNZs: 843, Bias: 1375475.759854, T: 14427595, Avg. loss: 80180596719179862321009770102784.000000\n",
      "Total training time: 30.85 seconds.\n",
      "-- Epoch 4286\n",
      "Norm: 5721352915898.83, NNZs: 843, Bias: -160875535.930492, T: 14430962, Avg. loss: 80171293129455484744028095774720.000000\n",
      "Total training time: 30.86 seconds.\n",
      "-- Epoch 4287\n",
      "Norm: 5760807248993.80, NNZs: 843, Bias: -25624668.525364, T: 14434329, Avg. loss: 80161884631566570294701035880448.000000\n",
      "Total training time: 30.87 seconds.\n",
      "-- Epoch 4288\n",
      "Norm: 5805551707406.22, NNZs: 843, Bias: -187855755.616098, T: 14437696, Avg. loss: 80152509721424453467542917218304.000000\n",
      "Total training time: 30.87 seconds.\n",
      "-- Epoch 4289\n",
      "Norm: 5974521850781.22, NNZs: 843, Bias: -350068043.163363, T: 14441063, Avg. loss: 80143260659686955319097005441024.000000\n",
      "Total training time: 30.88 seconds.\n",
      "-- Epoch 4290\n",
      "Norm: 5946083691257.06, NNZs: 843, Bias: -187843149.271601, T: 14444430, Avg. loss: 80134031637700540077411764535296.000000\n",
      "Total training time: 30.89 seconds.\n",
      "-- Epoch 4291\n",
      "Norm: 5876420498285.11, NNZs: 843, Bias: -25637900.929402, T: 14447797, Avg. loss: 80124704852948669088010229252096.000000\n",
      "Total training time: 30.89 seconds.\n",
      "-- Epoch 4292\n",
      "Norm: 5784405722216.73, NNZs: 843, Bias: -187831549.331937, T: 14451164, Avg. loss: 80115304235599615495891408912384.000000\n",
      "Total training time: 30.90 seconds.\n",
      "-- Epoch 4293\n",
      "Norm: 5851729394169.38, NNZs: 843, Bias: -25644396.487738, T: 14454531, Avg. loss: 80106003761605287895049466544128.000000\n",
      "Total training time: 30.91 seconds.\n",
      "-- Epoch 4294\n",
      "Norm: 5712468899052.20, NNZs: 843, Bias: -187818963.221180, T: 14457898, Avg. loss: 80096614901424317181681570152448.000000\n",
      "Total training time: 30.92 seconds.\n",
      "-- Epoch 4295\n",
      "Norm: 5632629360936.00, NNZs: 843, Bias: -25651566.851789, T: 14461265, Avg. loss: 80087185156739554696125518708736.000000\n",
      "Total training time: 30.92 seconds.\n",
      "-- Epoch 4296\n",
      "Norm: 5849094171833.81, NNZs: 843, Bias: 367037468.294118, T: 14464632, Avg. loss: 80077978847121339991069097459712.000000\n",
      "Total training time: 30.93 seconds.\n",
      "-- Epoch 4297\n",
      "Norm: 5853277044102.80, NNZs: 843, Bias: -119416107.786068, T: 14467999, Avg. loss: 80068709015342968491510713024512.000000\n",
      "Total training time: 30.94 seconds.\n",
      "-- Epoch 4298\n",
      "Norm: 5810798746587.58, NNZs: 843, Bias: 42720132.891754, T: 14471366, Avg. loss: 80059375449722566567287123345408.000000\n",
      "Total training time: 30.94 seconds.\n",
      "-- Epoch 4299\n",
      "Norm: 5834154135817.67, NNZs: 843, Bias: -119410822.460735, T: 14474733, Avg. loss: 80050078472360405562602469457920.000000\n",
      "Total training time: 30.95 seconds.\n",
      "-- Epoch 4300\n",
      "Norm: 6062275683463.64, NNZs: 843, Bias: 42707294.147589, T: 14478100, Avg. loss: 80040861834106353142307175268352.000000\n",
      "Total training time: 30.96 seconds.\n",
      "-- Epoch 4301\n",
      "Norm: 6068630118258.21, NNZs: 843, Bias: -209156643.654787, T: 14481467, Avg. loss: 80031591053134822587342920876032.000000\n",
      "Total training time: 30.97 seconds.\n",
      "-- Epoch 4302\n",
      "Norm: 5757657530608.15, NNZs: 843, Bias: -47053283.366903, T: 14484834, Avg. loss: 80022109855810857913843340804096.000000\n",
      "Total training time: 30.97 seconds.\n",
      "-- Epoch 4303\n",
      "Norm: 6078828490561.96, NNZs: 843, Bias: 120815452.986934, T: 14488201, Avg. loss: 80012935768686328275704901271552.000000\n",
      "Total training time: 30.98 seconds.\n",
      "-- Epoch 4304\n",
      "Norm: 6115598733728.70, NNZs: 843, Bias: -442934499.386172, T: 14491568, Avg. loss: 80003536594591967172957378183168.000000\n",
      "Total training time: 30.99 seconds.\n",
      "-- Epoch 4305\n",
      "Norm: 5837845997419.31, NNZs: 843, Bias: 43289802.817752, T: 14494935, Avg. loss: 79994219453944886403547545993216.000000\n",
      "Total training time: 30.99 seconds.\n",
      "-- Epoch 4306\n",
      "Norm: 6048423085304.51, NNZs: 843, Bias: -442891013.757255, T: 14498302, Avg. loss: 79984973399994838103252147372032.000000\n",
      "Total training time: 31.00 seconds.\n",
      "-- Epoch 4307\n",
      "Norm: 5875764156858.23, NNZs: 843, Bias: -2432261.621933, T: 14501669, Avg. loss: 79975734121961712026676744945664.000000\n",
      "Total training time: 31.01 seconds.\n",
      "-- Epoch 4308\n",
      "Norm: 5833128331443.39, NNZs: 843, Bias: -164476318.039721, T: 14505036, Avg. loss: 79966488854787915547422377377792.000000\n",
      "Total training time: 31.02 seconds.\n",
      "-- Epoch 4309\n",
      "Norm: 6028206376110.06, NNZs: 843, Bias: -326501437.798480, T: 14508403, Avg. loss: 79957273153300194694733269827584.000000\n",
      "Total training time: 31.02 seconds.\n",
      "-- Epoch 4310\n",
      "Norm: 6196296257056.80, NNZs: 843, Bias: -500919259.884715, T: 14511770, Avg. loss: 79948015728382737154899930251264.000000\n",
      "Total training time: 31.03 seconds.\n",
      "-- Epoch 4311\n",
      "Norm: 6260167428473.40, NNZs: 843, Bias: -14863039.936348, T: 14515137, Avg. loss: 79938789651797991189587832078336.000000\n",
      "Total training time: 31.04 seconds.\n",
      "-- Epoch 4312\n",
      "Norm: 6037751195489.44, NNZs: 843, Bias: -176869766.877737, T: 14518504, Avg. loss: 79929358708396781214275368648704.000000\n",
      "Total training time: 31.05 seconds.\n",
      "-- Epoch 4313\n",
      "Norm: 5969818067285.01, NNZs: 843, Bias: -338856424.210843, T: 14521871, Avg. loss: 79920137853202450888383149375488.000000\n",
      "Total training time: 31.05 seconds.\n",
      "-- Epoch 4314\n",
      "Norm: 5982263176620.39, NNZs: 843, Bias: -176858631.308190, T: 14525238, Avg. loss: 79910938780805850023325396369408.000000\n",
      "Total training time: 31.06 seconds.\n",
      "-- Epoch 4315\n",
      "Norm: 5947389038241.38, NNZs: 843, Bias: -216915052.804881, T: 14528605, Avg. loss: 79901570823003694444979572178944.000000\n",
      "Total training time: 31.07 seconds.\n",
      "-- Epoch 4316\n",
      "Norm: 6181225706173.80, NNZs: 843, Bias: -57164136.418398, T: 14531972, Avg. loss: 79892348365386913368115425312768.000000\n",
      "Total training time: 31.07 seconds.\n",
      "-- Epoch 4317\n",
      "Norm: 6252160305084.76, NNZs: 843, Bias: -219121058.108788, T: 14535339, Avg. loss: 79883154009749822843825873223680.000000\n",
      "Total training time: 31.08 seconds.\n",
      "-- Epoch 4318\n",
      "Norm: 6233853851399.26, NNZs: 843, Bias: -57167186.733744, T: 14538706, Avg. loss: 79873939414146570937845522890752.000000\n",
      "Total training time: 31.09 seconds.\n",
      "-- Epoch 4319\n",
      "Norm: 6196536088640.22, NNZs: 843, Bias: -219104324.528752, T: 14542073, Avg. loss: 79864738237110039020276484669440.000000\n",
      "Total training time: 31.10 seconds.\n",
      "-- Epoch 4320\n",
      "Norm: 6106293676173.87, NNZs: 843, Bias: -381023856.320935, T: 14545440, Avg. loss: 79855528333314806281159971962880.000000\n",
      "Total training time: 31.10 seconds.\n",
      "-- Epoch 4321\n",
      "Norm: 6023565840348.44, NNZs: 843, Bias: -219089643.676451, T: 14548807, Avg. loss: 79846240059773072043944097873920.000000\n",
      "Total training time: 31.11 seconds.\n",
      "-- Epoch 4322\n",
      "Norm: 6012015444428.67, NNZs: 843, Bias: 266641197.902956, T: 14552174, Avg. loss: 79837048731209547332864668860416.000000\n",
      "Total training time: 31.12 seconds.\n",
      "-- Epoch 4323\n",
      "Norm: 5894032394850.22, NNZs: 843, Bias: 104722371.727195, T: 14555541, Avg. loss: 79827860816987146811533906935808.000000\n",
      "Total training time: 31.12 seconds.\n",
      "-- Epoch 4324\n",
      "Norm: 5910410200531.56, NNZs: 843, Bias: -28989895.283400, T: 14558908, Avg. loss: 79818707048461123645816860311552.000000\n",
      "Total training time: 31.13 seconds.\n",
      "-- Epoch 4325\n",
      "Norm: 5981724837573.77, NNZs: 843, Bias: 132886484.590095, T: 14562275, Avg. loss: 79809504222204264471749558534144.000000\n",
      "Total training time: 31.14 seconds.\n",
      "-- Epoch 4326\n",
      "Norm: 5918577379642.48, NNZs: 843, Bias: -28995050.737482, T: 14565642, Avg. loss: 79800406749708789854779148861440.000000\n",
      "Total training time: 31.15 seconds.\n",
      "-- Epoch 4327\n",
      "Norm: 5937985705463.50, NNZs: 843, Bias: -190859190.501536, T: 14569009, Avg. loss: 79791170966800754802257026875392.000000\n",
      "Total training time: 31.15 seconds.\n",
      "-- Epoch 4328\n",
      "Norm: 5854253918128.40, NNZs: 843, Bias: -29000602.998631, T: 14572376, Avg. loss: 79781992106589626830274544271360.000000\n",
      "Total training time: 31.16 seconds.\n",
      "-- Epoch 4329\n",
      "Norm: 5816813028478.17, NNZs: 843, Bias: -190846053.922156, T: 14575743, Avg. loss: 79772821068882110627841082130432.000000\n",
      "Total training time: 31.17 seconds.\n",
      "-- Epoch 4330\n",
      "Norm: 6125023684034.99, NNZs: 843, Bias: 75834117.169822, T: 14579110, Avg. loss: 79763593407760815716470905372672.000000\n",
      "Total training time: 31.17 seconds.\n",
      "-- Epoch 4331\n",
      "Norm: 6011186593271.53, NNZs: 843, Bias: -85998304.851076, T: 14582477, Avg. loss: 79754435861284383518172318269440.000000\n",
      "Total training time: 31.18 seconds.\n",
      "-- Epoch 4332\n",
      "Norm: 5690019194297.54, NNZs: 843, Bias: 23489875.198151, T: 14585844, Avg. loss: 79745327212723915843365744148480.000000\n",
      "Total training time: 31.19 seconds.\n",
      "-- Epoch 4333\n",
      "Norm: 5968770432646.61, NNZs: 843, Bias: -138321408.282281, T: 14589211, Avg. loss: 79736226303000861399880497627136.000000\n",
      "Total training time: 31.20 seconds.\n",
      "-- Epoch 4334\n",
      "Norm: 5667451372028.75, NNZs: 843, Bias: 23477611.570302, T: 14592578, Avg. loss: 79726925923179792397508200628224.000000\n",
      "Total training time: 31.20 seconds.\n",
      "-- Epoch 4335\n",
      "Norm: 5697548789650.19, NNZs: 843, Bias: -138314559.023627, T: 14595945, Avg. loss: 79717614485951934209875928678400.000000\n",
      "Total training time: 31.21 seconds.\n",
      "-- Epoch 4336\n",
      "Norm: 5679816205499.91, NNZs: 843, Bias: -300087175.263416, T: 14599312, Avg. loss: 79708444845584624065395405357056.000000\n",
      "Total training time: 31.22 seconds.\n",
      "-- Epoch 4337\n",
      "Norm: 5575305061689.94, NNZs: 843, Bias: 129488189.544290, T: 14602679, Avg. loss: 79699322661947557638248113111040.000000\n",
      "Total training time: 31.22 seconds.\n",
      "-- Epoch 4338\n",
      "Norm: 5890979552312.57, NNZs: 843, Bias: -355798899.662025, T: 14606046, Avg. loss: 79690089272687601445595904999424.000000\n",
      "Total training time: 31.23 seconds.\n",
      "-- Epoch 4339\n",
      "Norm: 5732146756222.29, NNZs: 843, Bias: 129464935.226888, T: 14609413, Avg. loss: 79681018572540385445798060490752.000000\n",
      "Total training time: 31.24 seconds.\n",
      "-- Epoch 4340\n",
      "Norm: 5594069086641.14, NNZs: 843, Bias: -169410258.300179, T: 14612780, Avg. loss: 79671669687533740615675687206912.000000\n",
      "Total training time: 31.25 seconds.\n",
      "-- Epoch 4341\n",
      "Norm: 5845142987174.57, NNZs: 843, Bias: 360095611.053637, T: 14616147, Avg. loss: 79662525003961727843893310914560.000000\n",
      "Total training time: 31.25 seconds.\n",
      "-- Epoch 4342\n",
      "Norm: 5470077875360.06, NNZs: 843, Bias: -125091594.904562, T: 14619514, Avg. loss: 79653336424927398882148152770560.000000\n",
      "Total training time: 31.26 seconds.\n",
      "-- Epoch 4343\n",
      "Norm: 5485925568633.94, NNZs: 843, Bias: 36623805.324504, T: 14622881, Avg. loss: 79644265657387390316854967795712.000000\n",
      "Total training time: 31.27 seconds.\n",
      "-- Epoch 4344\n",
      "Norm: 5529194812931.86, NNZs: 843, Bias: -195561892.769137, T: 14626248, Avg. loss: 79635015142865790033104648273920.000000\n",
      "Total training time: 31.28 seconds.\n",
      "-- Epoch 4345\n",
      "Norm: 5494845078180.68, NNZs: 843, Bias: -123402405.047624, T: 14629615, Avg. loss: 79625688336416017391795545571328.000000\n",
      "Total training time: 31.28 seconds.\n",
      "-- Epoch 4346\n",
      "Norm: 5591543347135.93, NNZs: 843, Bias: -285082765.703739, T: 14632982, Avg. loss: 79616672152836876022929608933376.000000\n",
      "Total training time: 31.29 seconds.\n",
      "-- Epoch 4347\n",
      "Norm: 5589472604022.42, NNZs: 843, Bias: -123396070.928176, T: 14636349, Avg. loss: 79607395491466889307115239768064.000000\n",
      "Total training time: 31.30 seconds.\n",
      "-- Epoch 4348\n",
      "Norm: 5837062165150.63, NNZs: 843, Bias: -219758863.243433, T: 14639716, Avg. loss: 79598230449785236689677304463360.000000\n",
      "Total training time: 31.30 seconds.\n",
      "-- Epoch 4349\n",
      "Norm: 5867269268037.05, NNZs: 843, Bias: -169719171.405965, T: 14643083, Avg. loss: 79589148900193924476109114572800.000000\n",
      "Total training time: 31.31 seconds.\n",
      "-- Epoch 4350\n",
      "Norm: 5763494424086.05, NNZs: 843, Bias: 23178631.944572, T: 14646450, Avg. loss: 79579978958264538447786629136384.000000\n",
      "Total training time: 31.32 seconds.\n",
      "-- Epoch 4351\n",
      "Norm: 5675871174801.72, NNZs: 843, Bias: 13509200.808657, T: 14649817, Avg. loss: 79570836347490169894358164176896.000000\n",
      "Total training time: 31.33 seconds.\n",
      "-- Epoch 4352\n",
      "Norm: 5641946325520.25, NNZs: 843, Bias: 27316391.813349, T: 14653184, Avg. loss: 79561581738353587602970554400768.000000\n",
      "Total training time: 31.33 seconds.\n",
      "-- Epoch 4353\n",
      "Norm: 5858500357799.89, NNZs: 843, Bias: -457544591.602097, T: 14656551, Avg. loss: 79552283154024663679312047636480.000000\n",
      "Total training time: 31.34 seconds.\n",
      "-- Epoch 4354\n",
      "Norm: 6003164014682.68, NNZs: 843, Bias: -295911944.135437, T: 14659918, Avg. loss: 79543160864932461299161827901440.000000\n",
      "Total training time: 31.35 seconds.\n",
      "-- Epoch 4355\n",
      "Norm: 5735292038132.44, NNZs: 843, Bias: 70394981.970264, T: 14663285, Avg. loss: 79534122351952811982778106118144.000000\n",
      "Total training time: 31.35 seconds.\n",
      "-- Epoch 4356\n",
      "Norm: 5793252820097.01, NNZs: 843, Bias: 244156572.366414, T: 14666652, Avg. loss: 79524837606351620432852867350528.000000\n",
      "Total training time: 31.36 seconds.\n",
      "-- Epoch 4357\n",
      "Norm: 5638554604898.66, NNZs: 843, Bias: 82558077.623926, T: 14670019, Avg. loss: 79515891886054229514713084985344.000000\n",
      "Total training time: 31.37 seconds.\n",
      "-- Epoch 4358\n",
      "Norm: 5698631224239.95, NNZs: 843, Bias: -79022723.008157, T: 14673386, Avg. loss: 79506859278128559972772307009536.000000\n",
      "Total training time: 31.38 seconds.\n",
      "-- Epoch 4359\n",
      "Norm: 5964947107440.13, NNZs: 843, Bias: 82543170.874102, T: 14676753, Avg. loss: 79497785227048083283235645161472.000000\n",
      "Total training time: 31.38 seconds.\n",
      "-- Epoch 4360\n",
      "Norm: 5971150053955.31, NNZs: 843, Bias: -47931241.658073, T: 14680120, Avg. loss: 79488629811373541811998093737984.000000\n",
      "Total training time: 31.39 seconds.\n",
      "-- Epoch 4361\n",
      "Norm: 5606235711538.92, NNZs: 843, Bias: -180821387.251464, T: 14683487, Avg. loss: 79479517972475708186888790605824.000000\n",
      "Total training time: 31.40 seconds.\n",
      "-- Epoch 4362\n",
      "Norm: 5660106739053.03, NNZs: 843, Bias: -19279745.368601, T: 14686854, Avg. loss: 79470463880550066609476243292160.000000\n",
      "Total training time: 31.40 seconds.\n",
      "-- Epoch 4363\n",
      "Norm: 5641772623964.47, NNZs: 843, Bias: -180808505.188489, T: 14690221, Avg. loss: 79461346618950787900696323686400.000000\n",
      "Total training time: 31.41 seconds.\n",
      "-- Epoch 4364\n",
      "Norm: 5750819046867.81, NNZs: 843, Bias: 303749602.827059, T: 14693588, Avg. loss: 79452320550926524611286894903296.000000\n",
      "Total training time: 31.42 seconds.\n",
      "-- Epoch 4365\n",
      "Norm: 5611258982269.56, NNZs: 843, Bias: -180795185.094151, T: 14696955, Avg. loss: 79443242517012426278081715503104.000000\n",
      "Total training time: 31.43 seconds.\n",
      "-- Epoch 4366\n",
      "Norm: 5624154242152.36, NNZs: 843, Bias: -342286200.491728, T: 14700322, Avg. loss: 79434072089633497188834524266496.000000\n",
      "Total training time: 31.43 seconds.\n",
      "-- Epoch 4367\n",
      "Norm: 5456067688628.47, NNZs: 843, Bias: -180781273.882215, T: 14703689, Avg. loss: 79424818317126375283214973403136.000000\n",
      "Total training time: 31.44 seconds.\n",
      "-- Epoch 4368\n",
      "Norm: 5394526883826.67, NNZs: 843, Bias: -19294572.486696, T: 14707056, Avg. loss: 79415935691827364218301054976000.000000\n",
      "Total training time: 31.45 seconds.\n",
      "-- Epoch 4369\n",
      "Norm: 5423660161754.05, NNZs: 843, Bias: -149282237.302368, T: 14710423, Avg. loss: 79406745455172413831101089316864.000000\n",
      "Total training time: 31.46 seconds.\n",
      "-- Epoch 4370\n",
      "Norm: 5564627003148.05, NNZs: 843, Bias: 12184603.683498, T: 14713790, Avg. loss: 79397782985137440882062981595136.000000\n",
      "Total training time: 31.46 seconds.\n",
      "-- Epoch 4371\n",
      "Norm: 5625145186340.54, NNZs: 843, Bias: -149271491.458080, T: 14717157, Avg. loss: 79388735235998774360902949928960.000000\n",
      "Total training time: 31.47 seconds.\n",
      "-- Epoch 4372\n",
      "Norm: 5418118613710.15, NNZs: 843, Bias: -24798713.284265, T: 14720524, Avg. loss: 79379839290523508625914116177920.000000\n",
      "Total training time: 31.48 seconds.\n",
      "-- Epoch 4373\n",
      "Norm: 5434186614851.91, NNZs: 843, Bias: -186234462.349297, T: 14723891, Avg. loss: 79370933238065878884795761557504.000000\n",
      "Total training time: 31.48 seconds.\n",
      "-- Epoch 4374\n",
      "Norm: 5755649009808.85, NNZs: 843, Bias: -347652065.130400, T: 14727258, Avg. loss: 79361887501692810297863422607360.000000\n",
      "Total training time: 31.49 seconds.\n",
      "-- Epoch 4375\n",
      "Norm: 5664737125867.42, NNZs: 843, Bias: -170723392.417177, T: 14730625, Avg. loss: 79352873650384998713602389049344.000000\n",
      "Total training time: 31.50 seconds.\n",
      "-- Epoch 4376\n",
      "Norm: 5460493534632.13, NNZs: 843, Bias: -9310789.839036, T: 14733992, Avg. loss: 79343719276258681654678336307200.000000\n",
      "Total training time: 31.51 seconds.\n",
      "-- Epoch 4377\n",
      "Norm: 5621925489275.15, NNZs: 843, Bias: -124602018.221083, T: 14737359, Avg. loss: 79334580574531183630061550960640.000000\n",
      "Total training time: 31.51 seconds.\n",
      "-- Epoch 4378\n",
      "Norm: 5727929885977.30, NNZs: 843, Bias: -145709969.752796, T: 14740726, Avg. loss: 79325562350391641557120214630400.000000\n",
      "Total training time: 31.52 seconds.\n",
      "-- Epoch 4379\n",
      "Norm: 5531950570924.47, NNZs: 843, Bias: -236509425.487861, T: 14744093, Avg. loss: 79316464029937685518949149573120.000000\n",
      "Total training time: 31.53 seconds.\n",
      "-- Epoch 4380\n",
      "Norm: 5642465946656.22, NNZs: 843, Bias: -90754609.532679, T: 14747460, Avg. loss: 79307495096161823731030226370560.000000\n",
      "Total training time: 31.53 seconds.\n",
      "-- Epoch 4381\n",
      "Norm: 5582643430527.85, NNZs: 843, Bias: 70607212.172788, T: 14750827, Avg. loss: 79298541888901132978211528900608.000000\n",
      "Total training time: 31.54 seconds.\n",
      "-- Epoch 4382\n",
      "Norm: 5546272196908.11, NNZs: 843, Bias: 12667984.948211, T: 14754194, Avg. loss: 79289458589163807844373750611968.000000\n",
      "Total training time: 31.55 seconds.\n",
      "-- Epoch 4383\n",
      "Norm: 5734994363140.63, NNZs: 843, Bias: 174005594.289050, T: 14757561, Avg. loss: 79280269419233293657598179934208.000000\n",
      "Total training time: 31.56 seconds.\n",
      "-- Epoch 4384\n",
      "Norm: 5818465092366.51, NNZs: 843, Bias: -114648294.296015, T: 14760928, Avg. loss: 79271429865183580583937951399936.000000\n",
      "Total training time: 31.56 seconds.\n",
      "-- Epoch 4385\n",
      "Norm: 5627440250570.17, NNZs: 843, Bias: -63148891.146289, T: 14764295, Avg. loss: 79262426346854607979574254895104.000000\n",
      "Total training time: 31.57 seconds.\n",
      "-- Epoch 4386\n",
      "Norm: 5979796062008.05, NNZs: 843, Bias: -547091302.384183, T: 14767662, Avg. loss: 79253412187879253547039860981760.000000\n",
      "Total training time: 31.58 seconds.\n",
      "-- Epoch 4387\n",
      "Norm: 5794266541693.15, NNZs: 843, Bias: -272897226.471291, T: 14771029, Avg. loss: 79244444581974465679375642656768.000000\n",
      "Total training time: 31.58 seconds.\n",
      "-- Epoch 4388\n",
      "Norm: 6096663406766.43, NNZs: 843, Bias: 108140310.747137, T: 14774396, Avg. loss: 79235395131309395814341488410624.000000\n",
      "Total training time: 31.59 seconds.\n",
      "-- Epoch 4389\n",
      "Norm: 5861345072845.27, NNZs: 843, Bias: -375729468.088181, T: 14777763, Avg. loss: 79226145412528120136341492072448.000000\n",
      "Total training time: 31.60 seconds.\n",
      "-- Epoch 4390\n",
      "Norm: 5624770794463.00, NNZs: 843, Bias: -21127275.192712, T: 14781130, Avg. loss: 79217278749479437993713378263040.000000\n",
      "Total training time: 31.61 seconds.\n",
      "-- Epoch 4391\n",
      "Norm: 5575987736088.79, NNZs: 843, Bias: -34951319.576810, T: 14784497, Avg. loss: 79208351714715295680420290494464.000000\n",
      "Total training time: 31.61 seconds.\n",
      "-- Epoch 4392\n",
      "Norm: 5598066681208.43, NNZs: 843, Bias: 129398013.037049, T: 14787864, Avg. loss: 79199439112766916136996429627392.000000\n",
      "Total training time: 31.62 seconds.\n",
      "-- Epoch 4393\n",
      "Norm: 5696295086057.04, NNZs: 843, Bias: -319756309.482243, T: 14791231, Avg. loss: 79190643695248974726322800558080.000000\n",
      "Total training time: 31.63 seconds.\n",
      "-- Epoch 4394\n",
      "Norm: 5973995651208.61, NNZs: 843, Bias: -40791171.969264, T: 14794598, Avg. loss: 79181769607104985464435101401088.000000\n",
      "Total training time: 31.63 seconds.\n",
      "-- Epoch 4395\n",
      "Norm: 5722937634280.17, NNZs: 843, Bias: -202024217.694247, T: 14797965, Avg. loss: 79172983697084678201479949975552.000000\n",
      "Total training time: 31.64 seconds.\n",
      "-- Epoch 4396\n",
      "Norm: 5717385416943.37, NNZs: 843, Bias: -31562318.537558, T: 14801332, Avg. loss: 79164205690075662712438507175936.000000\n",
      "Total training time: 31.65 seconds.\n",
      "-- Epoch 4397\n",
      "Norm: 5798625585827.51, NNZs: 843, Bias: -192778272.930822, T: 14804699, Avg. loss: 79155295946836515101552217686016.000000\n",
      "Total training time: 31.66 seconds.\n",
      "-- Epoch 4398\n",
      "Norm: 5596534660222.60, NNZs: 843, Bias: -31568615.256245, T: 14808066, Avg. loss: 79146235550515941376915281018880.000000\n",
      "Total training time: 31.66 seconds.\n",
      "-- Epoch 4399\n",
      "Norm: 5867415300608.00, NNZs: 843, Bias: -427723119.168085, T: 14811433, Avg. loss: 79137235145322386916972116836352.000000\n",
      "Total training time: 31.67 seconds.\n",
      "-- Epoch 4400\n",
      "Norm: 5467105140033.53, NNZs: 843, Bias: -134008269.812119, T: 14814800, Avg. loss: 79128379665385714356307405832192.000000\n",
      "Total training time: 31.68 seconds.\n",
      "-- Epoch 4401\n",
      "Norm: 5522757934384.87, NNZs: 843, Bias: -69308482.487387, T: 14818167, Avg. loss: 79119493589106048937213139353600.000000\n",
      "Total training time: 31.69 seconds.\n",
      "-- Epoch 4402\n",
      "Norm: 5680578970401.58, NNZs: 843, Bias: -230475035.080779, T: 14821534, Avg. loss: 79110494746430602571542018654208.000000\n",
      "Total training time: 31.69 seconds.\n",
      "-- Epoch 4403\n",
      "Norm: 5460562255091.34, NNZs: 843, Bias: -69306968.247964, T: 14824901, Avg. loss: 79101451456648984307925396750336.000000\n",
      "Total training time: 31.70 seconds.\n",
      "-- Epoch 4404\n",
      "Norm: 5552906323924.05, NNZs: 843, Bias: 91842805.479305, T: 14828268, Avg. loss: 79092344580276614303218141233152.000000\n",
      "Total training time: 31.71 seconds.\n",
      "-- Epoch 4405\n",
      "Norm: 5523900778466.06, NNZs: 843, Bias: -69306030.802299, T: 14831635, Avg. loss: 79083346858929290654442135224320.000000\n",
      "Total training time: 31.71 seconds.\n",
      "-- Epoch 4406\n",
      "Norm: 5517960348997.84, NNZs: 843, Bias: 91825164.011121, T: 14835002, Avg. loss: 79074403726614058519298396127232.000000\n",
      "Total training time: 31.72 seconds.\n",
      "-- Epoch 4407\n",
      "Norm: 5525885065153.37, NNZs: 843, Bias: -200033259.419184, T: 14838369, Avg. loss: 79065289682537656537081958105088.000000\n",
      "Total training time: 31.73 seconds.\n",
      "-- Epoch 4408\n",
      "Norm: 5570505608367.04, NNZs: 843, Bias: -177372383.378585, T: 14841736, Avg. loss: 79056309457386438673749086568448.000000\n",
      "Total training time: 31.74 seconds.\n",
      "-- Epoch 4409\n",
      "Norm: 5812020211999.39, NNZs: 843, Bias: 122060482.795815, T: 14845103, Avg. loss: 79047391204908281910055334313984.000000\n",
      "Total training time: 31.74 seconds.\n",
      "-- Epoch 4410\n",
      "Norm: 5596440470723.63, NNZs: 843, Bias: -81356952.582880, T: 14848470, Avg. loss: 79038479133932469972164239425536.000000\n",
      "Total training time: 31.75 seconds.\n",
      "-- Epoch 4411\n",
      "Norm: 5980071732305.72, NNZs: 843, Bias: 79729916.315988, T: 14851837, Avg. loss: 79029557349674514889241287196672.000000\n",
      "Total training time: 31.76 seconds.\n",
      "-- Epoch 4412\n",
      "Norm: 5728304891840.95, NNZs: 843, Bias: -339128769.051184, T: 14855204, Avg. loss: 79020670407182891449645791182848.000000\n",
      "Total training time: 31.76 seconds.\n",
      "-- Epoch 4413\n",
      "Norm: 5854354429169.76, NNZs: 843, Bias: 144086758.678276, T: 14858571, Avg. loss: 79011875738529602959232951386112.000000\n",
      "Total training time: 31.77 seconds.\n",
      "-- Epoch 4414\n",
      "Norm: 5609404069276.33, NNZs: 843, Bias: -47157062.190949, T: 14861938, Avg. loss: 79002994044318974202920847802368.000000\n",
      "Total training time: 31.78 seconds.\n",
      "-- Epoch 4415\n",
      "Norm: 5577104252608.04, NNZs: 843, Bias: 113890949.657533, T: 14865305, Avg. loss: 78994178657981004505424773251072.000000\n",
      "Total training time: 31.79 seconds.\n",
      "-- Epoch 4416\n",
      "Norm: 5696611234180.64, NNZs: 843, Bias: -80055590.804280, T: 14868672, Avg. loss: 78985112941104408269567060606976.000000\n",
      "Total training time: 31.79 seconds.\n",
      "-- Epoch 4417\n",
      "Norm: 5608914167563.50, NNZs: 843, Bias: -178097472.469581, T: 14872039, Avg. loss: 78975966295793464047191675895808.000000\n",
      "Total training time: 31.80 seconds.\n",
      "-- Epoch 4418\n",
      "Norm: 5465032747196.74, NNZs: 843, Bias: -17070235.376423, T: 14875406, Avg. loss: 78966913202290963959077987680256.000000\n",
      "Total training time: 31.81 seconds.\n",
      "-- Epoch 4419\n",
      "Norm: 5572756006250.48, NNZs: 843, Bias: -178084231.419528, T: 14878773, Avg. loss: 78958011325875054813118363336704.000000\n",
      "Total training time: 31.81 seconds.\n",
      "-- Epoch 4420\n",
      "Norm: 5747421354383.81, NNZs: 843, Bias: 304930588.542626, T: 14882140, Avg. loss: 78949073252179005878746663092224.000000\n",
      "Total training time: 31.82 seconds.\n",
      "-- Epoch 4421\n",
      "Norm: 5645232669390.60, NNZs: 843, Bias: -178071473.754302, T: 14885507, Avg. loss: 78940124072055701787086528970752.000000\n",
      "Total training time: 31.83 seconds.\n",
      "-- Epoch 4422\n",
      "Norm: 5566264016085.59, NNZs: 843, Bias: -17080220.699632, T: 14888874, Avg. loss: 78931090191976493115601546379264.000000\n",
      "Total training time: 31.84 seconds.\n",
      "-- Epoch 4423\n",
      "Norm: 5515560290443.41, NNZs: 843, Bias: 58781231.801171, T: 14892241, Avg. loss: 78922267178780034459206405849088.000000\n",
      "Total training time: 31.84 seconds.\n",
      "-- Epoch 4424\n",
      "Norm: 5541546225191.47, NNZs: 843, Bias: -200727971.733107, T: 14895608, Avg. loss: 78913319425594548140581836554240.000000\n",
      "Total training time: 31.85 seconds.\n",
      "-- Epoch 4425\n",
      "Norm: 5603872905035.67, NNZs: 843, Bias: -39763217.156005, T: 14898975, Avg. loss: 78904357031655303820954910588928.000000\n",
      "Total training time: 31.86 seconds.\n",
      "-- Epoch 4426\n",
      "Norm: 5665672374619.94, NNZs: 843, Bias: -200712660.813805, T: 14902342, Avg. loss: 78895515540888406624793016664064.000000\n",
      "Total training time: 31.87 seconds.\n",
      "-- Epoch 4427\n",
      "Norm: 5715651056064.08, NNZs: 843, Bias: -361645169.138320, T: 14905709, Avg. loss: 78886560608282494674878022549504.000000\n",
      "Total training time: 31.87 seconds.\n",
      "-- Epoch 4428\n",
      "Norm: 5865918670136.65, NNZs: 843, Bias: -200698972.797617, T: 14909076, Avg. loss: 78877735872921464043818488692736.000000\n",
      "Total training time: 31.88 seconds.\n",
      "-- Epoch 4429\n",
      "Norm: 5923620766293.39, NNZs: 843, Bias: -313626436.834299, T: 14912443, Avg. loss: 78868784004911533742495010652160.000000\n",
      "Total training time: 31.89 seconds.\n",
      "-- Epoch 4430\n",
      "Norm: 5576329152585.11, NNZs: 843, Bias: -152699764.891550, T: 14915810, Avg. loss: 78859883532211666718949283725312.000000\n",
      "Total training time: 31.90 seconds.\n",
      "-- Epoch 4431\n",
      "Norm: 5619150637001.19, NNZs: 843, Bias: 8207770.293154, T: 14919177, Avg. loss: 78850894114594184299685538168832.000000\n",
      "Total training time: 31.90 seconds.\n",
      "-- Epoch 4432\n",
      "Norm: 5688039007538.96, NNZs: 843, Bias: -152690718.971261, T: 14922544, Avg. loss: 78842022846830209276109102514176.000000\n",
      "Total training time: 31.91 seconds.\n",
      "-- Epoch 4433\n",
      "Norm: 5732932484153.07, NNZs: 843, Bias: 8198532.777998, T: 14925911, Avg. loss: 78833106696929857353147723284480.000000\n",
      "Total training time: 31.92 seconds.\n",
      "-- Epoch 4434\n",
      "Norm: 5715623154447.20, NNZs: 843, Bias: 128478470.427478, T: 14929278, Avg. loss: 78824178066278834024927243272192.000000\n",
      "Total training time: 31.92 seconds.\n",
      "-- Epoch 4435\n",
      "Norm: 5546875299871.10, NNZs: 843, Bias: -32399226.223312, T: 14932645, Avg. loss: 78815171486235562036740571004928.000000\n",
      "Total training time: 31.93 seconds.\n",
      "-- Epoch 4436\n",
      "Norm: 5538874882924.43, NNZs: 843, Bias: -193259219.815552, T: 14936012, Avg. loss: 78806391550052936837039317319680.000000\n",
      "Total training time: 31.94 seconds.\n",
      "-- Epoch 4437\n",
      "Norm: 5691680663931.87, NNZs: 843, Bias: -62598384.486003, T: 14939379, Avg. loss: 78797637512715914941244398108672.000000\n",
      "Total training time: 31.95 seconds.\n",
      "-- Epoch 4438\n",
      "Norm: 5636549885071.45, NNZs: 843, Bias: 98241350.317144, T: 14942746, Avg. loss: 78788719866777322809042042617856.000000\n",
      "Total training time: 31.95 seconds.\n",
      "-- Epoch 4439\n",
      "Norm: 5947826020382.47, NNZs: 843, Bias: -62596875.027774, T: 14946113, Avg. loss: 78779810490207061512636065644544.000000\n",
      "Total training time: 31.96 seconds.\n",
      "-- Epoch 4440\n",
      "Norm: 5967801754653.19, NNZs: 843, Bias: 98224351.279665, T: 14949480, Avg. loss: 78770928670765396364386373730304.000000\n",
      "Total training time: 31.97 seconds.\n",
      "-- Epoch 4441\n",
      "Norm: 5814026986699.81, NNZs: 843, Bias: -384221453.635283, T: 14952847, Avg. loss: 78761951553984175750166727360512.000000\n",
      "Total training time: 31.98 seconds.\n",
      "-- Epoch 4442\n",
      "Norm: 5727624440957.82, NNZs: 843, Bias: 98206732.455602, T: 14956214, Avg. loss: 78753318947907341885252963401728.000000\n",
      "Total training time: 31.98 seconds.\n",
      "-- Epoch 4443\n",
      "Norm: 5976409611335.26, NNZs: 843, Bias: -158355003.783274, T: 14959581, Avg. loss: 78744406987675648266752432799744.000000\n",
      "Total training time: 31.99 seconds.\n",
      "-- Epoch 4444\n",
      "Norm: 5747065082534.72, NNZs: 843, Bias: -112325287.519779, T: 14962948, Avg. loss: 78735406719596346189925157175296.000000\n",
      "Total training time: 32.00 seconds.\n",
      "-- Epoch 4445\n",
      "Norm: 5998145352647.09, NNZs: 843, Bias: 48454599.835625, T: 14966315, Avg. loss: 78726478311214591762262906634240.000000\n",
      "Total training time: 32.00 seconds.\n",
      "-- Epoch 4446\n",
      "Norm: 5958451954761.50, NNZs: 843, Bias: -112317174.409839, T: 14969682, Avg. loss: 78717630953038376335324806643712.000000\n",
      "Total training time: 32.01 seconds.\n",
      "-- Epoch 4447\n",
      "Norm: 5957559028521.52, NNZs: 843, Bias: 48444045.642652, T: 14973049, Avg. loss: 78708831155764528095279794094080.000000\n",
      "Total training time: 32.02 seconds.\n",
      "-- Epoch 4448\n",
      "Norm: 5841395773646.72, NNZs: 843, Bias: -112310547.302713, T: 14976416, Avg. loss: 78699900771834328578828816875520.000000\n",
      "Total training time: 32.03 seconds.\n",
      "-- Epoch 4449\n",
      "Norm: 6102312420158.92, NNZs: 843, Bias: 48432134.342291, T: 14979783, Avg. loss: 78690929319936460679658900291584.000000\n",
      "Total training time: 32.03 seconds.\n",
      "-- Epoch 4450\n",
      "Norm: 6071799789707.72, NNZs: 843, Bias: -271754869.465027, T: 14983150, Avg. loss: 78682332834192236147424429604864.000000\n",
      "Total training time: 32.04 seconds.\n",
      "-- Epoch 4451\n",
      "Norm: 5920314538697.25, NNZs: 843, Bias: 96820304.814584, T: 14986517, Avg. loss: 78673424627074067990918400573440.000000\n",
      "Total training time: 32.05 seconds.\n",
      "-- Epoch 4452\n",
      "Norm: 5998352573919.63, NNZs: 843, Bias: 319642104.843085, T: 14989884, Avg. loss: 78664556799184537374054244417536.000000\n",
      "Total training time: 32.05 seconds.\n",
      "-- Epoch 4453\n",
      "Norm: 5904015897660.65, NNZs: 843, Bias: -193094484.164094, T: 14993251, Avg. loss: 78655778269356883534420903985152.000000\n",
      "Total training time: 32.06 seconds.\n",
      "-- Epoch 4454\n",
      "Norm: 6035733537970.40, NNZs: 843, Bias: -92256741.421585, T: 14996618, Avg. loss: 78646874835479706114184433369088.000000\n",
      "Total training time: 32.07 seconds.\n",
      "-- Epoch 4455\n",
      "Norm: 5970806497893.75, NNZs: 843, Bias: -259256352.201872, T: 14999985, Avg. loss: 78638266573495897274782142431232.000000\n",
      "Total training time: 32.08 seconds.\n",
      "-- Epoch 4456\n",
      "Norm: 5891697966922.02, NNZs: 843, Bias: -98568395.021538, T: 15003352, Avg. loss: 78629514575365908366240769900544.000000\n",
      "Total training time: 32.08 seconds.\n",
      "-- Epoch 4457\n",
      "Norm: 5735097257119.39, NNZs: 843, Bias: -259231943.098508, T: 15006719, Avg. loss: 78620692850351597150776835702784.000000\n",
      "Total training time: 32.09 seconds.\n",
      "-- Epoch 4458\n",
      "Norm: 5898260121897.03, NNZs: 843, Bias: -276199166.283669, T: 15010086, Avg. loss: 78611958460831649995307830214656.000000\n",
      "Total training time: 32.10 seconds.\n",
      "-- Epoch 4459\n",
      "Norm: 5828502745956.81, NNZs: 843, Bias: -28926099.898053, T: 15013453, Avg. loss: 78603301557287233787876167647232.000000\n",
      "Total training time: 32.10 seconds.\n",
      "-- Epoch 4460\n",
      "Norm: 5687960046748.69, NNZs: 843, Bias: -189568061.596234, T: 15016820, Avg. loss: 78594553041205693032554358112256.000000\n",
      "Total training time: 32.11 seconds.\n",
      "-- Epoch 4461\n",
      "Norm: 5728266898700.83, NNZs: 843, Bias: -92461028.156965, T: 15020187, Avg. loss: 78585904725209813047370835820544.000000\n",
      "Total training time: 32.12 seconds.\n",
      "-- Epoch 4462\n",
      "Norm: 5713332774236.29, NNZs: 843, Bias: 68164088.968460, T: 15023554, Avg. loss: 78577012659740811328024238620672.000000\n",
      "Total training time: 32.13 seconds.\n",
      "-- Epoch 4463\n",
      "Norm: 5650070812851.61, NNZs: 843, Bias: -92456064.716193, T: 15026921, Avg. loss: 78568043992006639766917164826624.000000\n",
      "Total training time: 32.13 seconds.\n",
      "-- Epoch 4464\n",
      "Norm: 5548353706835.84, NNZs: 843, Bias: 68150624.287484, T: 15030288, Avg. loss: 78559289665429426596562979520512.000000\n",
      "Total training time: 32.14 seconds.\n",
      "-- Epoch 4465\n",
      "Norm: 5675591509477.75, NNZs: 843, Bias: 75474651.702697, T: 15033655, Avg. loss: 78550477234394956488958933991424.000000\n",
      "Total training time: 32.15 seconds.\n",
      "-- Epoch 4466\n",
      "Norm: 5496510763477.43, NNZs: 843, Bias: -85118591.688226, T: 15037022, Avg. loss: 78541820062750678003409341120512.000000\n",
      "Total training time: 32.16 seconds.\n",
      "-- Epoch 4467\n",
      "Norm: 5478652069643.78, NNZs: 843, Bias: -245694463.022088, T: 15040389, Avg. loss: 78533040379475228904474508525568.000000\n",
      "Total training time: 32.16 seconds.\n",
      "-- Epoch 4468\n",
      "Norm: 5371954073733.42, NNZs: 843, Bias: -85114505.385452, T: 15043756, Avg. loss: 78524277864875529829338555875328.000000\n",
      "Total training time: 32.17 seconds.\n",
      "-- Epoch 4469\n",
      "Norm: 5454083860474.13, NNZs: 843, Bias: 75447058.693859, T: 15047123, Avg. loss: 78515464750350289697214396628992.000000\n",
      "Total training time: 32.18 seconds.\n",
      "-- Epoch 4470\n",
      "Norm: 5721326561111.90, NNZs: 843, Bias: -59217933.900354, T: 15050490, Avg. loss: 78506654840320841031119586459648.000000\n",
      "Total training time: 32.18 seconds.\n",
      "-- Epoch 4471\n",
      "Norm: 5535163521865.21, NNZs: 843, Bias: -20000907.123399, T: 15053857, Avg. loss: 78497875118249718410153174237184.000000\n",
      "Total training time: 32.19 seconds.\n",
      "-- Epoch 4472\n",
      "Norm: 5568406443765.33, NNZs: 843, Bias: -180535737.913370, T: 15057224, Avg. loss: 78489270277798750940654202781696.000000\n",
      "Total training time: 32.20 seconds.\n",
      "-- Epoch 4473\n",
      "Norm: 5445610234441.94, NNZs: 843, Bias: 78079392.255783, T: 15060591, Avg. loss: 78480535972730556199215871033344.000000\n",
      "Total training time: 32.21 seconds.\n",
      "-- Epoch 4474\n",
      "Norm: 5457987560058.54, NNZs: 843, Bias: -82441484.868409, T: 15063958, Avg. loss: 78471736085623793501579528634368.000000\n",
      "Total training time: 32.21 seconds.\n",
      "-- Epoch 4475\n",
      "Norm: 5385937777165.88, NNZs: 843, Bias: 78066400.562976, T: 15067325, Avg. loss: 78462752117710312710984853094400.000000\n",
      "Total training time: 32.22 seconds.\n",
      "-- Epoch 4476\n",
      "Norm: 5523709935001.38, NNZs: 843, Bias: -403431369.281893, T: 15070692, Avg. loss: 78454013207124463778985871933440.000000\n",
      "Total training time: 32.23 seconds.\n",
      "-- Epoch 4477\n",
      "Norm: 5514707360203.32, NNZs: 843, Bias: -54527193.425991, T: 15074059, Avg. loss: 78445279819795738856243326877696.000000\n",
      "Total training time: 32.23 seconds.\n",
      "-- Epoch 4478\n",
      "Norm: 5325115299230.93, NNZs: 843, Bias: -173766619.358381, T: 15077426, Avg. loss: 78436771688705556127153941118976.000000\n",
      "Total training time: 32.24 seconds.\n",
      "-- Epoch 4479\n",
      "Norm: 5751210058553.91, NNZs: 843, Bias: 100113943.417234, T: 15080793, Avg. loss: 78428065393657699866241111425024.000000\n",
      "Total training time: 32.25 seconds.\n",
      "-- Epoch 4480\n",
      "Norm: 5386991784591.95, NNZs: 843, Bias: -23685339.656736, T: 15084160, Avg. loss: 78419253010761161340486747684864.000000\n",
      "Total training time: 32.26 seconds.\n",
      "-- Epoch 4481\n",
      "Norm: 5394125937528.10, NNZs: 843, Bias: 46252008.140173, T: 15087527, Avg. loss: 78410539814245257624000602832896.000000\n",
      "Total training time: 32.26 seconds.\n",
      "-- Epoch 4482\n",
      "Norm: 5438103823303.98, NNZs: 843, Bias: 2031530.857928, T: 15090894, Avg. loss: 78401755071846222166028235833344.000000\n",
      "Total training time: 32.27 seconds.\n",
      "-- Epoch 4483\n",
      "Norm: 5637910560945.27, NNZs: 843, Bias: 385307854.695985, T: 15094261, Avg. loss: 78393324421146719537468824092672.000000\n",
      "Total training time: 32.28 seconds.\n",
      "-- Epoch 4484\n",
      "Norm: 5340125842502.18, NNZs: 843, Bias: -1186470.898991, T: 15097628, Avg. loss: 78384625683452992941471534415872.000000\n",
      "Total training time: 32.28 seconds.\n",
      "-- Epoch 4485\n",
      "Norm: 5337894858187.03, NNZs: 843, Bias: -161605981.719895, T: 15100995, Avg. loss: 78375912375450453427830996860928.000000\n",
      "Total training time: 32.29 seconds.\n",
      "-- Epoch 4486\n",
      "Norm: 5512691868687.91, NNZs: 843, Bias: 319622099.167040, T: 15104362, Avg. loss: 78367249831676739945463910760448.000000\n",
      "Total training time: 32.30 seconds.\n",
      "-- Epoch 4487\n",
      "Norm: 5479645887419.36, NNZs: 843, Bias: 159202566.144342, T: 15107729, Avg. loss: 78358656598903891339570946506752.000000\n",
      "Total training time: 32.31 seconds.\n",
      "-- Epoch 4488\n",
      "Norm: 5394520086573.44, NNZs: 843, Bias: -1198827.149227, T: 15111096, Avg. loss: 78350059931862823258304572030976.000000\n",
      "Total training time: 32.31 seconds.\n",
      "-- Epoch 4489\n",
      "Norm: 5414953983507.39, NNZs: 843, Bias: -191481767.324232, T: 15114463, Avg. loss: 78341405933873866134536266448896.000000\n",
      "Total training time: 32.32 seconds.\n",
      "-- Epoch 4490\n",
      "Norm: 5339463925238.12, NNZs: 843, Bias: -31103374.119707, T: 15117830, Avg. loss: 78332733396140332060112486137856.000000\n",
      "Total training time: 32.33 seconds.\n",
      "-- Epoch 4491\n",
      "Norm: 5505246654297.01, NNZs: 843, Bias: 129257338.649298, T: 15121197, Avg. loss: 78323815143878897517687056891904.000000\n",
      "Total training time: 32.34 seconds.\n",
      "-- Epoch 4492\n",
      "Norm: 5497795309722.65, NNZs: 843, Bias: -351814486.504708, T: 15124564, Avg. loss: 78315266209104612402958432206848.000000\n",
      "Total training time: 32.34 seconds.\n",
      "-- Epoch 4493\n",
      "Norm: 5486759922176.49, NNZs: 843, Bias: -191453669.089080, T: 15127931, Avg. loss: 78306761722071434676040510734336.000000\n",
      "Total training time: 32.35 seconds.\n",
      "-- Epoch 4494\n",
      "Norm: 5627856353323.77, NNZs: 843, Bias: -351782813.219318, T: 15131298, Avg. loss: 78298055908062863454558771740672.000000\n",
      "Total training time: 32.36 seconds.\n",
      "-- Epoch 4495\n",
      "Norm: 5414481438966.08, NNZs: 843, Bias: -191438964.277565, T: 15134665, Avg. loss: 78289347708835645831752749940736.000000\n",
      "Total training time: 32.36 seconds.\n",
      "-- Epoch 4496\n",
      "Norm: 5441942753816.97, NNZs: 843, Bias: -31113022.866445, T: 15138032, Avg. loss: 78280537073108240872576076742656.000000\n",
      "Total training time: 32.37 seconds.\n",
      "-- Epoch 4497\n",
      "Norm: 5703466625106.33, NNZs: 843, Bias: 449814330.498638, T: 15141399, Avg. loss: 78271663922081575718853311201280.000000\n",
      "Total training time: 32.38 seconds.\n",
      "-- Epoch 4498\n",
      "Norm: 5462586861721.51, NNZs: 843, Bias: -31114422.904821, T: 15144766, Avg. loss: 78262927278511618345918401609728.000000\n",
      "Total training time: 32.39 seconds.\n",
      "-- Epoch 4499\n",
      "Norm: 5529060999523.97, NNZs: 843, Bias: -128562479.488070, T: 15148133, Avg. loss: 78254313110459691153583369093120.000000\n",
      "Total training time: 32.39 seconds.\n",
      "-- Epoch 4500\n",
      "Norm: 5547431535198.83, NNZs: 843, Bias: 31724402.957915, T: 15151500, Avg. loss: 78245554248018819687871072436224.000000\n",
      "Total training time: 32.40 seconds.\n",
      "-- Epoch 4501\n",
      "Norm: 5517625942059.95, NNZs: 843, Bias: -128553664.858957, T: 15154867, Avg. loss: 78236698308218652427682521808896.000000\n",
      "Total training time: 32.41 seconds.\n",
      "-- Epoch 4502\n",
      "Norm: 5554652468249.27, NNZs: 843, Bias: 535474.011240, T: 15158234, Avg. loss: 78227958532196464410207312674816.000000\n",
      "Total training time: 32.41 seconds.\n",
      "-- Epoch 4503\n",
      "Norm: 5500484443227.38, NNZs: 843, Bias: -135093865.407771, T: 15161601, Avg. loss: 78219171304330168055118218395648.000000\n",
      "Total training time: 32.42 seconds.\n",
      "-- Epoch 4504\n",
      "Norm: 5543990222877.59, NNZs: 843, Bias: -332279767.747771, T: 15164968, Avg. loss: 78210533904086547929342981177344.000000\n",
      "Total training time: 32.43 seconds.\n",
      "-- Epoch 4505\n",
      "Norm: 5491509153429.28, NNZs: 843, Bias: -172026485.119907, T: 15168335, Avg. loss: 78201945007700959554759309656064.000000\n",
      "Total training time: 32.44 seconds.\n",
      "-- Epoch 4506\n",
      "Norm: 5442178008746.21, NNZs: 843, Bias: 51924400.114396, T: 15171702, Avg. loss: 78193386052902117882179914563584.000000\n",
      "Total training time: 32.44 seconds.\n",
      "-- Epoch 4507\n",
      "Norm: 5490621356310.80, NNZs: 843, Bias: -42606993.890966, T: 15175069, Avg. loss: 78184615115816618571443807977472.000000\n",
      "Total training time: 32.45 seconds.\n",
      "-- Epoch 4508\n",
      "Norm: 5657195674462.76, NNZs: 843, Bias: 231606563.552182, T: 15178436, Avg. loss: 78175942781119064656147703136256.000000\n",
      "Total training time: 32.46 seconds.\n",
      "-- Epoch 4509\n",
      "Norm: 5679400010127.36, NNZs: 843, Bias: -231705668.643254, T: 15181803, Avg. loss: 78167259772806140530947231055872.000000\n",
      "Total training time: 32.46 seconds.\n",
      "-- Epoch 4510\n",
      "Norm: 5673286704606.00, NNZs: 843, Bias: -7657530.132344, T: 15185170, Avg. loss: 78158606929532660896791919591424.000000\n",
      "Total training time: 32.47 seconds.\n",
      "-- Epoch 4511\n",
      "Norm: 5645667369283.41, NNZs: 843, Bias: 152524402.332314, T: 15188537, Avg. loss: 78149981214355187169908580417536.000000\n",
      "Total training time: 32.48 seconds.\n",
      "-- Epoch 4512\n",
      "Norm: 5611957505430.20, NNZs: 843, Bias: -7662779.519443, T: 15191904, Avg. loss: 78141242292541576136498801016832.000000\n",
      "Total training time: 32.49 seconds.\n",
      "-- Epoch 4513\n",
      "Norm: 5610254649301.80, NNZs: 843, Bias: -167832624.566196, T: 15195271, Avg. loss: 78132571620450227513348444913664.000000\n",
      "Total training time: 32.49 seconds.\n",
      "-- Epoch 4514\n",
      "Norm: 5685828622306.47, NNZs: 843, Bias: 95422340.724873, T: 15198638, Avg. loss: 78123986523603908669075986841600.000000\n",
      "Total training time: 32.50 seconds.\n",
      "-- Epoch 4515\n",
      "Norm: 5901005491238.02, NNZs: 843, Bias: -64735499.517118, T: 15202005, Avg. loss: 78115242729804048002416004562944.000000\n",
      "Total training time: 32.51 seconds.\n",
      "-- Epoch 4516\n",
      "Norm: 5864328100340.69, NNZs: 843, Bias: 414836834.578106, T: 15205372, Avg. loss: 78106548039617948549336023433216.000000\n",
      "Total training time: 32.52 seconds.\n",
      "-- Epoch 4517\n",
      "Norm: 5587938276461.92, NNZs: 843, Bias: -65582917.476213, T: 15208739, Avg. loss: 78097889518564329084015932866560.000000\n",
      "Total training time: 32.52 seconds.\n",
      "-- Epoch 4518\n",
      "Norm: 5762896253803.50, NNZs: 843, Bias: -225705110.391566, T: 15212106, Avg. loss: 78089303417020471763990550151168.000000\n",
      "Total training time: 32.53 seconds.\n",
      "-- Epoch 4519\n",
      "Norm: 5711091660485.43, NNZs: 843, Bias: -65580820.316751, T: 15215473, Avg. loss: 78080824394676904077606917242880.000000\n",
      "Total training time: 32.54 seconds.\n",
      "-- Epoch 4520\n",
      "Norm: 6014761796421.27, NNZs: 843, Bias: -225684250.894195, T: 15218840, Avg. loss: 78072152665184786454588482912256.000000\n",
      "Total training time: 32.54 seconds.\n",
      "-- Epoch 4521\n",
      "Norm: 5743352784993.21, NNZs: 843, Bias: -385770896.669483, T: 15222207, Avg. loss: 78063474917017833425358852980736.000000\n",
      "Total training time: 32.55 seconds.\n",
      "-- Epoch 4522\n",
      "Norm: 5571160922824.24, NNZs: 843, Bias: 92245422.253327, T: 15225574, Avg. loss: 78054750353056731694705703649280.000000\n",
      "Total training time: 32.56 seconds.\n",
      "-- Epoch 4523\n",
      "Norm: 5679755434650.46, NNZs: 843, Bias: -384162532.477130, T: 15228941, Avg. loss: 78046184120221647954345122070528.000000\n",
      "Total training time: 32.57 seconds.\n",
      "-- Epoch 4524\n",
      "Norm: 5652384175899.80, NNZs: 843, Bias: 96065348.095336, T: 15232308, Avg. loss: 78037564575672956055347076792320.000000\n",
      "Total training time: 32.57 seconds.\n",
      "-- Epoch 4525\n",
      "Norm: 5710962028636.22, NNZs: 843, Bias: -64003215.923203, T: 15235675, Avg. loss: 78028778341005904844983963222016.000000\n",
      "Total training time: 32.58 seconds.\n",
      "-- Epoch 4526\n",
      "Norm: 5736975815273.95, NNZs: 843, Bias: -224054196.372453, T: 15239042, Avg. loss: 78020362371850856316789579579392.000000\n",
      "Total training time: 32.59 seconds.\n",
      "-- Epoch 4527\n",
      "Norm: 5587591325241.60, NNZs: 843, Bias: -64002189.159384, T: 15242409, Avg. loss: 78011920340925516392337999134720.000000\n",
      "Total training time: 32.59 seconds.\n",
      "-- Epoch 4528\n",
      "Norm: 5810602117844.58, NNZs: 843, Bias: 416101451.002686, T: 15245776, Avg. loss: 78003446150312529875573512601600.000000\n",
      "Total training time: 32.60 seconds.\n",
      "-- Epoch 4529\n",
      "Norm: 5555933063345.82, NNZs: 843, Bias: 74241081.674276, T: 15249143, Avg. loss: 77994918076922179596598565142528.000000\n",
      "Total training time: 32.61 seconds.\n",
      "-- Epoch 4530\n",
      "Norm: 5878898794520.18, NNZs: 843, Bias: -201917736.373893, T: 15252510, Avg. loss: 77986251086889443818552541839360.000000\n",
      "Total training time: 32.62 seconds.\n",
      "-- Epoch 4531\n",
      "Norm: 5753946517482.31, NNZs: 843, Bias: -41901927.575547, T: 15255877, Avg. loss: 77977623366139332331093316599808.000000\n",
      "Total training time: 32.62 seconds.\n",
      "-- Epoch 4532\n",
      "Norm: 5659072993171.82, NNZs: 843, Bias: -201901387.754135, T: 15259244, Avg. loss: 77969044500998253374017353285632.000000\n",
      "Total training time: 32.63 seconds.\n",
      "-- Epoch 4533\n",
      "Norm: 5564704612832.13, NNZs: 843, Bias: -41902339.221982, T: 15262611, Avg. loss: 77960374505543322243318782361600.000000\n",
      "Total training time: 32.64 seconds.\n",
      "-- Epoch 4534\n",
      "Norm: 5606782894995.79, NNZs: 843, Bias: 118077785.864414, T: 15265978, Avg. loss: 77951734018464378055443471138816.000000\n",
      "Total training time: 32.64 seconds.\n",
      "-- Epoch 4535\n",
      "Norm: 5570151826104.63, NNZs: 843, Bias: -126280126.101570, T: 15269345, Avg. loss: 77943203093565403129750034055168.000000\n",
      "Total training time: 32.65 seconds.\n",
      "-- Epoch 4536\n",
      "Norm: 5694109104100.72, NNZs: 843, Bias: -286239623.659944, T: 15272712, Avg. loss: 77934698589658579671763820478464.000000\n",
      "Total training time: 32.66 seconds.\n",
      "-- Epoch 4537\n",
      "Norm: 5782136694281.62, NNZs: 843, Bias: -126272425.145636, T: 15276079, Avg. loss: 77925983429370306222532105076736.000000\n",
      "Total training time: 32.67 seconds.\n",
      "-- Epoch 4538\n",
      "Norm: 5792812163542.81, NNZs: 843, Bias: 33677393.162702, T: 15279446, Avg. loss: 77917407386401701177058626895872.000000\n",
      "Total training time: 32.67 seconds.\n",
      "-- Epoch 4539\n",
      "Norm: 5736762183179.49, NNZs: 843, Bias: -25724126.874410, T: 15282813, Avg. loss: 77908846959772836386641392173056.000000\n",
      "Total training time: 32.68 seconds.\n",
      "-- Epoch 4540\n",
      "Norm: 5789178404538.96, NNZs: 843, Bias: -185653797.914207, T: 15286180, Avg. loss: 77900412570435809643137756823552.000000\n",
      "Total training time: 32.69 seconds.\n",
      "-- Epoch 4541\n",
      "Norm: 5854325747344.92, NNZs: 843, Bias: -142267327.632944, T: 15289547, Avg. loss: 77891961839936559097459075186688.000000\n",
      "Total training time: 32.70 seconds.\n",
      "-- Epoch 4542\n",
      "Norm: 5978478684303.67, NNZs: 843, Bias: -260249248.727227, T: 15292914, Avg. loss: 77883364072847083704206038663168.000000\n",
      "Total training time: 32.70 seconds.\n",
      "-- Epoch 4543\n",
      "Norm: 6005395917101.72, NNZs: 843, Bias: -100335590.211734, T: 15296281, Avg. loss: 77874872101765729883590800441344.000000\n",
      "Total training time: 32.71 seconds.\n",
      "-- Epoch 4544\n",
      "Norm: 5776237341215.58, NNZs: 843, Bias: 59560351.084512, T: 15299648, Avg. loss: 77866282463152668539583608127488.000000\n",
      "Total training time: 32.72 seconds.\n",
      "-- Epoch 4545\n",
      "Norm: 5758390657602.45, NNZs: 843, Bias: -100330733.177701, T: 15303015, Avg. loss: 77857793588054008070611715227648.000000\n",
      "Total training time: 32.72 seconds.\n",
      "-- Epoch 4546\n",
      "Norm: 5759174338419.36, NNZs: 843, Bias: 59547857.886580, T: 15306382, Avg. loss: 77849204389436505425208758239232.000000\n",
      "Total training time: 32.73 seconds.\n",
      "-- Epoch 4547\n",
      "Norm: 5884609529595.65, NNZs: 843, Bias: 15810275.547298, T: 15309749, Avg. loss: 77840597128324039868540201730048.000000\n",
      "Total training time: 32.74 seconds.\n",
      "-- Epoch 4548\n",
      "Norm: 5963409414619.91, NNZs: 843, Bias: 51597928.260099, T: 15313116, Avg. loss: 77831991440138906268080644554752.000000\n",
      "Total training time: 32.75 seconds.\n",
      "-- Epoch 4549\n",
      "Norm: 5988619543822.12, NNZs: 843, Bias: 211441926.341274, T: 15316483, Avg. loss: 77823381213837080572279828512768.000000\n",
      "Total training time: 32.75 seconds.\n",
      "-- Epoch 4550\n",
      "Norm: 6001872642186.99, NNZs: 843, Bias: 51586717.158533, T: 15319850, Avg. loss: 77814741557321734460864274104320.000000\n",
      "Total training time: 32.76 seconds.\n",
      "-- Epoch 4551\n",
      "Norm: 6056892816322.01, NNZs: 843, Bias: 211413179.254408, T: 15323217, Avg. loss: 77806346438096841016113553735680.000000\n",
      "Total training time: 32.77 seconds.\n",
      "-- Epoch 4552\n",
      "Norm: 6033502898343.21, NNZs: 843, Bias: 93834629.289400, T: 15326584, Avg. loss: 77797655596534518252074453434368.000000\n",
      "Total training time: 32.77 seconds.\n",
      "-- Epoch 4553\n",
      "Norm: 6178375416924.42, NNZs: 843, Bias: 253641151.297312, T: 15329951, Avg. loss: 77789127295707468860096369393664.000000\n",
      "Total training time: 32.78 seconds.\n",
      "-- Epoch 4554\n",
      "Norm: 6165329814921.52, NNZs: 843, Bias: -225790707.613028, T: 15333318, Avg. loss: 77780549115371472428393815867392.000000\n",
      "Total training time: 32.79 seconds.\n",
      "-- Epoch 4555\n",
      "Norm: 6232876078974.88, NNZs: 843, Bias: -43256768.258663, T: 15336685, Avg. loss: 77772101471125752930552978079744.000000\n",
      "Total training time: 32.80 seconds.\n",
      "-- Epoch 4556\n",
      "Norm: 6327073581301.12, NNZs: 843, Bias: -203045045.007064, T: 15340052, Avg. loss: 77763554154258167784023756111872.000000\n",
      "Total training time: 32.80 seconds.\n",
      "-- Epoch 4557\n",
      "Norm: 6110866728988.87, NNZs: 843, Bias: -362815754.141671, T: 15343419, Avg. loss: 77755096489562923060294213173248.000000\n",
      "Total training time: 32.81 seconds.\n",
      "-- Epoch 4558\n",
      "Norm: 6049980030748.48, NNZs: 843, Bias: -42925498.249667, T: 15346786, Avg. loss: 77746651688102418781349997445120.000000\n",
      "Total training time: 32.82 seconds.\n",
      "-- Epoch 4559\n",
      "Norm: 6085582682695.40, NNZs: 843, Bias: -202688090.371616, T: 15350153, Avg. loss: 77738138855507138225336660000768.000000\n",
      "Total training time: 32.83 seconds.\n",
      "-- Epoch 4560\n",
      "Norm: 6016596618173.74, NNZs: 843, Bias: -313921643.686431, T: 15353520, Avg. loss: 77729577294856187714727518928896.000000\n",
      "Total training time: 32.83 seconds.\n",
      "-- Epoch 4561\n",
      "Norm: 6159843758805.97, NNZs: 843, Bias: 121456725.146393, T: 15356887, Avg. loss: 77721155764935389363954314641408.000000\n",
      "Total training time: 32.84 seconds.\n",
      "-- Epoch 4562\n",
      "Norm: 5988901838447.30, NNZs: 843, Bias: -81843055.224283, T: 15360254, Avg. loss: 77712628329918838114494399381504.000000\n",
      "Total training time: 32.85 seconds.\n",
      "-- Epoch 4563\n",
      "Norm: 6122731522776.48, NNZs: 843, Bias: 77885275.897868, T: 15363621, Avg. loss: 77704201731066946481760422068224.000000\n",
      "Total training time: 32.85 seconds.\n",
      "-- Epoch 4564\n",
      "Norm: 5989925186811.03, NNZs: 843, Bias: 237595937.589236, T: 15366988, Avg. loss: 77695631099245522452761655902208.000000\n",
      "Total training time: 32.86 seconds.\n",
      "-- Epoch 4565\n",
      "Norm: 5965894499764.65, NNZs: 843, Bias: 149608919.113309, T: 15370355, Avg. loss: 77687193620513757964524784189440.000000\n",
      "Total training time: 32.87 seconds.\n",
      "-- Epoch 4566\n",
      "Norm: 6036194187061.78, NNZs: 843, Bias: -363879071.131642, T: 15373722, Avg. loss: 77678530624441399199115411193856.000000\n",
      "Total training time: 32.88 seconds.\n",
      "-- Epoch 4567\n",
      "Norm: 5949085458193.11, NNZs: 843, Bias: -204170599.819541, T: 15377089, Avg. loss: 77670186873292887754267334541312.000000\n",
      "Total training time: 32.88 seconds.\n",
      "-- Epoch 4568\n",
      "Norm: 6201060473751.79, NNZs: 843, Bias: -8214318.399688, T: 15380456, Avg. loss: 77661636440532231332286823399424.000000\n",
      "Total training time: 32.89 seconds.\n",
      "-- Epoch 4569\n",
      "Norm: 5897660992081.32, NNZs: 843, Bias: -167890390.036291, T: 15383823, Avg. loss: 77653167983124969087953696456704.000000\n",
      "Total training time: 32.90 seconds.\n",
      "-- Epoch 4570\n",
      "Norm: 5860545595703.21, NNZs: 843, Bias: -8218586.920301, T: 15387190, Avg. loss: 77644804687723466379449017040896.000000\n",
      "Total training time: 32.90 seconds.\n",
      "-- Epoch 4571\n",
      "Norm: 5856253957772.35, NNZs: 843, Bias: -167876669.773769, T: 15390557, Avg. loss: 77636400688808986845909006417920.000000\n",
      "Total training time: 32.91 seconds.\n",
      "-- Epoch 4572\n",
      "Norm: 5893548580788.79, NNZs: 843, Bias: -8222749.305943, T: 15393924, Avg. loss: 77627810048284556950020156293120.000000\n",
      "Total training time: 32.92 seconds.\n",
      "-- Epoch 4573\n",
      "Norm: 6043126962373.61, NNZs: 843, Bias: 4094928.501728, T: 15397291, Avg. loss: 77619259943797078003395151790080.000000\n",
      "Total training time: 32.93 seconds.\n",
      "-- Epoch 4574\n",
      "Norm: 5981653660395.90, NNZs: 843, Bias: -77566466.952415, T: 15400658, Avg. loss: 77610751527899236824997089509376.000000\n",
      "Total training time: 32.93 seconds.\n",
      "-- Epoch 4575\n",
      "Norm: 5777869959356.84, NNZs: 843, Bias: 82057487.428453, T: 15404025, Avg. loss: 77602269710324160543505506107392.000000\n",
      "Total training time: 32.94 seconds.\n",
      "-- Epoch 4576\n",
      "Norm: 5950000487563.15, NNZs: 843, Bias: 241664446.711996, T: 15407392, Avg. loss: 77593707498984524987919570567168.000000\n",
      "Total training time: 32.95 seconds.\n",
      "-- Epoch 4577\n",
      "Norm: 5860240332705.91, NNZs: 843, Bias: -237163368.137771, T: 15410759, Avg. loss: 77585306384722416162772204126208.000000\n",
      "Total training time: 32.96 seconds.\n",
      "-- Epoch 4578\n",
      "Norm: 5750143357259.91, NNZs: 843, Bias: -102116905.375890, T: 15414126, Avg. loss: 77576895326803406754685879582720.000000\n",
      "Total training time: 32.96 seconds.\n",
      "-- Epoch 4579\n",
      "Norm: 6037723008715.47, NNZs: 843, Bias: -261700359.756652, T: 15417493, Avg. loss: 77568443042539847832842518134784.000000\n",
      "Total training time: 32.97 seconds.\n",
      "-- Epoch 4580\n",
      "Norm: 5703646199468.59, NNZs: 843, Bias: -102110599.040874, T: 15420860, Avg. loss: 77559906738816107481105711497216.000000\n",
      "Total training time: 32.98 seconds.\n",
      "-- Epoch 4581\n",
      "Norm: 5661155276772.56, NNZs: 843, Bias: -162797894.403101, T: 15424227, Avg. loss: 77551465202608988866684959850496.000000\n",
      "Total training time: 32.98 seconds.\n",
      "-- Epoch 4582\n",
      "Norm: 5733490462207.76, NNZs: 843, Bias: -87208839.280226, T: 15427594, Avg. loss: 77543208786126062056805329010688.000000\n",
      "Total training time: 32.99 seconds.\n",
      "-- Epoch 4583\n",
      "Norm: 5669351289969.46, NNZs: 843, Bias: 72344863.284935, T: 15430961, Avg. loss: 77534745609455174361585423482880.000000\n",
      "Total training time: 33.00 seconds.\n",
      "-- Epoch 4584\n",
      "Norm: 5624651620235.71, NNZs: 843, Bias: -8627505.103700, T: 15434328, Avg. loss: 77526150441755907953022124163072.000000\n",
      "Total training time: 33.01 seconds.\n",
      "-- Epoch 4585\n",
      "Norm: 5650801456714.65, NNZs: 843, Bias: 147176541.097416, T: 15437695, Avg. loss: 77517616442667347423715521462272.000000\n",
      "Total training time: 33.01 seconds.\n",
      "-- Epoch 4586\n",
      "Norm: 5854121073755.31, NNZs: 843, Bias: -79708656.938829, T: 15441062, Avg. loss: 77509290164339584359196323217408.000000\n",
      "Total training time: 33.02 seconds.\n",
      "-- Epoch 4587\n",
      "Norm: 5684590963860.69, NNZs: 843, Bias: -204540722.307375, T: 15444429, Avg. loss: 77500797356012623680593213784064.000000\n",
      "Total training time: 33.03 seconds.\n",
      "-- Epoch 4588\n",
      "Norm: 5649041680229.43, NNZs: 843, Bias: -45023781.991493, T: 15447796, Avg. loss: 77492383566624852828045583581184.000000\n",
      "Total training time: 33.03 seconds.\n",
      "-- Epoch 4589\n",
      "Norm: 5638559254300.51, NNZs: 843, Bias: 114474931.195306, T: 15451163, Avg. loss: 77484038878725430140715436867584.000000\n",
      "Total training time: 33.04 seconds.\n",
      "-- Epoch 4590\n",
      "Norm: 5663330479522.85, NNZs: 843, Bias: -23938305.884361, T: 15454530, Avg. loss: 77475479616488044653715216400384.000000\n",
      "Total training time: 33.05 seconds.\n",
      "-- Epoch 4591\n",
      "Norm: 5767668642289.24, NNZs: 843, Bias: 144089073.479433, T: 15457897, Avg. loss: 77466963867143644169006173126656.000000\n",
      "Total training time: 33.06 seconds.\n",
      "-- Epoch 4592\n",
      "Norm: 5775364221446.72, NNZs: 843, Bias: -15395307.208709, T: 15461264, Avg. loss: 77458526262155562804863800705024.000000\n",
      "Total training time: 33.06 seconds.\n",
      "-- Epoch 4593\n",
      "Norm: 5856122501256.20, NNZs: 843, Bias: -174863913.899970, T: 15464631, Avg. loss: 77450054982917388651867221786624.000000\n",
      "Total training time: 33.07 seconds.\n",
      "-- Epoch 4594\n",
      "Norm: 5812559751541.77, NNZs: 843, Bias: -334314793.770684, T: 15467998, Avg. loss: 77441704675541246765325642366976.000000\n",
      "Total training time: 33.08 seconds.\n",
      "-- Epoch 4595\n",
      "Norm: 5824836305767.33, NNZs: 843, Bias: 49708938.433425, T: 15471365, Avg. loss: 77433358615278671625431875059712.000000\n",
      "Total training time: 33.09 seconds.\n",
      "-- Epoch 4596\n",
      "Norm: 5936173741555.22, NNZs: 843, Bias: -428613309.471846, T: 15474732, Avg. loss: 77424910942337123187494569902080.000000\n",
      "Total training time: 33.09 seconds.\n",
      "-- Epoch 4597\n",
      "Norm: 5763610540276.69, NNZs: 843, Bias: 49697684.796262, T: 15478099, Avg. loss: 77416474120616872441081300516864.000000\n",
      "Total training time: 33.10 seconds.\n",
      "-- Epoch 4598\n",
      "Norm: 5772774907579.38, NNZs: 843, Bias: -106159407.219187, T: 15481466, Avg. loss: 77408141100525207905408498270208.000000\n",
      "Total training time: 33.11 seconds.\n",
      "-- Epoch 4599\n",
      "Norm: 5931854206813.91, NNZs: 843, Bias: -265569382.130992, T: 15484833, Avg. loss: 77399746396994377512376778883072.000000\n",
      "Total training time: 33.11 seconds.\n",
      "-- Epoch 4600\n",
      "Norm: 5897454717665.13, NNZs: 843, Bias: 316898159.753991, T: 15488200, Avg. loss: 77391352056443192705474518581248.000000\n",
      "Total training time: 33.12 seconds.\n",
      "-- Epoch 4601\n",
      "Norm: 5727065570458.39, NNZs: 843, Bias: -161309010.038261, T: 15491567, Avg. loss: 77382930276513266599749297897472.000000\n",
      "Total training time: 33.13 seconds.\n",
      "-- Epoch 4602\n",
      "Norm: 5841439924416.45, NNZs: 843, Bias: -1916351.670920, T: 15494934, Avg. loss: 77374477367536659383695320285184.000000\n",
      "Total training time: 33.14 seconds.\n",
      "-- Epoch 4603\n",
      "Norm: 5888843357146.13, NNZs: 843, Bias: -161297579.356368, T: 15498301, Avg. loss: 77365894197460811474519266951168.000000\n",
      "Total training time: 33.14 seconds.\n",
      "-- Epoch 4604\n",
      "Norm: 5873569223207.12, NNZs: 843, Bias: -1922287.899802, T: 15501668, Avg. loss: 77357494468865722588368589553664.000000\n",
      "Total training time: 33.15 seconds.\n",
      "-- Epoch 4605\n",
      "Norm: 5982137062141.77, NNZs: 843, Bias: -161286123.913732, T: 15505035, Avg. loss: 77349025567048527333878195552256.000000\n",
      "Total training time: 33.16 seconds.\n",
      "-- Epoch 4606\n",
      "Norm: 5826494454326.48, NNZs: 843, Bias: -201508608.349421, T: 15508402, Avg. loss: 77340739259008801650497843888128.000000\n",
      "Total training time: 33.16 seconds.\n",
      "-- Epoch 4607\n",
      "Norm: 5872962398710.11, NNZs: 843, Bias: -87834263.428504, T: 15511769, Avg. loss: 77332451370867698955691642322944.000000\n",
      "Total training time: 33.17 seconds.\n",
      "-- Epoch 4608\n",
      "Norm: 5917550418005.88, NNZs: 843, Bias: 23985750.173694, T: 15515136, Avg. loss: 77324225855905492798523148599296.000000\n",
      "Total training time: 33.18 seconds.\n",
      "-- Epoch 4609\n",
      "Norm: 6235440954013.57, NNZs: 843, Bias: -309896443.756844, T: 15518503, Avg. loss: 77315972403575341608743164968960.000000\n",
      "Total training time: 33.19 seconds.\n",
      "-- Epoch 4610\n",
      "Norm: 6306062440090.12, NNZs: 843, Bias: -317130415.560583, T: 15521870, Avg. loss: 77307565792272958321147504492544.000000\n",
      "Total training time: 33.19 seconds.\n",
      "-- Epoch 4611\n",
      "Norm: 6018070934222.30, NNZs: 843, Bias: 160810049.741108, T: 15525237, Avg. loss: 77299210173265604326543025242112.000000\n",
      "Total training time: 33.20 seconds.\n",
      "-- Epoch 4612\n",
      "Norm: 5965536341061.17, NNZs: 843, Bias: 1498186.158686, T: 15528604, Avg. loss: 77290785165712416480363926257664.000000\n",
      "Total training time: 33.21 seconds.\n",
      "-- Epoch 4613\n",
      "Norm: 6254100849462.77, NNZs: 843, Bias: -157796911.689741, T: 15531971, Avg. loss: 77282448649958975960243356303360.000000\n",
      "Total training time: 33.22 seconds.\n",
      "-- Epoch 4614\n",
      "Norm: 6175987687263.53, NNZs: 843, Bias: -45439383.381372, T: 15535338, Avg. loss: 77274145502979699088968701181952.000000\n",
      "Total training time: 33.22 seconds.\n",
      "-- Epoch 4615\n",
      "Norm: 6212256422473.26, NNZs: 843, Bias: -204714947.776071, T: 15538705, Avg. loss: 77265798766960871953397928951808.000000\n",
      "Total training time: 33.23 seconds.\n",
      "-- Epoch 4616\n",
      "Norm: 6099040120495.97, NNZs: 843, Bias: -78633298.997698, T: 15542072, Avg. loss: 77257562296962326042400494977024.000000\n",
      "Total training time: 33.24 seconds.\n",
      "-- Epoch 4617\n",
      "Norm: 6020794054321.78, NNZs: 843, Bias: 51753724.506780, T: 15545439, Avg. loss: 77249345555486603187705417302016.000000\n",
      "Total training time: 33.24 seconds.\n",
      "-- Epoch 4618\n",
      "Norm: 5981466387765.67, NNZs: 843, Bias: 210997509.145898, T: 15548806, Avg. loss: 77240865442208382611259700281344.000000\n",
      "Total training time: 33.25 seconds.\n",
      "-- Epoch 4619\n",
      "Norm: 5938632379214.05, NNZs: 843, Bias: 21974194.225331, T: 15552173, Avg. loss: 77232587906063613158036305084416.000000\n",
      "Total training time: 33.26 seconds.\n",
      "-- Epoch 4620\n",
      "Norm: 6059504474600.43, NNZs: 843, Bias: -137261252.039165, T: 15555540, Avg. loss: 77224200133002246409789965860864.000000\n",
      "Total training time: 33.27 seconds.\n",
      "-- Epoch 4621\n",
      "Norm: 6027660498824.49, NNZs: 843, Bias: 21967021.009598, T: 15558907, Avg. loss: 77215883837866962840221258874880.000000\n",
      "Total training time: 33.27 seconds.\n",
      "-- Epoch 4622\n",
      "Norm: 6121361925949.99, NNZs: 843, Bias: 181177562.842310, T: 15562274, Avg. loss: 77207485786143465253092999561216.000000\n",
      "Total training time: 33.28 seconds.\n",
      "-- Epoch 4623\n",
      "Norm: 6026690010174.76, NNZs: 843, Bias: -243998149.649992, T: 15565641, Avg. loss: 77199231844100312314348463718400.000000\n",
      "Total training time: 33.29 seconds.\n",
      "-- Epoch 4624\n",
      "Norm: 6241168214267.35, NNZs: 843, Bias: -84789055.319167, T: 15569008, Avg. loss: 77190876284226707614995629211648.000000\n",
      "Total training time: 33.29 seconds.\n",
      "-- Epoch 4625\n",
      "Norm: 6024097981371.50, NNZs: 843, Bias: -47582938.729693, T: 15572375, Avg. loss: 77182618760330880216870242222080.000000\n",
      "Total training time: 33.30 seconds.\n",
      "-- Epoch 4626\n",
      "Norm: 6295666338922.64, NNZs: 843, Bias: 357665959.805589, T: 15575742, Avg. loss: 77174175255888171516592427892736.000000\n",
      "Total training time: 33.31 seconds.\n",
      "-- Epoch 4627\n",
      "Norm: 6326338688627.57, NNZs: 843, Bias: -119869141.732802, T: 15579109, Avg. loss: 77165883465246396524139676434432.000000\n",
      "Total training time: 33.32 seconds.\n",
      "-- Epoch 4628\n",
      "Norm: 6018377376978.50, NNZs: 843, Bias: 39297280.613832, T: 15582476, Avg. loss: 77157500528092942794278496632832.000000\n",
      "Total training time: 33.32 seconds.\n",
      "-- Epoch 4629\n",
      "Norm: 6147269943290.47, NNZs: 843, Bias: -119861514.295633, T: 15585843, Avg. loss: 77149028751667313727556393369600.000000\n",
      "Total training time: 33.33 seconds.\n",
      "-- Epoch 4630\n",
      "Norm: 6500412091940.60, NNZs: 843, Bias: -597293432.753888, T: 15589210, Avg. loss: 77140783452237011697418171842560.000000\n",
      "Total training time: 33.34 seconds.\n",
      "-- Epoch 4631\n",
      "Norm: 6272952587518.87, NNZs: 843, Bias: 198419634.234835, T: 15592577, Avg. loss: 77132543905513939060549541167104.000000\n",
      "Total training time: 33.35 seconds.\n",
      "-- Epoch 4632\n",
      "Norm: 6133101065988.74, NNZs: 843, Bias: 39277261.649439, T: 15595944, Avg. loss: 77124135245753039279971511566336.000000\n",
      "Total training time: 33.35 seconds.\n",
      "-- Epoch 4633\n",
      "Norm: 6299618512613.02, NNZs: 843, Bias: -28379444.014364, T: 15599311, Avg. loss: 77115793870368632430720014352384.000000\n",
      "Total training time: 33.36 seconds.\n",
      "-- Epoch 4634\n",
      "Norm: 6197986423978.86, NNZs: 843, Bias: -187493055.482601, T: 15602678, Avg. loss: 77107555379129110403550822793216.000000\n",
      "Total training time: 33.37 seconds.\n",
      "-- Epoch 4635\n",
      "Norm: 6255496770907.09, NNZs: 843, Bias: -40740932.720206, T: 15606045, Avg. loss: 77099168756803110931033273401344.000000\n",
      "Total training time: 33.37 seconds.\n",
      "-- Epoch 4636\n",
      "Norm: 6295527018435.84, NNZs: 843, Bias: 151256301.926105, T: 15609412, Avg. loss: 77090829599650793829235708395520.000000\n",
      "Total training time: 33.38 seconds.\n",
      "-- Epoch 4637\n",
      "Norm: 6202336899286.22, NNZs: 843, Bias: -7840503.478626, T: 15612779, Avg. loss: 77082442843422545256639927156736.000000\n",
      "Total training time: 33.39 seconds.\n",
      "-- Epoch 4638\n",
      "Norm: 6113830048554.15, NNZs: 843, Bias: 104215624.122652, T: 15616146, Avg. loss: 77074111635825771717667409887232.000000\n",
      "Total training time: 33.40 seconds.\n",
      "-- Epoch 4639\n",
      "Norm: 6086455765078.19, NNZs: 843, Bias: -237798485.737159, T: 15619513, Avg. loss: 77065891246040736085438390861824.000000\n",
      "Total training time: 33.40 seconds.\n",
      "-- Epoch 4640\n",
      "Norm: 5970018577867.27, NNZs: 843, Bias: -78727914.213224, T: 15622880, Avg. loss: 77057609467103288476055258529792.000000\n",
      "Total training time: 33.41 seconds.\n",
      "-- Epoch 4641\n",
      "Norm: 6027691110319.71, NNZs: 843, Bias: -237776710.744068, T: 15626247, Avg. loss: 77049464590647052978985940025344.000000\n",
      "Total training time: 33.42 seconds.\n",
      "-- Epoch 4642\n",
      "Norm: 6262516463967.74, NNZs: 843, Bias: -153447967.876373, T: 15629614, Avg. loss: 77041161763880176782818454536192.000000\n",
      "Total training time: 33.42 seconds.\n",
      "-- Epoch 4643\n",
      "Norm: 5988369412553.91, NNZs: 843, Bias: 5591909.683048, T: 15632981, Avg. loss: 77032810635762058023972536057856.000000\n",
      "Total training time: 33.43 seconds.\n",
      "-- Epoch 4644\n",
      "Norm: 5976997332312.04, NNZs: 843, Bias: -153436749.506307, T: 15636348, Avg. loss: 77024591630090336488094403919872.000000\n",
      "Total training time: 33.44 seconds.\n",
      "-- Epoch 4645\n",
      "Norm: 6005958914783.57, NNZs: 843, Bias: 5585916.834808, T: 15639715, Avg. loss: 77016371874617600619030373203968.000000\n",
      "Total training time: 33.45 seconds.\n",
      "-- Epoch 4646\n",
      "Norm: 6024240133062.80, NNZs: 843, Bias: -153426004.815291, T: 15643082, Avg. loss: 77008093468154536041065603923968.000000\n",
      "Total training time: 33.45 seconds.\n",
      "-- Epoch 4647\n",
      "Norm: 6131707488071.38, NNZs: 843, Bias: 5579281.642218, T: 15646449, Avg. loss: 76999886144075658770661676941312.000000\n",
      "Total training time: 33.46 seconds.\n",
      "-- Epoch 4648\n",
      "Norm: 6097971812126.80, NNZs: 843, Bias: -153414788.482158, T: 15649816, Avg. loss: 76991538589767650227841358888960.000000\n",
      "Total training time: 33.47 seconds.\n",
      "-- Epoch 4649\n",
      "Norm: 6038855695410.11, NNZs: 843, Bias: 5573178.525533, T: 15653183, Avg. loss: 76983320776258278636808101167104.000000\n",
      "Total training time: 33.48 seconds.\n",
      "-- Epoch 4650\n",
      "Norm: 6328885626013.79, NNZs: 843, Bias: 97164463.233962, T: 15656550, Avg. loss: 76975065730811833901261290209280.000000\n",
      "Total training time: 33.48 seconds.\n",
      "-- Epoch 4651\n",
      "Norm: 6331131352526.05, NNZs: 843, Bias: -61808305.310090, T: 15659917, Avg. loss: 76966828703563162314945993900032.000000\n",
      "Total training time: 33.49 seconds.\n",
      "-- Epoch 4652\n",
      "Norm: 6319464432117.32, NNZs: 843, Bias: 46227973.401116, T: 15663284, Avg. loss: 76958668875423110312142714175488.000000\n",
      "Total training time: 33.50 seconds.\n",
      "-- Epoch 4653\n",
      "Norm: 6343518545829.34, NNZs: 843, Bias: 24412914.622931, T: 15666651, Avg. loss: 76950487315366718215359935021056.000000\n",
      "Total training time: 33.50 seconds.\n",
      "-- Epoch 4654\n",
      "Norm: 6351441743325.48, NNZs: 843, Bias: -134530262.911182, T: 15670018, Avg. loss: 76942292189951244248620325666816.000000\n",
      "Total training time: 33.51 seconds.\n",
      "-- Epoch 4655\n",
      "Norm: 6665059938094.18, NNZs: 843, Bias: 24406182.195734, T: 15673385, Avg. loss: 76933831847389342054171507949568.000000\n",
      "Total training time: 33.52 seconds.\n",
      "-- Epoch 4656\n",
      "Norm: 6845889717518.00, NNZs: 843, Bias: -452364842.022263, T: 15676752, Avg. loss: 76925717194818331176025670549504.000000\n",
      "Total training time: 33.53 seconds.\n",
      "-- Epoch 4657\n",
      "Norm: 6630507993910.22, NNZs: 843, Bias: 24399793.205976, T: 15680119, Avg. loss: 76917507117765887146876007874560.000000\n",
      "Total training time: 33.53 seconds.\n",
      "-- Epoch 4658\n",
      "Norm: 6675972015286.29, NNZs: 843, Bias: 167334345.845970, T: 15683486, Avg. loss: 76909352896798918319691570085888.000000\n",
      "Total training time: 33.54 seconds.\n",
      "-- Epoch 4659\n",
      "Norm: 6526720903445.08, NNZs: 843, Bias: 8425926.263495, T: 15686853, Avg. loss: 76901073702186562480691562414080.000000\n",
      "Total training time: 33.55 seconds.\n",
      "-- Epoch 4660\n",
      "Norm: 6633447423315.98, NNZs: 843, Bias: -150464591.695001, T: 15690220, Avg. loss: 76892826738620821876136921268224.000000\n",
      "Total training time: 33.55 seconds.\n",
      "-- Epoch 4661\n",
      "Norm: 6503843952195.58, NNZs: 843, Bias: 8421625.021277, T: 15693587, Avg. loss: 76884608147320330080629509062656.000000\n",
      "Total training time: 33.56 seconds.\n",
      "-- Epoch 4662\n",
      "Norm: 6492756595940.56, NNZs: 843, Bias: 269593130.083701, T: 15696954, Avg. loss: 76876321765224033621517298302976.000000\n",
      "Total training time: 33.57 seconds.\n",
      "-- Epoch 4663\n",
      "Norm: 6422087834298.62, NNZs: 843, Bias: 110714301.357317, T: 15700321, Avg. loss: 76868068686099437360571240939520.000000\n",
      "Total training time: 33.58 seconds.\n",
      "-- Epoch 4664\n",
      "Norm: 6090077450957.56, NNZs: 843, Bias: -48148338.325536, T: 15703688, Avg. loss: 76859768779060533023428147412992.000000\n",
      "Total training time: 33.58 seconds.\n",
      "-- Epoch 4665\n",
      "Norm: 6140605117819.53, NNZs: 843, Bias: -206993758.581120, T: 15707055, Avg. loss: 76851546464452689179752387313664.000000\n",
      "Total training time: 33.59 seconds.\n",
      "-- Epoch 4666\n",
      "Norm: 6251494053807.02, NNZs: 843, Bias: -365822739.942526, T: 15710422, Avg. loss: 76843417615654442497914827702272.000000\n",
      "Total training time: 33.60 seconds.\n",
      "-- Epoch 4667\n",
      "Norm: 6104307341799.97, NNZs: 843, Bias: 110681934.582093, T: 15713789, Avg. loss: 76835152506115025296354327920640.000000\n",
      "Total training time: 33.60 seconds.\n",
      "-- Epoch 4668\n",
      "Norm: 6097068922055.60, NNZs: 843, Bias: 269494561.255776, T: 15717156, Avg. loss: 76826978152602609083449362874368.000000\n",
      "Total training time: 33.61 seconds.\n",
      "-- Epoch 4669\n",
      "Norm: 5991001007859.55, NNZs: 843, Bias: -206957942.375573, T: 15720523, Avg. loss: 76818936827227291598727531200512.000000\n",
      "Total training time: 33.62 seconds.\n",
      "-- Epoch 4670\n",
      "Norm: 6013360845141.78, NNZs: 843, Bias: -48145232.971594, T: 15723890, Avg. loss: 76810682507147698511658304405504.000000\n",
      "Total training time: 33.63 seconds.\n",
      "-- Epoch 4671\n",
      "Norm: 6031215872909.13, NNZs: 843, Bias: 248733251.138491, T: 15727257, Avg. loss: 76802610968833245374329529565184.000000\n",
      "Total training time: 33.63 seconds.\n",
      "-- Epoch 4672\n",
      "Norm: 6001632098372.00, NNZs: 843, Bias: -227641910.294113, T: 15730624, Avg. loss: 76794537845756180604047186722816.000000\n",
      "Total training time: 33.64 seconds.\n",
      "-- Epoch 4673\n",
      "Norm: 6522849032539.50, NNZs: 843, Bias: -56079236.996926, T: 15733991, Avg. loss: 76786290572387578438296247730176.000000\n",
      "Total training time: 33.65 seconds.\n",
      "-- Epoch 4674\n",
      "Norm: 6222791901516.14, NNZs: 843, Bias: 102690954.638104, T: 15737358, Avg. loss: 76778098266141920425200156934144.000000\n",
      "Total training time: 33.66 seconds.\n",
      "-- Epoch 4675\n",
      "Norm: 6600192382039.35, NNZs: 843, Bias: -56077832.108639, T: 15740725, Avg. loss: 76769924790443131189670027722752.000000\n",
      "Total training time: 33.66 seconds.\n",
      "-- Epoch 4676\n",
      "Norm: 6231349672981.84, NNZs: 843, Bias: 102675157.536194, T: 15744092, Avg. loss: 76761724464772499526771456081920.000000\n",
      "Total training time: 33.67 seconds.\n",
      "-- Epoch 4677\n",
      "Norm: 6216318758358.97, NNZs: 843, Bias: 20364792.277958, T: 15747459, Avg. loss: 76753507713510427660202526375936.000000\n",
      "Total training time: 33.68 seconds.\n",
      "-- Epoch 4678\n",
      "Norm: 6089290332413.76, NNZs: 843, Bias: 179096673.406762, T: 15750826, Avg. loss: 76745310110064279695440230744064.000000\n",
      "Total training time: 33.68 seconds.\n",
      "-- Epoch 4679\n",
      "Norm: 6057696663403.29, NNZs: 843, Bias: 11588178.159348, T: 15754193, Avg. loss: 76737193965927292189216669171712.000000\n",
      "Total training time: 33.69 seconds.\n",
      "-- Epoch 4680\n",
      "Norm: 6053258112618.28, NNZs: 843, Bias: -147132339.263670, T: 15757560, Avg. loss: 76728913218597849995404443123712.000000\n",
      "Total training time: 33.70 seconds.\n",
      "-- Epoch 4681\n",
      "Norm: 6356815069166.65, NNZs: 843, Bias: 306787070.364550, T: 15760927, Avg. loss: 76720799076394294513266000396288.000000\n",
      "Total training time: 33.71 seconds.\n",
      "-- Epoch 4682\n",
      "Norm: 6135803390004.03, NNZs: 843, Bias: 148066309.530230, T: 15764294, Avg. loss: 76712595605426994931780634542080.000000\n",
      "Total training time: 33.71 seconds.\n",
      "-- Epoch 4683\n",
      "Norm: 6350815132039.62, NNZs: 843, Bias: -10637185.888061, T: 15767661, Avg. loss: 76704426948210115496469636055040.000000\n",
      "Total training time: 33.72 seconds.\n",
      "-- Epoch 4684\n",
      "Norm: 6049996511904.78, NNZs: 843, Bias: 148046716.437697, T: 15771028, Avg. loss: 76696225095366742507610261422080.000000\n",
      "Total training time: 33.73 seconds.\n",
      "-- Epoch 4685\n",
      "Norm: 6078944254308.00, NNZs: 843, Bias: 104971869.400851, T: 15774395, Avg. loss: 76688003626716223605111586291712.000000\n",
      "Total training time: 33.73 seconds.\n",
      "-- Epoch 4686\n",
      "Norm: 6068243684676.88, NNZs: 843, Bias: 52660776.849630, T: 15777762, Avg. loss: 76679650872604678336765019291648.000000\n",
      "Total training time: 33.74 seconds.\n",
      "-- Epoch 4687\n",
      "Norm: 6291950260468.62, NNZs: 843, Bias: 211315520.736696, T: 15781129, Avg. loss: 76671437392122800590229371617280.000000\n",
      "Total training time: 33.75 seconds.\n",
      "-- Epoch 4688\n",
      "Norm: 6026086230289.86, NNZs: 843, Bias: -264650395.807312, T: 15784496, Avg. loss: 76663355137623593478002835456000.000000\n",
      "Total training time: 33.76 seconds.\n",
      "-- Epoch 4689\n",
      "Norm: 6055551917351.48, NNZs: 843, Bias: 211289505.990716, T: 15787863, Avg. loss: 76655237782059784361683932151808.000000\n",
      "Total training time: 33.76 seconds.\n",
      "-- Epoch 4690\n",
      "Norm: 6079360088417.99, NNZs: 843, Bias: 31698863.787912, T: 15791230, Avg. loss: 76647162445005792901473887059968.000000\n",
      "Total training time: 33.77 seconds.\n",
      "-- Epoch 4691\n",
      "Norm: 6329614399584.70, NNZs: 843, Bias: -455437122.930708, T: 15794597, Avg. loss: 76639049291472167382818643509248.000000\n",
      "Total training time: 33.78 seconds.\n",
      "-- Epoch 4692\n",
      "Norm: 5900153124271.63, NNZs: 843, Bias: 20436546.075300, T: 15797964, Avg. loss: 76630989398480008264002857598976.000000\n",
      "Total training time: 33.79 seconds.\n",
      "-- Epoch 4693\n",
      "Norm: 5815012114026.83, NNZs: 843, Bias: -33412386.800054, T: 15801331, Avg. loss: 76622754461643811039383891476480.000000\n",
      "Total training time: 33.79 seconds.\n",
      "-- Epoch 4694\n",
      "Norm: 5995885936204.23, NNZs: 843, Bias: 125187321.665517, T: 15804698, Avg. loss: 76614594215593159356258673754112.000000\n",
      "Total training time: 33.80 seconds.\n",
      "-- Epoch 4695\n",
      "Norm: 5757706045448.16, NNZs: 843, Bias: -220546077.731648, T: 15808065, Avg. loss: 76606472097520977847649388462080.000000\n",
      "Total training time: 33.81 seconds.\n",
      "-- Epoch 4696\n",
      "Norm: 5835370740064.40, NNZs: 843, Bias: -139607526.037044, T: 15811432, Avg. loss: 76598427028216325621029707710464.000000\n",
      "Total training time: 33.81 seconds.\n",
      "-- Epoch 4697\n",
      "Norm: 5826816353013.65, NNZs: 843, Bias: 18973114.609371, T: 15814799, Avg. loss: 76590255482956504469063168163840.000000\n",
      "Total training time: 33.82 seconds.\n",
      "-- Epoch 4698\n",
      "Norm: 6096108090304.87, NNZs: 843, Bias: 392743345.788376, T: 15818166, Avg. loss: 76582270733301521380251327791104.000000\n",
      "Total training time: 33.83 seconds.\n",
      "-- Epoch 4699\n",
      "Norm: 6336324521227.42, NNZs: 843, Bias: -82952579.777988, T: 15821533, Avg. loss: 76574150753126750890817096777728.000000\n",
      "Total training time: 33.84 seconds.\n",
      "-- Epoch 4700\n",
      "Norm: 6394352528074.20, NNZs: 843, Bias: 149882908.221901, T: 15824900, Avg. loss: 76565855065746263007171136454656.000000\n",
      "Total training time: 33.84 seconds.\n",
      "-- Epoch 4701\n",
      "Norm: 6386986140595.85, NNZs: 843, Bias: -325750744.043794, T: 15828267, Avg. loss: 76557817300615637200082667831296.000000\n",
      "Total training time: 33.85 seconds.\n",
      "-- Epoch 4702\n",
      "Norm: 6191435107473.32, NNZs: 843, Bias: 149861410.700884, T: 15831634, Avg. loss: 76549612031118874879192301830144.000000\n",
      "Total training time: 33.86 seconds.\n",
      "-- Epoch 4703\n",
      "Norm: 6101982455297.38, NNZs: 843, Bias: -8673060.231461, T: 15835001, Avg. loss: 76541603730869470612900118790144.000000\n",
      "Total training time: 33.87 seconds.\n",
      "-- Epoch 4704\n",
      "Norm: 6049134377291.37, NNZs: 843, Bias: 149840585.316830, T: 15838368, Avg. loss: 76533565429007956702324458520576.000000\n",
      "Total training time: 33.87 seconds.\n",
      "-- Epoch 4705\n",
      "Norm: 6181758867321.86, NNZs: 843, Bias: -8677016.058901, T: 15841735, Avg. loss: 76525389285389817658631128088576.000000\n",
      "Total training time: 33.88 seconds.\n",
      "-- Epoch 4706\n",
      "Norm: 6036286487109.13, NNZs: 843, Bias: -167177899.882312, T: 15845102, Avg. loss: 76517264131218811647564972032000.000000\n",
      "Total training time: 33.89 seconds.\n",
      "-- Epoch 4707\n",
      "Norm: 5914649672550.21, NNZs: 843, Bias: -8681414.724712, T: 15848469, Avg. loss: 76509176512417614842075214249984.000000\n",
      "Total training time: 33.89 seconds.\n",
      "-- Epoch 4708\n",
      "Norm: 6461666512559.70, NNZs: 843, Bias: -484129556.736975, T: 15851836, Avg. loss: 76501036684818280770312750497792.000000\n",
      "Total training time: 33.90 seconds.\n",
      "-- Epoch 4709\n",
      "Norm: 6107348247143.09, NNZs: 843, Bias: -167699282.617271, T: 15855203, Avg. loss: 76493034005536034978312501592064.000000\n",
      "Total training time: 33.91 seconds.\n",
      "-- Epoch 4710\n",
      "Norm: 6032246877924.36, NNZs: 843, Bias: -82181150.916193, T: 15858570, Avg. loss: 76484769099803154824923514404864.000000\n",
      "Total training time: 33.92 seconds.\n",
      "-- Epoch 4711\n",
      "Norm: 6252887721860.57, NNZs: 843, Bias: 307319878.367382, T: 15861937, Avg. loss: 76476748719108927746317804896256.000000\n",
      "Total training time: 33.92 seconds.\n",
      "-- Epoch 4712\n",
      "Norm: 5861126952475.77, NNZs: 843, Bias: -168043674.307285, T: 15865304, Avg. loss: 76468544981599732134607803383808.000000\n",
      "Total training time: 33.93 seconds.\n",
      "-- Epoch 4713\n",
      "Norm: 5969782666877.41, NNZs: 843, Bias: -3479557.801285, T: 15868671, Avg. loss: 76460366417771580866388780122112.000000\n",
      "Total training time: 33.94 seconds.\n",
      "-- Epoch 4714\n",
      "Norm: 5905397439207.69, NNZs: 843, Bias: -161913762.510307, T: 15872038, Avg. loss: 76452195605457016614259762659328.000000\n",
      "Total training time: 33.94 seconds.\n",
      "-- Epoch 4715\n",
      "Norm: 5824877629791.58, NNZs: 843, Bias: -3484411.458183, T: 15875405, Avg. loss: 76444159612257684219318692216832.000000\n",
      "Total training time: 33.95 seconds.\n",
      "-- Epoch 4716\n",
      "Norm: 5931384182266.10, NNZs: 843, Bias: -478731647.910773, T: 15878772, Avg. loss: 76436071317791703526583824285696.000000\n",
      "Total training time: 33.96 seconds.\n",
      "-- Epoch 4717\n",
      "Norm: 5978915222834.24, NNZs: 843, Bias: -3488898.610845, T: 15882139, Avg. loss: 76427878326551010163031090921472.000000\n",
      "Total training time: 33.97 seconds.\n",
      "-- Epoch 4718\n",
      "Norm: 5747966921742.90, NNZs: 843, Bias: -161889724.050466, T: 15885506, Avg. loss: 76419683762727131912394502045696.000000\n",
      "Total training time: 33.97 seconds.\n",
      "-- Epoch 4719\n",
      "Norm: 5691072452567.19, NNZs: 843, Bias: -3494395.203862, T: 15888873, Avg. loss: 76411620693172947167499218059264.000000\n",
      "Total training time: 33.98 seconds.\n",
      "-- Epoch 4720\n",
      "Norm: 5757287095627.45, NNZs: 843, Bias: -161877275.974208, T: 15892240, Avg. loss: 76403537812416191276054608871424.000000\n",
      "Total training time: 33.99 seconds.\n",
      "-- Epoch 4721\n",
      "Norm: 5932742063934.65, NNZs: 843, Bias: -89931399.357543, T: 15895607, Avg. loss: 76395434593400500767699725451264.000000\n",
      "Total training time: 34.00 seconds.\n",
      "-- Epoch 4722\n",
      "Norm: 6087649536370.56, NNZs: 843, Bias: 68435763.231716, T: 15898974, Avg. loss: 76387455687926115708417018429440.000000\n",
      "Total training time: 34.00 seconds.\n",
      "-- Epoch 4723\n",
      "Norm: 5839993500100.17, NNZs: 843, Bias: -89925545.010686, T: 15902341, Avg. loss: 76379256760231787522040449204224.000000\n",
      "Total training time: 34.01 seconds.\n",
      "-- Epoch 4724\n",
      "Norm: 5704363344885.10, NNZs: 843, Bias: 167709412.015994, T: 15905708, Avg. loss: 76371201065640135881209071796224.000000\n",
      "Total training time: 34.02 seconds.\n",
      "-- Epoch 4725\n",
      "Norm: 5812989161040.13, NNZs: 843, Bias: 9358913.935857, T: 15909075, Avg. loss: 76363148325594856675314705104896.000000\n",
      "Total training time: 34.02 seconds.\n",
      "-- Epoch 4726\n",
      "Norm: 5864627456906.11, NNZs: 843, Bias: -148975765.880211, T: 15912442, Avg. loss: 76355149385347469578983069188096.000000\n",
      "Total training time: 34.03 seconds.\n",
      "-- Epoch 4727\n",
      "Norm: 5803891885614.92, NNZs: 843, Bias: -307293126.263205, T: 15915809, Avg. loss: 76346936776512800128034026815488.000000\n",
      "Total training time: 34.04 seconds.\n",
      "-- Epoch 4728\n",
      "Norm: 6208833655584.77, NNZs: 843, Bias: 167664287.676052, T: 15919176, Avg. loss: 76338774591379718834108432908288.000000\n",
      "Total training time: 34.05 seconds.\n",
      "-- Epoch 4729\n",
      "Norm: 5793981775352.01, NNZs: 843, Bias: -89016911.891121, T: 15922543, Avg. loss: 76330647402848877093129830793216.000000\n",
      "Total training time: 34.05 seconds.\n",
      "-- Epoch 4730\n",
      "Norm: 5648116055861.31, NNZs: 843, Bias: 69283017.043621, T: 15925910, Avg. loss: 76322761448533881690834427969536.000000\n",
      "Total training time: 34.06 seconds.\n",
      "-- Epoch 4731\n",
      "Norm: 5671645357583.74, NNZs: 843, Bias: -89010960.697022, T: 15929277, Avg. loss: 76314672166193369779204773642240.000000\n",
      "Total training time: 34.07 seconds.\n",
      "-- Epoch 4732\n",
      "Norm: 5713537753938.40, NNZs: 843, Bias: 164505522.694913, T: 15932644, Avg. loss: 76306519000056055917423343173632.000000\n",
      "Total training time: 34.07 seconds.\n",
      "-- Epoch 4733\n",
      "Norm: 5727017357092.51, NNZs: 843, Bias: -153481229.866555, T: 15936011, Avg. loss: 76298383776676294508425276030976.000000\n",
      "Total training time: 34.08 seconds.\n",
      "-- Epoch 4734\n",
      "Norm: 5922832590776.71, NNZs: 843, Bias: -311739097.491238, T: 15939378, Avg. loss: 76290368151272178425751574413312.000000\n",
      "Total training time: 34.09 seconds.\n",
      "-- Epoch 4735\n",
      "Norm: 5879965300112.63, NNZs: 843, Bias: 163042271.320024, T: 15942745, Avg. loss: 76282216977156297146577773920256.000000\n",
      "Total training time: 34.10 seconds.\n",
      "-- Epoch 4736\n",
      "Norm: 5885410478336.69, NNZs: 843, Bias: -14935003.413612, T: 15946112, Avg. loss: 76274010291161042974498143338496.000000\n",
      "Total training time: 34.10 seconds.\n",
      "-- Epoch 4737\n",
      "Norm: 5832495705218.48, NNZs: 843, Bias: 29276170.749710, T: 15949479, Avg. loss: 76266028514369157473090679078912.000000\n",
      "Total training time: 34.11 seconds.\n",
      "-- Epoch 4738\n",
      "Norm: 5863083924960.59, NNZs: 843, Bias: -128957326.706836, T: 15952846, Avg. loss: 76258028599487200565584937877504.000000\n",
      "Total training time: 34.12 seconds.\n",
      "-- Epoch 4739\n",
      "Norm: 5716398465472.87, NNZs: 843, Bias: 29270083.205007, T: 15956213, Avg. loss: 76249927550552670834309713625088.000000\n",
      "Total training time: 34.13 seconds.\n",
      "-- Epoch 4740\n",
      "Norm: 5750305665823.77, NNZs: 843, Bias: 187480465.734004, T: 15959580, Avg. loss: 76241931050767046738066397986816.000000\n",
      "Total training time: 34.13 seconds.\n",
      "-- Epoch 4741\n",
      "Norm: 5848802668798.97, NNZs: 843, Bias: -85136404.843568, T: 15962947, Avg. loss: 76233859611376240078958412955648.000000\n",
      "Total training time: 34.14 seconds.\n",
      "-- Epoch 4742\n",
      "Norm: 5830792712449.54, NNZs: 843, Bias: 1400300.386253, T: 15966314, Avg. loss: 76225966172977483393330006982656.000000\n",
      "Total training time: 34.15 seconds.\n",
      "-- Epoch 4743\n",
      "Norm: 5867946962842.21, NNZs: 843, Bias: -154292754.681300, T: 15969681, Avg. loss: 76217988007840996007814981746688.000000\n",
      "Total training time: 34.15 seconds.\n",
      "-- Epoch 4744\n",
      "Norm: 6065276477875.48, NNZs: 843, Bias: 3894070.958067, T: 15973048, Avg. loss: 76209926367896245399704733483008.000000\n",
      "Total training time: 34.16 seconds.\n",
      "-- Epoch 4745\n",
      "Norm: 5790446239276.34, NNZs: 843, Bias: -154280576.463537, T: 15976415, Avg. loss: 76201851326497102307302846758912.000000\n",
      "Total training time: 34.17 seconds.\n",
      "-- Epoch 4746\n",
      "Norm: 6144413107845.47, NNZs: 843, Bias: -312438719.022169, T: 15979782, Avg. loss: 76193808324794147293751660249088.000000\n",
      "Total training time: 34.18 seconds.\n",
      "-- Epoch 4747\n",
      "Norm: 5631338314284.39, NNZs: 843, Bias: -154268434.472015, T: 15983149, Avg. loss: 76185787390000512839136487407616.000000\n",
      "Total training time: 34.18 seconds.\n",
      "-- Epoch 4748\n",
      "Norm: 5628279372770.73, NNZs: 843, Bias: 3884870.850763, T: 15986516, Avg. loss: 76177700919013225252698042925056.000000\n",
      "Total training time: 34.19 seconds.\n",
      "-- Epoch 4749\n",
      "Norm: 5825773963814.49, NNZs: 843, Bias: -271883801.153643, T: 15989883, Avg. loss: 76169578095515527763257662111744.000000\n",
      "Total training time: 34.20 seconds.\n",
      "-- Epoch 4750\n",
      "Norm: 5579498539248.90, NNZs: 843, Bias: -113741265.038331, T: 15993250, Avg. loss: 76161578314366844897811211747328.000000\n",
      "Total training time: 34.21 seconds.\n",
      "-- Epoch 4751\n",
      "Norm: 5883843199052.66, NNZs: 843, Bias: 44385512.087495, T: 15996617, Avg. loss: 76153499292378746273616300605440.000000\n",
      "Total training time: 34.21 seconds.\n",
      "-- Epoch 4752\n",
      "Norm: 5714739483607.19, NNZs: 843, Bias: 202495122.961967, T: 15999984, Avg. loss: 76145335183854208816213366145024.000000\n",
      "Total training time: 34.22 seconds.\n",
      "-- Epoch 4753\n",
      "Norm: 6043323978360.65, NNZs: 843, Bias: -588045296.831529, T: 16003351, Avg. loss: 76137328665976865035795640090624.000000\n",
      "Total training time: 34.23 seconds.\n",
      "-- Epoch 4754\n",
      "Norm: 5742834467046.53, NNZs: 843, Bias: -113724875.235473, T: 16006718, Avg. loss: 76129397677196639657861111087104.000000\n",
      "Total training time: 34.23 seconds.\n",
      "-- Epoch 4755\n",
      "Norm: 5594893220867.32, NNZs: 843, Bias: 44368395.713538, T: 16010085, Avg. loss: 76121443339895554021204839890944.000000\n",
      "Total training time: 34.24 seconds.\n",
      "-- Epoch 4756\n",
      "Norm: 5781618374496.45, NNZs: 843, Bias: -113716365.992529, T: 16013452, Avg. loss: 76113401281758744515840711852032.000000\n",
      "Total training time: 34.25 seconds.\n",
      "-- Epoch 4757\n",
      "Norm: 5919919108533.29, NNZs: 843, Bias: 117439721.132815, T: 16016819, Avg. loss: 76105218330422460235672375525376.000000\n",
      "Total training time: 34.26 seconds.\n",
      "-- Epoch 4758\n",
      "Norm: 5715701691890.91, NNZs: 843, Bias: -356760672.583880, T: 16020186, Avg. loss: 76097227768804297786050228518912.000000\n",
      "Total training time: 34.26 seconds.\n",
      "-- Epoch 4759\n",
      "Norm: 5559381725144.70, NNZs: 843, Bias: -198688136.257448, T: 16023553, Avg. loss: 76089352677813924994645267316736.000000\n",
      "Total training time: 34.27 seconds.\n",
      "-- Epoch 4760\n",
      "Norm: 5584129765393.56, NNZs: 843, Bias: 123283796.023539, T: 16026920, Avg. loss: 76081299947966362597788028502016.000000\n",
      "Total training time: 34.28 seconds.\n",
      "-- Epoch 4761\n",
      "Norm: 5636222660534.63, NNZs: 843, Bias: -34764015.019982, T: 16030287, Avg. loss: 76073273102285181728201379938304.000000\n",
      "Total training time: 34.28 seconds.\n",
      "-- Epoch 4762\n",
      "Norm: 5874760670371.23, NNZs: 843, Bias: 123267289.823209, T: 16033654, Avg. loss: 76065422303490009181153168195584.000000\n",
      "Total training time: 34.29 seconds.\n",
      "-- Epoch 4763\n",
      "Norm: 5650159291866.18, NNZs: 843, Bias: 375816897.849399, T: 16037021, Avg. loss: 76057300925650998640576691175424.000000\n",
      "Total training time: 34.30 seconds.\n",
      "-- Epoch 4764\n",
      "Norm: 5380564127817.09, NNZs: 843, Bias: -98247240.655147, T: 16040388, Avg. loss: 76049178151450585377860724195328.000000\n",
      "Total training time: 34.31 seconds.\n",
      "-- Epoch 4765\n",
      "Norm: 5354348288600.30, NNZs: 843, Bias: 59762917.326462, T: 16043755, Avg. loss: 76041268917527334034189131448320.000000\n",
      "Total training time: 34.31 seconds.\n",
      "-- Epoch 4766\n",
      "Norm: 5447749068874.86, NNZs: 843, Bias: 217755755.495061, T: 16047122, Avg. loss: 76033161818993272472223566790656.000000\n",
      "Total training time: 34.32 seconds.\n",
      "-- Epoch 4767\n",
      "Norm: 5445210171167.57, NNZs: 843, Bias: -256226407.824775, T: 16050489, Avg. loss: 76025044897008725650995643678720.000000\n",
      "Total training time: 34.33 seconds.\n",
      "-- Epoch 4768\n",
      "Norm: 5482855956626.05, NNZs: 843, Bias: -98233224.806850, T: 16053856, Avg. loss: 76017220323292428431991806885888.000000\n",
      "Total training time: 34.34 seconds.\n",
      "-- Epoch 4769\n",
      "Norm: 5420469797377.09, NNZs: 843, Bias: 59743151.816918, T: 16057223, Avg. loss: 76009175298696612987746383822848.000000\n",
      "Total training time: 34.34 seconds.\n",
      "-- Epoch 4770\n",
      "Norm: 5620755655630.00, NNZs: 843, Bias: 217702507.311023, T: 16060590, Avg. loss: 76001021125808047162537044606976.000000\n",
      "Total training time: 34.35 seconds.\n",
      "-- Epoch 4771\n",
      "Norm: 5561445393239.62, NNZs: 843, Bias: -185017004.548867, T: 16063957, Avg. loss: 75993362649806983565752676646912.000000\n",
      "Total training time: 34.36 seconds.\n",
      "-- Epoch 4772\n",
      "Norm: 5503695561698.73, NNZs: 843, Bias: -27060539.011132, T: 16067324, Avg. loss: 75985455780931233273973416394752.000000\n",
      "Total training time: 34.36 seconds.\n",
      "-- Epoch 4773\n",
      "Norm: 5760562365208.19, NNZs: 843, Bias: 130877767.513002, T: 16070691, Avg. loss: 75977505399698867688323550281728.000000\n",
      "Total training time: 34.37 seconds.\n",
      "-- Epoch 4774\n",
      "Norm: 5562987094540.22, NNZs: 843, Bias: -27062841.246157, T: 16074058, Avg. loss: 75969615156525864132530904498176.000000\n",
      "Total training time: 34.38 seconds.\n",
      "-- Epoch 4775\n",
      "Norm: 5550252665136.64, NNZs: 843, Bias: -184986326.923980, T: 16077425, Avg. loss: 75961734268335606954795133304832.000000\n",
      "Total training time: 34.39 seconds.\n",
      "-- Epoch 4776\n",
      "Norm: 5509288734618.09, NNZs: 843, Bias: -168907952.910626, T: 16080792, Avg. loss: 75953869086320922822265471500288.000000\n",
      "Total training time: 34.39 seconds.\n",
      "-- Epoch 4777\n",
      "Norm: 5483479205131.84, NNZs: 843, Bias: -10994357.090313, T: 16084159, Avg. loss: 75945876938651560799908000694272.000000\n",
      "Total training time: 34.40 seconds.\n",
      "-- Epoch 4778\n",
      "Norm: 5504462038943.34, NNZs: 843, Bias: -161130765.390910, T: 16087526, Avg. loss: 75937929740271272339096438046720.000000\n",
      "Total training time: 34.41 seconds.\n",
      "-- Epoch 4779\n",
      "Norm: 5419831990176.12, NNZs: 843, Bias: -3234172.669416, T: 16090893, Avg. loss: 75930083496430670030217200795648.000000\n",
      "Total training time: 34.41 seconds.\n",
      "-- Epoch 4780\n",
      "Norm: 5436803211340.01, NNZs: 843, Bias: 154646207.995646, T: 16094260, Avg. loss: 75921947842191847522657697267712.000000\n",
      "Total training time: 34.42 seconds.\n",
      "-- Epoch 4781\n",
      "Norm: 5498268181205.70, NNZs: 843, Bias: -3238280.352053, T: 16097627, Avg. loss: 75913941407528964216244639105024.000000\n",
      "Total training time: 34.43 seconds.\n",
      "-- Epoch 4782\n",
      "Norm: 5433602187538.88, NNZs: 843, Bias: 154624109.825602, T: 16100994, Avg. loss: 75906046856782487650679255990272.000000\n",
      "Total training time: 34.44 seconds.\n",
      "-- Epoch 4783\n",
      "Norm: 5414130425994.72, NNZs: 843, Bias: -143342577.307068, T: 16104361, Avg. loss: 75898116855182815810659484172288.000000\n",
      "Total training time: 34.44 seconds.\n",
      "-- Epoch 4784\n",
      "Norm: 5515985607639.39, NNZs: 843, Bias: -24951550.990972, T: 16107728, Avg. loss: 75890168808075839918608916414464.000000\n",
      "Total training time: 34.45 seconds.\n",
      "-- Epoch 4785\n",
      "Norm: 5578781763855.26, NNZs: 843, Bias: -174273659.274995, T: 16111095, Avg. loss: 75882346760353165419555594436608.000000\n",
      "Total training time: 34.46 seconds.\n",
      "-- Epoch 4786\n",
      "Norm: 5732556713195.44, NNZs: 843, Bias: 299230281.735080, T: 16114462, Avg. loss: 75874491732536763930492828385280.000000\n",
      "Total training time: 34.46 seconds.\n",
      "-- Epoch 4787\n",
      "Norm: 5541894133722.47, NNZs: 843, Bias: -174260066.989967, T: 16117829, Avg. loss: 75866312009585273066168957009920.000000\n",
      "Total training time: 34.47 seconds.\n",
      "-- Epoch 4788\n",
      "Norm: 5549365460820.88, NNZs: 843, Bias: -104286243.266417, T: 16121196, Avg. loss: 75858472822391593248505343246336.000000\n",
      "Total training time: 34.48 seconds.\n",
      "-- Epoch 4789\n",
      "Norm: 5543200788426.09, NNZs: 843, Bias: 53524893.541924, T: 16124563, Avg. loss: 75850306412830143984550174261248.000000\n",
      "Total training time: 34.49 seconds.\n",
      "-- Epoch 4790\n",
      "Norm: 5718540536138.25, NNZs: 843, Bias: 211320042.340042, T: 16127930, Avg. loss: 75842335827194102323556282007552.000000\n",
      "Total training time: 34.49 seconds.\n",
      "-- Epoch 4791\n",
      "Norm: 5509164935594.61, NNZs: 843, Bias: -131859832.452566, T: 16131297, Avg. loss: 75834229924526376219966456201216.000000\n",
      "Total training time: 34.50 seconds.\n",
      "-- Epoch 4792\n",
      "Norm: 5738204522654.66, NNZs: 843, Bias: 25927560.574914, T: 16134664, Avg. loss: 75826338128475309644280861032448.000000\n",
      "Total training time: 34.51 seconds.\n",
      "-- Epoch 4793\n",
      "Norm: 5753318004394.36, NNZs: 843, Bias: -131850134.124580, T: 16138031, Avg. loss: 75818507918872824501391121711104.000000\n",
      "Total training time: 34.52 seconds.\n",
      "-- Epoch 4794\n",
      "Norm: 5705823464620.71, NNZs: 843, Bias: 276778323.514440, T: 16141398, Avg. loss: 75810627754681702786898279268352.000000\n",
      "Total training time: 34.52 seconds.\n",
      "-- Epoch 4795\n",
      "Norm: 5498531454632.12, NNZs: 843, Bias: 119004155.634958, T: 16144765, Avg. loss: 75802829394408834941859932405760.000000\n",
      "Total training time: 34.53 seconds.\n",
      "-- Epoch 4796\n",
      "Norm: 5418523194824.67, NNZs: 843, Bias: -38753963.017148, T: 16148132, Avg. loss: 75794773840892837945176550801408.000000\n",
      "Total training time: 34.54 seconds.\n",
      "-- Epoch 4797\n",
      "Norm: 5471595784663.65, NNZs: 843, Bias: -196494597.877712, T: 16151499, Avg. loss: 75786848213649968550052062298112.000000\n",
      "Total training time: 34.54 seconds.\n",
      "-- Epoch 4798\n",
      "Norm: 5514209366828.97, NNZs: 843, Bias: -305246201.956446, T: 16154866, Avg. loss: 75778889512225465786586409992192.000000\n",
      "Total training time: 34.55 seconds.\n",
      "-- Epoch 4799\n",
      "Norm: 5430279306605.69, NNZs: 843, Bias: -147507150.193367, T: 16158233, Avg. loss: 75771005637528221763146721787904.000000\n",
      "Total training time: 34.56 seconds.\n",
      "-- Epoch 4800\n",
      "Norm: 5435747107789.58, NNZs: 843, Bias: 10215440.194234, T: 16161600, Avg. loss: 75763199094707380669963399331840.000000\n",
      "Total training time: 34.57 seconds.\n",
      "-- Epoch 4801\n",
      "Norm: 5321037901664.04, NNZs: 843, Bias: -36529477.959398, T: 16164967, Avg. loss: 75755404457249342210644908179456.000000\n",
      "Total training time: 34.57 seconds.\n",
      "-- Epoch 4802\n",
      "Norm: 5412752766142.44, NNZs: 843, Bias: 121171158.810134, T: 16168334, Avg. loss: 75747495556551138262209267761152.000000\n",
      "Total training time: 34.58 seconds.\n",
      "-- Epoch 4803\n",
      "Norm: 5310967665062.61, NNZs: 843, Bias: -26971377.991265, T: 16171701, Avg. loss: 75739607872532328936249147523072.000000\n",
      "Total training time: 34.59 seconds.\n",
      "-- Epoch 4804\n",
      "Norm: 5732032586114.51, NNZs: 843, Bias: 130711921.453013, T: 16175068, Avg. loss: 75731875794261300176988072312832.000000\n",
      "Total training time: 34.60 seconds.\n",
      "-- Epoch 4805\n",
      "Norm: 5312080694648.85, NNZs: 843, Bias: -14716862.442070, T: 16178435, Avg. loss: 75724071172131854148286147461120.000000\n",
      "Total training time: 34.60 seconds.\n",
      "-- Epoch 4806\n",
      "Norm: 5331935591212.46, NNZs: 843, Bias: 142949498.891051, T: 16181802, Avg. loss: 75716294332476889239338382000128.000000\n",
      "Total training time: 34.61 seconds.\n",
      "-- Epoch 4807\n",
      "Norm: 5411100471279.10, NNZs: 843, Bias: -14719627.211543, T: 16185169, Avg. loss: 75708422291713511283820236636160.000000\n",
      "Total training time: 34.62 seconds.\n",
      "-- Epoch 4808\n",
      "Norm: 5297186529661.52, NNZs: 843, Bias: -192061064.944267, T: 16188536, Avg. loss: 75700489268403493333235553796096.000000\n",
      "Total training time: 34.62 seconds.\n",
      "-- Epoch 4809\n",
      "Norm: 5392350555893.89, NNZs: 843, Bias: -349697019.120105, T: 16191903, Avg. loss: 75692468244343245052573126754304.000000\n",
      "Total training time: 34.63 seconds.\n",
      "-- Epoch 4810\n",
      "Norm: 5353621354192.92, NNZs: 843, Bias: -302136941.229084, T: 16195270, Avg. loss: 75684490804045877511200410959872.000000\n",
      "Total training time: 34.64 seconds.\n",
      "-- Epoch 4811\n",
      "Norm: 5449986296010.91, NNZs: 843, Bias: 170758011.792188, T: 16198637, Avg. loss: 75676638399991752059873512128512.000000\n",
      "Total training time: 34.65 seconds.\n",
      "-- Epoch 4812\n",
      "Norm: 5328825588299.51, NNZs: 843, Bias: -118534956.069350, T: 16202004, Avg. loss: 75668665044130546821137405313024.000000\n",
      "Total training time: 34.65 seconds.\n",
      "-- Epoch 4813\n",
      "Norm: 5550553889850.60, NNZs: 843, Bias: 383797709.951248, T: 16205371, Avg. loss: 75660715238448683996625412030464.000000\n",
      "Total training time: 34.66 seconds.\n",
      "-- Epoch 4814\n",
      "Norm: 5385087534271.73, NNZs: 843, Bias: -40027415.652555, T: 16208738, Avg. loss: 75652923357920660597494656270336.000000\n",
      "Total training time: 34.67 seconds.\n",
      "-- Epoch 4815\n",
      "Norm: 5426185329168.93, NNZs: 843, Bias: -135658087.167836, T: 16212105, Avg. loss: 75645037896862126152576676134912.000000\n",
      "Total training time: 34.67 seconds.\n",
      "-- Epoch 4816\n",
      "Norm: 5485942373398.38, NNZs: 843, Bias: -293238450.935597, T: 16215472, Avg. loss: 75637140483715915918227204472832.000000\n",
      "Total training time: 34.68 seconds.\n",
      "-- Epoch 4817\n",
      "Norm: 5481695496114.67, NNZs: 843, Bias: -135647015.678968, T: 16218839, Avg. loss: 75629365944420800655052047908864.000000\n",
      "Total training time: 34.69 seconds.\n",
      "-- Epoch 4818\n",
      "Norm: 5309335492967.78, NNZs: 843, Bias: 21927971.864736, T: 16222206, Avg. loss: 75621620340210004009865478930432.000000\n",
      "Total training time: 34.70 seconds.\n",
      "-- Epoch 4819\n",
      "Norm: 5463811877105.75, NNZs: 843, Bias: -80499815.773017, T: 16225573, Avg. loss: 75613744752888256550702596227072.000000\n",
      "Total training time: 34.70 seconds.\n",
      "-- Epoch 4820\n",
      "Norm: 5630395601128.38, NNZs: 843, Bias: 234974425.071788, T: 16228940, Avg. loss: 75605996180909924087702979870720.000000\n",
      "Total training time: 34.71 seconds.\n",
      "-- Epoch 4821\n",
      "Norm: 5447480367295.54, NNZs: 843, Bias: -237674862.598569, T: 16232307, Avg. loss: 75598103193677535681201880170496.000000\n",
      "Total training time: 34.72 seconds.\n",
      "-- Epoch 4822\n",
      "Norm: 5784709594778.73, NNZs: 843, Bias: 120553565.707043, T: 16235674, Avg. loss: 75590366853674688130412590399488.000000\n",
      "Total training time: 34.73 seconds.\n",
      "-- Epoch 4823\n",
      "Norm: 5332356338611.67, NNZs: 843, Bias: -138732454.157025, T: 16239041, Avg. loss: 75582651151678927251260267560960.000000\n",
      "Total training time: 34.73 seconds.\n",
      "-- Epoch 4824\n",
      "Norm: 5574536778588.73, NNZs: 843, Bias: 37334275.463649, T: 16242408, Avg. loss: 75574681792172662947033703776256.000000\n",
      "Total training time: 34.74 seconds.\n",
      "-- Epoch 4825\n",
      "Norm: 5429908194990.51, NNZs: 843, Bias: -120181606.487747, T: 16245775, Avg. loss: 75566910881928306526052386078720.000000\n",
      "Total training time: 34.75 seconds.\n",
      "-- Epoch 4826\n",
      "Norm: 5497983603841.07, NNZs: 843, Bias: 37327486.192691, T: 16249142, Avg. loss: 75559102225439285300958456709120.000000\n",
      "Total training time: 34.75 seconds.\n",
      "-- Epoch 4827\n",
      "Norm: 5371287715073.41, NNZs: 843, Bias: 52259996.841014, T: 16252509, Avg. loss: 75551348032459756157683240009728.000000\n",
      "Total training time: 34.76 seconds.\n",
      "-- Epoch 4828\n",
      "Norm: 5443570620677.37, NNZs: 843, Bias: -105231941.062252, T: 16255876, Avg. loss: 75543618520298064719983570583552.000000\n",
      "Total training time: 34.77 seconds.\n",
      "-- Epoch 4829\n",
      "Norm: 5524292843429.69, NNZs: 843, Bias: 20729446.308936, T: 16259243, Avg. loss: 75535930936985052777788594978816.000000\n",
      "Total training time: 34.78 seconds.\n",
      "-- Epoch 4830\n",
      "Norm: 5403359349308.01, NNZs: 843, Bias: -136745510.903962, T: 16262610, Avg. loss: 75527974871377980469810860916736.000000\n",
      "Total training time: 34.78 seconds.\n",
      "-- Epoch 4831\n",
      "Norm: 5586906736227.07, NNZs: 843, Bias: 20723082.078544, T: 16265977, Avg. loss: 75520232690682870038466293399552.000000\n",
      "Total training time: 34.79 seconds.\n",
      "-- Epoch 4832\n",
      "Norm: 5392583188706.52, NNZs: 843, Bias: -136735905.346716, T: 16269344, Avg. loss: 75512411645922361970751222841344.000000\n",
      "Total training time: 34.80 seconds.\n",
      "-- Epoch 4833\n",
      "Norm: 5698165322033.87, NNZs: 843, Bias: -284510886.797970, T: 16272711, Avg. loss: 75504693228293928081653762097152.000000\n",
      "Total training time: 34.80 seconds.\n",
      "-- Epoch 4834\n",
      "Norm: 5472955599340.92, NNZs: 843, Bias: -127058902.716073, T: 16276078, Avg. loss: 75497018148045698719240050180096.000000\n",
      "Total training time: 34.81 seconds.\n",
      "-- Epoch 4835\n",
      "Norm: 5510806157133.66, NNZs: 843, Bias: -284484307.842898, T: 16279445, Avg. loss: 75489175039321584536859459977216.000000\n",
      "Total training time: 34.82 seconds.\n",
      "-- Epoch 4836\n",
      "Norm: 5737034443664.36, NNZs: 843, Bias: 187797174.209507, T: 16282812, Avg. loss: 75481362231919523514250428416000.000000\n",
      "Total training time: 34.83 seconds.\n",
      "-- Epoch 4837\n",
      "Norm: 5493023465984.15, NNZs: 843, Bias: 30371043.323005, T: 16286179, Avg. loss: 75473457748308707620765564928000.000000\n",
      "Total training time: 34.83 seconds.\n",
      "-- Epoch 4838\n",
      "Norm: 5627424237574.44, NNZs: 843, Bias: -441851816.758650, T: 16289546, Avg. loss: 75465597152927012584520748630016.000000\n",
      "Total training time: 34.84 seconds.\n",
      "-- Epoch 4839\n",
      "Norm: 5472328705056.35, NNZs: 843, Bias: 30364397.521887, T: 16292913, Avg. loss: 75457688075244513011031389241344.000000\n",
      "Total training time: 34.85 seconds.\n",
      "-- Epoch 4840\n",
      "Norm: 5372983291853.30, NNZs: 843, Bias: -127029357.061294, T: 16296280, Avg. loss: 75449936192464694296175049703424.000000\n",
      "Total training time: 34.86 seconds.\n",
      "-- Epoch 4841\n",
      "Norm: 5311121641895.77, NNZs: 843, Bias: 55339242.420326, T: 16299647, Avg. loss: 75442177765102770149670147588096.000000\n",
      "Total training time: 34.86 seconds.\n",
      "-- Epoch 4842\n",
      "Norm: 5456808911932.02, NNZs: 843, Bias: -102038684.909151, T: 16303014, Avg. loss: 75434528538009450532427706400768.000000\n",
      "Total training time: 34.87 seconds.\n",
      "-- Epoch 4843\n",
      "Norm: 5338887475271.88, NNZs: 843, Bias: 55330923.931844, T: 16306381, Avg. loss: 75426737605571708683155314573312.000000\n",
      "Total training time: 34.88 seconds.\n",
      "-- Epoch 4844\n",
      "Norm: 5539152163568.74, NNZs: 843, Bias: 244325346.555004, T: 16309748, Avg. loss: 75418850270044122668502087630848.000000\n",
      "Total training time: 34.88 seconds.\n",
      "-- Epoch 4845\n",
      "Norm: 5358760599757.70, NNZs: 843, Bias: 86962263.038039, T: 16313115, Avg. loss: 75411165996741179415646017945600.000000\n",
      "Total training time: 34.89 seconds.\n",
      "-- Epoch 4846\n",
      "Norm: 5363509645774.60, NNZs: 843, Bias: 91960883.880903, T: 16316482, Avg. loss: 75403261017584506701409194868736.000000\n",
      "Total training time: 34.90 seconds.\n",
      "-- Epoch 4847\n",
      "Norm: 5374559485495.48, NNZs: 843, Bias: -65378611.488018, T: 16319849, Avg. loss: 75395411102628885519548599500800.000000\n",
      "Total training time: 34.91 seconds.\n",
      "-- Epoch 4848\n",
      "Norm: 5461908644541.55, NNZs: 843, Bias: -222701272.566797, T: 16323216, Avg. loss: 75387713156173203315622877855744.000000\n",
      "Total training time: 34.91 seconds.\n",
      "-- Epoch 4849\n",
      "Norm: 5853073255809.21, NNZs: 843, Bias: -65373404.522580, T: 16326583, Avg. loss: 75379895612216735330082735783936.000000\n",
      "Total training time: 34.92 seconds.\n",
      "-- Epoch 4850\n",
      "Norm: 5449334193402.91, NNZs: 843, Bias: 91938542.297222, T: 16329950, Avg. loss: 75372061836838291490418718670848.000000\n",
      "Total training time: 34.93 seconds.\n",
      "-- Epoch 4851\n",
      "Norm: 5572038591540.54, NNZs: 843, Bias: -65367867.882914, T: 16333317, Avg. loss: 75364283393295287418162354585600.000000\n",
      "Total training time: 34.93 seconds.\n",
      "-- Epoch 4852\n",
      "Norm: 5535518087886.96, NNZs: 843, Bias: 3169993.257499, T: 16336684, Avg. loss: 75356477592579268262150744834048.000000\n",
      "Total training time: 34.94 seconds.\n",
      "-- Epoch 4853\n",
      "Norm: 5530927799954.59, NNZs: 843, Bias: 160453255.266850, T: 16340051, Avg. loss: 75348663642825506842945215528960.000000\n",
      "Total training time: 34.95 seconds.\n",
      "-- Epoch 4854\n",
      "Norm: 5653064666022.73, NNZs: 843, Bias: 3166597.733911, T: 16343418, Avg. loss: 75340937771395686701739806818304.000000\n",
      "Total training time: 34.96 seconds.\n",
      "-- Epoch 4855\n",
      "Norm: 5557402086379.57, NNZs: 843, Bias: 90457292.742730, T: 16346785, Avg. loss: 75333019752596186468388228300800.000000\n",
      "Total training time: 34.96 seconds.\n",
      "-- Epoch 4856\n",
      "Norm: 5480567787906.37, NNZs: 843, Bias: -1509532.545985, T: 16350152, Avg. loss: 75325172049965661705128384659456.000000\n",
      "Total training time: 34.97 seconds.\n",
      "-- Epoch 4857\n",
      "Norm: 5463266974886.09, NNZs: 843, Bias: -158764341.196617, T: 16353519, Avg. loss: 75317400220522344927010068889600.000000\n",
      "Total training time: 34.98 seconds.\n",
      "-- Epoch 4858\n",
      "Norm: 5576295606629.48, NNZs: 843, Bias: -264612423.533859, T: 16356886, Avg. loss: 75309581555650608209615120236544.000000\n",
      "Total training time: 34.99 seconds.\n",
      "-- Epoch 4859\n",
      "Norm: 5612328681251.11, NNZs: 843, Bias: -45943371.009048, T: 16360253, Avg. loss: 75301816427653455828954931789824.000000\n",
      "Total training time: 34.99 seconds.\n",
      "-- Epoch 4860\n",
      "Norm: 5494461976368.84, NNZs: 843, Bias: 111285827.116777, T: 16363620, Avg. loss: 75294059140187049907071951568896.000000\n",
      "Total training time: 35.00 seconds.\n",
      "-- Epoch 4861\n",
      "Norm: 5714732366467.99, NNZs: 843, Bias: -360381449.629706, T: 16366987, Avg. loss: 75286327279009852435786733977600.000000\n",
      "Total training time: 35.01 seconds.\n",
      "-- Epoch 4862\n",
      "Norm: 5478404120639.99, NNZs: 843, Bias: 111271516.640171, T: 16370354, Avg. loss: 75278585725195021518535267450880.000000\n",
      "Total training time: 35.01 seconds.\n",
      "-- Epoch 4863\n",
      "Norm: 5592743336556.86, NNZs: 843, Bias: -360346663.570386, T: 16373721, Avg. loss: 75271002036216959929321795878912.000000\n",
      "Total training time: 35.02 seconds.\n",
      "-- Epoch 4864\n",
      "Norm: 5507739314403.11, NNZs: 843, Bias: -203133664.230138, T: 16377088, Avg. loss: 75263300499762023885666223390720.000000\n",
      "Total training time: 35.03 seconds.\n",
      "-- Epoch 4865\n",
      "Norm: 5543544193174.20, NNZs: 843, Bias: 201974468.846860, T: 16380455, Avg. loss: 75255653466002688511956200980480.000000\n",
      "Total training time: 35.04 seconds.\n",
      "-- Epoch 4866\n",
      "Norm: 5481601620055.62, NNZs: 843, Bias: -16979125.717318, T: 16383822, Avg. loss: 75247879045589506943645745414144.000000\n",
      "Total training time: 35.04 seconds.\n",
      "-- Epoch 4867\n",
      "Norm: 5685561228068.40, NNZs: 843, Bias: -488495482.021924, T: 16387189, Avg. loss: 75240120508830058780732566798336.000000\n",
      "Total training time: 35.05 seconds.\n",
      "-- Epoch 4868\n",
      "Norm: 5573312069706.00, NNZs: 843, Bias: -168612748.733385, T: 16390556, Avg. loss: 75232344998046345211684455448576.000000\n",
      "Total training time: 35.06 seconds.\n",
      "-- Epoch 4869\n",
      "Norm: 5611962406245.51, NNZs: 843, Bias: -56430315.138000, T: 16393923, Avg. loss: 75224659663855865062656776863744.000000\n",
      "Total training time: 35.06 seconds.\n",
      "-- Epoch 4870\n",
      "Norm: 5634073901295.00, NNZs: 843, Bias: -213575409.664967, T: 16397290, Avg. loss: 75217025014937183342636306530304.000000\n",
      "Total training time: 35.07 seconds.\n",
      "-- Epoch 4871\n",
      "Norm: 5768786968530.82, NNZs: 843, Bias: -56426368.990060, T: 16400657, Avg. loss: 75209390984679810895949584138240.000000\n",
      "Total training time: 35.08 seconds.\n",
      "-- Epoch 4872\n",
      "Norm: 5663127754334.35, NNZs: 843, Bias: 100707003.340651, T: 16404024, Avg. loss: 75201679009784648923110956859392.000000\n",
      "Total training time: 35.09 seconds.\n",
      "-- Epoch 4873\n",
      "Norm: 5728152084044.08, NNZs: 843, Bias: -56421974.757144, T: 16407391, Avg. loss: 75193926419363921513125936889856.000000\n",
      "Total training time: 35.09 seconds.\n",
      "-- Epoch 4874\n",
      "Norm: 5769806862572.13, NNZs: 843, Bias: 100695312.993135, T: 16410758, Avg. loss: 75186272945555315485881820774400.000000\n",
      "Total training time: 35.10 seconds.\n",
      "-- Epoch 4875\n",
      "Norm: 5608288458219.20, NNZs: 843, Bias: -56417166.305225, T: 16414125, Avg. loss: 75178635270972775700811485282304.000000\n",
      "Total training time: 35.11 seconds.\n",
      "-- Epoch 4876\n",
      "Norm: 5614658185572.21, NNZs: 843, Bias: -111264957.417180, T: 16417492, Avg. loss: 75171158315272534582193198989312.000000\n",
      "Total training time: 35.12 seconds.\n",
      "-- Epoch 4877\n",
      "Norm: 5796076559118.20, NNZs: 843, Bias: 45830773.450324, T: 16420859, Avg. loss: 75163500108614813149440774242304.000000\n",
      "Total training time: 35.12 seconds.\n",
      "-- Epoch 4878\n",
      "Norm: 5818442824970.94, NNZs: 843, Bias: 202909522.136361, T: 16424226, Avg. loss: 75155900023491439994573336608768.000000\n",
      "Total training time: 35.13 seconds.\n",
      "-- Epoch 4879\n",
      "Norm: 5742590863690.52, NNZs: 843, Bias: -213870099.975196, T: 16427593, Avg. loss: 75148066106362233912219976859648.000000\n",
      "Total training time: 35.14 seconds.\n",
      "-- Epoch 4880\n",
      "Norm: 5625806751860.80, NNZs: 843, Bias: 98003250.846803, T: 16430960, Avg. loss: 75140268665104246293149486415872.000000\n",
      "Total training time: 35.14 seconds.\n",
      "-- Epoch 4881\n",
      "Norm: 5726635538210.30, NNZs: 843, Bias: -59061896.335861, T: 16434327, Avg. loss: 75132601782666302006471407173632.000000\n",
      "Total training time: 35.15 seconds.\n",
      "-- Epoch 4882\n",
      "Norm: 5984297723594.71, NNZs: 843, Bias: -216110130.923320, T: 16437694, Avg. loss: 75124865263879871700302083129344.000000\n",
      "Total training time: 35.16 seconds.\n",
      "-- Epoch 4883\n",
      "Norm: 5876512870129.67, NNZs: 843, Bias: -59057451.149115, T: 16441061, Avg. loss: 75117161591858948092568614731776.000000\n",
      "Total training time: 35.17 seconds.\n",
      "-- Epoch 4884\n",
      "Norm: 5811251547945.19, NNZs: 843, Bias: -216090241.525002, T: 16444428, Avg. loss: 75109410359395411769501803347968.000000\n",
      "Total training time: 35.17 seconds.\n",
      "-- Epoch 4885\n",
      "Norm: 5905664718097.46, NNZs: 843, Bias: -59053714.935205, T: 16447795, Avg. loss: 75101661620759690749396345946112.000000\n",
      "Total training time: 35.18 seconds.\n",
      "-- Epoch 4886\n",
      "Norm: 5680381513312.28, NNZs: 843, Bias: 97967433.320947, T: 16451162, Avg. loss: 75093931382709180466406851870720.000000\n",
      "Total training time: 35.19 seconds.\n",
      "-- Epoch 4887\n",
      "Norm: 5938893042081.87, NNZs: 843, Bias: 254971691.629682, T: 16454529, Avg. loss: 75086334313441266740939821416448.000000\n",
      "Total training time: 35.19 seconds.\n",
      "-- Epoch 4888\n",
      "Norm: 5652280830695.84, NNZs: 843, Bias: 31836358.357537, T: 16457896, Avg. loss: 75078614383517705873302226993152.000000\n",
      "Total training time: 35.20 seconds.\n",
      "-- Epoch 4889\n",
      "Norm: 5610521831970.62, NNZs: 843, Bias: -125161696.313616, T: 16461263, Avg. loss: 75070821017480227689774882750464.000000\n",
      "Total training time: 35.21 seconds.\n",
      "-- Epoch 4890\n",
      "Norm: 5871689822540.01, NNZs: 843, Bias: 31829331.774906, T: 16464630, Avg. loss: 75063218227471589690117768347648.000000\n",
      "Total training time: 35.22 seconds.\n",
      "-- Epoch 4891\n",
      "Norm: 5907897049632.62, NNZs: 843, Bias: 275014602.379501, T: 16467997, Avg. loss: 75055678814520659313193094479872.000000\n",
      "Total training time: 35.22 seconds.\n",
      "-- Epoch 4892\n",
      "Norm: 6222135627794.99, NNZs: 843, Bias: -195911647.147033, T: 16471364, Avg. loss: 75048037305150418707879827603456.000000\n",
      "Total training time: 35.23 seconds.\n",
      "-- Epoch 4893\n",
      "Norm: 6002504043372.47, NNZs: 843, Bias: 51511055.912017, T: 16474731, Avg. loss: 75040202331788605311404258361344.000000\n",
      "Total training time: 35.24 seconds.\n",
      "-- Epoch 4894\n",
      "Norm: 6271860006405.75, NNZs: 843, Bias: -105447780.237370, T: 16478098, Avg. loss: 75032566336035749885742994685952.000000\n",
      "Total training time: 35.24 seconds.\n",
      "-- Epoch 4895\n",
      "Norm: 6294480722596.53, NNZs: 843, Bias: 51502152.429919, T: 16481465, Avg. loss: 75024898889940669942665435938816.000000\n",
      "Total training time: 35.25 seconds.\n",
      "-- Epoch 4896\n",
      "Norm: 5852360309231.32, NNZs: 843, Bias: 248111885.719253, T: 16484832, Avg. loss: 75017253740496452881623445143552.000000\n",
      "Total training time: 35.26 seconds.\n",
      "-- Epoch 4897\n",
      "Norm: 5868759343974.62, NNZs: 843, Bias: 91167744.534614, T: 16488199, Avg. loss: 75009682932700831936195927736320.000000\n",
      "Total training time: 35.27 seconds.\n",
      "-- Epoch 4898\n",
      "Norm: 6041809032914.70, NNZs: 843, Bias: -379604593.085449, T: 16491566, Avg. loss: 75002118848878876120292769398784.000000\n",
      "Total training time: 35.27 seconds.\n",
      "-- Epoch 4899\n",
      "Norm: 5708056601206.89, NNZs: 843, Bias: 91156285.748274, T: 16494933, Avg. loss: 74994387936396757600024754388992.000000\n",
      "Total training time: 35.28 seconds.\n",
      "-- Epoch 4900\n",
      "Norm: 5674856615192.38, NNZs: 843, Bias: -91545247.611694, T: 16498300, Avg. loss: 74986585514024897056964098392064.000000\n",
      "Total training time: 35.29 seconds.\n",
      "-- Epoch 4901\n",
      "Norm: 5746208705768.65, NNZs: 843, Bias: -5415421.714166, T: 16501667, Avg. loss: 74978916925404809196429570473984.000000\n",
      "Total training time: 35.30 seconds.\n",
      "-- Epoch 4902\n",
      "Norm: 5783462121989.63, NNZs: 843, Bias: -162306859.463336, T: 16505034, Avg. loss: 74971200664299323305423324315648.000000\n",
      "Total training time: 35.30 seconds.\n",
      "-- Epoch 4903\n",
      "Norm: 6042334247205.07, NNZs: 843, Bias: -319182550.532457, T: 16508401, Avg. loss: 74963538235884652108007191085056.000000\n",
      "Total training time: 35.31 seconds.\n",
      "-- Epoch 4904\n",
      "Norm: 5892528803658.10, NNZs: 843, Bias: 55929719.163119, T: 16511768, Avg. loss: 74955988659202831885947077918720.000000\n",
      "Total training time: 35.32 seconds.\n",
      "-- Epoch 4905\n",
      "Norm: 5717074053936.68, NNZs: 843, Bias: 118795424.890478, T: 16515135, Avg. loss: 74948155599816582739223239458816.000000\n",
      "Total training time: 35.32 seconds.\n",
      "-- Epoch 4906\n",
      "Norm: 5589055978863.49, NNZs: 843, Bias: -38070632.546499, T: 16518502, Avg. loss: 74940403965611396218809611190272.000000\n",
      "Total training time: 35.33 seconds.\n",
      "-- Epoch 4907\n",
      "Norm: 5622581091099.55, NNZs: 843, Bias: 118780286.039303, T: 16521869, Avg. loss: 74932887549291380265711348744192.000000\n",
      "Total training time: 35.34 seconds.\n",
      "-- Epoch 4908\n",
      "Norm: 5620433138586.62, NNZs: 843, Bias: -38069082.734880, T: 16525236, Avg. loss: 74925414629568866696798540922880.000000\n",
      "Total training time: 35.35 seconds.\n",
      "-- Epoch 4909\n",
      "Norm: 5608316705863.62, NNZs: 843, Bias: 118765197.708040, T: 16528603, Avg. loss: 74917892736333550727304132427776.000000\n",
      "Total training time: 35.35 seconds.\n",
      "-- Epoch 4910\n",
      "Norm: 5956274567582.25, NNZs: 843, Bias: -38068866.904147, T: 16531970, Avg. loss: 74910259173089191431787440504832.000000\n",
      "Total training time: 35.36 seconds.\n",
      "-- Epoch 4911\n",
      "Norm: 5687002863910.16, NNZs: 843, Bias: 94710965.302365, T: 16535337, Avg. loss: 74902606132079716157782677782528.000000\n",
      "Total training time: 35.37 seconds.\n",
      "-- Epoch 4912\n",
      "Norm: 5580262492997.89, NNZs: 843, Bias: -44245159.580191, T: 16538704, Avg. loss: 74895002428110218498786170241024.000000\n",
      "Total training time: 35.37 seconds.\n",
      "-- Epoch 4913\n",
      "Norm: 5788559785916.31, NNZs: 843, Bias: 112557947.199830, T: 16542071, Avg. loss: 74887452673642137658827219140608.000000\n",
      "Total training time: 35.38 seconds.\n",
      "-- Epoch 4914\n",
      "Norm: 6023943790081.82, NNZs: 843, Bias: -44243692.699277, T: 16545438, Avg. loss: 74879792466136937180137523249152.000000\n",
      "Total training time: 35.39 seconds.\n",
      "-- Epoch 4915\n",
      "Norm: 5871371272948.86, NNZs: 843, Bias: -156391709.850884, T: 16548805, Avg. loss: 74872188020634367571062496952320.000000\n",
      "Total training time: 35.40 seconds.\n",
      "-- Epoch 4916\n",
      "Norm: 5590579581909.49, NNZs: 843, Bias: -124780396.113094, T: 16552172, Avg. loss: 74864632340880218897071074181120.000000\n",
      "Total training time: 35.40 seconds.\n",
      "-- Epoch 4917\n",
      "Norm: 5867375840352.66, NNZs: 843, Bias: -281546625.728223, T: 16555539, Avg. loss: 74856998403161100225734902808576.000000\n",
      "Total training time: 35.41 seconds.\n",
      "-- Epoch 4918\n",
      "Norm: 5621174002010.76, NNZs: 843, Bias: -105067014.565221, T: 16558906, Avg. loss: 74849366361308049284936922824704.000000\n",
      "Total training time: 35.42 seconds.\n",
      "-- Epoch 4919\n",
      "Norm: 5592841052273.68, NNZs: 843, Bias: 51691458.010096, T: 16562273, Avg. loss: 74841699901999836214747901984768.000000\n",
      "Total training time: 35.43 seconds.\n",
      "-- Epoch 4920\n",
      "Norm: 5916236679675.43, NNZs: 843, Bias: -311581417.889318, T: 16565640, Avg. loss: 74834130798054304884131843014656.000000\n",
      "Total training time: 35.43 seconds.\n",
      "-- Epoch 4921\n",
      "Norm: 5922019964025.94, NNZs: 843, Bias: 158647988.625383, T: 16569007, Avg. loss: 74826460933467858391927210639360.000000\n",
      "Total training time: 35.44 seconds.\n",
      "-- Epoch 4922\n",
      "Norm: 5824126143303.38, NNZs: 843, Bias: -311553618.132681, T: 16572374, Avg. loss: 74818739450230673370035156680704.000000\n",
      "Total training time: 35.45 seconds.\n",
      "-- Epoch 4923\n",
      "Norm: 5808425882777.85, NNZs: 843, Bias: -154816764.264928, T: 16575741, Avg. loss: 74811197629085775927172755619840.000000\n",
      "Total training time: 35.45 seconds.\n",
      "-- Epoch 4924\n",
      "Norm: 5617175607045.34, NNZs: 843, Bias: -180940658.416288, T: 16579108, Avg. loss: 74803636917307740083104633585664.000000\n",
      "Total training time: 35.46 seconds.\n",
      "-- Epoch 4925\n",
      "Norm: 5692893325863.37, NNZs: 843, Bias: -24226695.500690, T: 16582475, Avg. loss: 74796103259803803932760391811072.000000\n",
      "Total training time: 35.47 seconds.\n",
      "-- Epoch 4926\n",
      "Norm: 5870891498713.83, NNZs: 843, Bias: -180925455.653323, T: 16585842, Avg. loss: 74788416794849484307552303841280.000000\n",
      "Total training time: 35.48 seconds.\n",
      "-- Epoch 4927\n",
      "Norm: 5742848851204.84, NNZs: 843, Bias: -24226962.359353, T: 16589209, Avg. loss: 74780820390786717313164642353152.000000\n",
      "Total training time: 35.48 seconds.\n",
      "-- Epoch 4928\n",
      "Norm: 5745515677752.84, NNZs: 843, Bias: 65315830.151548, T: 16592576, Avg. loss: 74773199547038888696986080378880.000000\n",
      "Total training time: 35.49 seconds.\n",
      "-- Epoch 4929\n",
      "Norm: 5897560302935.60, NNZs: 843, Bias: -91364033.322322, T: 16595943, Avg. loss: 74765496141780915803587101065216.000000\n",
      "Total training time: 35.50 seconds.\n",
      "-- Epoch 4930\n",
      "Norm: 5794231640159.25, NNZs: 843, Bias: 65306755.292968, T: 16599310, Avg. loss: 74757966757882259639156489584640.000000\n",
      "Total training time: 35.50 seconds.\n",
      "-- Epoch 4931\n",
      "Norm: 5708438796471.04, NNZs: 843, Bias: 147267931.349117, T: 16602677, Avg. loss: 74750291957233735124159467356160.000000\n",
      "Total training time: 35.51 seconds.\n",
      "-- Epoch 4932\n",
      "Norm: 5693272839017.80, NNZs: 843, Bias: -9392357.354595, T: 16606044, Avg. loss: 74742800644571976853063529922560.000000\n",
      "Total training time: 35.52 seconds.\n",
      "-- Epoch 4933\n",
      "Norm: 5651429084774.79, NNZs: 843, Bias: -237043003.156515, T: 16609411, Avg. loss: 74735172287010648632601543180288.000000\n",
      "Total training time: 35.53 seconds.\n",
      "-- Epoch 4934\n",
      "Norm: 5590209473727.89, NNZs: 843, Bias: -80397040.222132, T: 16612778, Avg. loss: 74727435410789899997480325480448.000000\n",
      "Total training time: 35.53 seconds.\n",
      "-- Epoch 4935\n",
      "Norm: 5600450347241.23, NNZs: 843, Bias: 118001909.100963, T: 16616145, Avg. loss: 74719798008148297576715942625280.000000\n",
      "Total training time: 35.54 seconds.\n",
      "-- Epoch 4936\n",
      "Norm: 5817154830403.54, NNZs: 843, Bias: -38624593.144749, T: 16619512, Avg. loss: 74712238188721850580746297671680.000000\n",
      "Total training time: 35.55 seconds.\n",
      "-- Epoch 4937\n",
      "Norm: 5576035680968.14, NNZs: 843, Bias: 117987427.128678, T: 16622879, Avg. loss: 74704506173704437034257632722944.000000\n",
      "Total training time: 35.56 seconds.\n",
      "-- Epoch 4938\n",
      "Norm: 5657629503772.40, NNZs: 843, Bias: -129140357.359336, T: 16626246, Avg. loss: 74696724199158847852065159380992.000000\n",
      "Total training time: 35.56 seconds.\n",
      "-- Epoch 4939\n",
      "Norm: 5637012677571.79, NNZs: 843, Bias: -78529765.307328, T: 16629613, Avg. loss: 74689198588530279843467574116352.000000\n",
      "Total training time: 35.57 seconds.\n",
      "-- Epoch 4940\n",
      "Norm: 5979503772535.83, NNZs: 843, Bias: -47293977.981759, T: 16632980, Avg. loss: 74681619261721235272371273728000.000000\n",
      "Total training time: 35.58 seconds.\n",
      "-- Epoch 4941\n",
      "Norm: 5674097265042.54, NNZs: 843, Bias: -146613690.173800, T: 16636347, Avg. loss: 74673997299444964662050975383552.000000\n",
      "Total training time: 35.58 seconds.\n",
      "-- Epoch 4942\n",
      "Norm: 5767155414228.08, NNZs: 843, Bias: 9963826.026425, T: 16639714, Avg. loss: 74666477128609867014123494047744.000000\n",
      "Total training time: 35.59 seconds.\n",
      "-- Epoch 4943\n",
      "Norm: 5635564392512.13, NNZs: 843, Bias: -146602165.944230, T: 16643081, Avg. loss: 74658929967780484103536438673408.000000\n",
      "Total training time: 35.60 seconds.\n",
      "-- Epoch 4944\n",
      "Norm: 5741251493725.32, NNZs: 843, Bias: -73121166.444999, T: 16646448, Avg. loss: 74651366828858757099037791354880.000000\n",
      "Total training time: 35.61 seconds.\n",
      "-- Epoch 4945\n",
      "Norm: 5684730624167.61, NNZs: 843, Bias: 130820112.694622, T: 16649815, Avg. loss: 74643784899957821148389957435392.000000\n",
      "Total training time: 35.61 seconds.\n",
      "-- Epoch 4946\n",
      "Norm: 5740821702795.63, NNZs: 843, Bias: -25728533.431017, T: 16653182, Avg. loss: 74636163677167060917564181839872.000000\n",
      "Total training time: 35.62 seconds.\n",
      "-- Epoch 4947\n",
      "Norm: 5719993774962.87, NNZs: 843, Bias: -182261168.844010, T: 16656549, Avg. loss: 74628637421045511674895341191168.000000\n",
      "Total training time: 35.63 seconds.\n",
      "-- Epoch 4948\n",
      "Norm: 5792439715822.51, NNZs: 843, Bias: 287318609.060187, T: 16659916, Avg. loss: 74621138329624172268453059624960.000000\n",
      "Total training time: 35.63 seconds.\n",
      "-- Epoch 4949\n",
      "Norm: 5864862566082.60, NNZs: 843, Bias: -182246679.026217, T: 16663283, Avg. loss: 74613696130674011187438428356608.000000\n",
      "Total training time: 35.64 seconds.\n",
      "-- Epoch 4950\n",
      "Norm: 5622893684925.00, NNZs: 843, Bias: -25730769.510458, T: 16666650, Avg. loss: 74606301298124531406458534756352.000000\n",
      "Total training time: 35.65 seconds.\n",
      "-- Epoch 4951\n",
      "Norm: 5860918660538.12, NNZs: 843, Bias: 130769685.691375, T: 16670017, Avg. loss: 74598643742486500584095381192704.000000\n",
      "Total training time: 35.66 seconds.\n",
      "-- Epoch 4952\n",
      "Norm: 5723974447427.83, NNZs: 843, Bias: -25731565.111213, T: 16673384, Avg. loss: 74591180171293930811146727587840.000000\n",
      "Total training time: 35.66 seconds.\n",
      "-- Epoch 4953\n",
      "Norm: 5721588413016.82, NNZs: 843, Bias: 4669003.430849, T: 16676751, Avg. loss: 74583631403686485805972643119104.000000\n",
      "Total training time: 35.67 seconds.\n",
      "-- Epoch 4954\n",
      "Norm: 5696512725113.52, NNZs: 843, Bias: 119066900.386615, T: 16680118, Avg. loss: 74576145826763204180147194298368.000000\n",
      "Total training time: 35.68 seconds.\n",
      "-- Epoch 4955\n",
      "Norm: 5744556414933.04, NNZs: 843, Bias: -37409375.787395, T: 16683485, Avg. loss: 74568604412403763337305197117440.000000\n",
      "Total training time: 35.68 seconds.\n",
      "-- Epoch 4956\n",
      "Norm: 5808823196906.33, NNZs: 843, Bias: -134803293.768858, T: 16686852, Avg. loss: 74561236022373334998088836710400.000000\n",
      "Total training time: 35.69 seconds.\n",
      "-- Epoch 4957\n",
      "Norm: 5629582564158.48, NNZs: 843, Bias: 21653917.035484, T: 16690219, Avg. loss: 74553660714980570778998300213248.000000\n",
      "Total training time: 35.70 seconds.\n",
      "-- Epoch 4958\n",
      "Norm: 5772480237286.94, NNZs: 843, Bias: -200539212.110895, T: 16693586, Avg. loss: 74546252983012683750283094786048.000000\n",
      "Total training time: 35.71 seconds.\n",
      "-- Epoch 4959\n",
      "Norm: 5781910537354.10, NNZs: 843, Bias: -44092696.920908, T: 16696953, Avg. loss: 74538692943927785622535189561344.000000\n",
      "Total training time: 35.71 seconds.\n",
      "-- Epoch 4960\n",
      "Norm: 5642782849719.74, NNZs: 843, Bias: 112337902.088992, T: 16700320, Avg. loss: 74531140971497380652801798963200.000000\n",
      "Total training time: 35.72 seconds.\n",
      "-- Epoch 4961\n",
      "Norm: 5775877213884.22, NNZs: 843, Bias: -44090970.999539, T: 16703687, Avg. loss: 74523730762379867828365652131840.000000\n",
      "Total training time: 35.73 seconds.\n",
      "-- Epoch 4962\n",
      "Norm: 5859396913923.13, NNZs: 843, Bias: 241455747.270704, T: 16707054, Avg. loss: 74516282206707738760084990197760.000000\n",
      "Total training time: 35.74 seconds.\n",
      "-- Epoch 4963\n",
      "Norm: 5756183201043.22, NNZs: 843, Bias: 85035959.955108, T: 16710421, Avg. loss: 74508694092285184467390680793088.000000\n",
      "Total training time: 35.74 seconds.\n",
      "-- Epoch 4964\n",
      "Norm: 5692383982707.80, NNZs: 843, Bias: -71368155.265818, T: 16713788, Avg. loss: 74501110320279570795907671654400.000000\n",
      "Total training time: 35.75 seconds.\n",
      "-- Epoch 4965\n",
      "Norm: 5599347155671.04, NNZs: 843, Bias: 85023830.330128, T: 16717155, Avg. loss: 74493528851392279556360209694720.000000\n",
      "Total training time: 35.76 seconds.\n",
      "-- Epoch 4966\n",
      "Norm: 5577266266951.67, NNZs: 843, Bias: -71363793.634639, T: 16720522, Avg. loss: 74486049618133985762940173680640.000000\n",
      "Total training time: 35.76 seconds.\n",
      "-- Epoch 4967\n",
      "Norm: 5658643885872.67, NNZs: 843, Bias: 85013282.737976, T: 16723889, Avg. loss: 74478626053102209670205201711104.000000\n",
      "Total training time: 35.77 seconds.\n",
      "-- Epoch 4968\n",
      "Norm: 5600713088761.51, NNZs: 843, Bias: 85715007.539938, T: 16727256, Avg. loss: 74471093892525524295544872108032.000000\n",
      "Total training time: 35.78 seconds.\n",
      "-- Epoch 4969\n",
      "Norm: 5709685499925.88, NNZs: 843, Bias: 242067961.082922, T: 16730623, Avg. loss: 74463533347474682802806833807360.000000\n",
      "Total training time: 35.79 seconds.\n",
      "-- Epoch 4970\n",
      "Norm: 5580517506645.78, NNZs: 843, Bias: -116733423.849742, T: 16733990, Avg. loss: 74456008485424672044541699686400.000000\n",
      "Total training time: 35.79 seconds.\n",
      "-- Epoch 4971\n",
      "Norm: 5594471672197.05, NNZs: 843, Bias: 39613747.791552, T: 16737357, Avg. loss: 74448489562167618877943967645696.000000\n",
      "Total training time: 35.80 seconds.\n",
      "-- Epoch 4972\n",
      "Norm: 5734439554243.47, NNZs: 843, Bias: -251206517.409604, T: 16740724, Avg. loss: 74440929953181304015679716851712.000000\n",
      "Total training time: 35.81 seconds.\n",
      "-- Epoch 4973\n",
      "Norm: 5666051069191.99, NNZs: 843, Bias: 100572249.125986, T: 16744091, Avg. loss: 74433400523505620287657349218304.000000\n",
      "Total training time: 35.82 seconds.\n",
      "-- Epoch 4974\n",
      "Norm: 5667096815429.22, NNZs: 843, Bias: -55753874.472331, T: 16747458, Avg. loss: 74425892984195935954630885769216.000000\n",
      "Total training time: 35.82 seconds.\n",
      "-- Epoch 4975\n",
      "Norm: 5761918728674.37, NNZs: 843, Bias: -212063909.028779, T: 16750825, Avg. loss: 74418403819172010005583930327040.000000\n",
      "Total training time: 35.83 seconds.\n",
      "-- Epoch 4976\n",
      "Norm: 5783444550518.70, NNZs: 843, Bias: 226188404.134124, T: 16754192, Avg. loss: 74410849994742939547707212038144.000000\n",
      "Total training time: 35.84 seconds.\n",
      "-- Epoch 4977\n",
      "Norm: 5948090796409.48, NNZs: 843, Bias: -242712312.579337, T: 16757559, Avg. loss: 74403342039851061577705630728192.000000\n",
      "Total training time: 35.84 seconds.\n",
      "-- Epoch 4978\n",
      "Norm: 5937473689241.21, NNZs: 843, Bias: -60545933.262532, T: 16760926, Avg. loss: 74395937278866766889505762836480.000000\n",
      "Total training time: 35.85 seconds.\n",
      "-- Epoch 4979\n",
      "Norm: 5907915039269.70, NNZs: 843, Bias: -122700275.049560, T: 16764293, Avg. loss: 74388452439307998377066937450496.000000\n",
      "Total training time: 35.86 seconds.\n",
      "-- Epoch 4980\n",
      "Norm: 5970222061674.65, NNZs: 843, Bias: 33575871.381538, T: 16767660, Avg. loss: 74380921380355881104781141344256.000000\n",
      "Total training time: 35.87 seconds.\n",
      "-- Epoch 4981\n",
      "Norm: 5639533993877.42, NNZs: 843, Bias: -19647575.453175, T: 16771027, Avg. loss: 74373510670209721186841136201728.000000\n",
      "Total training time: 35.87 seconds.\n",
      "-- Epoch 4982\n",
      "Norm: 5864465986883.67, NNZs: 843, Bias: -175906110.070152, T: 16774394, Avg. loss: 74366040347077051332721326948352.000000\n",
      "Total training time: 35.88 seconds.\n",
      "-- Epoch 4983\n",
      "Norm: 5582957108997.71, NNZs: 843, Bias: 71532425.232413, T: 16777761, Avg. loss: 74358579010992793097281753579520.000000\n",
      "Total training time: 35.89 seconds.\n",
      "-- Epoch 4984\n",
      "Norm: 5688740969423.00, NNZs: 843, Bias: -159890879.660157, T: 16781128, Avg. loss: 74351053718575343182978201157632.000000\n",
      "Total training time: 35.89 seconds.\n",
      "-- Epoch 4985\n",
      "Norm: 5845698763707.23, NNZs: 843, Bias: 308813818.422682, T: 16784495, Avg. loss: 74343667922669076045986306981888.000000\n",
      "Total training time: 35.90 seconds.\n",
      "-- Epoch 4986\n",
      "Norm: 5613865850773.52, NNZs: 843, Bias: -27507581.184049, T: 16787862, Avg. loss: 74336320697873234634607941386240.000000\n",
      "Total training time: 35.91 seconds.\n",
      "-- Epoch 4987\n",
      "Norm: 5776772424115.73, NNZs: 843, Bias: 6675458.356821, T: 16791229, Avg. loss: 74328830664372337290067803373568.000000\n",
      "Total training time: 35.92 seconds.\n",
      "-- Epoch 4988\n",
      "Norm: 5745771420531.81, NNZs: 843, Bias: -149537099.510304, T: 16794596, Avg. loss: 74321442978270781950935315775488.000000\n",
      "Total training time: 35.92 seconds.\n",
      "-- Epoch 4989\n",
      "Norm: 5753303382507.18, NNZs: 843, Bias: 6669861.379647, T: 16797963, Avg. loss: 74313973442965269883704188076032.000000\n",
      "Total training time: 35.93 seconds.\n",
      "-- Epoch 4990\n",
      "Norm: 5909663620583.27, NNZs: 843, Bias: -149527035.739886, T: 16801330, Avg. loss: 74306493974105674191223279058944.000000\n",
      "Total training time: 35.94 seconds.\n",
      "-- Epoch 4991\n",
      "Norm: 5874809270887.05, NNZs: 843, Bias: 6664411.189434, T: 16804697, Avg. loss: 74299031291431661609119563907072.000000\n",
      "Total training time: 35.95 seconds.\n",
      "-- Epoch 4992\n",
      "Norm: 5727778131110.92, NNZs: 843, Bias: 112504657.452862, T: 16808064, Avg. loss: 74291641928805436257460262273024.000000\n",
      "Total training time: 35.95 seconds.\n",
      "-- Epoch 4993\n",
      "Norm: 5787851872576.26, NNZs: 843, Bias: -171649885.586665, T: 16811431, Avg. loss: 74284130087152462284313749094400.000000\n",
      "Total training time: 35.96 seconds.\n",
      "-- Epoch 4994\n",
      "Norm: 6098385915200.68, NNZs: 843, Bias: -317555223.345856, T: 16814798, Avg. loss: 74276689071359640642821902303232.000000\n",
      "Total training time: 35.97 seconds.\n",
      "-- Epoch 4995\n",
      "Norm: 5957935615446.20, NNZs: 843, Bias: 150923015.989515, T: 16818165, Avg. loss: 74269150363865165976168359264256.000000\n",
      "Total training time: 35.97 seconds.\n",
      "-- Epoch 4996\n",
      "Norm: 5924993766120.10, NNZs: 843, Bias: 307059883.173284, T: 16821532, Avg. loss: 74261788300427326879534004830208.000000\n",
      "Total training time: 35.98 seconds.\n",
      "-- Epoch 4997\n",
      "Norm: 5660287708004.94, NNZs: 843, Bias: 150902902.962063, T: 16824899, Avg. loss: 74254265012839024470522998030336.000000\n",
      "Total training time: 35.99 seconds.\n",
      "-- Epoch 4998\n",
      "Norm: 5967034671590.14, NNZs: 843, Bias: 111413970.638501, T: 16828266, Avg. loss: 74246840932686892093437414211584.000000\n",
      "Total training time: 36.00 seconds.\n",
      "-- Epoch 4999\n",
      "Norm: 5728412613815.94, NNZs: 843, Bias: 267530187.160831, T: 16831633, Avg. loss: 74239334393198933106790462652416.000000\n",
      "Total training time: 36.00 seconds.\n",
      "-- Epoch 5000\n",
      "Norm: 5651464508599.92, NNZs: 843, Bias: -122381970.931700, T: 16835000, Avg. loss: 74231801789181390282591194578944.000000\n",
      "Total training time: 36.01 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-5.7311666994651513e+28"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "model_sgd = SGDRegressor(n_iter=5000, verbose=2)\n",
    "model_sgd.fit(X_train, Y_train)\n",
    "model_sgd.score(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "#from sklearn.svm import SVR\n",
    "\n",
    "model_svr = SVR(kernel='linear', degree=3, verbose=2)\n",
    "model_svr.fit(X_train, Y_train)\n",
    "model_svr.score(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "          metric_params=None, n_jobs=-1, n_neighbors=3, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model_knr = KNeighborsRegressor(n_neighbors=3, n_jobs=-1)\n",
    "model_knr.fit(X_train, Y_train)\n",
    "#model_knr.score(X_val, Y_val)\n",
    "#model_knr.fit(data_train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  78.3       ,  102.12      ,  112.53      , ...,   99.25666667,\n",
       "         93.52666667,   94.35333333])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_knr.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.42453774095102292"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "model_abr = AdaBoostRegressor(n_estimators=200)\n",
    "#model_abr.fit(X_train, Y_train)\n",
    "model_abr.score(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 200building tree 3 of 200building tree 4 of 200building tree 5 of 200building tree 6 of 200building tree 7 of 200building tree 8 of 200building tree 9 of 200building tree 10 of 200building tree 11 of 200building tree 12 of 200building tree 13 of 200\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "building tree 14 of 200\n",
      "building tree 15 of 200\n",
      "building tree 16 of 200\n",
      "building tree 1 of 200\n",
      "\n",
      "building tree 17 of 200\n",
      "building tree 18 of 200\n",
      "building tree 19 of 200\n",
      "building tree 20 of 200\n",
      "building tree 21 of 200\n",
      "building tree 22 of 200\n",
      "building tree 23 of 200\n",
      "building tree 24 of 200\n",
      "building tree 25 of 200\n",
      "building tree 26 of 200\n",
      "building tree 27 of 200\n",
      "building tree 28 of 200building tree 29 of 200\n",
      "\n",
      "building tree 30 of 200\n",
      "building tree 31 of 200\n",
      "building tree 32 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 33 of 200\n",
      "building tree 34 of 200\n",
      "building tree 35 of 200\n",
      "building tree 36 of 200\n",
      "building tree 37 of 200\n",
      "building tree 38 of 200\n",
      "building tree 39 of 200\n",
      "building tree 40 of 200\n",
      "building tree 41 of 200\n",
      "building tree 42 of 200\n",
      "building tree 43 of 200\n",
      "building tree 44 of 200\n",
      "building tree 45 of 200\n",
      "building tree 46 of 200\n",
      "building tree 47 of 200\n",
      "building tree 48 of 200\n",
      "building tree 49 of 200\n",
      "building tree 50 of 200\n",
      "building tree 51 of 200\n",
      "building tree 52 of 200\n",
      "building tree 53 of 200\n",
      "building tree 54 of 200\n",
      "building tree 55 of 200\n",
      "building tree 56 of 200\n",
      "building tree 57 of 200\n",
      "building tree 58 of 200\n",
      "building tree 59 of 200\n",
      "building tree 60 of 200\n",
      "building tree 61 of 200\n",
      "building tree 62 of 200\n",
      "building tree 63 of 200\n",
      "building tree 64 of 200\n",
      "building tree 65 of 200\n",
      "building tree 66 of 200\n",
      "building tree 67 of 200\n",
      "building tree 68 of 200\n",
      "building tree 69 of 200\n",
      "building tree 70 of 200\n",
      "building tree 71 of 200\n",
      "building tree 72 of 200\n",
      "building tree 73 of 200\n",
      "building tree 74 of 200\n",
      "building tree 75 of 200\n",
      "building tree 76 of 200\n",
      "building tree 77 of 200\n",
      "building tree 78 of 200\n",
      "building tree 79 of 200\n",
      "building tree 80 of 200\n",
      "building tree 81 of 200\n",
      "building tree 82 of 200\n",
      "building tree 83 of 200\n",
      "building tree 84 of 200\n",
      "building tree 85 of 200\n",
      "building tree 86 of 200\n",
      "building tree 87 of 200\n",
      "building tree 88 of 200\n",
      "building tree 89 of 200\n",
      "building tree 91 of 200building tree 90 of 200\n",
      "\n",
      "building tree 92 of 200\n",
      "building tree 93 of 200\n",
      "building tree 94 of 200\n",
      "building tree 95 of 200\n",
      "building tree 96 of 200\n",
      "building tree 97 of 200\n",
      "building tree 98 of 200\n",
      "building tree 99 of 200\n",
      "building tree 100 of 200\n",
      "building tree 101 of 200\n",
      "building tree 102 of 200\n",
      "building tree 103 of 200\n",
      "building tree 104 of 200\n",
      "building tree 105 of 200\n",
      "building tree 106 of 200\n",
      "building tree 107 of 200\n",
      "building tree 108 of 200\n",
      "building tree 109 of 200\n",
      "building tree 110 of 200\n",
      "building tree 111 of 200\n",
      "building tree 112 of 200\n",
      "building tree 113 of 200\n",
      "building tree 114 of 200\n",
      "building tree 115 of 200\n",
      "building tree 116 of 200\n",
      "building tree 117 of 200\n",
      "building tree 118 of 200\n",
      "building tree 119 of 200\n",
      "building tree 120 of 200\n",
      "building tree 121 of 200\n",
      "building tree 122 of 200\n",
      "building tree 123 of 200\n",
      "building tree 124 of 200\n",
      "building tree 125 of 200\n",
      "building tree 126 of 200\n",
      "building tree 127 of 200\n",
      "building tree 128 of 200\n",
      "building tree 129 of 200\n",
      "building tree 130 of 200\n",
      "building tree 131 of 200\n",
      "building tree 132 of 200\n",
      "building tree 133 of 200\n",
      "building tree 134 of 200\n",
      "building tree 135 of 200\n",
      "building tree 136 of 200\n",
      "building tree 137 of 200\n",
      "building tree 138 of 200\n",
      "building tree 139 of 200\n",
      "building tree 140 of 200\n",
      "building tree 141 of 200\n",
      "building tree 142 of 200\n",
      "building tree 143 of 200\n",
      "building tree 144 of 200\n",
      "building tree 145 of 200\n",
      "building tree 146 of 200\n",
      "building tree 147 of 200\n",
      "building tree 148 of 200\n",
      "building tree 149 of 200\n",
      "building tree 150 of 200\n",
      "building tree 151 of 200\n",
      "building tree 152 of 200\n",
      "building tree 153 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 154 of 200\n",
      "building tree 155 of 200\n",
      "building tree 156 of 200\n",
      "building tree 157 of 200\n",
      "building tree 158 of 200\n",
      "building tree 159 of 200\n",
      "building tree 160 of 200\n",
      "building tree 161 of 200\n",
      "building tree 162 of 200\n",
      "building tree 163 of 200\n",
      "building tree 164 of 200\n",
      "building tree 165 of 200\n",
      "building tree 166 of 200\n",
      "building tree 167 of 200\n",
      "building tree 168 of 200\n",
      "building tree 169 of 200\n",
      "building tree 170 of 200\n",
      "building tree 171 of 200\n",
      "building tree 172 of 200\n",
      "building tree 173 of 200\n",
      "building tree 174 of 200\n",
      "building tree 175 of 200\n",
      "building tree 176 of 200\n",
      "building tree 177 of 200\n",
      "building tree 178 of 200\n",
      "building tree 179 of 200\n",
      "building tree 180 of 200\n",
      "building tree 181 of 200\n",
      "building tree 182 of 200\n",
      "building tree 183 of 200\n",
      "building tree 184 of 200\n",
      "building tree 185 of 200\n",
      "building tree 186 of 200\n",
      "building tree 187 of 200\n",
      "building tree 188 of 200\n",
      "building tree 189 of 200\n",
      "building tree 190 of 200\n",
      "building tree 191 of 200\n",
      "building tree 192 of 200\n",
      "building tree 193 of 200\n",
      "building tree 194 of 200\n",
      "building tree 195 of 200\n",
      "building tree 196 of 200building tree 197 of 200\n",
      "\n",
      "building tree 198 of 200\n",
      "building tree 199 of 200\n",
      "building tree 200 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    8.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
       "          max_features='auto', max_leaf_nodes=None,\n",
       "          min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "          n_estimators=200, n_jobs=-1, oob_score=False, random_state=None,\n",
       "          verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "model_etr = ExtraTreesRegressor(n_estimators=200, verbose=2, n_jobs=-1)\n",
    "model_etr.fit(X_train, Y_train)\n",
    "#model_etr.score(X_val, Y_val)\n",
    "#model_etr.fit(data_train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          64.2795           6.5887            1.62m\n",
      "         2          56.6342           7.3414            1.61m\n",
      "         3          50.2029           6.2425            1.58m\n",
      "         4          46.1008           4.2766            1.58m\n",
      "         5          40.1591           4.1000            1.59m\n",
      "         6          37.5173           3.3777            1.58m\n",
      "         7          33.3692           2.9817            1.58m\n",
      "         8          30.5172           2.1287            1.55m\n",
      "         9          28.0382           2.1503            1.56m\n",
      "        10          26.5677           1.5151            1.55m\n",
      "        11          24.4955           1.2944            1.56m\n",
      "        12          23.4937           1.0828            1.55m\n",
      "        13          23.2530           0.7671            1.54m\n",
      "        14          21.3093           0.6464            1.54m\n",
      "        15          20.3541           0.6639            1.53m\n",
      "        16          19.8502           0.5575            1.53m\n",
      "        17          20.6439           0.3483            1.52m\n",
      "        18          19.8190           0.3705            1.52m\n",
      "        19          18.6398           0.1991            1.52m\n",
      "        20          18.0650           0.2044            1.51m\n",
      "        21          17.5153           0.1606            1.51m\n",
      "        22          18.1067           0.1451            1.50m\n",
      "        23          17.8079           0.1073            1.50m\n",
      "        24          17.3326           0.0604            1.49m\n",
      "        25          17.4519           0.1224            1.49m\n",
      "        26          16.2027           0.0002            1.49m\n",
      "        27          17.0356           0.0643            1.49m\n",
      "        28          16.5775           0.0782            1.48m\n",
      "        29          16.1149           0.1168            1.48m\n",
      "        30          15.7808           0.0623            1.47m\n",
      "        31          16.0056          -0.0468            1.46m\n",
      "        32          15.4980           0.0604            1.46m\n",
      "        33          15.1267           0.0526            1.45m\n",
      "        34          15.1013           0.0050            1.46m\n",
      "        35          15.0453           0.0710            1.45m\n",
      "        36          14.7995          -0.0126            1.44m\n",
      "        37          14.9282           0.0661            1.44m\n",
      "        38          14.1895          -0.0494            1.43m\n",
      "        39          14.6666           0.0082            1.42m\n",
      "        40          14.0573          -0.0046            1.41m\n",
      "        41          14.1937           0.0039            1.40m\n",
      "        42          14.0579          -0.0204            1.40m\n",
      "        43          13.6063           0.1044            1.39m\n",
      "        44          14.0559          -0.0487            1.38m\n",
      "        45          14.0523           0.0112            1.37m\n",
      "        46          14.3008           0.0002            1.36m\n",
      "        47          14.3777           0.0191            1.35m\n",
      "        48          13.5656          -0.0380            1.35m\n",
      "        49          13.7327          -0.0525            1.34m\n",
      "        50          13.6344           0.0475            1.33m\n",
      "        51          13.3661           0.0210            1.33m\n",
      "        52          12.9577           0.0659            1.32m\n",
      "        53          13.4485          -0.0557            1.31m\n",
      "        54          13.8914          -0.0150            1.31m\n",
      "        55          12.7615          -0.0016            1.31m\n",
      "        56          13.4130           0.0247            1.30m\n",
      "        57          12.9988          -0.0306            1.29m\n",
      "        58          12.8514           0.1034            1.29m\n",
      "        59          12.5883           0.0119            1.29m\n",
      "        60          13.0117           0.0002            1.28m\n",
      "        61          12.3724          -0.0554            1.28m\n",
      "        62          12.3590           0.0386            1.27m\n",
      "        63          12.2353           0.0136            1.27m\n",
      "        64          11.9407          -0.0245            1.26m\n",
      "        65          11.9043           0.0203            1.26m\n",
      "        66          12.8099          -0.0401            1.26m\n",
      "        67          12.1121          -0.0132            1.26m\n",
      "        68          11.7993           0.1147            1.25m\n",
      "        69          11.9959          -0.0174            1.24m\n",
      "        70          11.9589          -0.0463            1.24m\n",
      "        71          11.3326          -0.0306            1.24m\n",
      "        72          11.6316          -0.0256            1.23m\n",
      "        73          11.5273          -0.0344            1.23m\n",
      "        74          11.8939           0.0736            1.23m\n",
      "        75          10.9062          -0.0600            1.22m\n",
      "        76          10.9300           0.0345            1.22m\n",
      "        77          10.4339          -0.0038            1.22m\n",
      "        78          10.6510          -0.0111            1.22m\n",
      "        79          10.7122           0.0145            1.21m\n",
      "        80          10.1266           0.0019            1.21m\n",
      "        81          11.3219          -0.0105            1.21m\n",
      "        82          10.9930           0.0388            1.20m\n",
      "        83          10.4313           0.0298            1.20m\n",
      "        84          10.7443          -0.0474            1.20m\n",
      "        85          10.6698           0.0096            1.19m\n",
      "        86          10.5874          -0.0272            1.19m\n",
      "        87           9.9423          -0.0007            1.19m\n",
      "        88           9.8130          -0.0026            1.19m\n",
      "        89           9.9532          -0.0244            1.19m\n",
      "        90           9.8011           0.0001            1.18m\n",
      "        91           9.7421          -0.0204            1.18m\n",
      "        92          10.1348          -0.0203            1.17m\n",
      "        93           9.7306          -0.0430            1.17m\n",
      "        94          10.0511           0.0271            1.17m\n",
      "        95          10.1137          -0.0453            1.16m\n",
      "        96           9.5055          -0.0075            1.16m\n",
      "        97           9.5428           0.0111            1.15m\n",
      "        98           9.4123          -0.0075            1.15m\n",
      "        99           9.4808           0.0591            1.15m\n",
      "       100           9.6641           0.0194            1.14m\n",
      "       101           9.4541           0.0010            1.14m\n",
      "       102           9.2574           0.0312            1.14m\n",
      "       103           9.5426          -0.0038            1.13m\n",
      "       104           9.4377          -0.0375            1.13m\n",
      "       105           9.5321          -0.0067            1.13m\n",
      "       106           9.1114           0.0293            1.13m\n",
      "       107           9.2477          -0.0053            1.12m\n",
      "       108           9.5662          -0.0218            1.12m\n",
      "       109           8.9226          -0.0006            1.11m\n",
      "       110           9.1575          -0.0327            1.11m\n",
      "       111           8.8846           0.0015            1.11m\n",
      "       112           8.8079          -0.0492            1.11m\n",
      "       113           9.0294          -0.0027            1.10m\n",
      "       114           8.4965           0.0012            1.10m\n",
      "       115           8.9582           0.0395            1.10m\n",
      "       116           8.6672          -0.0157            1.09m\n",
      "       117           8.6195           0.0015            1.09m\n",
      "       118           8.3677          -0.0266            1.08m\n",
      "       119           8.2688           0.0104            1.08m\n",
      "       120           8.2544          -0.0172            1.08m\n",
      "       121           8.3683          -0.0049            1.07m\n",
      "       122           8.6452          -0.0017            1.07m\n",
      "       123           8.2996          -0.0039            1.07m\n",
      "       124           8.5897           0.0151            1.06m\n",
      "       125           8.2859           0.0176            1.06m\n",
      "       126           8.0704           0.0151            1.06m\n",
      "       127           8.2794          -0.0268            1.05m\n",
      "       128           7.6756           0.0049            1.05m\n",
      "       129           8.2179          -0.0178            1.05m\n",
      "       130           7.3053           0.0809            1.05m\n",
      "       131           7.9615           0.0129            1.05m\n",
      "       132           7.7212           0.0322            1.04m\n",
      "       133           7.5208          -0.0092            1.04m\n",
      "       134           7.6637          -0.0023            1.03m\n",
      "       135           7.6046          -0.0100            1.03m\n",
      "       136           7.4818          -0.0016            1.03m\n",
      "       137           7.5227          -0.0276            1.02m\n",
      "       138           7.3210           0.0120            1.02m\n",
      "       139           7.2727          -0.0431            1.02m\n",
      "       140           7.2966           0.0790            1.02m\n",
      "       141           7.4102          -0.0080            1.01m\n",
      "       142           7.3972          -0.0475            1.01m\n",
      "       143           7.5221           0.0105            1.00m\n",
      "       144           7.2684          -0.0008            1.00m\n",
      "       145           6.9499          -0.0018           59.98s\n",
      "       146           7.2500           0.0322           59.70s\n",
      "       147           7.3322          -0.0239           59.60s\n",
      "       148           7.0558          -0.0060           59.39s\n",
      "       149           7.0005           0.0012           59.23s\n",
      "       150           7.5888          -0.0163           59.07s\n",
      "       151           7.3520           0.0056           58.83s\n",
      "       152           6.5587          -0.0245           58.69s\n",
      "       153           6.6540          -0.0043           58.52s\n",
      "       154           6.7514           0.0233           58.30s\n",
      "       155           6.9133          -0.0182           58.18s\n",
      "       156           6.6502          -0.0147           58.01s\n",
      "       157           6.5296           0.0644           57.84s\n",
      "       158           6.7014          -0.0167           57.71s\n",
      "       159           6.6014           0.0013           57.60s\n",
      "       160           6.4770           0.0251           57.47s\n",
      "       161           6.5508           0.0072           57.34s\n",
      "       162           6.3608           0.0042           57.15s\n",
      "       163           6.3791           0.0078           56.93s\n",
      "       164           6.4779           0.0037           56.65s\n",
      "       165           6.6615          -0.0116           56.49s\n",
      "       166           6.1343          -0.0126           56.33s\n",
      "       167           5.6780          -0.0392           56.14s\n",
      "       168           6.3694          -0.0035           55.99s\n",
      "       169           6.4398           0.0245           55.81s\n",
      "       170           5.9562          -0.0047           55.68s\n",
      "       171           5.7058          -0.0092           55.50s\n",
      "       172           6.0372           0.0261           55.29s\n",
      "       173           5.8638          -0.0378           55.06s\n",
      "       174           5.6017          -0.0278           54.87s\n",
      "       175           6.2804           0.0064           54.64s\n",
      "       176           6.0656           0.0108           54.54s\n",
      "       177           6.0936          -0.0004           54.35s\n",
      "       178           5.9098           0.0156           54.18s\n",
      "       179           5.8472          -0.0246           53.95s\n",
      "       180           5.8033           0.0101           53.79s\n",
      "       181           5.6792          -0.0100           53.64s\n",
      "       182           5.9344          -0.0085           53.50s\n",
      "       183           5.4443          -0.0345           53.33s\n",
      "       184           5.6984          -0.0034           53.21s\n",
      "       185           5.5688           0.0512           53.07s\n",
      "       186           5.6321           0.0110           52.89s\n",
      "       187           5.4944          -0.0122           52.70s\n",
      "       188           5.8249          -0.0177           52.50s\n",
      "       189           5.7669          -0.0094           52.29s\n",
      "       190           5.2209           0.0172           52.17s\n",
      "       191           5.3802           0.0266           51.96s\n",
      "       192           5.6335          -0.0186           51.77s\n",
      "       193           5.9293           0.0124           51.60s\n",
      "       194           5.4358          -0.0173           51.47s\n",
      "       195           5.3697           0.0088           51.29s\n",
      "       196           5.4353           0.0019           51.14s\n",
      "       197           5.1798           0.0072           50.98s\n",
      "       198           4.7899           0.0259           50.83s\n",
      "       199           5.3233          -0.0433           50.69s\n",
      "       200           5.5635          -0.0200           50.50s\n",
      "       201           5.5308          -0.0075           50.33s\n",
      "       202           5.2883           0.0030           50.11s\n",
      "       203           5.3311          -0.0057           49.95s\n",
      "       204           5.2819           0.0352           49.79s\n",
      "       205           5.1442           0.0188           49.64s\n",
      "       206           4.8178          -0.0107           49.42s\n",
      "       207           5.1485          -0.0065           49.21s\n",
      "       208           4.8290           0.0434           49.03s\n",
      "       209           5.2991          -0.0024           48.82s\n",
      "       210           4.6204          -0.0284           48.68s\n",
      "       211           5.3290          -0.0114           48.53s\n",
      "       212           5.1389           0.0011           48.36s\n",
      "       213           4.9915          -0.0001           48.19s\n",
      "       214           5.1774          -0.0041           47.99s\n",
      "       215           4.9532          -0.0207           47.82s\n",
      "       216           5.0506          -0.0086           47.61s\n",
      "       217           5.1063           0.0096           47.39s\n",
      "       218           4.7188           0.0057           47.17s\n",
      "       219           4.8683          -0.0069           46.95s\n",
      "       220           5.0117          -0.0103           46.79s\n",
      "       221           4.9528           0.0017           46.59s\n",
      "       222           4.7445          -0.0059           46.39s\n",
      "       223           5.1198          -0.0147           46.19s\n",
      "       224           4.9581           0.0005           46.04s\n",
      "       225           4.7909          -0.0051           45.89s\n",
      "       226           4.8401           0.0117           45.71s\n",
      "       227           4.8390          -0.0122           45.54s\n",
      "       228           4.7807          -0.0173           45.41s\n",
      "       229           4.7758           0.0231           45.21s\n",
      "       230           4.6815          -0.0100           45.02s\n",
      "       231           4.7118           0.0089           44.85s\n",
      "       232           4.4359          -0.0082           44.65s\n",
      "       233           4.6082          -0.0049           44.53s\n",
      "       234           4.8076           0.0138           44.38s\n",
      "       235           4.6267          -0.0076           44.22s\n",
      "       236           4.5998           0.0115           44.04s\n",
      "       237           4.2243          -0.0149           43.84s\n",
      "       238           4.7870          -0.0006           43.63s\n",
      "       239           4.7071          -0.0012           43.45s\n",
      "       240           4.3951           0.0009           43.31s\n",
      "       241           4.4390           0.0097           43.11s\n",
      "       242           4.4448          -0.0077           42.92s\n",
      "       243           4.4429          -0.0147           42.73s\n",
      "       244           4.5287           0.0032           42.54s\n",
      "       245           4.5836          -0.0005           42.36s\n",
      "       246           4.1071          -0.0056           42.17s\n",
      "       247           4.3929           0.0080           42.00s\n",
      "       248           4.3910          -0.0104           41.82s\n",
      "       249           4.6621          -0.0139           41.68s\n",
      "       250           4.2228           0.0522           41.53s\n",
      "       251           4.4481          -0.0201           41.38s\n",
      "       252           4.3917          -0.0274           41.24s\n",
      "       253           4.4648          -0.0040           41.07s\n",
      "       254           4.1852           0.0322           40.87s\n",
      "       255           4.1531          -0.0017           40.73s\n",
      "       256           4.2524          -0.0055           40.56s\n",
      "       257           4.2172          -0.0060           40.37s\n",
      "       258           4.1525           0.0020           40.24s\n",
      "       259           4.0529           0.0397           40.04s\n",
      "       260           4.2713           0.0005           39.84s\n",
      "       261           3.9939          -0.0035           39.69s\n",
      "       262           4.0461          -0.0019           39.52s\n",
      "       263           4.0877          -0.0021           39.36s\n",
      "       264           4.1055           0.0092           39.20s\n",
      "       265           4.0921           0.0051           39.00s\n",
      "       266           3.8644          -0.0178           38.86s\n",
      "       267           4.1315           0.0019           38.69s\n",
      "       268           4.0725           0.0085           38.51s\n",
      "       269           3.5964           0.0021           38.35s\n",
      "       270           4.1324          -0.0175           38.20s\n",
      "       271           3.9784           0.0145           38.04s\n",
      "       272           3.7976           0.0096           37.89s\n",
      "       273           3.9246           0.0037           37.70s\n",
      "       274           4.0455          -0.0057           37.53s\n",
      "       275           3.7830          -0.0132           37.36s\n",
      "       276           3.7622           0.0046           37.21s\n",
      "       277           3.7595           0.0313           37.07s\n",
      "       278           3.8258          -0.0130           36.93s\n",
      "       279           3.9030           0.0052           36.75s\n",
      "       280           3.5632          -0.0046           36.57s\n",
      "       281           3.7609          -0.0090           36.38s\n",
      "       282           3.8660          -0.0002           36.20s\n",
      "       283           3.3719          -0.0040           36.03s\n",
      "       284           3.5950          -0.0114           35.84s\n",
      "       285           3.6747           0.0088           35.69s\n",
      "       286           3.6767           0.0115           35.50s\n",
      "       287           3.4324          -0.0015           35.32s\n",
      "       288           3.4381          -0.0056           35.13s\n",
      "       289           3.5740          -0.0026           34.97s\n",
      "       290           3.5414           0.0203           34.82s\n",
      "       291           3.4350          -0.0073           34.62s\n",
      "       292           3.6397          -0.0148           34.44s\n",
      "       293           3.7236          -0.0008           34.29s\n",
      "       294           3.5793          -0.0085           34.13s\n",
      "       295           3.5069          -0.0059           33.99s\n",
      "       296           3.4419           0.0003           33.81s\n",
      "       297           3.5518          -0.0072           33.63s\n",
      "       298           3.4951           0.0125           33.48s\n",
      "       299           3.4101           0.0088           33.31s\n",
      "       300           3.3970           0.0063           33.16s\n",
      "       301           3.5083          -0.0175           33.00s\n",
      "       302           3.5284          -0.0079           32.83s\n",
      "       303           3.6025           0.0120           32.64s\n",
      "       304           3.2869          -0.0041           32.46s\n",
      "       305           3.4382          -0.0010           32.27s\n",
      "       306           3.5028          -0.0114           32.09s\n",
      "       307           3.3527          -0.0118           31.94s\n",
      "       308           3.3362          -0.0024           31.79s\n",
      "       309           3.3084           0.0007           31.61s\n",
      "       310           3.4140          -0.0114           31.45s\n",
      "       311           3.3949          -0.0043           31.29s\n",
      "       312           3.4194          -0.0080           31.13s\n",
      "       313           3.2612          -0.0170           30.95s\n",
      "       314           3.2507           0.0189           30.78s\n",
      "       315           3.4975           0.0023           30.61s\n",
      "       316           3.1353           0.0163           30.45s\n",
      "       317           3.0324          -0.0227           30.27s\n",
      "       318           3.0430           0.0071           30.10s\n",
      "       319           3.2995          -0.0072           29.93s\n",
      "       320           3.3224           0.0028           29.75s\n",
      "       321           3.2249          -0.0053           29.59s\n",
      "       322           2.9670          -0.0030           29.45s\n",
      "       323           3.2330           0.0331           29.27s\n",
      "       324           3.1928          -0.0080           29.09s\n",
      "       325           2.9901          -0.0063           28.91s\n",
      "       326           3.0086          -0.0040           28.76s\n",
      "       327           3.1697           0.0148           28.58s\n",
      "       328           3.1589          -0.0078           28.43s\n",
      "       329           3.0485           0.0176           28.27s\n",
      "       330           2.9636           0.0101           28.10s\n",
      "       331           2.9835          -0.0112           27.92s\n",
      "       332           3.0709          -0.0046           27.76s\n",
      "       333           2.9958           0.0162           27.58s\n",
      "       334           3.0601          -0.0045           27.40s\n",
      "       335           3.0268          -0.0028           27.24s\n",
      "       336           3.0482          -0.0041           27.08s\n",
      "       337           3.0318           0.0043           26.90s\n",
      "       338           3.1025          -0.0156           26.74s\n",
      "       339           3.0770           0.0005           26.57s\n",
      "       340           2.9829           0.0087           26.40s\n",
      "       341           3.0816          -0.0047           26.25s\n",
      "       342           2.8519          -0.0015           26.10s\n",
      "       343           2.8796           0.0247           25.93s\n",
      "       344           3.0998           0.0031           25.76s\n",
      "       345           2.9521           0.0005           25.59s\n",
      "       346           2.8719           0.0019           25.41s\n",
      "       347           3.0444          -0.0054           25.25s\n",
      "       348           2.9918           0.0009           25.09s\n",
      "       349           2.9012           0.0069           24.92s\n",
      "       350           2.7968           0.0029           24.76s\n",
      "       351           2.7865          -0.0027           24.61s\n",
      "       352           2.7498          -0.0147           24.45s\n",
      "       353           2.8252          -0.0031           24.28s\n",
      "       354           2.8440           0.0013           24.12s\n",
      "       355           2.7508           0.0206           23.96s\n",
      "       356           2.7154           0.0394           23.77s\n",
      "       357           2.7909           0.0017           23.61s\n",
      "       358           2.6124          -0.0281           23.44s\n",
      "       359           2.7160           0.0051           23.28s\n",
      "       360           2.8155           0.0013           23.10s\n",
      "       361           2.6254          -0.0039           22.93s\n",
      "       362           2.5343           0.0021           22.75s\n",
      "       363           2.7737          -0.0122           22.58s\n",
      "       364           2.7411           0.0305           22.40s\n",
      "       365           2.6603          -0.0408           22.23s\n",
      "       366           2.7664          -0.0033           22.08s\n",
      "       367           2.7649          -0.0134           21.92s\n",
      "       368           2.6980          -0.0013           21.77s\n",
      "       369           2.6650           0.0413           21.62s\n",
      "       370           2.6209           0.0026           21.45s\n",
      "       371           2.5133           0.0038           21.28s\n",
      "       372           2.4962          -0.0104           21.13s\n",
      "       373           2.4303           0.0073           20.96s\n",
      "       374           2.5248           0.0130           20.80s\n",
      "       375           2.6529          -0.0044           20.65s\n",
      "       376           2.6574           0.0056           20.48s\n",
      "       377           2.4762          -0.0061           20.31s\n",
      "       378           2.6947          -0.0036           20.13s\n",
      "       379           2.6268          -0.0041           19.96s\n",
      "       380           2.6356           0.0050           19.80s\n",
      "       381           2.5902          -0.0021           19.63s\n",
      "       382           2.4663          -0.0143           19.47s\n",
      "       383           2.4528          -0.0193           19.32s\n",
      "       384           2.6837           0.0015           19.15s\n",
      "       385           2.3902           0.0072           18.99s\n",
      "       386           2.5152           0.0138           18.84s\n",
      "       387           2.4520          -0.0126           18.67s\n",
      "       388           2.4535          -0.0007           18.50s\n",
      "       389           2.3612          -0.0065           18.34s\n",
      "       390           2.4457          -0.0076           18.18s\n",
      "       391           2.4662           0.0064           18.02s\n",
      "       392           2.2959           0.0026           17.84s\n",
      "       393           2.5063          -0.0030           17.69s\n",
      "       394           2.4483          -0.0057           17.52s\n",
      "       395           2.5030           0.0076           17.35s\n",
      "       396           2.3493          -0.0012           17.18s\n",
      "       397           2.3838          -0.0105           17.01s\n",
      "       398           2.3982          -0.0047           16.84s\n",
      "       399           2.2938           0.0150           16.68s\n",
      "       400           2.4823          -0.0016           16.52s\n",
      "       401           2.2684          -0.0076           16.36s\n",
      "       402           2.3766           0.0184           16.19s\n",
      "       403           2.3987          -0.0045           16.03s\n",
      "       404           2.4186          -0.0001           15.86s\n",
      "       405           2.2016           0.0168           15.70s\n",
      "       406           2.3173           0.0130           15.53s\n",
      "       407           2.2503          -0.0033           15.37s\n",
      "       408           2.2479           0.0066           15.22s\n",
      "       409           2.3329           0.0067           15.05s\n",
      "       410           2.2526          -0.0009           14.89s\n",
      "       411           2.2469          -0.0197           14.73s\n",
      "       412           2.2774           0.0067           14.56s\n",
      "       413           2.2240           0.0023           14.40s\n",
      "       414           2.2835           0.0151           14.23s\n",
      "       415           2.2535          -0.0028           14.07s\n",
      "       416           2.3162           0.0014           13.90s\n",
      "       417           2.0851          -0.0064           13.74s\n",
      "       418           2.2843           0.0107           13.57s\n",
      "       419           2.1654           0.0215           13.39s\n",
      "       420           2.2517          -0.0039           13.23s\n",
      "       421           2.0513          -0.0222           13.07s\n",
      "       422           1.9920           0.0054           12.90s\n",
      "       423           2.1390          -0.0020           12.74s\n",
      "       424           2.0674          -0.0190           12.58s\n",
      "       425           2.1355           0.0041           12.41s\n",
      "       426           2.1419          -0.0022           12.25s\n",
      "       427           1.9427           0.0250           12.08s\n",
      "       428           2.0821           0.0012           11.92s\n",
      "       429           2.1197           0.0040           11.75s\n",
      "       430           1.9403           0.0137           11.58s\n",
      "       431           1.9613           0.0156           11.41s\n",
      "       432           1.8910           0.0013           11.25s\n",
      "       433           2.0664           0.0039           11.08s\n",
      "       434           1.9483           0.0048           10.92s\n",
      "       435           1.9659          -0.0033           10.76s\n",
      "       436           2.0497           0.0125           10.58s\n",
      "       437           2.0432          -0.0003           10.41s\n",
      "       438           2.0334          -0.0112           10.24s\n",
      "       439           1.9614          -0.0017           10.08s\n",
      "       440           1.9883           0.0047            9.91s\n",
      "       441           2.0777          -0.0150            9.75s\n",
      "       442           1.9769           0.0067            9.58s\n",
      "       443           2.0669          -0.0013            9.42s\n",
      "       444           2.0278           0.0096            9.25s\n",
      "       445           1.8302           0.0090            9.09s\n",
      "       446           2.0002           0.0041            8.92s\n",
      "       447           1.9943           0.0067            8.75s\n",
      "       448           1.9436          -0.0011            8.59s\n",
      "       449           1.9588           0.0085            8.42s\n",
      "       450           1.8832          -0.0031            8.26s\n",
      "       451           1.9673          -0.0059            8.09s\n",
      "       452           1.8636           0.0003            7.93s\n",
      "       453           1.9078          -0.0013            7.76s\n",
      "       454           1.9970          -0.0030            7.60s\n",
      "       455           1.8611           0.0023            7.43s\n",
      "       456           1.9369          -0.0137            7.27s\n",
      "       457           1.8669          -0.0099            7.11s\n",
      "       458           1.8136           0.0146            6.94s\n",
      "       459           2.0172           0.0011            6.78s\n",
      "       460           1.8418          -0.0040            6.61s\n",
      "       461           2.0222          -0.0050            6.45s\n",
      "       462           1.8262           0.0030            6.28s\n",
      "       463           1.8000           0.0024            6.12s\n",
      "       464           1.8245           0.0156            5.95s\n",
      "       465           1.9312          -0.0070            5.79s\n",
      "       466           1.7550           0.0218            5.62s\n",
      "       467           1.6752           0.0188            5.46s\n",
      "       468           1.6819           0.0103            5.30s\n",
      "       469           1.7440          -0.0042            5.13s\n",
      "       470           1.8140           0.0044            4.96s\n",
      "       471           1.6422          -0.0042            4.80s\n",
      "       472           1.7033           0.0010            4.63s\n",
      "       473           1.7600          -0.0003            4.47s\n",
      "       474           1.8869           0.0029            4.30s\n",
      "       475           1.7670           0.0024            4.14s\n",
      "       476           1.7622           0.0017            3.97s\n",
      "       477           1.7203           0.0001            3.80s\n",
      "       478           1.6739          -0.0028            3.64s\n",
      "       479           1.7106           0.0136            3.47s\n",
      "       480           1.6240           0.0133            3.31s\n",
      "       481           1.6348          -0.0024            3.14s\n",
      "       482           1.5882          -0.0112            2.98s\n",
      "       483           1.7064          -0.0007            2.81s\n",
      "       484           1.7382           0.0056            2.64s\n",
      "       485           1.7352           0.0079            2.48s\n",
      "       486           1.7567          -0.0006            2.31s\n",
      "       487           1.4610           0.0005            2.15s\n",
      "       488           1.6609          -0.0031            1.98s\n",
      "       489           1.6032          -0.0010            1.82s\n",
      "       490           1.5935           0.0030            1.65s\n",
      "       491           1.5979           0.0005            1.49s\n",
      "       492           1.6135          -0.0015            1.32s\n",
      "       493           1.6184           0.0147            1.16s\n",
      "       494           1.5374           0.0044            0.99s\n",
      "       495           1.4604          -0.0030            0.83s\n",
      "       496           1.6563          -0.0081            0.66s\n",
      "       497           1.6284           0.0031            0.50s\n",
      "       498           1.4515          -0.0006            0.33s\n",
      "       499           1.5610           0.0118            0.17s\n",
      "       500           1.6077          -0.0019            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='huber', max_depth=6,\n",
       "             max_features=0.8, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=18,\n",
       "             min_samples_split=14, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=500, presort='auto', random_state=None,\n",
       "             subsample=0.8, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model_gbr = GradientBoostingRegressor(learning_rate=0.1, loss=\"huber\", max_depth=6, \n",
    "                                      max_features=0.8, min_samples_leaf=18, min_samples_split=14, \n",
    "                                      subsample=0.8, n_estimators=500, verbose=2)\n",
    "model_gbr.fit(X_train, Y_train)\n",
    "#model_gbr.score(X_val, Y_val)\n",
    "#model_gbr.fit(data_train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 800building tree 6 of 800building tree 2 of 800building tree 3 of 800building tree 4 of 800building tree 5 of 800building tree 7 of 800building tree 8 of 800building tree 9 of 800building tree 10 of 800\n",
      "building tree 11 of 800building tree 12 of 800building tree 13 of 800building tree 14 of 800building tree 15 of 800building tree 16 of 800\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "building tree 17 of 800\n",
      "building tree 18 of 800\n",
      "building tree 19 of 800\n",
      "building tree 20 of 800\n",
      "building tree 21 of 800\n",
      "building tree 22 of 800\n",
      "building tree 23 of 800\n",
      "building tree 24 of 800\n",
      "building tree 26 of 800\n",
      "building tree 25 of 800\n",
      "building tree 27 of 800\n",
      "building tree 28 of 800\n",
      "building tree 29 of 800\n",
      "building tree 30 of 800\n",
      "building tree 31 of 800\n",
      "building tree 32 of 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 33 of 800\n",
      "building tree 34 of 800\n",
      "building tree 35 of 800\n",
      "building tree 36 of 800\n",
      "building tree 37 of 800\n",
      "building tree 38 of 800\n",
      "building tree 39 of 800\n",
      "building tree 40 of 800\n",
      "building tree 41 of 800\n",
      "building tree 42 of 800\n",
      "building tree 43 of 800\n",
      "building tree 44 of 800\n",
      "building tree 45 of 800\n",
      "building tree 46 of 800\n",
      "building tree 47 of 800\n",
      "building tree 48 of 800\n",
      "building tree 49 of 800\n",
      "building tree 50 of 800\n",
      "building tree 51 of 800\n",
      "building tree 52 of 800\n",
      "building tree 53 of 800\n",
      "building tree 54 of 800\n",
      "building tree 55 of 800\n",
      "building tree 56 of 800building tree 57 of 800\n",
      "\n",
      "building tree 58 of 800\n",
      "building tree 59 of 800\n",
      "building tree 60 of 800\n",
      "building tree 61 of 800\n",
      "building tree 62 of 800\n",
      "building tree 63 of 800\n",
      "building tree 64 of 800\n",
      "building tree 65 of 800\n",
      "building tree 66 of 800\n",
      "building tree 67 of 800building tree 68 of 800\n",
      "\n",
      "building tree 69 of 800\n",
      "building tree 70 of 800\n",
      "building tree 71 of 800building tree 72 of 800\n",
      "\n",
      "building tree 73 of 800\n",
      "building tree 74 of 800\n",
      "building tree 75 of 800\n",
      "building tree 76 of 800\n",
      "building tree 77 of 800\n",
      "building tree 78 of 800\n",
      "building tree 79 of 800\n",
      "building tree 80 of 800\n",
      "building tree 81 of 800\n",
      "building tree 82 of 800\n",
      "building tree 83 of 800\n",
      "building tree 84 of 800\n",
      "building tree 85 of 800\n",
      "building tree 86 of 800\n",
      "building tree 87 of 800\n",
      "building tree 88 of 800\n",
      "building tree 89 of 800\n",
      "building tree 90 of 800\n",
      "building tree 91 of 800\n",
      "building tree 92 of 800building tree 93 of 800\n",
      "\n",
      "building tree 94 of 800\n",
      "building tree 95 of 800\n",
      "building tree 96 of 800\n",
      "building tree 97 of 800\n",
      "building tree 98 of 800\n",
      "building tree 99 of 800\n",
      "building tree 100 of 800\n",
      "building tree 101 of 800\n",
      "building tree 102 of 800\n",
      "building tree 103 of 800\n",
      "building tree 104 of 800\n",
      "building tree 105 of 800\n",
      "building tree 106 of 800\n",
      "building tree 107 of 800\n",
      "building tree 108 of 800\n",
      "building tree 109 of 800\n",
      "building tree 110 of 800\n",
      "building tree 111 of 800\n",
      "building tree 112 of 800\n",
      "building tree 113 of 800\n",
      "building tree 114 of 800\n",
      "building tree 115 of 800\n",
      "building tree 116 of 800\n",
      "building tree 117 of 800building tree 118 of 800\n",
      "\n",
      "building tree 119 of 800\n",
      "building tree 120 of 800\n",
      "building tree 121 of 800building tree 122 of 800\n",
      "\n",
      "building tree 123 of 800\n",
      "building tree 124 of 800\n",
      "building tree 125 of 800\n",
      "building tree 126 of 800\n",
      "building tree 127 of 800\n",
      "building tree 128 of 800\n",
      "building tree 129 of 800\n",
      "building tree 130 of 800\n",
      "building tree 131 of 800\n",
      "building tree 132 of 800\n",
      "building tree 133 of 800\n",
      "building tree 134 of 800\n",
      "building tree 135 of 800\n",
      "building tree 136 of 800\n",
      "building tree 137 of 800\n",
      "building tree 138 of 800\n",
      "building tree 139 of 800\n",
      "building tree 140 of 800\n",
      "building tree 141 of 800\n",
      "building tree 142 of 800\n",
      "building tree 143 of 800\n",
      "building tree 144 of 800\n",
      "building tree 145 of 800\n",
      "building tree 146 of 800\n",
      "building tree 147 of 800\n",
      "building tree 148 of 800\n",
      "building tree 149 of 800\n",
      "building tree 150 of 800\n",
      "building tree 151 of 800\n",
      "building tree 152 of 800\n",
      "building tree 153 of 800\n",
      "building tree 154 of 800\n",
      "building tree 155 of 800\n",
      "building tree 156 of 800\n",
      "building tree 157 of 800\n",
      "building tree 158 of 800\n",
      "building tree 159 of 800\n",
      "building tree 160 of 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    4.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 161 of 800\n",
      "building tree 162 of 800\n",
      "building tree 163 of 800\n",
      "building tree 164 of 800building tree 165 of 800\n",
      "\n",
      "building tree 166 of 800\n",
      "building tree 167 of 800\n",
      "building tree 168 of 800\n",
      "building tree 169 of 800\n",
      "building tree 170 of 800building tree 171 of 800\n",
      "\n",
      "building tree 172 of 800\n",
      "building tree 173 of 800\n",
      "building tree 174 of 800\n",
      "building tree 175 of 800\n",
      "building tree 176 of 800\n",
      "building tree 177 of 800\n",
      "building tree 178 of 800\n",
      "building tree 179 of 800\n",
      "building tree 180 of 800\n",
      "building tree 181 of 800\n",
      "building tree 182 of 800\n",
      "building tree 183 of 800\n",
      "building tree 184 of 800\n",
      "building tree 185 of 800building tree 186 of 800\n",
      "\n",
      "building tree 187 of 800\n",
      "building tree 188 of 800\n",
      "building tree 189 of 800\n",
      "building tree 190 of 800\n",
      "building tree 191 of 800\n",
      "building tree 192 of 800\n",
      "building tree 193 of 800\n",
      "building tree 194 of 800\n",
      "building tree 195 of 800\n",
      "building tree 196 of 800building tree 197 of 800\n",
      "\n",
      "building tree 198 of 800\n",
      "building tree 199 of 800\n",
      "building tree 200 of 800\n",
      "building tree 201 of 800\n",
      "building tree 202 of 800building tree 203 of 800\n",
      "\n",
      "building tree 204 of 800\n",
      "building tree 205 of 800\n",
      "building tree 206 of 800\n",
      "building tree 207 of 800\n",
      "building tree 208 of 800\n",
      "building tree 209 of 800\n",
      "building tree 210 of 800\n",
      "building tree 211 of 800\n",
      "building tree 212 of 800\n",
      "building tree 213 of 800\n",
      "building tree 214 of 800\n",
      "building tree 215 of 800\n",
      "building tree 216 of 800\n",
      "building tree 217 of 800\n",
      "building tree 218 of 800\n",
      "building tree 219 of 800\n",
      "building tree 220 of 800\n",
      "building tree 221 of 800\n",
      "building tree 222 of 800\n",
      "building tree 223 of 800\n",
      "building tree 224 of 800\n",
      "building tree 225 of 800\n",
      "building tree 226 of 800\n",
      "building tree 227 of 800\n",
      "building tree 228 of 800\n",
      "building tree 229 of 800\n",
      "building tree 230 of 800\n",
      "building tree 231 of 800\n",
      "building tree 232 of 800\n",
      "building tree 233 of 800\n",
      "building tree 234 of 800\n",
      "building tree 235 of 800\n",
      "building tree 236 of 800\n",
      "building tree 237 of 800\n",
      "building tree 238 of 800\n",
      "building tree 239 of 800\n",
      "building tree 240 of 800\n",
      "building tree 241 of 800\n",
      "building tree 242 of 800\n",
      "building tree 243 of 800\n",
      "building tree 244 of 800\n",
      "building tree 245 of 800\n",
      "building tree 246 of 800\n",
      "building tree 247 of 800\n",
      "building tree 248 of 800\n",
      "building tree 249 of 800\n",
      "building tree 250 of 800\n",
      "building tree 251 of 800\n",
      "building tree 252 of 800\n",
      "building tree 253 of 800\n",
      "building tree 254 of 800\n",
      "building tree 255 of 800\n",
      "building tree 256 of 800\n",
      "building tree 257 of 800\n",
      "building tree 258 of 800\n",
      "building tree 259 of 800\n",
      "building tree 260 of 800\n",
      "building tree 261 of 800\n",
      "building tree 262 of 800\n",
      "building tree 263 of 800\n",
      "building tree 264 of 800\n",
      "building tree 265 of 800\n",
      "building tree 266 of 800\n",
      "building tree 267 of 800\n",
      "building tree 268 of 800\n",
      "building tree 269 of 800\n",
      "building tree 270 of 800\n",
      "building tree 271 of 800\n",
      "building tree 272 of 800\n",
      "building tree 273 of 800\n",
      "building tree 274 of 800\n",
      "building tree 275 of 800\n",
      "building tree 276 of 800\n",
      "building tree 277 of 800\n",
      "building tree 278 of 800\n",
      "building tree 279 of 800\n",
      "building tree 280 of 800\n",
      "building tree 281 of 800\n",
      "building tree 282 of 800\n",
      "building tree 283 of 800\n",
      "building tree 284 of 800\n",
      "building tree 285 of 800\n",
      "building tree 286 of 800\n",
      "building tree 287 of 800\n",
      "building tree 288 of 800\n",
      "building tree 289 of 800\n",
      "building tree 290 of 800\n",
      "building tree 291 of 800\n",
      "building tree 292 of 800building tree 293 of 800\n",
      "\n",
      "building tree 294 of 800\n",
      "building tree 295 of 800\n",
      "building tree 296 of 800\n",
      "building tree 297 of 800\n",
      "building tree 298 of 800\n",
      "building tree 299 of 800\n",
      "building tree 300 of 800\n",
      "building tree 301 of 800\n",
      "building tree 302 of 800\n",
      "building tree 303 of 800\n",
      "building tree 304 of 800\n",
      "building tree 305 of 800\n",
      "building tree 306 of 800\n",
      "building tree 307 of 800\n",
      "building tree 308 of 800\n",
      "building tree 309 of 800\n",
      "building tree 310 of 800\n",
      "building tree 311 of 800\n",
      "building tree 312 of 800\n",
      "building tree 313 of 800\n",
      "building tree 314 of 800\n",
      "building tree 315 of 800\n",
      "building tree 316 of 800\n",
      "building tree 317 of 800\n",
      "building tree 318 of 800\n",
      "building tree 319 of 800\n",
      "building tree 320 of 800\n",
      "building tree 321 of 800\n",
      "building tree 322 of 800\n",
      "building tree 323 of 800\n",
      "building tree 324 of 800\n",
      "building tree 325 of 800\n",
      "building tree 326 of 800\n",
      "building tree 327 of 800\n",
      "building tree 328 of 800\n",
      "building tree 329 of 800\n",
      "building tree 330 of 800\n",
      "building tree 331 of 800\n",
      "building tree 332 of 800\n",
      "building tree 333 of 800\n",
      "building tree 334 of 800\n",
      "building tree 335 of 800\n",
      "building tree 336 of 800\n",
      "building tree 337 of 800\n",
      "building tree 338 of 800\n",
      "building tree 339 of 800\n",
      "building tree 340 of 800\n",
      "building tree 341 of 800\n",
      "building tree 342 of 800\n",
      "building tree 343 of 800\n",
      "building tree 344 of 800\n",
      "building tree 345 of 800\n",
      "building tree 346 of 800\n",
      "building tree 347 of 800\n",
      "building tree 348 of 800\n",
      "building tree 349 of 800\n",
      "building tree 350 of 800\n",
      "building tree 351 of 800\n",
      "building tree 352 of 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   11.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 353 of 800\n",
      "building tree 354 of 800\n",
      "building tree 355 of 800\n",
      "building tree 356 of 800\n",
      "building tree 357 of 800\n",
      "building tree 358 of 800\n",
      "building tree 359 of 800\n",
      "building tree 360 of 800\n",
      "building tree 361 of 800\n",
      "building tree 363 of 800building tree 362 of 800\n",
      "\n",
      "building tree 364 of 800\n",
      "building tree 365 of 800\n",
      "building tree 366 of 800\n",
      "building tree 367 of 800\n",
      "building tree 368 of 800\n",
      "building tree 369 of 800\n",
      "building tree 370 of 800\n",
      "building tree 371 of 800\n",
      "building tree 372 of 800\n",
      "building tree 374 of 800building tree 373 of 800\n",
      "\n",
      "building tree 375 of 800\n",
      "building tree 376 of 800\n",
      "building tree 377 of 800\n",
      "building tree 378 of 800\n",
      "building tree 379 of 800building tree 380 of 800\n",
      "\n",
      "building tree 381 of 800\n",
      "building tree 382 of 800\n",
      "building tree 383 of 800\n",
      "building tree 384 of 800\n",
      "building tree 385 of 800\n",
      "building tree 386 of 800\n",
      "building tree 387 of 800\n",
      "building tree 388 of 800\n",
      "building tree 389 of 800\n",
      "building tree 390 of 800\n",
      "building tree 391 of 800\n",
      "building tree 392 of 800\n",
      "building tree 393 of 800\n",
      "building tree 394 of 800\n",
      "building tree 395 of 800\n",
      "building tree 396 of 800\n",
      "building tree 397 of 800\n",
      "building tree 398 of 800\n",
      "building tree 399 of 800\n",
      "building tree 400 of 800\n",
      "building tree 401 of 800\n",
      "building tree 402 of 800\n",
      "building tree 403 of 800\n",
      "building tree 404 of 800\n",
      "building tree 405 of 800\n",
      "building tree 406 of 800\n",
      "building tree 407 of 800\n",
      "building tree 408 of 800\n",
      "building tree 409 of 800\n",
      "building tree 410 of 800\n",
      "building tree 411 of 800\n",
      "building tree 412 of 800\n",
      "building tree 413 of 800\n",
      "building tree 414 of 800\n",
      "building tree 415 of 800\n",
      "building tree 416 of 800\n",
      "building tree 417 of 800\n",
      "building tree 418 of 800\n",
      "building tree 419 of 800\n",
      "building tree 420 of 800building tree 421 of 800\n",
      "\n",
      "building tree 422 of 800\n",
      "building tree 423 of 800\n",
      "building tree 424 of 800\n",
      "building tree 425 of 800\n",
      "building tree 426 of 800\n",
      "building tree 427 of 800\n",
      "building tree 428 of 800\n",
      "building tree 429 of 800\n",
      "building tree 430 of 800\n",
      "building tree 431 of 800\n",
      "building tree 432 of 800\n",
      "building tree 433 of 800\n",
      "building tree 434 of 800\n",
      "building tree 435 of 800\n",
      "building tree 436 of 800building tree 437 of 800\n",
      "\n",
      "building tree 438 of 800\n",
      "building tree 439 of 800\n",
      "building tree 440 of 800\n",
      "building tree 441 of 800\n",
      "building tree 442 of 800\n",
      "building tree 443 of 800\n",
      "building tree 444 of 800\n",
      "building tree 445 of 800\n",
      "building tree 446 of 800\n",
      "building tree 447 of 800\n",
      "building tree 448 of 800\n",
      "building tree 449 of 800\n",
      "building tree 450 of 800\n",
      "building tree 451 of 800\n",
      "building tree 452 of 800\n",
      "building tree 453 of 800\n",
      "building tree 454 of 800\n",
      "building tree 455 of 800\n",
      "building tree 456 of 800\n",
      "building tree 457 of 800\n",
      "building tree 458 of 800\n",
      "building tree 459 of 800\n",
      "building tree 460 of 800\n",
      "building tree 461 of 800\n",
      "building tree 462 of 800\n",
      "building tree 463 of 800\n",
      "building tree 464 of 800\n",
      "building tree 465 of 800\n",
      "building tree 466 of 800\n",
      "building tree 467 of 800\n",
      "building tree 468 of 800\n",
      "building tree 469 of 800\n",
      "building tree 470 of 800\n",
      "building tree 471 of 800\n",
      "building tree 472 of 800\n",
      "building tree 473 of 800\n",
      "building tree 474 of 800\n",
      "building tree 475 of 800\n",
      "building tree 476 of 800\n",
      "building tree 477 of 800\n",
      "building tree 478 of 800\n",
      "building tree 479 of 800\n",
      "building tree 480 of 800\n",
      "building tree 481 of 800\n",
      "building tree 482 of 800\n",
      "building tree 483 of 800\n",
      "building tree 484 of 800\n",
      "building tree 485 of 800\n",
      "building tree 486 of 800building tree 487 of 800\n",
      "\n",
      "building tree 488 of 800\n",
      "building tree 489 of 800\n",
      "building tree 490 of 800\n",
      "building tree 491 of 800\n",
      "building tree 492 of 800\n",
      "building tree 493 of 800\n",
      "building tree 494 of 800\n",
      "building tree 495 of 800\n",
      "building tree 496 of 800\n",
      "building tree 497 of 800\n",
      "building tree 498 of 800\n",
      "building tree 499 of 800\n",
      "building tree 500 of 800\n",
      "building tree 501 of 800\n",
      "building tree 502 of 800\n",
      "building tree 503 of 800\n",
      "building tree 504 of 800\n",
      "building tree 505 of 800\n",
      "building tree 506 of 800\n",
      "building tree 507 of 800\n",
      "building tree 508 of 800\n",
      "building tree 509 of 800\n",
      "building tree 510 of 800\n",
      "building tree 511 of 800\n",
      "building tree 512 of 800\n",
      "building tree 513 of 800\n",
      "building tree 514 of 800\n",
      "building tree 515 of 800\n",
      "building tree 516 of 800\n",
      "building tree 517 of 800\n",
      "building tree 518 of 800\n",
      "building tree 519 of 800\n",
      "building tree 520 of 800\n",
      "building tree 521 of 800\n",
      "building tree 522 of 800\n",
      "building tree 523 of 800\n",
      "building tree 524 of 800\n",
      "building tree 525 of 800\n",
      "building tree 526 of 800\n",
      "building tree 527 of 800\n",
      "building tree 528 of 800\n",
      "building tree 529 of 800\n",
      "building tree 530 of 800\n",
      "building tree 531 of 800building tree 532 of 800\n",
      "\n",
      "building tree 533 of 800\n",
      "building tree 534 of 800\n",
      "building tree 535 of 800\n",
      "building tree 536 of 800\n",
      "building tree 537 of 800\n",
      "building tree 538 of 800\n",
      "building tree 539 of 800\n",
      "building tree 540 of 800\n",
      "building tree 541 of 800\n",
      "building tree 542 of 800\n",
      "building tree 543 of 800\n",
      "building tree 544 of 800\n",
      "building tree 545 of 800\n",
      "building tree 546 of 800\n",
      "building tree 547 of 800\n",
      "building tree 548 of 800\n",
      "building tree 549 of 800\n",
      "building tree 550 of 800building tree 551 of 800\n",
      "\n",
      "building tree 552 of 800\n",
      "building tree 553 of 800\n",
      "building tree 554 of 800\n",
      "building tree 555 of 800\n",
      "building tree 556 of 800\n",
      "building tree 557 of 800\n",
      "building tree 558 of 800\n",
      "building tree 559 of 800\n",
      "building tree 560 of 800\n",
      "building tree 561 of 800\n",
      "building tree 562 of 800\n",
      "building tree 563 of 800\n",
      "building tree 564 of 800\n",
      "building tree 565 of 800\n",
      "building tree 566 of 800\n",
      "building tree 567 of 800\n",
      "building tree 568 of 800\n",
      "building tree 569 of 800\n",
      "building tree 570 of 800\n",
      "building tree 571 of 800\n",
      "building tree 572 of 800\n",
      "building tree 573 of 800\n",
      "building tree 574 of 800\n",
      "building tree 575 of 800\n",
      "building tree 576 of 800\n",
      "building tree 577 of 800\n",
      "building tree 578 of 800\n",
      "building tree 579 of 800\n",
      "building tree 580 of 800\n",
      "building tree 581 of 800\n",
      "building tree 582 of 800\n",
      "building tree 583 of 800\n",
      "building tree 584 of 800\n",
      "building tree 585 of 800\n",
      "building tree 586 of 800\n",
      "building tree 587 of 800\n",
      "building tree 588 of 800\n",
      "building tree 589 of 800\n",
      "building tree 590 of 800\n",
      "building tree 591 of 800\n",
      "building tree 592 of 800\n",
      "building tree 593 of 800\n",
      "building tree 594 of 800\n",
      "building tree 595 of 800\n",
      "building tree 596 of 800\n",
      "building tree 597 of 800\n",
      "building tree 598 of 800\n",
      "building tree 599 of 800\n",
      "building tree 600 of 800\n",
      "building tree 601 of 800\n",
      "building tree 602 of 800\n",
      "building tree 603 of 800\n",
      "building tree 604 of 800\n",
      "building tree 605 of 800\n",
      "building tree 606 of 800\n",
      "building tree 607 of 800\n",
      "building tree 608 of 800\n",
      "building tree 609 of 800\n",
      "building tree 610 of 800\n",
      "building tree 611 of 800\n",
      "building tree 612 of 800\n",
      "building tree 613 of 800\n",
      "building tree 614 of 800\n",
      "building tree 615 of 800\n",
      "building tree 616 of 800\n",
      "building tree 617 of 800\n",
      "building tree 618 of 800\n",
      "building tree 619 of 800\n",
      "building tree 620 of 800\n",
      "building tree 621 of 800\n",
      "building tree 622 of 800\n",
      "building tree 623 of 800\n",
      "building tree 624 of 800\n",
      "building tree 625 of 800\n",
      "building tree 626 of 800\n",
      "building tree 627 of 800\n",
      "building tree 628 of 800\n",
      "building tree 629 of 800\n",
      "building tree 630 of 800\n",
      "building tree 631 of 800\n",
      "building tree 632 of 800\n",
      "building tree 633 of 800\n",
      "building tree 634 of 800\n",
      "building tree 635 of 800\n",
      "building tree 636 of 800\n",
      "building tree 637 of 800\n",
      "building tree 638 of 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   21.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 639 of 800\n",
      "building tree 640 of 800\n",
      "building tree 641 of 800\n",
      "building tree 642 of 800\n",
      "building tree 643 of 800\n",
      "building tree 644 of 800\n",
      "building tree 645 of 800\n",
      "building tree 646 of 800\n",
      "building tree 647 of 800\n",
      "building tree 648 of 800\n",
      "building tree 649 of 800\n",
      "building tree 650 of 800\n",
      "building tree 651 of 800\n",
      "building tree 652 of 800\n",
      "building tree 653 of 800\n",
      "building tree 654 of 800\n",
      "building tree 655 of 800\n",
      "building tree 656 of 800\n",
      "building tree 657 of 800\n",
      "building tree 658 of 800\n",
      "building tree 659 of 800\n",
      "building tree 660 of 800\n",
      "building tree 661 of 800\n",
      "building tree 662 of 800\n",
      "building tree 663 of 800\n",
      "building tree 664 of 800\n",
      "building tree 665 of 800building tree 666 of 800\n",
      "\n",
      "building tree 667 of 800\n",
      "building tree 668 of 800\n",
      "building tree 669 of 800\n",
      "building tree 670 of 800\n",
      "building tree 671 of 800\n",
      "building tree 672 of 800\n",
      "building tree 673 of 800\n",
      "building tree 674 of 800building tree 675 of 800\n",
      "\n",
      "building tree 676 of 800\n",
      "building tree 677 of 800\n",
      "building tree 678 of 800\n",
      "building tree 679 of 800\n",
      "building tree 680 of 800\n",
      "building tree 681 of 800\n",
      "building tree 682 of 800building tree 683 of 800\n",
      "\n",
      "building tree 684 of 800\n",
      "building tree 685 of 800\n",
      "building tree 686 of 800\n",
      "building tree 687 of 800\n",
      "building tree 688 of 800\n",
      "building tree 689 of 800\n",
      "building tree 690 of 800\n",
      "building tree 691 of 800\n",
      "building tree 692 of 800\n",
      "building tree 693 of 800\n",
      "building tree 694 of 800\n",
      "building tree 695 of 800\n",
      "building tree 696 of 800\n",
      "building tree 697 of 800\n",
      "building tree 698 of 800\n",
      "building tree 699 of 800\n",
      "building tree 700 of 800\n",
      "building tree 701 of 800\n",
      "building tree 702 of 800\n",
      "building tree 703 of 800\n",
      "building tree 704 of 800\n",
      "building tree 705 of 800\n",
      "building tree 706 of 800\n",
      "building tree 707 of 800\n",
      "building tree 708 of 800\n",
      "building tree 709 of 800\n",
      "building tree 710 of 800\n",
      "building tree 711 of 800\n",
      "building tree 712 of 800\n",
      "building tree 713 of 800\n",
      "building tree 714 of 800\n",
      "building tree 715 of 800\n",
      "building tree 716 of 800\n",
      "building tree 717 of 800\n",
      "building tree 718 of 800\n",
      "building tree 719 of 800\n",
      "building tree 720 of 800\n",
      "building tree 721 of 800\n",
      "building tree 722 of 800\n",
      "building tree 723 of 800\n",
      "building tree 724 of 800\n",
      "building tree 725 of 800\n",
      "building tree 726 of 800\n",
      "building tree 727 of 800\n",
      "building tree 728 of 800building tree 729 of 800\n",
      "\n",
      "building tree 730 of 800\n",
      "building tree 731 of 800\n",
      "building tree 732 of 800\n",
      "building tree 733 of 800\n",
      "building tree 734 of 800\n",
      "building tree 735 of 800\n",
      "building tree 736 of 800\n",
      "building tree 738 of 800building tree 737 of 800\n",
      "\n",
      "building tree 739 of 800\n",
      "building tree 740 of 800\n",
      "building tree 741 of 800\n",
      "building tree 742 of 800\n",
      "building tree 743 of 800building tree 744 of 800\n",
      "\n",
      "building tree 745 of 800\n",
      "building tree 746 of 800\n",
      "building tree 747 of 800\n",
      "building tree 748 of 800\n",
      "building tree 749 of 800\n",
      "building tree 750 of 800\n",
      "building tree 751 of 800\n",
      "building tree 752 of 800\n",
      "building tree 753 of 800\n",
      "building tree 754 of 800\n",
      "building tree 755 of 800\n",
      "building tree 756 of 800\n",
      "building tree 757 of 800\n",
      "building tree 758 of 800\n",
      "building tree 759 of 800\n",
      "building tree 760 of 800\n",
      "building tree 761 of 800\n",
      "building tree 762 of 800\n",
      "building tree 763 of 800\n",
      "building tree 764 of 800\n",
      "building tree 765 of 800\n",
      "building tree 766 of 800\n",
      "building tree 767 of 800\n",
      "building tree 768 of 800\n",
      "building tree 769 of 800\n",
      "building tree 770 of 800\n",
      "building tree 771 of 800\n",
      "building tree 772 of 800\n",
      "building tree 773 of 800\n",
      "building tree 774 of 800\n",
      "building tree 775 of 800\n",
      "building tree 776 of 800\n",
      "building tree 777 of 800\n",
      "building tree 778 of 800\n",
      "building tree 779 of 800\n",
      "building tree 780 of 800\n",
      "building tree 781 of 800\n",
      "building tree 782 of 800\n",
      "building tree 783 of 800\n",
      "building tree 784 of 800\n",
      "building tree 785 of 800\n",
      "building tree 786 of 800\n",
      "building tree 787 of 800\n",
      "building tree 788 of 800\n",
      "building tree 789 of 800\n",
      "building tree 790 of 800\n",
      "building tree 791 of 800\n",
      "building tree 792 of 800\n",
      "building tree 793 of 800\n",
      "building tree 794 of 800\n",
      "building tree 795 of 800\n",
      "building tree 796 of 800\n",
      "building tree 797 of 800\n",
      "building tree 798 of 800\n",
      "building tree 799 of 800\n",
      "building tree 800 of 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:   27.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=10,\n",
       "           max_features=0.8, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=800, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_rfr = RandomForestRegressor(n_estimators=800, max_features=0.8, max_depth=10,\n",
    "                                  n_jobs=-1, verbose=2)\n",
    "model_rfr.fit(X_train, Y_train)\n",
    "#model_rfr.score(X_val, Y_val)\n",
    "#model_rfr.fit(data_train, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "10644b60c0bf670e48b406ed8a5c27a11a3e60ac",
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6. Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 6.1 Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_lgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-700b23358126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_lgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_lgb' is not defined"
     ]
    }
   ],
   "source": [
    "Y_pred = model_lgb.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 6.2 Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#performance\n",
    "preds = []\n",
    "i = 0\n",
    "for bag in subfeatures:\n",
    "    #preds.append(models_xgb_sub[i].predict(X_val[:,bag]))\n",
    "    preds.append(models_xgb_sub[i].predict(data_test[:,bag]))\n",
    "    i += 1\n",
    "Y_pred = (preds[0]*0.628+preds[1]*0.590+preds[2]*0.557+preds[3]*0.592+preds[4]*0.609)/2.976\n",
    "\n",
    "#print( r2_score(Y_val, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 130 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=16)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 130 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 333 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0 0.650970606312\n",
      "0.9 0.1 0.650232598725\n",
      "0.8 0.2 0.649174298977\n",
      "0.7 0.3 0.647795700192\n",
      "0.6 0.4 0.646096822998\n",
      "0.5 0.5 0.644077640897\n",
      "0.4 0.6 0.641738162677\n",
      "0.3 0.7 0.639078409014\n",
      "0.2 0.8 0.636098365108\n",
      "0.1 0.9 0.632798018919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done 616 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 800 out of 800 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#performance\n",
    "Y_pred_llc = model_llc.predict(X_val)\n",
    "Y_pred_br = model_br.predict(X_val)\n",
    "Y_pred_hr = model_hr.predict(X_val)\n",
    "Y_pred_knr = model_knr.predict(X_val)\n",
    "Y_pred_etr = model_etr.predict(X_val)\n",
    "Y_pred_gbr = model_gbr.predict(X_val)\n",
    "Y_pred_rfr = model_rfr.predict(X_val)\n",
    "Y_pred_lgb = model_lgb.predict(X_val)\n",
    "Y_pred_xgb = model_xgb.predict(X_val)\n",
    "\n",
    "for p in np.arange(0.0, 1.0, 0.1):\n",
    "    Y_pred = ((Y_pred_llc*0.572 + Y_pred_br*0.618 + Y_pred_hr*0.552 + \\\n",
    "         Y_pred_knr*0.2 + Y_pred_etr*0.566 + Y_pred_gbr*0.602 + \\\n",
    "         Y_pred_rfr*0.628 + Y_pred_lgb*0.395)/4.132) * (1-p) + Y_pred_xgb*p\n",
    "    print((1-p), p, r2_score(Y_val, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 130 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=16)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 130 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 333 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 616 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 800 out of 800 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.678730571794\n"
     ]
    }
   ],
   "source": [
    "#performance\n",
    "Y_pred_llc = model_llc.predict(data_train)\n",
    "Y_pred_br = model_br.predict(data_train)\n",
    "Y_pred_hr = model_hr.predict(data_train)\n",
    "Y_pred_knr = model_knr.predict(data_train)\n",
    "Y_pred_etr = model_etr.predict(data_train)\n",
    "Y_pred_gbr = model_gbr.predict(data_train)\n",
    "Y_pred_rfr = model_rfr.predict(data_train)\n",
    "Y_pred_lgb = model_lgb.predict(data_train)\n",
    "Y_pred_xgb = model_xgb2.predict(data_train)\n",
    "\n",
    "Y_pred = ((Y_pred_llc*0.572 + Y_pred_br*0.618 + Y_pred_hr*0.552 + \\\n",
    "         Y_pred_knr*0.2 + Y_pred_etr*0.566 + Y_pred_gbr*0.602 + \\\n",
    "         Y_pred_rfr*0.628 + Y_pred_lgb*0.395)/4.132) * 0.25 + Y_pred_xgb*0.75\n",
    "\n",
    "print(r2_score(target, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 130 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=16)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 130 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 333 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 616 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 800 out of 800 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "Y_pred_llc = model_llc.predict(data_test)\n",
    "Y_pred_br = model_br.predict(data_test)\n",
    "Y_pred_hr = model_hr.predict(data_test)\n",
    "Y_pred_knr = model_knr.predict(data_test)\n",
    "Y_pred_etr = model_etr.predict(data_test)\n",
    "Y_pred_gbr = model_gbr.predict(data_test)\n",
    "Y_pred_rfr = model_rfr.predict(data_test)\n",
    "Y_pred_lgb = model_lgb.predict(data_test)\n",
    "Y_pred_xgb = model_xgb2.predict(data_test)\n",
    "\n",
    "Y_pred = (Y_pred_llc*0.572 + Y_pred_br*0.618 + Y_pred_hr*0.552 + \\\n",
    "         Y_pred_knr*0.2 + Y_pred_etr*0.566 + Y_pred_gbr*0.602 + \\\n",
    "         Y_pred_rfr*0.628 + Y_pred_lgb*0.395 + Y_pred_xgb*0.631)/4.764"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 6.3 Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#save submission\n",
    "output = pd.DataFrame({'ID': test['ID'], 'y': Y_pred})\n",
    "output.to_csv('submission9.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Output Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data).to_csv('train_processed.csv', index=False)\n",
    "pd.DataFrame(test).to_csv('test_processed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
